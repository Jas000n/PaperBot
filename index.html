<!DOCTYPE html>
<html lang="en">

<head>
    <title>Jason's Paper Bot</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-20T00:00:00Z">2025-05-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">48</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Traversability-aware path planning in dynamic environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaroslav Marchukov, Luis Montano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning in environments with moving obstacles remains a significant
challenge in robotics. While many works focus on navigation and path planning
in obstacle-dense spaces, traversing such congested regions is often avoidable
by selecting alternative routes. This paper presents Traversability-aware FMM
(Tr-FMM), a path planning method that computes paths in dynamic environments,
avoiding crowded regions. The method operates in two steps: first, it
discretizes the environment, identifying regions and their distribution;
second, it computes the traversability of regions, aiming to minimize both
obstacle risks and goal deviation. The path is then computed by propagating the
wavefront through regions with higher traversability. Simulated and real-world
experiments demonstrate that the approach enhances significant safety by
keeping the robot away from regions with obstacles while reducing unnecessary
deviations from the goal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based
  Autonomous Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo El-Hariry, Antoine Richard, Ricard M. Castan, Luis F. W. Batista, Matthieu Geist, Cedric Pradalier, Miguel Olivares-Mendez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous robots must navigate and operate in diverse environments, from
terrestrial and aquatic settings to aerial and space domains. While
Reinforcement Learning (RL) has shown promise in training policies for specific
autonomous robots, existing benchmarks are often constrained to unique
platforms, limiting generalization and fair comparisons across different
mobility systems. In this paper, we present NavBench, a multi-domain benchmark
for training and evaluating RL-based navigation policies across diverse robotic
platforms and operational environments. Built on IsaacLab, our framework
standardizes task definitions, enabling different robots to tackle various
navigation challenges without the need for ad-hoc task redesigns or custom
evaluation metrics. Our benchmark addresses three key challenges: (1) Unified
cross-medium benchmarking, enabling direct evaluation of diverse actuation
methods (thrusters, wheels, water-based propulsion) in realistic environments;
(2) Scalable and modular design, facilitating seamless robot-task
interchangeability and reproducible training pipelines; and (3) Robust
sim-to-real validation, demonstrated through successful policy transfer to
multiple real-world robots, including a satellite robotic simulator, an
unmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency
between simulation and real-world deployment, NavBench simplifies the
development of adaptable RL-based navigation strategies. Its modular design
allows researchers to easily integrate custom robots and tasks by following the
framework's predefined templates, making it accessible for a wide range of
applications. Our code is publicly available at NavBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for publication. Under review (2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Immersive Bilateral Teleoperation of Dissimilar Systems with
  Enhanced Transparency and Sense of Embodiment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Hejrati, Jouni Mattila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In human-in-the-loop systems such as teleoperation, especially those
involving heavy-duty manipulators, achieving high task performance requires
both robust control and strong human engagement. This paper presents a
bilateral teleoperation framework that enhances the operator's Sense of
Embodiment (SoE), specifically, the senses of agency and self-location, through
an immersive virtual reality interface and distributed haptic feedback via an
exoskeleton. To support this embodiment and stablish high level of motion and
force transparency, we develop a force-sensorless, robust control architecture
that tackles input nonlinearities, master-slave asymmetries, unknown
uncertainties, and arbitrary time delays. A human-robot augmented dynamic model
is integrated into the control loop to enhance human-adaptability of the
controller. Theoretical analysis confirms semi-global uniform ultimate
boundedness of the closed-loop system. Extensive real-world experiments
demonstrate high accuracy tracking under up to 1:13 motion scaling and 1:1000
force scaling, showcasing the significance of the results. Additionally, the
stability-transparency tradeoff for motion tracking and force
reflection-tracking is establish up to 150 ms of one-way fix and time-varying
communication delay. The results of user study with 10 participants (9 male and
1 female) demonstrated that the system can imply a good level of SoE (76.4%),
at the same time is very user friendly with no gender limitation. These results
are significant given the scale and weight of the heavy-duty manipulators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantically-driven Deep Reinforcement Learning for Inspection Path
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grzegorz Malczyk, Mihir Kulkarni, Kostas Alexis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel semantics-aware inspection planning policy
derived through deep reinforcement learning. Reflecting the fact that within
autonomous informative path planning missions in unknown environments, it is
often only a sparse set of objects of interest that need to be inspected, the
method contributes an end-to-end policy that simultaneously performs semantic
object visual inspection combined with collision-free navigation. Assuming
access only to the instantaneous depth map, the associated segmentation image,
the ego-centric local occupancy, and the history of past positions in the
robot's neighborhood, the method demonstrates robust generalizability and
successful crossing of the sim2real gap. Beyond simulations and extensive
comparison studies, the approach is verified in experimental evaluations
onboard a flying robot deployed in novel environments with previously unseen
semantics and overall geometric configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Robotics and Automation Letters
  (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Embodied Cognition in Robots via Spatially Grounded Synthetic
  Worlds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Currie, Gioele Migno, Enrico Piacenti, Maria Elena Giannaccini, Patric Bach, Davide De Tommaso, Agnieszka Wykowska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a conceptual framework for training Vision-Language Models (VLMs)
to perform Visual Perspective Taking (VPT), a core capability for embodied
cognition essential for Human-Robot Interaction (HRI). As a first step toward
this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,
that enables supervised learning for spatial reasoning tasks. Each instance
includes an RGB image, a natural language description, and a ground-truth 4X4
transformation matrix representing object pose. We focus on inferring Z-axis
distance as a foundational skill, with future extensions targeting full 6
Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to
support further research. This work serves as a foundational step toward
embodied AI systems capable of spatial understanding in interactive human-robot
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to: Intelligent Autonomous Systems (IAS) 2025 as Late
  Breaking Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Minima Prediction using Dynamic Bayesian Filtering for UGV
  Navigation in Unstructured Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seung Hun Lee, Wonse Jo, Lionel P. Robert Jr., Dawn M. Tilbury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Path planning is crucial for the navigation of autonomous vehicles, yet these
vehicles face challenges in complex and real-world environments. Although a
global view may be provided, it is often outdated, necessitating the reliance
of Unmanned Ground Vehicles (UGVs) on real-time local information. This
reliance on partial information, without considering the global context, can
lead to UGVs getting stuck in local minima. This paper develops a method to
proactively predict local minima using Dynamic Bayesian filtering, based on the
detected obstacles in the local view and the global goal. This approach aims to
enhance the autonomous navigation of self-driving vehicles by allowing them to
predict potential pitfalls before they get stuck, and either ask for help from
a human, or re-plan an alternate trajectory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sampling-Based System Identification with Active Exploration for Legged
  Robot Sim2Real Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Sobanbabu, Guanqi He, Tairan He, Yuxiang Yang, Guanya Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sim-to-real discrepancies hinder learning-based policies from achieving
high-precision tasks in the real world. While Domain Randomization (DR) is
commonly used to bridge this gap, it often relies on heuristics and can lead to
overly conservative policies with degrading performance when not properly
tuned. System Identification (Sys-ID) offers a targeted approach, but standard
techniques rely on differentiable dynamics and/or direct torque measurement,
assumptions that rarely hold for contact-rich legged systems. To this end, we
present SPI-Active (Sampling-based Parameter Identification with Active
Exploration), a two-stage framework that estimates physical parameters of
legged robots to minimize the sim-to-real gap. SPI-Active robustly identifies
key physical parameters through massive parallel sampling, minimizing state
prediction errors between simulated and real-world trajectories. To further
improve the informativeness of collected data, we introduce an active
exploration strategy that maximizes the Fisher Information of the collected
real-world trajectories via optimizing the input commands of an exploration
policy. This targeted exploration leads to accurate identification and better
generalization across diverse tasks. Experiments demonstrate that SPI-Active
enables precise sim-to-real transfer of learned policies to the real world,
outperforming baselines by 42-63% in various locomotion tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M3Depth: Wavelet-Enhanced Depth Estimation on Mars via Mutual Boosting
  of Dual-Modal Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Li, Jiawei Wang, Miyu Li, Yu Liu, Yumei Wang, Haitao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation plays a great potential role in obstacle avoidance and
navigation for further Mars exploration missions. Compared to traditional
stereo matching, learning-based stereo depth estimation provides a data-driven
approach to infer dense and precise depth maps from stereo image pairs.
However, these methods always suffer performance degradation in environments
with sparse textures and lacking geometric constraints, such as the
unstructured terrain of Mars. To address these challenges, we propose M3Depth,
a depth estimation model tailored for Mars rovers. Considering the sparse and
smooth texture of Martian terrain, which is primarily composed of low-frequency
features, our model incorporates a convolutional kernel based on wavelet
transform that effectively captures low-frequency response and expands the
receptive field. Additionally, we introduce a consistency loss that explicitly
models the complementary relationship between depth map and surface normal map,
utilizing the surface normal as a geometric constraint to enhance the accuracy
of depth estimation. Besides, a pixel-wise refinement module with mutual
boosting mechanism is designed to iteratively refine both depth and surface
normal predictions. Experimental results on synthetic Mars datasets with depth
annotations show that M3Depth achieves a significant 16% improvement in depth
estimation accuracy compared to other state-of-the-art methods in depth
estimation. Furthermore, the model demonstrates strong applicability in
real-world Martian scenarios, offering a promising solution for future Mars
exploration missions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Alles, Nutan Chen, Patrick van der Smagt, Botond Cseke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of guidance to steer sampling toward desired outcomes has been widely
explored within diffusion models, especially in applications such as image and
trajectory generation. However, incorporating guidance during training remains
relatively underexplored. In this work, we introduce energy-guided flow
matching, a novel approach that enhances the training of flow models and
eliminates the need for guidance at inference time. We learn a conditional
velocity field corresponding to the flow policy by approximating an
energy-guided probability path as a Gaussian path. Learning guided trajectories
is appealing for tasks where the target distribution is defined by a
combination of data and an energy function, as in reinforcement learning.
Diffusion-based policies have recently attracted attention for their expressive
power and ability to capture multi-modal action distributions. Typically, these
policies are optimized using weighted objectives or by back-propagating
gradients through actions sampled by the policy. As an alternative, we propose
FlowQ, an offline reinforcement learning algorithm based on energy-guided flow
matching. Our method achieves competitive performance while the policy training
time is constant in the number of flow sampling steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unconventional Hexacopters via Evolution and Learning: Performance Gains
  and New Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jed Muff, Keiichi Ito, Elijah H. W. Ang, Karine Miras, A. E. Eiben
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evolution and learning have historically been interrelated topics, and their
interplay is attracting increased interest lately. The emerging new factor in
this trend is morphological evolution, the evolution of physical forms within
embodied AI systems such as robots. In this study, we investigate a system of
hexacopter-type drones with evolvable morphologies and learnable controllers
and make contributions to two fields. For aerial robotics, we demonstrate that
the combination of evolution and learning can deliver non-conventional drones
that significantly outperform the traditional hexacopter on several tasks that
are more complex than previously considered in the literature. For the field of
Evolutionary Computing, we introduce novel metrics and perform new analyses
into the interaction of morphological evolution and learning, uncovering
hitherto unidentified effects. Our analysis tools are domain-agnostic, making a
methodological contribution towards building solid foundations for embodied AI
systems that integrate evolution and learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 14 figures, currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On-Demand Scenario Generation for Testing Automated Driving Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songyang Yan, Xiaodong Zhang, Kunkun Hao, haojie xin, Yonggang Luo, Jucheng Yang, Ming Fan, Chao Yang, Jun Sun, Zijiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The safety and reliability of Automated Driving Systems (ADS) are paramount,
necessitating rigorous testing methodologies to uncover potential failures
before deployment. Traditional testing approaches often prioritize either
natural scenario sampling or safety-critical scenario generation, resulting in
overly simplistic or unrealistic hazardous tests. In practice, the demand for
natural scenarios (e.g., when evaluating the ADS's reliability in real-world
conditions), critical scenarios (e.g., when evaluating safety in critical
situations), or somewhere in between (e.g., when testing the ADS in regions
with less civilized drivers) varies depending on the testing objectives. To
address this issue, we propose the On-demand Scenario Generation (OSG)
Framework, which generates diverse scenarios with varying risk levels.
Achieving the goal of OSG is challenging due to the complexity of quantifying
the criticalness and naturalness stemming from intricate vehicle-environment
interactions, as well as the need to maintain scenario diversity across various
risk levels. OSG learns from real-world traffic datasets and employs a Risk
Intensity Regulator to quantitatively control the risk level. It also leverages
an improved heuristic search method to ensure scenario diversity. We evaluate
OSG on the Carla simulators using various ADSs. We verify OSG's ability to
generate scenarios with different risk levels and demonstrate its necessity by
comparing accident types across risk levels. With the help of OSG, we are now
able to systematically and objectively compare the performance of different
ADSs based on different risk levels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 9 figures. Accepted by FSE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoBio: A Simulation and Benchmark for Robotic Automation in Digital
  Biology Laboratory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqian Lan, Yuxuan Jiang, Ruiqi Wang, Xuanbing Xie, Rongkui Zhang, Yicheng Zhu, Peihang Li, Tianshuo Yang, Tianxing Chen, Haoyu Gao, Xiaokang Yang, Xuelong Li, Hongyuan Zhang, Yao Mu, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language-action (VLA) models have shown promise as generalist robotic
policies by jointly leveraging visual, linguistic, and proprioceptive
modalities to generate action trajectories. While recent benchmarks have
advanced VLA research in domestic tasks, professional science-oriented domains
remain underexplored. We introduce AutoBio, a simulation framework and
benchmark designed to evaluate robotic automation in biology laboratory
environments--an application domain that combines structured protocols with
demanding precision and multimodal interaction. AutoBio extends existing
simulation capabilities through a pipeline for digitizing real-world laboratory
instruments, specialized physics plugins for mechanisms ubiquitous in
laboratory workflows, and a rendering stack that support dynamic instrument
interfaces and transparent materials through physically based rendering. Our
benchmark comprises biologically grounded tasks spanning three difficulty
levels, enabling standardized evaluation of language-guided robotic
manipulation in experimental protocols. We provide infrastructure for
demonstration generation and seamless integration with VLA models. Baseline
evaluations with two SOTA VLA models reveal significant gaps in precision
manipulation, visual reasoning, and instruction following in scientific
workflows. By releasing AutoBio, we aim to catalyze research on generalist
robotic systems for complex, high-precision, and multimodal professional
environments. The simulator and benchmark are publicly available to facilitate
reproducible research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Visuo-Tactile Fusion with Predictive Force Attention for
  Dexterous Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinzhou Li, Tianhao Wu, Jiyao Zhang, Zeyuan Chen, Haotian Jin, Mingdong Wu, Yujun Shen, Yaodong Yang, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively utilizing multi-sensory data is important for robots to
generalize across diverse tasks. However, the heterogeneous nature of these
modalities makes fusion challenging. Existing methods propose strategies to
obtain comprehensively fused features but often ignore the fact that each
modality requires different levels of attention at different manipulation
stages. To address this, we propose a force-guided attention fusion module that
adaptively adjusts the weights of visual and tactile features without human
labeling. We also introduce a self-supervised future force prediction auxiliary
task to reinforce the tactile modality, improve data imbalance, and encourage
proper adjustment. Our method achieves an average success rate of 93% across
three fine-grained, contactrich tasks in real-world experiments. Further
analysis shows that our policy appropriately adjusts attention to each modality
at different manipulation stages. The videos can be viewed at
https://adaptac-dex.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hypothesis on the Functional Advantages of the Selection-Broadcast Cycle
  Structure: Global Workspace Theory and Dealing with a Real-Time World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junya Nakanishi, Jun Baba, Yuichiro Yoshikawa, Hiroko Kamide, Hiroshi Ishiguro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper discusses the functional advantages of the Selection-Broadcast
Cycle structure proposed by Global Workspace Theory (GWT), inspired by human
consciousness, particularly focusing on its applicability to artificial
intelligence and robotics in dynamic, real-time scenarios. While previous
studies often examined the Selection and Broadcast processes independently,
this research emphasizes their combined cyclic structure and the resulting
benefits for real-time cognitive systems. Specifically, the paper identifies
three primary benefits: Dynamic Thinking Adaptation, Experience-Based
Adaptation, and Immediate Real-Time Adaptation. This work highlights GWT's
potential as a cognitive architecture suitable for sophisticated
decision-making and adaptive performance in unsupervised, dynamic environments.
It suggests new directions for the development and implementation of robust,
general-purpose AI and robotics systems capable of managing complex, real-world
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiDrive: A Co-Simulation Framework Bridging 2D and 3D Driving
  Simulation for AV Software Validation <span class="chip">SC 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Kaufeld, Korbinian Moller, Alessio Gambi, Paolo Arcaini, Johannes Betz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scenario-based testing using simulations is a cornerstone of Autonomous
Vehicles (AVs) software validation. So far, developers needed to choose between
low-fidelity 2D simulators to explore the scenario space efficiently, and
high-fidelity 3D simulators to study relevant scenarios in more detail, thus
reducing testing costs while mitigating the sim-to-real gap. This paper
presents a novel framework that leverages multi-agent co-simulation and
procedural scenario generation to support scenario-based testing across low-
and high-fidelity simulators for the development of motion planning algorithms.
Our framework limits the effort required to transition scenarios between
simulators and automates experiment execution, trajectory analysis, and
visualization. Experiments with a reference motion planner show that our
framework uncovers discrepancies between the planner's intended and actual
behavior, thus exposing weaknesses in planning assumptions under more realistic
conditions. Our framework is available at:
https://github.com/TUM-AVS/MultiDrive
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, Submitted to the IEEE International Conference on
  Intelligent Transportation Systems (ITSC 2025), Australia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sketch Interface for Teleoperation of Mobile Manipulator to Enable
  Intuitive and Intended Operation: A Proof of Concept 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuka Iwanaga, Masayoshi Tsuchinaga, Kosei Tanada, Yuji Nakamura, Takemitsu Mori, Takashi Yamamoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in robotics have underscored the need for effective
collaboration between humans and robots. Traditional interfaces often struggle
to balance robot autonomy with human oversight, limiting their practical
application in complex tasks like mobile manipulation. This study aims to
develop an intuitive interface that enables a mobile manipulator to
autonomously interpret user-provided sketches, enhancing user experience while
minimizing burden. We implemented a web-based application utilizing machine
learning algorithms to process sketches, making the interface accessible on
mobile devices for use anytime, anywhere, by anyone. In the first validation,
we examined natural sketches drawn by users for 27 selected manipulation and
navigation tasks, gaining insights into trends related to sketch instructions.
The second validation involved comparative experiments with five grasping
tasks, showing that the sketch interface reduces workload and enhances
intuitiveness compared to conventional axis control interfaces. These findings
suggest that the proposed sketch interface improves the efficiency of mobile
manipulators and opens new avenues for integrating intuitive human-robot
collaboration in various applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to the the 20th edition of the IEEE/ACM
  International Conference on Human-Robot Interaction (HRI'25), which will be
  held in Melbourne, Australia on March 4-6, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time Reversal Symmetry for Efficient Robotic Manipulations in Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Jiang, Jianshu Hu, Paul Weng, Yutong Ban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symmetry is pervasive in robotics and has been widely exploited to improve
sample efficiency in deep reinforcement learning (DRL). However, existing
approaches primarily focus on spatial symmetries, such as reflection, rotation,
and translation, while largely neglecting temporal symmetries. To address this
gap, we explore time reversal symmetry, a form of temporal symmetry commonly
found in robotics tasks such as door opening and closing. We propose Time
Reversal symmetry enhanced Deep Reinforcement Learning (TR-DRL), a framework
that combines trajectory reversal augmentation and time reversal guided reward
shaping to efficiently solve temporally symmetric tasks. Our method generates
reversed transitions from fully reversible transitions, identified by a
proposed dynamics-consistent filter, to augment the training data. For
partially reversible transitions, we apply reward shaping to guide learning,
according to successful trajectories from the reversed task. Extensive
experiments on the Robosuite and MetaWorld benchmarks demonstrate that TR-DRL
is effective in both single-task and multi-task settings, achieving higher
sample efficiency and stronger final performance compared to baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ APEX: Empowering LLMs with Physics-Based Task Planning for Real-time
  Insight 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanjing Huang, Weixiang Yan, Zhen Zhang, Ambuj Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate strong reasoning and task planning
capabilities but remain fundamentally limited in physical interaction modeling.
Existing approaches integrate perception via Vision-Language Models (VLMs) or
adaptive decision-making through Reinforcement Learning (RL), but they fail to
capture dynamic object interactions or require task-specific training, limiting
their real-world applicability. We introduce APEX (Anticipatory
Physics-Enhanced Execution), a framework that equips LLMs with physics-driven
foresight for real-time task planning. APEX constructs structured graphs to
identify and model the most relevant dynamic interactions in the environment,
providing LLMs with explicit physical state updates. Simultaneously, APEX
provides low-latency forward simulations of physically feasible actions,
allowing LLMs to select optimal strategies based on predictive outcomes rather
than static observations. We evaluate APEX on three benchmarks designed to
assess perception, prediction, and decision-making: (1) Physics Reasoning
Benchmark, testing causal inference and object motion prediction; (2) Tetris,
evaluating whether physics-informed prediction enhances decision-making
performance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance,
assessing the immediate integration of perception and action feasibility
analysis. APEX significantly outperforms standard LLMs and VLM-based models,
demonstrating the necessity of explicit physics reasoning for bridging the gap
between language-based intelligence and real-world task execution. The source
code and experiment setup are publicly available at
https://github.com/hwj20/APEX_EXP .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robotic Monitoring of Colorimetric Leaf Sensors for Precision
  Agriculture <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malakhi Hopkins, Alice Kate Li, Shobhita Kramadhati, Jackson Arnold, Akhila Mallavarapu, Chavez Lawrence, Varun Murali, Sanjeev J. Koppal, Cherie Kagan, Vijay Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current remote sensing technologies that measure crop health e.g. RGB,
multispectral, hyperspectral, and LiDAR, are indirect, and cannot capture plant
stress indicators directly. Instead, low-cost leaf sensors that directly
interface with the crop surface present an opportunity to advance real-time
direct monitoring. To this end, we co-design a sensor-detector system, where
the sensor is a novel colorimetric leaf sensor that directly measures crop
health in a precision agriculture setting, and the detector autonomously
obtains optical signals from these leaf sensors. This system integrates a
ground robot platform with an on-board monocular RGB camera and object detector
to localize the leaf sensor, and a hyperspectral camera with motorized mirror
and an on-board halogen light to acquire a hyperspectral reflectance image of
the leaf sensor, from which a spectral response characterizing crop health can
be extracted. We show a successful demonstration of our co-designed system
operating in outdoor environments, obtaining spectra that are interpretable
when compared to controlled laboratory-grade spectrometer measurements. The
system is demonstrated in row-crop environments both indoors and outdoors where
it is able to autonomously navigate, locate and obtain a hyperspectral image of
all leaf sensors present, and retrieve interpretable spectral resonance from
leaf sensors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Novel Approaches for Precision Agriculture and
  Forestry with Autonomous Robots IEEE ICRA Workshop - 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 4D-ROLLS: 4D Radar Occupancy Learning via LiDAR Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihan Liu, Xiaoyi Wu, Xijun Chen, Liang Hu, Yunjiang Lou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A comprehensive understanding of 3D scenes is essential for autonomous
vehicles (AVs), and among various perception tasks, occupancy estimation plays
a central role by providing a general representation of drivable and occupied
space. However, most existing occupancy estimation methods rely on LiDAR or
cameras, which perform poorly in degraded environments such as smoke, rain,
snow, and fog. In this paper, we propose 4D-ROLLS, the first weakly supervised
occupancy estimation method for 4D radar using the LiDAR point cloud as the
supervisory signal. Specifically, we introduce a method for generating
pseudo-LiDAR labels, including occupancy queries and LiDAR height maps, as
multi-stage supervision to train the 4D radar occupancy estimation model. Then
the model is aligned with the occupancy map produced by LiDAR, fine-tuning its
accuracy in occupancy estimation. Extensive comparative experiments validate
the exceptional performance of 4D-ROLLS. Its robustness in degraded
environments and effectiveness in cross-dataset training are qualitatively
demonstrated. The model is also seamlessly transferred to downstream tasks BEV
segmentation and point cloud occupancy prediction, highlighting its potential
for broader applications. The lightweight network enables 4D-ROLLS model to
achieve fast inference speeds at about 30 Hz on a 4060 GPU. The code of
4D-ROLLS will be made available at https://github.com/CLASS-Lab/4D-ROLLS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Insert for Constructive Neural Vehicle Routing Solver 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu Luo, Xi Lin, Mengyuan Zhong, Fei Liu, Zhenkun Wang, Jianyong Sun, Qingfu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Combinatorial Optimisation (NCO) is a promising learning-based
approach for solving Vehicle Routing Problems (VRPs) without extensive manual
design. While existing constructive NCO methods typically follow an
appending-based paradigm that sequentially adds unvisited nodes to partial
solutions, this rigid approach often leads to suboptimal results. To overcome
this limitation, we explore the idea of insertion-based paradigm and propose
Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel
learning-based method for constructive NCO. Unlike traditional approaches,
L2C-Insert builds solutions by strategically inserting unvisited nodes at any
valid position in the current partial solution, which can significantly enhance
the flexibility and solution quality. The proposed framework introduces three
key components: a novel model architecture for precise insertion position
prediction, an efficient training scheme for model optimization, and an
advanced inference technique that fully exploits the insertion paradigm's
flexibility. Extensive experiments on both synthetic and real-world instances
of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior
performance across various problem sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Certifiably Safe Manipulation of Deformable Linear Objects via Joint
  Shape and Tension Prediction <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiting Zhang, Shichen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulating deformable linear objects (DLOs) is challenging due to their
complex dynamics and the need for safe interaction in contact-rich
environments. Most existing models focus on shape prediction alone and fail to
account for contact and tension constraints, which can lead to damage to both
the DLO and the robot. In this work, we propose a certifiably safe motion
planning and control framework for DLO manipulation. At the core of our method
is a predictive model that jointly estimates the DLO's future shape and
tension. These predictions are integrated into a real-time trajectory optimizer
based on polynomial zonotopes, allowing us to enforce safety constraints
throughout the execution. We evaluate our framework on a simulated wire harness
assembly task using a 7-DOF robotic arm. Compared to state-of-the-art methods,
our approach achieves a higher task success rate while avoiding all safety
violations. The results demonstrate that our method enables robust and safe DLO
manipulation in contact-rich environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2025 Workshop on Learning Meets Model-Based Methods
  for Contact-Rich Manipulation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Zhang, Shihan Wu, Xu Luo, Hao Wu, Lianli Gao, Heng Tao Shen, Jingkuan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging pretrained Vision-Language Models (VLMs) to map language
instruction and visual observations to raw low-level actions,
Vision-Language-Action models (VLAs) hold great promise for achieving
general-purpose robotic systems. Despite their advancements, existing VLAs tend
to spuriously correlate task-irrelevant visual features with actions, limiting
their generalization capacity beyond the training data. To tackle this
challenge, we propose Intrinsic Spatial Reasoning (InSpire), a simple yet
effective approach that mitigates the adverse effects of spurious correlations
by boosting the spatial reasoning ability of VLAs. Specifically, InSpire
redirects the VLA's attention to task-relevant factors by prepending the
question "In which direction is the [object] relative to the robot?" to the
language instruction and aligning the answer
"right/left/up/down/front/back/grasped" and predicted actions with the
ground-truth. Notably, InSpire can be used as a plugin to enhance existing
autoregressive VLAs, requiring no extra training data or interaction with other
large models. Extensive experimental results in both simulation and real-world
environments demonstrate the effectiveness and flexibility of our approach. Our
code, pretrained models and demos are publicly available at:
https://Koorye.github.io/proj/Inspire.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safety2Drive: Safety-Critical Scenario Benchmark for the Evaluation of
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingzheng Li, Tiancheng Wang, Xingyu Peng, Jiacheng Chen, Zhijun Chen, Bing Li, Xianglong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous Driving (AD) systems demand the high levels of safety assurance.
Despite significant advancements in AD demonstrated on open-source benchmarks
like Longest6 and Bench2Drive, existing datasets still lack
regulatory-compliant scenario libraries for closed-loop testing to
comprehensively evaluate the functional safety of AD. Meanwhile, real-world AD
accidents are underrepresented in current driving datasets. This scarcity leads
to inadequate evaluation of AD performance, posing risks to safety validation
and practical deployment. To address these challenges, we propose Safety2Drive,
a safety-critical scenario library designed to evaluate AD systems.
Safety2Drive offers three key contributions. (1) Safety2Drive comprehensively
covers the test items required by standard regulations and contains 70 AD
function test items. (2) Safety2Drive supports the safety-critical scenario
generalization. It has the ability to inject safety threats such as natural
environment corruptions and adversarial attacks cross camera and LiDAR sensors.
(3) Safety2Drive supports multi-dimensional evaluation. In addition to the
evaluation of AD systems, it also supports the evaluation of various perception
tasks, such as object detection and lane detection. Safety2Drive provides a
paradigm from scenario construction to validation, establishing a standardized
test framework for the safe deployment of AD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Robot Navigation Policies with Task-Specific Uncertainty
  Managements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokul Puthumanaillam, Paulo Padrao, Jose Fuentes, Leonardo Bobadilla, Melkior Ornik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots navigating complex environments must manage uncertainty from sensor
noise, environmental changes, and incomplete information, with different tasks
requiring varying levels of precision in different areas. For example, precise
localization may be crucial near obstacles but less critical in open spaces. We
present GUIDE (Generalized Uncertainty Integration for Decision-Making and
Execution), a framework that integrates these task-specific requirements into
navigation policies via Task-Specific Uncertainty Maps (TSUMs). By assigning
acceptable uncertainty levels to different locations, TSUMs enable robots to
adapt uncertainty management based on context. When combined with reinforcement
learning, GUIDE learns policies that balance task completion and uncertainty
management without extensive reward engineering. Real-world tests show
significant performance gains over methods lacking task-specific uncertainty
awareness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Duawlfin: A Drone with Unified Actuation for Wheeled Locomotion and
  Flight Operation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Tang, Ruiqi Zhang, Kaan Beyduz, Yiwei Jiang, Cody Wiebe, Haoyu Zhang, Osaruese Asoro, Mark W. Mueller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Duawlfin, a drone with unified actuation for wheeled
locomotion and flight operation that achieves efficient, bidirectional ground
mobility. Unlike existing hybrid designs, Duawlfin eliminates the need for
additional actuators or propeller-driven ground propulsion by leveraging only
its standard quadrotor motors and introducing a differential drivetrain with
one-way bearings. This innovation simplifies the mechanical system,
significantly reduces energy usage, and prevents the disturbance caused by
propellers spinning near the ground, such as dust interference with sensors.
Besides, the one-way bearings minimize the power transfer from motors to
propellers in the ground mode, which enables the vehicle to operate safely near
humans. We provide a detailed mechanical design, present control strategies for
rapid and smooth mode transitions, and validate the concept through extensive
experimental testing. Flight-mode tests confirm stable aerial performance
comparable to conventional quadcopters, while ground-mode experiments
demonstrate efficient slope climbing (up to 30{\deg}) and agile turning
maneuvers approaching 1g lateral acceleration. The seamless transitions between
aerial and ground modes further underscore the practicality and effectiveness
of our approach for applications like urban logistics and indoor navigation.
All the materials including 3-D model files, demonstration video and other
assets are open-sourced at https://sites.google.com/view/Duawlfin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Real-World Cooperative and Competitive Soccer with Quadrupedal
  Robot Teams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Su, Yuman Gao, Emily Lukas, Yunfei Li, Jiaze Cai, Faris Tulbah, Fei Gao, Chao Yu, Zhongyu Li, Yi Wu, Koushil Sreenath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving coordinated teamwork among legged robots requires both fine-grained
locomotion control and long-horizon strategic decision-making. Robot soccer
offers a compelling testbed for this challenge, combining dynamic, competitive,
and multi-agent interactions. In this work, we present a hierarchical
multi-agent reinforcement learning (MARL) framework that enables fully
autonomous and decentralized quadruped robot soccer. First, a set of highly
dynamic low-level skills is trained for legged locomotion and ball
manipulation, such as walking, dribbling, and kicking. On top of these, a
high-level strategic planning policy is trained with Multi-Agent Proximal
Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning
framework allows agents to adapt to diverse opponent strategies and gives rise
to sophisticated team behaviors, including coordinated passing, interception,
and dynamic role allocation. With an extensive ablation study, the proposed
learning method shows significant advantages in the cooperative and competitive
multi-agent soccer game. We deploy the learned policies to real quadruped
robots relying solely on onboard proprioception and decentralized localization,
with the resulting system supporting autonomous robot-robot and robot-human
soccer matches on indoor and outdoor soccer courts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ C*: A Coverage Path Planning Algorithm for Unknown Environments using
  Rapidly Covering Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyuan Shen, James P. Wilson, Shalabh Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents a novel sample-based algorithm, called C*, for real-time
coverage path planning (CPP) of unknown environments. The C* algorithm is built
upon the concept of Rapidly Covering Graph (RCGs). The RCG is constructed
incrementally via progressive sampling during robot navigation, which
eliminates the need for cellular decomposition of the search space. The RCG has
a sparse-graph structure formed by efficient sampling and pruning techniques,
which produces non-myopic waypoints of the coverage trajectory. While C*
produces the desired back and forth coverage pattern, it adapts to the
TSP-based locally optimal coverage of small uncovered regions, called coverage
holes, that are surrounded by obstacles and covered regions. Thus, C*
proactively detects and covers the coverage holes in situ, which reduces the
coverage time by preventing the longer return trajectories from distant regions
to cover such holes later. The algorithmic simplicity and low computational
complexity of C* makes it easy to implement and suitable for real-time onboard
applications. It is analytically proven that C* provides complete coverage of
unknown environments. The performance of C* is validated by 1) extensive
high-fidelity simulations and 2) real laboratory experiments using autonomous
robots. A comparative evaluation with seven existing CPP methods demonstrate
that C* yields significant performance improvements in terms of coverage time,
number of turns, trajectory length and overlap ratio, while preventing the
formation of coverage holes. Finally, C* is evaluated on two different
applications of CPP using 1) energy-constrained robots and 2) multi-robot
teams.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models <span class="chip">ICRA-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15250v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15250v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sombit Dey, Jan-Nico Zaech, Nikolay Nikolov, Luc Van Gool, Danda Pani Paudel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in large language models and access to large-scale robotic
datasets has sparked a paradigm shift in robotics models transforming them into
generalists able to adapt to various tasks, scenes, and robot modalities. A
large step for the community are open Vision Language Action models which
showcase strong performance in a wide variety of tasks. In this work, we study
the visual generalization capabilities of three existing robotic foundation
models, and propose a corresponding evaluation framework. Our study shows that
the existing models do not exhibit robustness to visual out-of-domain
scenarios. This is potentially caused by limited variations in the training
data and/or catastrophic forgetting, leading to domain limitations in the
vision foundation models. We further explore OpenVLA, which uses two
pre-trained vision foundation models and is, therefore, expected to generalize
to out-of-domain experiments. However, we showcase catastrophic forgetting by
DINO-v2 in OpenVLA through its failure to fulfill the task of depth regression.
To overcome the aforementioned issue of visual catastrophic forgetting, we
propose a gradual backbone reversal approach founded on model merging. This
enables OpenVLA -- which requires the adaptation of the visual backbones during
initial training -- to regain its visual generalization ability. Regaining this
capability enables our ReVLA model to improve over OpenVLA by a factor of 77\%
and 66\% for grasping and lifting in visual OOD tasks. Comprehensive
evaluations, episode rollouts and model weights are available on the ReVLA Page
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA-2025, Atlanta</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion-Based Failure Sampling for Evaluating Safety-Critical
  Autonomous Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harrison Delecki, Marc R. Schlichting, Mansur Arief, Anthony Corso, Marcell Vazquez-Chanlatte, Mykel J. Kochenderfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Validating safety-critical autonomous systems in high-dimensional domains
such as robotics presents a significant challenge. Existing black-box
approaches based on Markov chain Monte Carlo may require an enormous number of
samples, while methods based on importance sampling often rely on simple
parametric families that may struggle to represent the distribution over
failures. We propose to sample the distribution over failures using a
conditional denoising diffusion model, which has shown success in complex
high-dimensional problems such as robotic task planning. We iteratively train a
diffusion model to produce state trajectories closer to failure. We demonstrate
the effectiveness of our approach on high-dimensional robotic validation tasks,
improving sample efficiency and mode coverage compared to existing black-box
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appears in IEEE International Conference on Engineering Reliable
  Autonomous Systems (ERAS) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SG-Reg: Generalizable and Efficient Scene Graph Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.14440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.14440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhao Liu, Zhijian Qiao, Jieqi Shi, Ke Wang, Peize Liu, Shaojie Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenges of registering two rigid semantic scene
graphs, an essential capability when an autonomous agent needs to register its
map against a remote agent, or against a prior map. The hand-crafted
descriptors in classical semantic-aided registration, or the ground-truth
annotation reliance in learning-based scene graph registration, impede their
application in practical real-world environments. To address the challenges, we
design a scene graph network to encode multiple modalities of semantic nodes:
open-set semantic feature, local topology with spatial awareness, and shape
feature. These modalities are fused to create compact semantic node features.
The matching layers then search for correspondences in a coarse-to-fine manner.
In the back-end, we employ a robust pose estimator to decide transformation
according to the correspondences. We manage to maintain a sparse and
hierarchical scene representation. Our approach demands fewer GPU resources and
fewer communication bandwidth in multi-agent tasks. Moreover, we design a new
data generation approach using vision foundation models and a semantic mapping
module to reconstruct semantic scene graphs. It differs significantly from
previous works, which rely on ground-truth semantic annotations to generate
data. We validate our method in a two-agent SLAM benchmark. It significantly
outperforms the hand-crafted baseline in terms of registration success rate.
Compared to visual loop closure networks, our method achieves a slightly higher
registration recall while requiring only 52 KB of communication bandwidth for
each query frame. Code available at:
\href{http://github.com/HKUST-Aerial-Robotics/SG-Reg}{http://github.com/HKUST-Aerial-Robotics/SG-Reg}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions Robotics Regular Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Practical Guide for Incorporating Symmetry in Diffusion Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13431v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13431v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dian Wang, Boce Hu, Shuran Song, Robin Walters, Robert Platt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, equivariant neural networks for policy learning have shown
promising improvements in sample efficiency and generalization, however, their
wide adoption faces substantial barriers due to implementation complexity.
Equivariant architectures typically require specialized mathematical
formulations and custom network design, posing significant challenges when
integrating with modern policy frameworks like diffusion-based models. In this
paper, we explore a number of straightforward and practical approaches to
incorporate symmetry benefits into diffusion policies without the overhead of
full equivariant designs. Specifically, we investigate (i) invariant
representations via relative trajectory actions and eye-in-hand perception,
(ii) integrating equivariant vision encoders, and (iii) symmetric feature
extraction with pretrained encoders using Frame Averaging. We first prove that
combining eye-in-hand perception with relative or delta action parameterization
yields inherent SE(3)-invariance, thus improving policy generalization. We then
perform a systematic experimental study on those design choices for integrating
symmetry in diffusion policies, and conclude that an invariant representation
with equivariant feature extraction significantly improves the policy
performance. Our method achieves performance on par with or exceeding fully
equivariant architectures while greatly simplifying implementation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AC-LIO: Towards Asymptotic Compensation for Distortion in LiDAR-Inertial
  Odometry via Selective Intra-Frame Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05873v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05873v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxiang Zhang, Xuanxuan Zhang, Wenlei Fan, Xin Xia, Huai Yu, Lin Wang, You Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing LiDAR-Inertial Odometry (LIO) methods typically utilize the prior
trajectory derived from the IMU integration to compensate for the motion
distortion within LiDAR frames. However, discrepancies between the prior and
true trajectory can lead to residual motion distortions that compromise the
consistency of LiDAR frame with its corresponding geometric environment. This
imbalance may result in pointcloud registration becoming trapped in local
optima, thereby exacerbating drift during long-term and large-scale
localization. To this end, we propose a novel LIO framework with selective
intra-frame smoothing dubbed AC-LIO. Our core idea is to asymptotically
backpropagate current update term and compensate for residual motion distortion
under the guidance of convergence criteria, aiming to improve the accuracy of
discrete-state LIO system with minimal computational increase. Extensive
experiments demonstrate that our AC-LIO framework further enhances odometry
accuracy compared to prior arts, with about 30.4% reduction in average RMSE
over the second best result, leading to marked improvements in the accuracy of
long-term and large-scale localization and mapping.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-End and Highly-Efficient Differentiable Simulation for Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Le Lidec, Louis Montaut, Yann de Mont-Marin, Fabian Schramm, Justin Carpentier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past few years, robotics simulators have largely improved in
efficiency and scalability, enabling them to generate years of simulated data
in a few hours. Yet, efficiently and accurately computing the simulation
derivatives remains an open challenge, with potentially high gains on the
convergence speed of reinforcement learning and trajectory optimization
algorithms, especially for problems involving physical contact interactions.
This paper contributes to this objective by introducing a unified and efficient
algorithmic solution for computing the analytical derivatives of robotic
simulators. The approach considers both the collision and frictional stages,
accounting for their intrinsic nonsmoothness and also exploiting the sparsity
induced by the underlying multibody systems. These derivatives have been
implemented in C++, and the code will be open-sourced in the Simple simulator.
They depict state-of-the-art timings ranging from 5 microseconds for a 7-dof
manipulator up to 95 microseconds for 36-dof humanoid, outperforming
alternative solutions by a factor of at least 100.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Distributed Control of Multi-Robot Systems with Communication
  Delays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09382v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09382v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Ballotta, Rajat Talak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe operation of multi-robot systems is critical, especially in
communication-degraded environments such as underwater for seabed mapping,
underground caves for navigation, and in extraterrestrial missions for assembly
and construction. We address safety of networked autonomous systems where the
information exchanged between robots incurs communication delays. We formalize
a notion of distributed control barrier function for multi-robot systems, a
safety certificate amenable to a distributed implementation, which provides
formal ground to using graph neural networks to learn safe distributed
controllers. Further, we observe that learning a distributed controller
ignoring delays can severely degrade safety. We finally propose a
predictor-based framework to train a safe distributed controller under
communication delays, where the current state of nearby robots is predicted
from received data and age-of-information. Numerical experiments on multi-robot
collision avoidance show that our predictor-based approach can significantly
improve the safety of a learned distributed controller under communication
delays. A video abstract is available at https://youtu.be/Hcu1Ri32Spk.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Copyright (c) 2025 IEEE. Personal use of this material is permitted.
  However, permission to use this material for any other purposes must be
  obtained from the IEEE by sending a request to pubs-permissions@ieee.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRoQ-Loco: Generalist and Robot-agnostic Quadruped Locomotion Control
  using Offline <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Narayanan PP, Sarvesh Prasanth Venkatesan, Srinivas Kantha Reddy, Shishir Kolathaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large-scale offline training have demonstrated the
potential of generalist policy learning for complex robotic tasks. However,
applying these principles to legged locomotion remains a challenge due to
continuous dynamics and the need for real-time adaptation across diverse
terrains and robot morphologies. In this work, we propose GRoQ-Loco, a
scalable, attention-based framework that learns a single generalist locomotion
policy across multiple quadruped robots and terrains, relying solely on offline
datasets. Our approach leverages expert demonstrations from two distinct
locomotion behaviors - stair traversal (non-periodic gaits) and flat terrain
traversal (periodic gaits) - collected across multiple quadruped robots, to
train a generalist model that enables behavior fusion for both behaviors.
Crucially, our framework operates directly on proprioceptive data from all
robots without incorporating any robot-specific encodings. The policy is
directly deployable on an Intel i7 nuc, producing low-latency control outputs
without any test-time optimization. Our extensive experiments demonstrate
strong zero-shot transfer across highly diverse quadruped robots and terrains,
including hardware deployment on the Unitree Go1, a commercially available 12kg
robot. Notably, we evaluate challenging cross-robot training setups where
different locomotion skills are unevenly distributed across robots, yet observe
successful transfer of both flat walking and stair traversal behaviors to all
robots at test time. We also show preliminary walking on Stoch 5, a 70kg
quadruped, on flat and outdoor terrains without requiring any fine tuning.
These results highlight the potential for robust generalist locomotion across
diverse robots and terrains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18pages, 16figures, 6tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Triplane Grasping: Efficient 6-DoF Grasping with Single RGB Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Li, Hanchi Ren, Yue Yang, Jingjing Deng, Xianghua Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable object grasping is one of the fundamental tasks in robotics.
However, determining grasping pose based on single-image input has long been a
challenge due to limited visual information and the complexity of real-world
objects. In this paper, we propose Triplane Grasping, a fast grasping
decision-making method that relies solely on a single RGB-only image as input.
Triplane Grasping creates a hybrid Triplane-Gaussian 3D representation through
a point decoder and a triplane decoder, which produce an efficient and
high-quality reconstruction of the object to be grasped to meet real-time
grasping requirements. We propose to use an end-to-end network to generate
6-DoF parallel-jaw grasp distributions directly from 3D points in the point
cloud as potential grasp contacts and anchor the grasp pose in the observed
data. Experiments on the OmniObject3D and GraspNet-1Billion datasets
demonstrate that our method achieves rapid modeling and grasping pose
decision-making for daily objects, and strong generalization capability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-oriented Robotic Manipulation with Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurhan Bulus Guran, Hanchi Ren, Jingjing Deng, Xianghua Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) play a crucial role in robotic manipulation by
enabling robots to understand and interpret the visual properties of objects
and their surroundings, allowing them to perform manipulation based on this
multimodal understanding. Accurately understanding spatial relationships
remains a non-trivial challenge, yet it is essential for effective robotic
manipulation. In this work, we introduce a novel framework that integrates VLMs
with a structured spatial reasoning pipeline to perform object manipulation
based on high-level, task-oriented input. Our approach is the transformation of
visual scenes into tree-structured representations that encode the spatial
relations. These trees are subsequently processed by a Large Language Model
(LLM) to infer restructured configurations that determine how these objects
should be organised for a given high-level task. To support our framework, we
also present a new dataset containing manually annotated captions that describe
spatial relations among objects, along with object-level attribute annotations
such as fragility, mass, material, and transparency. We demonstrate that our
method not only improves the comprehension of spatial relationships among
objects in the visual environment but also enables robots to interact with
these objects more effectively. As a result, this approach significantly
enhances spatial reasoning in robotic manipulation tasks. To our knowledge,
this is the first method of its kind in the literature, offering a novel
solution that allows robots to more efficiently organize and utilize objects in
their surroundings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aux-Think: Exploring Reasoning Strategies for Data-Efficient
  Vision-Language Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Wang, Yongcai Wang, Wanting Li, Xudong Cai, Yucheng Wang, Maiyue Chen, Kaihui Wang, Zhizhong Su, Deying Li, Zhaoxin Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Navigation (VLN) is a critical task for developing embodied
agents that can follow natural language instructions to navigate in complex
real-world environments. Recent advances in VLN by large pretrained models have
significantly improved generalization and instruction grounding compared to
traditional approaches. However, the role of reasoning strategies in
navigation-an action-centric, long-horizon task-remains underexplored, despite
Chain-of-Thought (CoT) reasoning's demonstrated success in static tasks like
visual question answering. To address this gap, we conduct the first systematic
evaluation of reasoning strategies for VLN, including No-Think (direct action
prediction), Pre-Think (reason before action), and Post-Think (reason after
action). Surprisingly, our findings reveal the Inference-time Reasoning
Collapse issue, where inference-time reasoning degrades navigation accuracy,
highlighting the challenges of integrating reasoning into VLN. Based on this
insight, we propose Aux-Think, a framework that trains models to internalize
structured reasoning patterns through CoT supervision, while inferring action
directly without reasoning in online prediction. To support this framework, we
release R2R-CoT-320k, the first Chain-of-Thought annotated dataset for VLN.
Extensive experiments show that Aux-Think reduces training effort greatly and
achieves the best performance under the same data scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Probabilistic Model for Skill Acquisition with Switching Latent
  Feedback Controllers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyan Zhang, Dana Kulic, Michael Burke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulation tasks often consist of subtasks, each representing a distinct
skill. Mastering these skills is essential for robots, as it enhances their
autonomy, efficiency, adaptability, and ability to work in their environment.
Learning from demonstrations allows robots to rapidly acquire new skills
without starting from scratch, with demonstrations typically sequencing skills
to achieve tasks. Behaviour cloning approaches to learning from demonstration
commonly rely on mixture density network output heads to predict robot actions.
In this work, we first reinterpret the mixture density network as a library of
feedback controllers (or skills) conditioned on latent states. This arises from
the observation that a one-layer linear network is functionally equivalent to a
classical feedback controller, with network weights corresponding to controller
gains. We use this insight to derive a probabilistic graphical model that
combines these elements, describing the skill acquisition process as
segmentation in a latent space, where each skill policy functions as a feedback
control law in this latent space. Our approach significantly improves not only
task success rate, but also robustness to observation noise when trained with
human demonstrations. Our physical robot experiments further show that the
induced robustness improves model deployment on robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSCEKF-MIO: Magnetic-Inertial Odometry Based on Multi-State Constraint
  Extended Kalman Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhu Li, Jian Kuang, Xiaoji Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To overcome the limitation of existing indoor odometry technologies which
often cannot simultaneously meet requirements for accuracy cost-effectiveness,
and robustness-this paper proposes a novel magnetometer array-aided inertial
odometry approach, MSCEKF-MIO (Multi-State Constraint Extended Kalman
Filter-based Magnetic-Inertial Odometry). We construct a magnetic field model
by fitting measurements from the magnetometer array and then use temporal
variations in this model-extracted from continuous observations-to estimate the
carrier's absolute velocity. Furthermore, we implement the MSCEKF framework to
fuse observed magnetic field variations with position and attitude estimates
from inertial navigation system (INS) integration, thereby enabling autonomous,
high-precision indoor relative positioning. Experimental results demonstrate
that the proposed algorithm achieves superior velocity estimation accuracy and
horizontal positioning precision relative to state-of-the-art magnetic
array-aided INS algorithms (MAINS). On datasets with trajectory lengths of
150-250m, the proposed method yields an average horizontal position RMSE of
approximately 2.5m. In areas with distinctive magnetic features, the
magneto-inertial odometry achieves a velocity estimation accuracy of 0.07m/s.
Consequently, the proposed method offers a novel positioning solution
characterized by low power consumption, cost-effectiveness, and high
reliability in complex indoor environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Practical Equivalence Testing and Its Application in Synthetic Pre-Crash
  Scenario Validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Wu, Ulrich Sander, Carol Flannagan, Minxiang Zhao, Jonas Bärgman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of representative pre-crash scenarios is critical for assessing the
safety impact of driving automation systems through simulation. However, a gap
remains in the robust evaluation of the similarity between synthetic and
real-world pre-crash scenarios and their crash characteristics. Without proper
validation, it cannot be ensured that the synthetic test scenarios adequately
represent real-world driving behaviors and crash characteristics. One reason
for this validation gap is the lack of focus on methods to confirm that the
synthetic test scenarios are practically equivalent to real-world ones, given
the assessment scope. Traditional statistical methods, like significance
testing, focus on detecting differences rather than establishing equivalence;
since failure to detect a difference does not imply equivalence, they are of
limited applicability for validating synthetic pre-crash scenarios and crash
characteristics. This study addresses this gap by proposing an equivalence
testing method based on the Bayesian Region of Practical Equivalence (ROPE)
framework. This method is designed to assess the practical equivalence of
scenario characteristics that are most relevant for the intended assessment,
making it particularly appropriate for the domain of virtual safety
assessments. We first review existing equivalence testing methods. Then we
propose and demonstrate the Bayesian ROPE-based method by testing the
equivalence of two rear-end pre-crash datasets. Our approach focuses on the
most relevant scenario characteristics. Our analysis provides insights into the
practicalities and effectiveness of equivalence testing in synthetic test
scenario validation and demonstrates the importance of testing for improving
the credibility of synthetic data for automated vehicle safety assessment, as
well as the credibility of subsequent safety impact assessments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and
  Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12224v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12224v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weifeng Lu, Minghao Ye, Zewei Ye, Ruihan Tao, Shuo Yang, Bo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language-Action (VLA) models have recently advanced robotic
manipulation by translating natural-language instructions and image information
into sequential control actions. However, these models often underperform in
open-world scenarios, as they are predominantly trained on successful expert
demonstrations and exhibit a limited capacity for failure recovery. In this
work, we present a Robotic Failure Analysis and Correction (RoboFAC) framework
to address this issue. Firstly, we construct RoboFAC dataset comprising 9,440
erroneous manipulation trajectories and 78,623 QA pairs across 16 diverse tasks
and 53 scenes in both simulation and real-world environments. Leveraging our
dataset, we develop RoboFAC model, which is capable of Task Understanding,
Failure Analysis and Failure Correction. Experimental results demonstrate that
the RoboFAC model outperforms GPT-4o by 34.1% on our evaluation benchmark.
Furthermore, we integrate the RoboFAC model into a real-world VLA control
pipeline as an external supervision providing correction instructions, yielding
a 29.1% relative improvement on average on four real-world tasks. The results
show that our RoboFAC framework effectively handles robotic failures and
assists the VLA model in recovering from failures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mapless Collision-Free Flight via MPC using Dual KD-Trees in Cluttered
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.10141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.10141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linzuo Zhang, Yu Hu, Yang Deng, Feng Yu, Danping Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collision-free flight in cluttered environments is a critical capability for
autonomous quadrotors. Traditional methods often rely on detailed 3D map
construction, trajectory generation, and tracking. However, this cascade
pipeline can introduce accumulated errors and computational delays, limiting
flight agility and safety. In this paper, we propose a novel method for
enabling collision-free flight in cluttered environments without explicitly
constructing 3D maps or generating and tracking collision-free trajectories.
Instead, we leverage Model Predictive Control (MPC) to directly produce safe
actions from sparse waypoints and point clouds from a depth camera. These
sparse waypoints are dynamically adjusted online based on nearby obstacles
detected from point clouds. To achieve this, we introduce a dual KD-Tree
mechanism: the Obstacle KD-Tree quickly identifies the nearest obstacle for
avoidance, while the Edge KD-Tree provides a robust initial guess for the MPC
solver, preventing it from getting stuck in local minima during obstacle
avoidance. We validate our approach through extensive simulations and
real-world experiments. The results show that our approach significantly
outperforms the mapping-based methods and is also superior to imitation
learning-based methods, demonstrating reliable obstacle avoidance at up to 12
m/s in simulations and 6 m/s in real-world tests. Our method provides a simple
and robust alternative to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Impact-Rich Rotational Maneuvers via Centroidal Velocity
  Rewards and Sim-to-Real Techniques: A One-Leg Hopper Flip Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyun Kang, Gijeong Kim, JongHun Choe, Hajun Kim, Hae-Won Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic rotational maneuvers, such as front flips, inherently involve large
angular momentum generation and intense impact forces, presenting major
challenges for reinforcement learning and sim-to-real transfer. In this work,
we propose a general framework for learning and deploying impact-rich,
rotation-intensive behaviors through centroidal velocity-based rewards and
actuator-aware sim-to-real techniques. We identify that conventional link-level
reward formulations fail to induce true whole-body rotation and introduce a
centroidal angular velocity reward that accurately captures system-wide
rotational dynamics. To bridge the sim-to-real gap under extreme conditions, we
model motor operating regions (MOR) and apply transmission load regularization
to ensure realistic torque commands and mechanical robustness. Using the
one-leg hopper front flip as a representative case study, we demonstrate the
first successful hardware realization of a full front flip. Our results
highlight that incorporating centroidal dynamics and actuator constraints is
critical for reliably executing highly dynamic motions. A supplementary video
is available at: https://youtu.be/atMAVI4s1RY
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Group and Grasp Multiple Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takahiro Yonemaru, Weiwei Wan, Tatsuki Nishimura, Kensuke Harada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneously grasping and delivering multiple objects can significantly
enhance robotic work efficiency and has been a key research focus for decades.
The primary challenge lies in determining how to push objects, group them, and
execute simultaneous grasping for respective groups while considering object
distribution and the hardware constraints of the robot. Traditional rule-based
methods struggle to flexibly adapt to diverse scenarios. To address this
challenge, this paper proposes an imitation learning-based approach. We collect
a series of expert demonstrations through teleoperation and train a diffusion
policy network, enabling the robot to dynamically generate action sequences for
pushing, grouping, and grasping, thereby facilitating efficient multi-object
grasping and delivery. We conducted experiments to evaluate the method under
different training dataset sizes, varying object quantities, and real-world
object scenarios. The results demonstrate that the proposed approach can
effectively and adaptively generate multi-object grouping and grasping
strategies. With the support of more training data, imitation learning is
expected to be an effective approach for solving the multi-object grasping
problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reachability Barrier Networks: Learning Hamilton-Jacobi Solutions for
  Smooth and Flexible Control Barrier Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Kim, William Sharpless, Hyun Joe Jeong, Sander Tonkens, Somil Bansal, Sylvia Herbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in autonomous driving and robotics underscore the
necessity of safety-critical controllers. Control barrier functions (CBFs) are
a popular method for appending safety guarantees to a general control
framework, but they are notoriously difficult to generate beyond low
dimensions. Existing methods often yield non-differentiable or inaccurate
approximations that lack integrity, and thus fail to ensure safety. In this
work, we use physics-informed neural networks (PINNs) to generate smooth
approximations of CBFs by computing Hamilton-Jacobi (HJ) optimal control
solutions. These reachability barrier networks (RBNs) avoid traditional
dimensionality constraints and support the tuning of their conservativeness
post-training through a parameterized discount term. To ensure robustness of
the discounted solutions, we leverage conformal prediction methods to derive
probabilistic safety guarantees for RBNs. We demonstrate that RBNs are highly
accurate in low dimensions, and safer than the standard neural CBF approach in
high dimensions. Namely, we showcase the RBNs in a 9D multi-vehicle collision
avoidance problem where it empirically proves to be 5.5x safer and 1.9x less
conservative than the neural CBFs, offering a promising method to synthesize
CBFs for general nonlinear autonomous systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoCoDA: Counterfactual Data Augmentation for Data-Efficient Robot
  Learning from Demonstrations <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezra Ameperosa, Jeremy A. Collins, Mrinal Jain, Animesh Garg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning in robotics faces significant challenges in generalization
due to the complexity of robotic environments and the high cost of data
collection. We introduce RoCoDA, a novel method that unifies the concepts of
invariance, equivariance, and causality within a single framework to enhance
data augmentation for imitation learning. RoCoDA leverages causal invariance by
modifying task-irrelevant subsets of the environment state without affecting
the policy's output. Simultaneously, we exploit SE(3) equivariance by applying
rigid body transformations to object poses and adjusting corresponding actions
to generate synthetic demonstrations. We validate RoCoDA through extensive
experiments on five robotic manipulation tasks, demonstrating improvements in
policy performance, generalization, and sample efficiency compared to
state-of-the-art data augmentation methods. Our policies exhibit robust
generalization to unseen object poses, textures, and the presence of
distractors. Furthermore, we observe emergent behavior such as re-grasping,
indicating policies trained with RoCoDA possess a deeper understanding of task
dynamics. By leveraging invariance, equivariance, and causality, RoCoDA
provides a principled approach to data augmentation in imitation learning,
bridging the gap between geometric symmetries and causal reasoning. Project
Page: https://rocoda.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2025 IEEE International Conference on Robotics and
  Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolei Xu, Yuchen Yan, Yongliang Shen, Wenqi Zhang, Guiyang Hou, Shengpei Jiang, Kaitao Song, Weiming Lu, Jun Xiao, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable progress on
mathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing
mathematical CoT datasets often suffer from Thought Leaps due to experts
omitting intermediate steps, which negatively impacts model learning and
generalization. We propose the CoT Thought Leap Bridge Task, which aims to
automatically detect leaps and generate missing intermediate reasoning steps to
restore the completeness and coherence of CoT. To facilitate this, we
constructed a specialized training dataset called ScaleQM+, based on the
structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought
leaps. Through comprehensive experiments on mathematical reasoning benchmarks,
we demonstrate that models fine-tuned on bridged datasets consistently
outperform those trained on original datasets, with improvements of up to
+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)
and provides better starting points for reinforcement learning (+3.1%),
functioning as a plug-and-play module compatible with existing optimization
techniques. Furthermore, CoT-Bridge demonstrate improved generalization to
out-of-domain logical reasoning tasks, confirming that enhancing reasoning
completeness yields broadly applicable benefits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two Experts Are All You Need for Steering Thinking: Reinforcing
  Cognitive Effort in MoE Reasoning Models Without Additional Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengru Wang, Xingyu Chen, Yue Wang, Zhiwei He, Jiahao Xu, Tian Liang, Qiuzhi Liu, Yunzhi Yao, Wenxuan Wang, Ruotian Ma, Haitao Mi, Ningyu Zhang, Zhaopeng Tu, Xiaolong Li, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)
have achieved impressive reasoning capabilities by selectively activating
experts to facilitate structured cognitive processes. Despite notable advances,
existing reasoning models often suffer from cognitive inefficiencies like
overthinking and underthinking. To address these limitations, we introduce a
novel inference-time steering methodology called Reinforcing Cognitive Experts
(RICE), designed to improve reasoning performance without additional training
or complex heuristics. Leveraging normalized Pointwise Mutual Information
(nPMI), we systematically identify specialized experts, termed ''cognitive
experts'' that orchestrate meta-level reasoning operations characterized by
tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs
(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning
benchmarks demonstrate noticeable and consistent improvements in reasoning
accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our
lightweight approach substantially outperforms prevalent reasoning-steering
techniques, such as prompt design and decoding constraints, while preserving
the model's general instruction-following skills. These results highlight
reinforcing cognitive experts as a promising, practical, and interpretable
direction to enhance cognitive efficiency within advanced reasoning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunhao Dai, Wenjie Wang, Liang Pang, Jun Xu, See-Kiong Ng, Ji-Rong Wen, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI search is reshaping information retrieval by offering
end-to-end answers to complex queries, reducing users' reliance on manually
browsing and summarizing multiple web pages. However, while this paradigm
enhances convenience, it disrupts the feedback-driven improvement loop that has
historically powered the evolution of traditional Web search. Web search can
continuously improve their ranking models by collecting large-scale,
fine-grained user feedback (e.g., clicks, dwell time) at the document level. In
contrast, generative AI search operates through a much longer search pipeline,
spanning query decomposition, document retrieval, and answer generation, yet
typically receives only coarse-grained feedback on the final answer. This
introduces a feedback loop disconnect, where user feedback for the final output
cannot be effectively mapped back to specific system components, making it
difficult to improve each intermediate stage and sustain the feedback loop. In
this paper, we envision NExT-Search, a next-generation paradigm designed to
reintroduce fine-grained, process-level feedback into generative AI search.
NExT-Search integrates two complementary modes: User Debug Mode, which allows
engaged users to intervene at key stages; and Shadow User Mode, where a
personalized user agent simulates user preferences and provides AI-assisted
feedback for less interactive users. Furthermore, we envision how these
feedback signals can be leveraged through online adaptation, which refines
current search outputs in real-time, and offline update, which aggregates
interaction logs to periodically fine-tune query decomposition, retrieval, and
generation models. By restoring human control over key stages of the generative
AI search pipeline, we believe NExT-Search offers a promising direction for
building feedback-rich AI search systems that can evolve continuously alongside
human feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2025 Perspective Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training-Free Watermarking for Autoregressive Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Tong, Zihao Pan, Shuai Yang, Kaiyang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Invisible image watermarking can protect image ownership and prevent
malicious misuse of visual generative models. However, existing generative
watermarking methods are mainly designed for diffusion models while
watermarking for autoregressive image generation models remains largely
underexplored. We propose IndexMark, a training-free watermarking framework for
autoregressive image generation models. IndexMark is inspired by the redundancy
property of the codebook: replacing autoregressively generated indices with
similar indices produces negligible visual differences. The core component in
IndexMark is a simple yet effective match-then-replace method, which carefully
selects watermark tokens from the codebook based on token similarity, and
promotes the use of watermark tokens through token replacement, thereby
embedding the watermark without affecting the image quality. Watermark
verification is achieved by calculating the proportion of watermark tokens in
generated images, with precision further improved by an Index Encoder.
Furthermore, we introduce an auxiliary validation scheme to enhance robustness
against cropping attacks. Experiments demonstrate that IndexMark achieves
state-of-the-art performance in terms of image quality and verification
accuracy, and exhibits robustness against various perturbations, including
cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG
compression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory
  Perceptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bufang Yang, Lilin Xu, Liekang Zeng, Kaiwei Liu, Siyang Jiang, Wenrui Lu, Hongkai Chen, Xiaofan Jiang, Guoliang Xing, Zhenyu Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have propelled intelligent
agents from reactive responses to proactive support. While promising, existing
proactive agents either rely exclusively on observations from enclosed
environments (e.g., desktop UIs) with direct LLM inference or employ rule-based
proactive notifications, leading to suboptimal user intent understanding and
limited functionality for proactive service. In this paper, we introduce
ContextAgent, the first context-aware proactive agent that incorporates
extensive sensory contexts to enhance the proactive capabilities of LLM agents.
ContextAgent first extracts multi-dimensional contexts from massive sensory
perceptions on wearables (e.g., video and audio) to understand user intentions.
ContextAgent then leverages the sensory contexts and the persona contexts from
historical data to predict the necessity for proactive services. When proactive
assistance is needed, ContextAgent further automatically calls the necessary
tools to assist users unobtrusively. To evaluate this new task, we curate
ContextAgentBench, the first benchmark for evaluating context-aware proactive
LLM agents, covering 1,000 samples across nine daily scenarios and twenty
tools. Experiments on ContextAgentBench show that ContextAgent outperforms
baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive
predictions and tool calling, respectively. We hope our research can inspire
the development of more advanced, human-centric, proactive AI assistants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonje Jeung, Sangyeon Yoon, Minsuk Kahng, Albert No
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Reasoning Models (LRMs) have become powerful tools for complex problem
solving, but their structured reasoning pathways can lead to unsafe outputs
when exposed to harmful prompts. Existing safety alignment methods reduce
harmful outputs but can degrade reasoning depth, leading to significant
trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated
jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight
alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at
the start of their reasoning, in response to harmful prompts, while leaving the
rest of the reasoning process unsupervised. Empirical results across multiple
benchmarks indicate that SAFEPATH effectively reduces harmful outputs while
maintaining reasoning performance. Specifically, SAFEPATH reduces harmful
responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the
DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than
Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot
variant that requires no fine-tuning. In addition, we provide a comprehensive
analysis of how existing methods in LLMs generalize, or fail, when applied to
reasoning-centric models, revealing critical gaps and new directions for safer
AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of
  Cross-Modal Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilin Ye, Junchao Huang, Xingchen Zeng, Jiazhi Xia, Wei Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal embeddings form the foundation for multi-modal models. However,
visualization methods for interpreting cross-modal embeddings have been
primarily confined to traditional dimensionality reduction (DR) techniques like
PCA and t-SNE. These DR methods primarily focus on feature distributions within
a single modality, whilst failing to incorporate metrics (e.g., CLIPScore)
across multiple modalities.This paper introduces AKRMap, a new DR technique
designed to visualize cross-modal embeddings metric with enhanced accuracy by
learning kernel regression of the metric landscape in the projection space.
Specifically, AKRMap constructs a supervised projection network guided by a
post-projection kernel regression loss, and employs adaptive generalized
kernels that can be jointly optimized with the projection. This approach
enables AKRMap to efficiently generate visualizations that capture complex
metric distributions, while also supporting interactive features such as zoom
and overlay for deeper exploration. Quantitative experiments demonstrate that
AKRMap outperforms existing DR methods in generating more accurate and
trustworthy visualizations. We further showcase the effectiveness of AKRMap in
visualizing and comparing cross-modal embeddings for text-to-image models. Code
and demo are available at https://github.com/yilinye/AKRMap.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Abacus: A Cost-Based Optimizer for Semantic Operator Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Russo, Sivaprasad Sudhir, Gerardo Vitagliano, Chunwei Liu, Tim Kraska, Samuel Madden, Michael Cafarella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs enable an exciting new class of data processing applications over large
collections of unstructured documents. Several new programming frameworks have
enabled developers to build these applications by composing them out of
semantic operators: a declarative set of AI-powered data transformations with
natural language specifications. These include LLM-powered maps, filters,
joins, etc. used for document processing tasks such as information extraction,
summarization, and more. While systems of semantic operators have achieved
strong performance on benchmarks, they can be difficult to optimize. An
optimizer for this setting must determine how to physically implement each
semantic operator in a way that optimizes the system globally. Existing
optimizers are limited in the number of optimizations they can apply, and most
(if not all) cannot optimize system quality, cost, or latency subject to
constraint(s) on the other dimensions. In this paper we present Abacus, an
extensible, cost-based optimizer which searches for the best implementation of
a semantic operator system given a (possibly constrained) optimization
objective. Abacus estimates operator performance by leveraging a minimal set of
validation examples and, if available, prior beliefs about operator
performance. We evaluate Abacus on document processing workloads in the
biomedical and legal domains (BioDEX; CUAD) and multi-modal question answering
(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%
better quality and up to 23.6x lower cost and 4.2x lower latency than the next
best system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmoGist: Efficient In-Context Learning for Visual Emotion Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronald Seoh, Dan Goldwasser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce EmoGist, a training-free, in-context learning
method for performing visual emotion classification with LVLMs. The key
intuition of our approach is that context-dependent definition of emotion
labels could allow more accurate predictions of emotions, as the ways in which
emotions manifest within images are highly context dependent and nuanced.
EmoGist pre-generates multiple explanations of emotion labels, by analyzing the
clusters of example images belonging to each category. At test time, we
retrieve a version of explanation based on embedding similarity, and feed it to
a fast VLM for classification. Through our experiments, we show that EmoGist
allows up to 13 points improvement in micro F1 scores with the multi-label
Memotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navneet Kaur, Lav Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As healthcare systems increasingly adopt advanced wireless networks and
connected devices, securing medical applications has become critical. The
integration of Internet of Medical Things devices, such as robotic surgical
tools, intensive care systems, and wearable monitors has enhanced patient care
but introduced serious security risks. Cyberattacks on these devices can lead
to life threatening consequences, including surgical errors, equipment failure,
and data breaches. While the ITU IMT 2030 vision highlights 6G's transformative
role in healthcare through AI and cloud integration, it also raises new
security concerns. This paper explores how explainable AI techniques like SHAP,
LIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve
trust and transparency in 6G enabled healthcare. We support our approach with
experimental analysis and highlight promising results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Zhang, Fei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While LLMs excel at open-ended reasoning, they often struggle with
cost-sensitive planning, either treating all actions as having equal cost or
failing to stay within strict budgets. In this paper, we introduce
Cost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings
explicit cost-awareness into LLM-guided planning. Tight cost constraints push
the planner to quickly identify infeasible solutions, while looser constraints
encourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,
Claude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their
performance in cost-sensitive scenarios. Our experiments suggest that raw LLMs
such as GPT-4.1 often falter under tight budgets, whereas CATS consistently
delivers strong performance, achieving higher task success rates and better
cost efficiency. CATS provides an effective solution for budget-aware
decision-making by combining the reasoning power of LLMs with structured
search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Words: Multimodal LLM Knows When to Speak 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikai Liao, Yi Ouyang, Yi-Lun Lee, Chen-Ping Yu, Yi-Hsuan Tsai, Zhaozheng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language model (LLM)-based chatbots have demonstrated strong
capabilities in generating coherent and contextually relevant responses, they
often struggle with understanding when to speak, particularly in delivering
brief, timely reactions during ongoing conversations. This limitation arises
largely from their reliance on text input, lacking the rich contextual cues in
real-world human dialogue. In this work, we focus on real-time prediction of
response types, with an emphasis on short, reactive utterances that depend on
subtle, multimodal signals across vision, audio, and text. To support this, we
introduce a new multimodal dataset constructed from real-world conversational
videos, containing temporally aligned visual, auditory, and textual streams.
This dataset enables fine-grained modeling of response timing in dyadic
interactions. Building on this dataset, we propose MM-When2Speak, a multimodal
LLM-based model that adaptively integrates visual, auditory, and textual
context to predict when a response should occur, and what type of response is
appropriate. Experiments show that MM-When2Speak significantly outperforms
state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x
improvement in response timing accuracy over leading commercial LLMs. These
results underscore the importance of multimodal inputs for producing timely,
natural, and engaging conversational AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/lzk901372/MM-When2Speak</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided
  Design Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna C. Doris, Md Ferdous Alam, Amin Heyrani Nobari, Faez Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient creation of accurate and editable 3D CAD models is critical in
engineering design, significantly impacting cost and time-to-market in product
innovation. Current manual workflows remain highly time-consuming and demand
extensive user expertise. While recent developments in AI-driven CAD generation
show promise, existing models are limited by incomplete representations of CAD
operations, inability to generalize to real-world images, and low output
accuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model
(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)
directly from visual input. Leveraging a novel dataset that we
created--GenCAD-Code, consisting of over 163k CAD-model image and code
pairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and
Qwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in
3D solid similarity. Notably, our VLM demonstrates some signs of
generalizability, successfully generating CAD code from real-world images and
executing CAD operations unseen during fine-tuning. The performance and
adaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code
to streamline CAD workflows for engineers and designers. CAD-Coder is publicly
available at: https://github.com/anniedoris/CAD-Coder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values
  Prioritization with AIRiskDilemmas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ying Chiu, Zhilin Wang, Sharan Maiya, Yejin Choi, Kyle Fish, Sydney Levine, Evan Hubinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting AI risks becomes more challenging as stronger models emerge and
find novel methods such as Alignment Faking to circumvent these detection
attempts. Inspired by how risky behaviors in humans (i.e., illegal activities
that may hurt others) are sometimes guided by strongly-held values, we believe
that identifying values within AI models can be an early warning system for
AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal
AI models' priorities on a range of AI value classes. Then, we collect
AIRiskDilemmas, a diverse collection of dilemmas that pit values against one
another in scenarios relevant to AI safety risks such as Power Seeking. By
measuring an AI model's value prioritization using its aggregate choices, we
obtain a self-consistent set of predicted value priorities that uncover
potential risks. We show that values in LitmusValues (including seemingly
innocuous ones like Care) can predict for both seen risky behaviors in
AIRiskDilemmas and unseen risky behaviors in HarmBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 11 figures, see associated data at
  https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at
  https://github.com/kellycyy/LitmusValues</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large
  Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fnu Mohbat, Mohammed J Zaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) and the abundance of food
data have resulted in studies to improve food understanding using LLMs. Despite
several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there
has been limited research on integrating food related KGs with LLMs. We
introduce KERL, a unified system that leverages food KGs and LLMs to provide
personalized food recommendations and generates recipes with associated
micro-nutritional information. Given a natural language question, KERL extracts
entities, retrieves subgraphs from the KG, which are then fed into the LLM as
context to select the recipes that satisfy the constraints. Next, our system
generates the cooking steps and nutritional information for each recipe. To
evaluate our approach, we also develop a benchmark dataset by curating recipe
related questions, combined with constraints and personal preferences. Through
extensive experiments, we show that our proposed KG-augmented LLM significantly
outperforms existing approaches, offering a complete and coherent solution for
food recommendation, recipe generation, and nutritional analysis. Our code and
benchmark datasets are publicly available at
https://github.com/mohbattharani/KERL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debating for Better Reasoning: An Unsupervised Multimodal Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashutosh Adhikari, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) gain expertise across diverse domains and
modalities, scalable oversight becomes increasingly challenging, particularly
when their capabilities may surpass human evaluators. Debate has emerged as a
promising mechanism for enabling such oversight. In this work, we extend the
debate paradigm to a multimodal setting, exploring its potential for weaker
models to supervise and enhance the performance of stronger models. We focus on
visual question answering (VQA), where two "sighted" expert vision-language
models debate an answer, while a "blind" (text-only) judge adjudicates based
solely on the quality of the arguments. In our framework, the experts defend
only answers aligned with their beliefs, thereby obviating the need for
explicit role-playing and concentrating the debate on instances of expert
disagreement. Experiments on several multimodal tasks demonstrate that the
debate framework consistently outperforms individual expert models. Moreover,
judgments from weaker LLMs can help instill reasoning capabilities in
vision-language models through finetuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TinyV: Reducing False Negatives in Verification Improves RL for LLM
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangchen Xu, Yuetai Li, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has become a powerful tool for enhancing the
reasoning abilities of large language models (LLMs) by optimizing their
policies with reward signals. Yet, RL's success relies on the reliability of
rewards, which are provided by verifiers. In this paper, we expose and analyze
a widespread problem--false negatives--where verifiers wrongly reject correct
model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals
that over 38% of model-generated responses suffer from false negatives, where
the verifier fails to recognize correct answers. We show, both empirically and
theoretically, that these false negatives severely impair RL training by
depriving the model of informative gradient signals and slowing convergence. To
mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments
existing rule-based methods, which dynamically identifies potential false
negatives and recovers valid responses to produce more accurate reward
estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts
pass rates by up to 10% and accelerates convergence relative to the baseline.
Our findings highlight the critical importance of addressing verifier false
negatives and offer a practical approach to improve RL-based fine-tuning of
LLMs. Our code is available at https://github.com/uw-nsl/TinyV.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle
  Generation from SAT Formulas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjiang Wei, Yuheng Wu, Yingjia Wan, Tarun Suresh, Huanmi Tan, Zhanke Zhou, Sanmi Koyejo, Ke Wang, Alex Aiken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SATBench, a benchmark for evaluating the logical reasoning
capabilities of large language models (LLMs) through logical puzzles derived
from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on
inference rule-based reasoning, which often involves deducing conclusions from
a set of premises, our approach leverages the search-based nature of SAT
problems, where the objective is to find a solution that fulfills a specified
set of logical constraints. Each instance in SATBench is generated from a SAT
formula, then translated into a story context and conditions using LLMs. The
generation process is fully automated and allows for adjustable difficulty by
varying the number of clauses. All 2100 puzzles are validated through both
LLM-assisted and solver-based consistency checks, with human validation on a
subset. Experimental results show that even the strongest model, o4-mini,
achieves only 65.0% accuracy on hard UNSAT problems, close to the random
baseline of 50%. SATBench exposes fundamental limitations in the search-based
logical reasoning abilities of current LLMs and provides a scalable testbed for
future research in logical reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models Optimized to Fool Detectors Still Have a Distinct Style
  (And How to Change It) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Rivera Soto, Barry Chen, Nicholas Andrews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite considerable progress in the development of machine-text detectors,
it has been suggested that the problem is inherently hard, and therefore, that
stakeholders should proceed under the assumption that machine-generated text
cannot be reliably detected as such. We examine a recent such claim by Nicks et
al. (2024) regarding the ease with which language models can be optimized to
degrade the performance of machine-text detectors, including detectors not
specifically optimized against. We identify a feature space$\unicode{x2013}$the
stylistic feature space$\unicode{x2013}$that is robust to such optimization,
and show that it may be used to reliably detect samples from language models
optimized to prevent detection. Furthermore, we show that even when models are
explicitly optimized against stylistic detectors, detection performance remains
surprisingly unaffected. We then seek to understand if stylistic detectors are
inherently more robust. To study this question, we explore a new paraphrasing
approach that simultaneously aims to close the gap between human writing and
machine writing in stylistic feature space while avoiding detection using
traditional features. We show that when only a single sample is available for
detection, this attack is universally effective across all detectors
considered, including those that use writing style. However, as the number of
samples available for detection grows, the human and machine distributions
become distinguishable. This observation encourages us to introduce AURA, a
metric that estimates the overlap between human and machine-generated
distributions by analyzing how detector performance improves as more samples
become available. Overall, our findings underscore previous recommendations to
avoid reliance on machine-text detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Let LLMs Break Free from Overthinking via Self-Braking Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Zhao, Yuchen Yan, Yongliang Shen, Haolei Xu, Wenqi Zhang, Kaitao Song, Jian Shao, Weiming Lu, Jun Xiao, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have
significantly enhanced their reasoning capabilities by generating longer chains
of thought, demonstrating outstanding performance across a variety of tasks.
However, this performance gain comes at the cost of a substantial increase in
redundant reasoning during the generation process, leading to high
computational overhead and exacerbating the issue of overthinking. Although
numerous existing approaches aim to address the problem of overthinking, they
often rely on external interventions. In this paper, we propose a novel
framework, Self-Braking Tuning (SBT), which tackles overthinking from the
perspective of allowing the model to regulate its own reasoning process, thus
eliminating the reliance on external control mechanisms. We construct a set of
overthinking identification metrics based on standard answers and design a
systematic method to detect redundant reasoning. This method accurately
identifies unnecessary steps within the reasoning trajectory and generates
training signals for learning self-regulation behaviors. Building on this
foundation, we develop a complete strategy for constructing data with adaptive
reasoning lengths and introduce an innovative braking prompt mechanism that
enables the model to naturally learn when to terminate reasoning at an
appropriate point. Experiments across mathematical benchmarks (AIME, AMC,
MATH500, GSM8K) demonstrate that our method reduces token consumption by up to
60% while maintaining comparable accuracy to unconstrained models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github:https://github.com/CCAI-Lab/Self-Braking-Tuning; Project:
  https://CCAI-Lab.github.io/SBT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Foundation Model for Communication Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Buffelli, Sowmen Das, Yu-Wei Lin, Sattar Vakili, Chien-Yi Wang, Masoud Attarifar, Pritthijit Nath, Da-shan Shiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) has demonstrated unprecedented performance
across various domains, and its application to communication systems is an
active area of research. While current methods focus on task-specific
solutions, the broader trend in AI is shifting toward large general models
capable of supporting multiple applications. In this work, we take a step
toward a foundation model for communication data--a transformer-based,
multi-modal model designed to operate directly on communication data. We
propose methodologies to address key challenges, including tokenization,
positional embedding, multimodality, variable feature sizes, and normalization.
Furthermore, we empirically demonstrate that such a model can successfully
estimate multiple features, including transmission rank, selected precoder,
Doppler spread, and delay profile.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Reliable Biomedical Hypothesis Generation: Evaluating
  Truthfulness and Hallucination in Large Language Models <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzhi Xiong, Eric Xie, Corey Williams, Myles Kim, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown significant potential in scientific
disciplines such as biomedicine, particularly in hypothesis generation, where
they can analyze vast literature, identify patterns, and suggest research
directions. However, a key challenge lies in evaluating the truthfulness of
generated hypotheses, as verifying their accuracy often requires substantial
time and resources. Additionally, the hallucination problem in LLMs can lead to
the generation of hypotheses that appear plausible but are ultimately
incorrect, undermining their reliability. To facilitate the systematic study of
these challenges, we introduce TruthHypo, a benchmark for assessing the
capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,
a knowledge-based hallucination detector to evaluate how well hypotheses are
grounded in existing knowledge. Our results show that LLMs struggle to generate
truthful hypotheses. By analyzing hallucinations in reasoning steps, we
demonstrate that the groundedness scores provided by KnowHD serve as an
effective metric for filtering truthful hypotheses from the diverse outputs of
LLMs. Human evaluations further validate the utility of KnowHD in identifying
truthful hypotheses and accelerating scientific discovery. Our data and source
code are available at https://github.com/Teddy-XiongGZ/TruthHypo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent Context Protocols Enhance Collective Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devansh Bhardwaj, Arjun Beniwal, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Karthik R. Narasimhan, Ameet Deshpande, Vishvak Murahari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI agents have become increasingly adept at complex tasks such as coding,
reasoning, and multimodal understanding. However, building generalist systems
requires moving beyond individual agents to collective inference -- a paradigm
where multi-agent systems with diverse, task-specialized agents complement one
another through structured communication and collaboration. Today, coordination
is usually handled with imprecise, ad-hoc natural language, which limits
complex interaction and hinders interoperability with domain-specific agents.
We introduce Agent context protocols (ACPs): a domain- and agent-agnostic
family of structured protocols for agent-agent communication, coordination, and
error handling. ACPs combine (i) persistent execution blueprints -- explicit
dependency graphs that store intermediate agent outputs -- with (ii)
standardized message schemas, enabling robust and fault-tolerant multi-agent
collective inference. ACP-powered generalist systems reach state-of-the-art
performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance
and best-in-class multimodal technical reports, outperforming commercial AI
systems in human evaluation. ACPs are highly modular and extensible, allowing
practitioners to build top-tier generalist agents quickly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KIPPO: Koopman-Inspired Proximal Policy Optimization <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Cozma, Landon Harris, Hairong Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has made significant strides in various domains,
and policy gradient methods like Proximal Policy Optimization (PPO) have gained
popularity due to their balance in performance, training stability, and
computational efficiency. These methods directly optimize policies through
gradient-based updates. However, developing effective control policies for
environments with complex and non-linear dynamics remains a challenge. High
variance in gradient estimates and non-convex optimization landscapes often
lead to unstable learning trajectories. Koopman Operator Theory has emerged as
a powerful framework for studying non-linear systems through an
infinite-dimensional linear operator that acts on a higher-dimensional space of
measurement functions. In contrast with their non-linear counterparts, linear
systems are simpler, more predictable, and easier to analyze. In this paper, we
present Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an
approximately linear latent-space representation of the underlying system's
dynamics while retaining essential features for effective policy learning. This
is achieved through a Koopman-approximation auxiliary network that can be added
to the baseline policy optimization algorithms without altering the
architecture of the core policy or value function. Extensive experimental
results demonstrate consistent improvements over the PPO baseline with 6-60%
increased performance while reducing variability by up to 91% when evaluated on
various continuous control tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for IJCAI 2025. This arXiv submission is the full version of
  the conference paper, including the appendix and supplementary material
  omitted from the IJCAI proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bellman operator convergence enhancements in reinforcement learning
  algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Krame Kadurha, Domini Jocema Leko Moutouo, Yae Ulrich Gaba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reviews the topological groundwork for the study of reinforcement
learning (RL) by focusing on the structure of state, action, and policy spaces.
We begin by recalling key mathematical concepts such as complete metric spaces,
which form the foundation for expressing RL problems. By leveraging the Banach
contraction principle, we illustrate how the Banach fixed-point theorem
explains the convergence of RL algorithms and how Bellman operators, expressed
as operators on Banach spaces, ensure this convergence. The work serves as a
bridge between theoretical mathematics and practical algorithm design, offering
new approaches to enhance the efficiency of RL. In particular, we investigate
alternative formulations of Bellman operators and demonstrate their impact on
improving convergence rates and performance in standard RL environments such as
MountainCar, CartPole, and Acrobot. Our findings highlight how a deeper
mathematical understanding of RL can lead to more effective algorithms for
decision-making problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSPS: <span class="highlight-title">Self-Supervised</span> Positive Sampling for Robust <span class="highlight-title">Self-Supervised</span>
  Speaker Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theo Lepage, Reda Dehak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-Supervised Learning (SSL) has led to considerable progress in Speaker
Verification (SV). The standard framework uses same-utterance positive sampling
and data-augmentation to generate anchor-positive pairs of the same speaker.
This is a major limitation, as this strategy primarily encodes channel
information from the recording condition, shared by the anchor and positive. We
propose a new positive sampling technique to address this bottleneck:
Self-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find
an appropriate positive, i.e., of the same speaker identity but a different
recording condition, in the latent space using clustering assignments and a
memory queue of positive embeddings. SSPS improves SV performance for both
SimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods
on VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by
lowering intra-speaker variance, providing comparable performance to DINO-SSPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at Interspeech 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Guided Learning of Meteorological Dynamics for Weather
  Downscaling and Forecasting <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingtao Luo, Shikai Fang, Binqing Wu, Qingsong Wen, Liang Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weather forecasting is essential but remains computationally intensive and
physically incomplete in traditional numerical weather prediction (NWP)
methods. Deep learning (DL) models offer efficiency and accuracy but often
ignore physical laws, limiting interpretability and generalization. We propose
PhyDL-NWP, a physics-guided deep learning framework that integrates physical
equations with latent force parameterization into data-driven models. It
predicts weather variables from arbitrary spatiotemporal coordinates, computes
physical terms via automatic differentiation, and uses a physics-informed loss
to align predictions with governing dynamics. PhyDL-NWP enables resolution-free
downscaling by modeling weather as a continuous function and fine-tunes
pre-trained models with minimal overhead, achieving up to 170x faster inference
with only 55K parameters. Experiments show that PhyDL-NWP improves both
forecasting performance and physical consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published/Accepted in KDD 2025 (February Cycle)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Shi, Jian Yang, Jiaheng Liu, Xingyuan Bu, Jiangjie Chen, Junting Zhou, Kaijing Ma, Zhoufutu Wen, Bingli Wang, Yancheng He, Liang Song, Hualei Zhu, Shilong Li, Xingjian Wang, Wei Zhang, Ruibin Yuan, Yifan Yao, Wenjun Yang, Yunli Wang, Siyuan Fang, Siyu Yuan, Qianyu He, Xiangru Tang, Yingshui Tan, Wangchunshu Zhou, Zhaoxiang Zhang, Zhoujun Li, Wenhao Huang, Ge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) underscore the need for
more comprehensive evaluation methods to accurately assess their reasoning
capabilities. Existing benchmarks are often domain-specific and thus cannot
fully capture an LLM's general reasoning potential. To address this limitation,
we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic
evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over
fifty games in either textual or visual formats and supports interactive,
multi-turn assessments with reinforcement learning scenarios. Using KORGym, we
conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent
reasoning patterns within model families and demonstrating the superior
performance of closed-source models. Further analysis examines the effects of
modality, reasoning strategies, reinforcement learning techniques, and response
length on model performance. We expect KORGym to become a valuable resource for
advancing LLM reasoning research and developing evaluation methodologies suited
to complex, interactive environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trustworthy Reputation Games and Applications to Proof-of-Reputation
  Blockchains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petros Drineas, Rohit Nema, Rafail Ostrovsky, Vassilis Zikas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reputation systems play an essential role in the Internet era, as they enable
people to decide whom to trust, by collecting and aggregating data about users'
behavior. Recently, several works proposed the use of reputation for the design
and scalability improvement of decentralized (blockchain) ledgers; however,
such systems are prone to manipulation and to our knowledge no game-theoretic
treatment exists that can support their economic robustness.
  In this work we put forth a new model for the design of what we call, {\em
trustworthy reputation systems}. Concretely, we describe a class of games,
which we term {\em trustworthy reputation games}, that enable a set of users to
report a function of their beliefs about the trustworthiness of each server in
a set -- i.e., their estimate of the probability that this server will behave
according to its specified strategy -- in a way that satisfies the following
properties:
  1. It is $(\epsilon$-)best response for any rational user in the game to play
a prescribed (truthful) strategy according to their true belief.
  2. Assuming that the users' beliefs are not too far from the {\em true}
trustworthiness of the servers, playing the above ($\epsilon-$)Nash equilibrium
allows anyone who observes the users' strategies to estimate the relative
trustworthiness of any two servers.
  Our utilities and decoding function build on a connection between the well
known PageRank algorithm and the problem of trustworthiness discovery, which
can be of independent interest. Finally, we show how the above games are
motivated by and can be leveraged in proof-of-reputation (PoR) blockchains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models Really Recognize Your Name? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dzung Pham, Peter Kairouz, Niloofar Mireshghallah, Eugene Bagdasarian, Chau Minh Pham, Amir Houmansadr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly being used to protect sensitive
user data. However, current LLM-based privacy solutions assume that these
models can reliably detect personally identifiable information (PII),
particularly named entities. In this paper, we challenge that assumption by
revealing systematic failures in LLM-based privacy tasks. Specifically, we show
that modern LLMs regularly overlook human names even in short text snippets due
to ambiguous contexts, which cause the names to be misinterpreted or
mishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous
human names, leveraging the name regularity bias phenomenon, embedded within
concise text snippets along with benign prompt injections. Our experiments on
modern LLMs tasked to detect PII as well as specialized tools show that recall
of ambiguous names drops by 20--40% compared to more recognizable names.
Furthermore, ambiguous human names are four times more likely to be ignored in
supposedly privacy-preserving summaries generated by LLMs when benign prompt
injections are present. These findings highlight the underexplored risks of
relying solely on LLMs to safeguard user privacy and underscore the need for a
more systematic investigation into their privacy failure modes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic
  Signal Optimization: A Simulation Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saahil Mahato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban traffic congestion, particularly at intersections, significantly
impacts travel time, fuel consumption, and emissions. Traditional fixed-time
signal control systems often lack the adaptability to manage dynamic traffic
patterns effectively. This study explores the application of multi-agent
reinforcement learning (MARL) to optimize traffic signal coordination across
multiple intersections within a simulated environment. Utilizing Pygame, a
simulation was developed to model a network of interconnected intersections
with randomly generated vehicle flows to reflect realistic traffic variability.
A decentralized MARL controller was implemented, in which each traffic signal
operates as an autonomous agent, making decisions based on local observations
and information from neighboring agents. Performance was evaluated against a
baseline fixed-time controller using metrics such as average vehicle wait time
and overall throughput. The MARL approach demonstrated statistically
significant improvements, including reduced average waiting times and improved
throughput. These findings suggest that MARL-based dynamic control strategies
hold substantial promise for improving urban traffic management efficiency.
More research is recommended to address scalability and real-world
implementation challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Logic of General Attention Using Edge-Conditioned Event Models
  (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaia Belardinelli, Thomas Bolander, Sebastian Watzl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present the first general logic of attention. Attention is a
powerful cognitive ability that allows agents to focus on potentially complex
information, such as logically structured propositions, higher-order beliefs,
or what other agents pay attention to. This ability is a strength, as it helps
to ignore what is irrelevant, but it can also introduce biases when some types
of information or agents are systematically ignored. Existing dynamic epistemic
logics for attention cannot model such complex attention scenarios, as they
only model attention to atomic formulas. Additionally, such logics quickly
become cumbersome, as their size grows exponentially in the number of agents
and announced literals. Here, we introduce a logic that overcomes both
limitations. First, we generalize edge-conditioned event models, which we show
to be as expressive as standard event models yet exponentially more succinct
(generalizing both standard event models and generalized arrow updates).
Second, we extend attention to arbitrary formulas, allowing agents to also
attend to other agents' beliefs or attention. Our work treats attention as a
modality, like belief or awareness. We introduce attention principles that
impose closure properties on that modality and that can be used in its
axiomatization. Throughout, we illustrate our framework with examples of AI
agents reasoning about human attentional biases, demonstrating how such agents
can discover attentional biases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy-Efficient Deep Reinforcement Learning with Spiking <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Irfan Uddin, Nishad Tasnim, Md Omor Faruk, Zejian Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agent-based Transformers have been widely adopted in recent reinforcement
learning advances due to their demonstrated ability to solve complex tasks.
However, the high computational complexity of Transformers often results in
significant energy consumption, limiting their deployment in real-world
autonomous systems. Spiking neural networks (SNNs), with their biologically
inspired structure, offer an energy-efficient alternative for machine learning.
In this paper, a novel Spike-Transformer Reinforcement Learning (STRL)
algorithm that combines the energy efficiency of SNNs with the powerful
decision-making capabilities of reinforcement learning is developed.
Specifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons
and attention mechanisms capable of processing spatio-temporal patterns over
multiple time steps is designed. The architecture is further enhanced with
state, action, and reward encodings to create a Transformer-like structure
optimized for reinforcement learning tasks. Comprehensive numerical experiments
conducted on state-of-the-art benchmarks demonstrate that the proposed SNN
Transformer achieves significantly improved policy performance compared to
conventional agent-based Transformers. With both enhanced energy efficiency and
policy optimality, this work highlights a promising direction for deploying
bio-inspired, low-cost machine learning models in complex real-world
decision-making scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based
  Autonomous Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo El-Hariry, Antoine Richard, Ricard M. Castan, Luis F. W. Batista, Matthieu Geist, Cedric Pradalier, Miguel Olivares-Mendez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous robots must navigate and operate in diverse environments, from
terrestrial and aquatic settings to aerial and space domains. While
Reinforcement Learning (RL) has shown promise in training policies for specific
autonomous robots, existing benchmarks are often constrained to unique
platforms, limiting generalization and fair comparisons across different
mobility systems. In this paper, we present NavBench, a multi-domain benchmark
for training and evaluating RL-based navigation policies across diverse robotic
platforms and operational environments. Built on IsaacLab, our framework
standardizes task definitions, enabling different robots to tackle various
navigation challenges without the need for ad-hoc task redesigns or custom
evaluation metrics. Our benchmark addresses three key challenges: (1) Unified
cross-medium benchmarking, enabling direct evaluation of diverse actuation
methods (thrusters, wheels, water-based propulsion) in realistic environments;
(2) Scalable and modular design, facilitating seamless robot-task
interchangeability and reproducible training pipelines; and (3) Robust
sim-to-real validation, demonstrated through successful policy transfer to
multiple real-world robots, including a satellite robotic simulator, an
unmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency
between simulation and real-world deployment, NavBench simplifies the
development of adaptable RL-based navigation strategies. Its modular design
allows researchers to easily integrate custom robots and tasks by following the
framework's predefined templates, making it accessible for a wide range of
applications. Our code is publicly available at NavBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for publication. Under review (2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guarded Query Routing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Šléher, William Brach, Tibor Sloboda, Kristián Košťál, Lukas Galke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query routing, the task to route user queries to different large language
model (LLM) endpoints, can be considered as a text classification problem.
However, out-of-distribution queries must be handled properly, as those could
be questions about unrelated domains, queries in other languages, or even
contain unsafe text. Here, we thus study a \emph{guarded} query routing
problem, for which we first introduce the Guarded Query Routing Benchmark
(GQR-Bench), which covers three exemplary target domains (law, finance, and
healthcare), and seven datasets to test robustness against out-of-distribution
queries. We then use GQR-Bench to contrast the effectiveness and efficiency of
LLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),
standard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo
Guardrails), continuous bag-of-words classifiers (WideMLP, fastText), and
traditional machine learning models (SVM, XGBoost). Our results show that
WideMLP, enhanced with out-of-domain detection capabilities, yields the best
trade-off between accuracy (88\%) and speed (<4ms). The embedding-based
fastText excels at speed (<1ms) with acceptable accuracy (80\%), whereas LLMs
yield the highest accuracy (91\%) but are comparatively slow (62ms for local
Llama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge
the automatic reliance on LLMs for (guarded) query routing and provide concrete
recommendations for practical applications. GQR-Bench will be released as a
Python package -- \texttt{gqr}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Graph Representations of Logical Forms for Language Modeling <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Sullivan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We make the case for language models over logical forms (LFLMs), arguing that
such models are more data-efficient than their textual counterparts. To that
end, we introduce the Graph-based Formal-Logical Distributional Semantics
(GFoLDS) prototype, a pretrained LM over graph representations of logical
forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong
experimental evidence that LFLMs can leverage the built-in, basic linguistic
knowledge inherent in such models to immediately begin learning more complex
patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,
transformer LMs pretrained on similar amounts of data, indicating that LFLMs
can learn with substantially less data than models over plain text.
Furthermore, we show that the performance of this model is likely to scale with
additional parameters and pretraining data, suggesting the viability of LFLMs
in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Flow <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Chen Wu, Feng-Ting Liao, Meng-Hsi Chen, Pei-Chen Ho, Farhang Nabiei, Da-shan Shiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers, the standard implementation for large language models (LLMs),
typically consist of tens to hundreds of discrete layers. While more layers can
lead to better performance, this approach has been challenged as far from
efficient, especially given the superiority of continuous layers demonstrated
by diffusion and flow-based models for image generation. We propose the Latent
Flow Transformer (LFT), which replaces a block of layers with a single learned
transport operator trained via flow matching, offering significant compression
while maintaining compatibility with the original architecture. Additionally,
we address the limitations of existing flow-based methods in \textit{preserving
coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M
model, LFT trained with flow matching compresses 6 of 24 layers and outperforms
directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),
demonstrating the feasibility of this design. When trained with FW, LFT further
distills 12 layers into one while reducing the KL to 0.736 surpassing that from
skipping 3 layers (0.932), significantly narrowing the gap between
autoregressive and flow-based generation paradigms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BACON: A fully explainable AI model with graded logic for decision
  making problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haishi Bai, Jozo Dujmovic, Jianwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As machine learning models and autonomous agents are increasingly deployed in
high-stakes, real-world domains such as healthcare, security, finance, and
robotics, the need for transparent and trustworthy explanations has become
critical. To ensure end-to-end transparency of AI decisions, we need models
that are not only accurate but also fully explainable and human-tunable. We
introduce BACON, a novel framework for automatically training explainable AI
models for decision making problems using graded logic. BACON achieves high
predictive accuracy while offering full structural transparency and precise,
logic-based symbolic explanations, enabling effective human-AI collaboration
and expert-guided refinement. We evaluate BACON with a diverse set of
scenarios: classic Boolean approximation, Iris flower classification, house
purchasing decisions and breast cancer diagnosis. In each case, BACON provides
high-performance models while producing compact, human-verifiable decision
logic. These results demonstrate BACON's potential as a practical and
principled approach for delivering crisp, trustworthy explainable AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ModRWKV: <span class="highlight-title">Transformer</span> Multimodality in Linear Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiale Kang, Ziyin Yue, Qingyu Yin, Jiang Rui, Weile Li, Zening Lu, Zhouran Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, most multimodal studies are based on large language models (LLMs)
with quadratic-complexity Transformer architectures. While linear models like
RNNs enjoy low inference costs, their application has been largely limited to
the text-only modality. This work explores the capabilities of modern RNN
architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal
framework built upon the RWKV7 architecture as its LLM backbone-which achieves
multi-source information fusion through dynamically adaptable heterogeneous
modality encoders. We designed the multimodal modules in ModRWKV with an
extremely lightweight architecture and, through extensive experiments,
identified a configuration that achieves an optimal balance between performance
and computational efficiency. ModRWKV leverages the pretrained weights of the
RWKV7 LLM for initialization, which significantly accelerates multimodal
training. Comparative experiments with different pretrained checkpoints further
demonstrate that such initialization plays a crucial role in enhancing the
model's ability to understand multimodal signals. Supported by extensive
experiments, we conclude that modern RNN architectures present a viable
alternative to Transformers in the domain of multimodal large language models
(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV
architecture through systematic exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated
  Rationales 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Cao, Jiyi Li, Ziwei Yang, Renjie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been growing interest in Multimodal Aspect-Based Sentiment Analysis
(MABSA) in recent years. Existing methods predominantly rely on pre-trained
small language models (SLMs) to collect information related to aspects and
sentiments from both image and text, with an aim to align these two modalities.
However, small SLMs possess limited capacity and knowledge, often resulting in
inaccurate identification of meaning, aspects, sentiments, and their
interconnections in textual and visual data. On the other hand, Large language
models (LLMs) have shown exceptional capabilities in various tasks by
effectively exploring fine-grained information in multimodal data. However,
some studies indicate that LLMs still fall short compared to fine-tuned small
models in the field of ABSA. Based on these findings, we propose a novel
framework, termed LRSA, which combines the decision-making capabilities of SLMs
with additional information provided by LLMs for MABSA. Specifically, we inject
explanations generated by LLMs as rationales into SLMs and employ a dual
cross-attention mechanism for enhancing feature interaction and fusion, thereby
augmenting the SLMs' ability to identify aspects and sentiments. We evaluated
our method using two baseline models, numerous experiments highlight the
superiority of our approach on three widely-used benchmarks, indicating its
generalizability and applicability to most pre-trained models for MABSA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning Models Better Express Their Confidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongkeun Yoon, Seungone Kim, Sohee Yang, Sunkyoung Kim, Soyeon Kim, Yongil Kim, Eunbi Choi, Yireun Kim, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their strengths, large language models (LLMs) often fail to
communicate their confidence accurately, making it difficult to assess when
they might be wrong and limiting their reliability. In this work, we
demonstrate that reasoning models-LLMs that engage in extended chain-of-thought
(CoT) reasoning-exhibit superior performance not only in problem-solving but
also in accurately expressing their confidence. Specifically, we benchmark six
reasoning models across six datasets and find that they achieve strictly better
confidence calibration than their non-reasoning counterparts in 33 out of the
36 settings. Our detailed analysis reveals that these gains in calibration stem
from the slow thinking behaviors of reasoning models-such as exploring
alternative approaches and backtracking-which enable them to adjust their
confidence dynamically throughout their CoT, making it progressively more
accurate. In particular, we find that reasoning models become increasingly
better calibrated as their CoT unfolds, a trend not observed in non-reasoning
models. Moreover, removing slow thinking behaviors from the CoT leads to a
significant drop in calibration. Lastly, we show that these gains are not
exclusive to reasoning models-non-reasoning models also benefit when guided to
perform slow thinking via in-context learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oren Sultan, Eitan Stern, Dafna Shahaf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) struggle with formal domains that require
rigorous logical deduction and symbolic reasoning, such as mathematical proof
generation. We propose a neuro-symbolic approach that combines LLMs' generative
strengths with structured components to overcome this challenge. As a
proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)
we retrieve analogous problems and use their proofs to guide the LLM, and (2) a
formal verifier evaluates the generated proofs and provides feedback, helping
the model fix incorrect proofs. We demonstrate that our method significantly
improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both
analogous problems and the verifier's feedback contribute to these gains. More
broadly, shifting to LLMs that generate provably correct conclusions could
dramatically improve their reliability, accuracy and consistency, unlocking
complex tasks and critical real-world applications that require
trustworthiness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attributional Safety Failures in Large Language Models under Code-Mixed
  Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somnath Banerjee, Pratyush Chatterjee, Shanu Kumar, Sayan Layek, Parag Agrawal, Rima Hazra, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in LLMs have raised significant safety concerns,
particularly when dealing with code-mixed inputs and outputs. Our study
systematically investigates the increased susceptibility of LLMs to produce
unsafe outputs from code-mixed prompts compared to monolingual English prompts.
Utilizing explainability methods, we dissect the internal attribution shifts
causing model's harmful behaviors. In addition, we explore cultural dimensions
by distinguishing between universally unsafe and culturally-specific unsafe
queries. This paper presents novel experimental insights, clarifying the
mechanisms driving this phenomenon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block
  Prediction and Controllable Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chihan Huang, Hao Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although autoregressive models have dominated language modeling in recent
years, there has been a growing interest in exploring alternative paradigms to
the conventional next-token prediction framework. Diffusion-based language
models have emerged as a compelling alternative due to their powerful parallel
generation capabilities and inherent editability. However, these models are
often constrained by fixed-length generation. A promising direction is to
combine the strengths of both paradigms, segmenting sequences into blocks,
modeling autoregressive dependencies across blocks while leveraging discrete
diffusion to estimate the conditional distribution within each block given the
preceding context. Nevertheless, their practical application is often hindered
by two key limitations: rigid fixed-length outputs and a lack of flexible
control mechanisms. In this work, we address the critical limitations of fixed
granularity and weak controllability in current large diffusion language
models. We propose CtrlDiff, a dynamic and controllable semi-autoregressive
framework that adaptively determines the size of each generation block based on
local semantics using reinforcement learning. Furthermore, we introduce a
classifier-guided control mechanism tailored to discrete diffusion, which
significantly reduces computational overhead while facilitating efficient
post-hoc conditioning without retraining. Extensive experiments demonstrate
that CtrlDiff sets a new standard among hybrid diffusion models, narrows the
performance gap to state-of-the-art autoregressive approaches, and enables
effective conditional text generation across diverse tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Managers Perceive AI-Assisted Conversational Training for Workplace
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lance T Wilhelm, Xiaohan Ding, Kirk McInnis Knutsen, Buse Carik, Eugenia H Rho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective workplace communication is essential for managerial success, yet
many managers lack access to tailored and sustained training. Although
AI-assisted communication systems may offer scalable training solutions, little
is known about how managers envision the role of AI in helping them improve
their communication skills. To investigate this, we designed a conversational
role-play system, CommCoach, as a functional probe to understand how managers
anticipate using AI to practice their communication skills. Through
semi-structured interviews, participants emphasized the value of adaptive,
low-risk simulations for practicing difficult workplace conversations. They
also highlighted opportunities, including human-AI teaming, transparent and
context-aware feedback, and greater control over AI-generated personas.
AI-assisted communication training should balance personalization, structured
learning objectives, and adaptability to different user styles and contexts.
However, achieving this requires carefully navigating tensions between adaptive
and consistent AI feedback, realism and potential bias, and the open-ended
nature of AI conversations versus structured workplace discourse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to CUI '25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data
  Imputation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Atik Ahamed, Qiang Ye, Qiang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Missing values in high-dimensional, mixed-type datasets pose significant
challenges for data imputation, particularly under Missing Not At Random (MNAR)
mechanisms. Existing methods struggle to integrate local and global data
characteristics, limiting performance in MNAR and high-dimensional settings. We
propose an innovative framework, RefiDiff, combining local machine learning
predictions with a novel Mamba-based denoising network capturing
interrelationships among distant features and samples. Our approach leverages
pre-refinement for initial warm-up imputations and post-refinement to polish
results, enhancing stability and accuracy. By encoding mixed-type data into
unified tokens, RefiDiff enables robust imputation without architectural or
hyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods
across missing-value settings, excelling in MNAR with a 4x faster training time
than SOTA DDPM-based approaches. Extensive evaluations on nine real-world
datasets demonstrate its robustness, scalability, and effectiveness in handling
complex missingness patterns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Creative Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Ismayilzada, Antonio Laverghetta Jr., Simone A. Luchini, Reet Patel, Antoine Bosselut, Lonneke van der Plas, Roger Beaty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) have demonstrated impressive performance
across natural language generation tasks, their ability to generate truly
creative content-characterized by novelty, diversity, surprise, and
quality-remains limited. Existing methods for enhancing LLM creativity often
focus narrowly on diversity or specific tasks, failing to address creativity's
multifaceted nature in a generalizable way. In this work, we propose Creative
Preference Optimization (CrPO), a novel alignment method that injects signals
from multiple creativity dimensions into the preference optimization objective
in a modular fashion. We train and evaluate creativity-augmented versions of
several models using CrPO and MuCE, a new large-scale human preference dataset
spanning over 200,000 human-generated responses and ratings from more than 30
psychological creativity assessments. Our models outperform strong baselines,
including GPT-4o, on both automated and human evaluations, producing more
novel, diverse, and surprising generations while maintaining high output
quality. Additional evaluations on NoveltyBench further confirm the
generalizability of our approach. Together, our results demonstrate that
directly optimizing for creativity within preference frameworks is a promising
direction for advancing the creative capabilities of LLMs without compromising
output quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric
  Knowledge Transfer in Large Language Models <span class="chip">ACL'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqiao Tan, Shizhu He, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) offer a transparent brain with accessible
parameters that encode extensive knowledge, which can be analyzed, located and
transferred. Consequently, a key research challenge is to transcend traditional
knowledge transfer paradigms rooted in symbolic language and achieve genuine
Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods
for transferring knowledge across LLMs of different scales through parameters
presents an intriguing and valuable research direction. In this paper, we first
demonstrate $\textbf{Alignment}$ in parametric space is the fundamental
prerequisite to achieve successful cross-scale PKT. We redefine the previously
explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes
extracted parameters for LoRA initialization and requires subsequent fine-tune
for alignment. Hence, to reduce cost for further fine-tuning, we introduce a
novel Pre-Align PKT (PrePKT) paradigm and propose a solution called
$\textbf{LaTen}$
($\textbf{L}$oc$\textbf{a}$te-$\textbf{T}$h$\textbf{e}$n-Alig$\textbf{n}$) that
aligns the parametric spaces of LLMs across scales only using several training
steps without following training. Comprehensive experiments on four benchmarks
demonstrate that both PostPKT and PrePKT face challenges in achieving
consistently stable transfer. Through in-depth analysis, we identify
$\textbf{Neural Incompatibility}$ as the ethological and parametric structural
differences between LLMs of varying scales, presenting fundamental challenges
to achieving effective PKT. These findings provide fresh insights into the
parametric architectures of LLMs and highlight promising directions for future
research on efficient PKT. Our code is available at
https://github.com/Trae1ounG/Neural_Incompatibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL'25 Main. Code link:
  https://github.com/Trae1ounG/Neural_Incompatibility</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Choosing a Model, Shaping a Future: Comparing LLM Perspectives on
  Sustainability and its Relationship with AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annika Bush, Meltem Aksoy, Markus Pauly, Greta Ontrup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As organizations increasingly rely on AI systems for decision support in
sustainability contexts, it becomes critical to understand the inherent biases
and perspectives embedded in Large Language Models (LLMs). This study
systematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek,
GPT, LLaMA, and Mistral - conceptualize sustainability and its relationship
with AI. We administered validated, psychometric sustainability-related
questionnaires - each 100 times per model -- to capture response patterns and
variability. Our findings revealed significant inter-model differences: For
example, GPT exhibited skepticism about the compatibility of AI and
sustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect
scores for several Sustainable Development Goals (SDGs). Models also diverged
in attributing institutional responsibility for AI and sustainability
integration, a results that holds implications for technology governance
approaches. Our results demonstrate that model selection could substantially
influence organizational sustainability strategies, highlighting the need for
awareness of model-specific biases when deploying LLMs for
sustainability-related decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Neural System Dynamics: Combining Deep Learning with
  System Dynamics Modeling to Support Critical Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo D'Elia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of this proposal is to bridge the gap between Deep Learning
(DL) and System Dynamics (SD) by developing an interpretable neural system
dynamics framework. While DL excels at learning complex models and making
accurate predictions, it lacks interpretability and causal reliability.
Traditional SD approaches, on the other hand, provide transparency and causal
insights but are limited in scalability and require extensive domain knowledge.
To overcome these limitations, this project introduces a Neural System Dynamics
pipeline, integrating Concept-Based Interpretability, Mechanistic
Interpretability, and Causal Machine Learning. This framework combines the
predictive power of DL with the interpretability of traditional SD models,
resulting in both causal reliability and scalability. The efficacy of the
proposed pipeline will be validated through real-world applications of the
EU-funded AutoMoTIF project, which is focused on autonomous multimodal
transportation systems. The long-term goal is to collect actionable insights
that support the integration of explainability and safety in autonomous
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be submitted to CEUR-WS.org for publication in the Doctoral
  Consortium Proceedings of XAI 2025, The World Conference on Explainable
  Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated
  Process Annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huimin Xu, Xin Mao, Feng-Lin Li, Xiaobao Wu, Wang Chen, Wei Zhang, Anh Tuan Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process Reward Models (PRMs) have demonstrated promising results in
mathematical reasoning, but existing process annotation approaches, whether
through human annotations or Monte Carlo simulations, remain computationally
expensive. In this paper, we introduce Step COmpression for Process Estimation
(SCOPE), a novel compression-based approach that significantly reduces
annotation costs. We first translate natural language reasoning steps into code
and normalize them through Abstract Syntax Tree, then merge equivalent steps to
construct a prefix tree. Unlike simulation-based methods that waste numerous
samples on estimation, SCOPE leverages a compression-based prefix tree where
each root-to-leaf path serves as a training sample, reducing the complexity
from $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K
samples with only 5% of the computational resources required by previous
methods. Empirical results demonstrate that PRMs trained on our dataset
consistently outperform existing automated annotation approaches on both
Best-of-N strategy and ProcessBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRL: <span class="highlight-title">Prompt</span>s from Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paweł Batorski, Adrian Kosmala, Paul Swoboda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective prompt engineering remains a central challenge in fully harnessing
the capabilities of LLMs. While well-designed prompts can dramatically enhance
performance, crafting them typically demands expert intuition and a nuanced
understanding of the task. Moreover, the most impactful prompts often hinge on
subtle semantic cues, ones that may elude human perception but are crucial for
guiding LLM behavior. In this paper, we introduce PRL (Prompts from
Reinforcement Learning), a novel RL-based approach for automatic prompt
generation. Unlike previous methods, PRL can produce novel few-shot examples
that were not seen during training. Our approach achieves state-of-the-art
performance across a range of benchmarks, including text classification,
simplification, and summarization. On the classification task, it surpasses
prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it
improves the average ROUGE scores on the summarization task by 4.32 over APE
and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over
APE and by 6.01 over EvoPrompt. Our code is available at
https://github.com/Batorskq/prl .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unearthing Gems from Stones: Policy Optimization with Negative Sample
  Augmentation for LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaohui Yang, Shilei Jiang, Chen Hu, Linjing Li, Shihong Deng, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in reasoning language models have witnessed a paradigm shift
from short to long CoT pattern. Given the substantial computational cost of
rollouts in long CoT models, maximizing the utility of fixed training datasets
becomes crucial. Our analysis reveals that negative responses contain valuable
components such as self-reflection and error-correction steps, yet primary
existing methods either completely discard negative samples (RFT) or apply
equal penalization across all tokens (RL), failing to leverage these potential
learning signals. In light of this, we propose Behavior Constrained Policy
Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline
RL framework that encompasses three stages: 1) sample segmentation, 2)
consensus-based step correctness assessment combining LLM and PRM judgers, and
3) policy optimization with NSA designed to effectively mine positive steps
within negative samples. Experimental results show that BCPG-NSA outperforms
baselines on several challenging math/coding reasoning benchmarks using the
same training dataset, achieving improved sample efficiency and demonstrating
robustness and scalability when extended to multiple iterations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable
  Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Baile Chen, Yi Zhang, Dan Roth, Samuel Madden, Jacob Andreas, Michael Cafarella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While humans naturally learn and adapt from past experiences, large language
models (LLMs) and their agentic counterparts struggle to retain reasoning from
previous tasks and apply them in future contexts. To address this limitation,
we propose a novel framework, log-augmented generation (LAG) that directly
reuses prior computation and reasoning from past logs at test time to enhance
model's ability to learn from previous tasks and perform better on new, unseen
challenges, all while keeping the system efficient and scalable. Specifically,
our system represents task logs using key-value (KV) caches, encoding the full
reasoning context of prior tasks while storing KV caches for only a selected
subset of tokens. When a new task arises, LAG retrieves the KV values from
relevant logs to augment generation. Our approach differs from reflection-based
memory mechanisms by directly reusing prior reasoning and computations without
requiring additional steps for knowledge extraction or distillation. Our method
also goes beyond existing KV caching techniques, which primarily target
efficiency gains rather than improving accuracy. Experiments on knowledge- and
reasoning-intensive datasets demonstrate that our method significantly
outperforms standard agentic systems that do not utilize logs, as well as
existing solutions based on reflection and KV cache techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Data and code are available at https://peterbaile.github.io/lag/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Cartographer: From Mapping to Reasoning Over Counterfactual
  Worlds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaël Gendron, Jože M. Rožanec, Michael Witbrock, Gillian Dobbie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal world models are systems that can answer counterfactual questions
about an environment of interest, i.e. predict how it would have evolved if an
arbitrary subset of events had been realized differently. It requires
understanding the underlying causes behind chains of events and conducting
causal inference for arbitrary unseen distributions. So far, this task eludes
foundation models, notably large language models (LLMs), which do not have
demonstrated causal reasoning capabilities beyond the memorization of existing
causal relationships. Furthermore, evaluating counterfactuals in real-world
applications is challenging since only the factual world is observed, limiting
evaluation to synthetic datasets. We address these problems by explicitly
extracting and modeling causal relationships and propose the Causal
Cartographer framework. First, we introduce a graph retrieval-augmented
generation agent tasked to retrieve causal relationships from data. This
approach allows us to construct a large network of real-world causal
relationships that can serve as a repository of causal knowledge and build
real-world counterfactuals. In addition, we create a counterfactual reasoning
agent constrained by causal relationships to perform reliable step-by-step
causal inference. We show that our approach can extract causal knowledge and
improve the robustness of LLMs for causal reasoning tasks while reducing
inference costs and spurious correlations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 9 pages for the main paper, 20 pages for the references and
  appendix, 25 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation
  Capabilities in Any Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyoung Song, Seogyeong Jeong, Eunsu Kim, Jiho Jin, Dongkwan Kim, Jay Shin, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating text generation capabilities of large language models (LLMs) is
challenging, particularly for low-resource languages where methods for direct
assessment are scarce. We propose MUG-Eval, a novel framework that evaluates
LLMs' multilingual generation capabilities by transforming existing benchmarks
into conversational tasks and measuring the LLMs' accuracies on those tasks. We
specifically designed these conversational tasks to require effective
communication in the target language. Then, we simply use task success rate as
a proxy of successful conversation generation. Our approach offers two key
advantages: it is independent of language-specific NLP tools or annotated
datasets, which are limited for most languages, and it does not rely on
LLMs-as-judges, whose evaluation quality degrades outside a few high-resource
languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and
low-resource categories, and we find that MUG-Eval correlates strongly with
established benchmarks ($r$ > 0.75) while enabling standardized comparisons
across languages and models. Our framework provides a robust and
resource-efficient solution for evaluating multilingual generation that can be
extended to thousands of languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Graph Based Repository-Level Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Athale, Vishal Vaddina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have transformed code
generation from natural language queries. However, despite their extensive
knowledge and ability to produce high-quality code, LLMs often struggle with
contextual accuracy, particularly in evolving codebases. Current code search
and retrieval methods frequently lack robustness in both the quality and
contextual relevance of retrieved results, leading to suboptimal code
generation. This paper introduces a novel knowledge graph-based approach to
improve code search and retrieval leading to better quality of code generation
in the context of repository-level tasks. The proposed approach represents code
repositories as graphs, capturing structural and relational information for
enhanced context-aware code generation. Our framework employs a hybrid approach
for code retrieval to improve contextual relevance, track inter-file modular
dependencies, generate more robust code and ensure consistency with the
existing codebase. We benchmark the proposed approach on the Evolutionary Code
Benchmark (EvoCodeBench) dataset, a repository-level code generation benchmark,
and demonstrate that our method significantly outperforms the baseline
approach. These findings suggest that knowledge graph based code generation
could advance robust, context-sensitive coding assistance tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond the First Error: Process Reward Models for Reflective
  Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaohui Yang, Chenghua He, Xiaowen Shi, Linjing Li, Qiyue Yin, Shihong Deng, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many studies focus on data annotation techniques for training effective PRMs.
However, current methods encounter a significant issue when applied to long CoT
reasoning processes: they tend to focus solely on the first incorrect step and
all preceding steps, assuming that all subsequent steps are incorrect. These
methods overlook the unique self-correction and reflection mechanisms inherent
in long CoT, where correct reasoning steps may still occur after initial
reasoning mistakes. To address this issue, we propose a novel data annotation
method for PRMs specifically designed to score the long CoT reasoning process.
Given that under the reflection pattern, correct and incorrect steps often
alternate, we introduce the concepts of Error Propagation and Error Cessation,
enhancing PRMs' ability to identify both effective self-correction behaviors
and reasoning based on erroneous steps. Leveraging an LLM-based judger for
annotation, we collect 1.7 million data samples to train a 7B PRM and evaluate
it at both solution and step levels. Experimental results demonstrate that
compared to existing open-source PRMs and PRMs trained on open-source datasets,
our PRM achieves superior performance across various metrics, including search
guidance, BoN, and F1 scores. Compared to widely used MC-based annotation
methods, our annotation approach not only achieves higher data efficiency but
also delivers superior performance. Detailed analysis is also conducted to
demonstrate the stability and generalizability of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCAN: Semantic Document Layout Analysis for Textual and Visual
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Dong, Nobuhiro Ueda, Krisztián Boros, Daiki Ito, Takuya Sera, Masafumi Oyamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing adoption of Large Language Models (LLMs) and
Vision-Language Models (VLMs), rich document analysis technologies for
applications like Retrieval-Augmented Generation (RAG) and visual RAG are
gaining significant attention. Recent research indicates that using VLMs can
achieve better RAG performance, but processing rich documents still remains a
challenge since a single page contains large amounts of information. In this
paper, we present SCAN (\textbf{S}emanti\textbf{C} Document Layout
\textbf{AN}alysis), a novel approach enhancing both textual and visual
Retrieval-Augmented Generation (RAG) systems working with visually rich
documents. It is a VLM-friendly approach that identifies document components
with appropriate semantic granularity, balancing context preservation with
processing efficiency. SCAN uses a coarse-grained semantic approach that
divides documents into coherent regions covering continuous components. We
trained the SCAN model by fine-tuning object detection models with
sophisticated annotation datasets. Our experimental results across English and
Japanese datasets demonstrate that applying SCAN improves end-to-end textual
RAG performance by up to 9.0\% and visual RAG performance by up to 6.4\%,
outperforming conventional approaches and even commercial document processing
solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Bias Backfires: The Modulatory Role of Counterfactual Explanations
  on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ulrike Kuhl, Annika Bush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although the integration of artificial intelligence (AI) into everyday tasks
improves efficiency and objectivity, it also risks transmitting bias to human
decision-making. In this study, we conducted a controlled experiment that
simulated hiring decisions to examine how biased AI recommendations - augmented
with or without counterfactual explanations - influence human judgment over
time. Participants, acting as hiring managers, completed 60 decision trials
divided into a baseline phase without AI, followed by a phase with biased (X)AI
recommendations (favoring either male or female candidates), and a final
post-interaction phase without AI. Our results indicate that the participants
followed the AI recommendations 70% of the time when the qualifications of the
given candidates were comparable. Yet, only a fraction of participants detected
the gender bias (8 out of 294). Crucially, exposure to biased AI altered
participants' inherent preferences: in the post-interaction phase,
participants' independent decisions aligned with the bias when no
counterfactual explanations were provided before, but reversed the bias when
explanations were given. Reported trust did not differ significantly across
conditions. Confidence varied throughout the study phases after exposure to
male-biased AI, indicating nuanced effects of AI bias on decision certainty.
Our findings point to the importance of calibrating XAI to avoid unintended
behavioral shifts in order to safeguard equitable decision-making and prevent
the adoption of algorithmic bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for XAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Embodied Cognition in Robots via Spatially Grounded Synthetic
  Worlds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Currie, Gioele Migno, Enrico Piacenti, Maria Elena Giannaccini, Patric Bach, Davide De Tommaso, Agnieszka Wykowska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a conceptual framework for training Vision-Language Models (VLMs)
to perform Visual Perspective Taking (VPT), a core capability for embodied
cognition essential for Human-Robot Interaction (HRI). As a first step toward
this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,
that enables supervised learning for spatial reasoning tasks. Each instance
includes an RGB image, a natural language description, and a ground-truth 4X4
transformation matrix representing object pose. We focus on inferring Z-axis
distance as a foundational skill, with future extensions targeting full 6
Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to
support further research. This work serves as a foundational step toward
embodied AI systems capable of spatial understanding in interactive human-robot
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to: Intelligent Autonomous Systems (IAS) 2025 as Late
  Breaking Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis
  for Ü-Tsang, Amdo and Kham Speech <span class="highlight-title">Dataset</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Liu, Ziyue Zhang, Ban Ma-bao, Yuqing Cai, Yongbin Yu, Renzeng Duojie, Xiangxiang Wang, Fan Gao, Cheng Huang, Nyima Tashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tibetan is a low-resource language with minimal parallel speech corpora
spanning its three major dialects-\"U-Tsang, Amdo, and Kham-limiting progress
in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,
multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel
dialectal speech from limited reference audio and explicit dialect labels. Our
method features a novel speaker-dialect fusion module and a Dialect-Specialized
Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and
linguistic variations across dialects while preserving speaker identity.
Extensive objective and subjective evaluations demonstrate that FMSD-TTS
significantly outperforms baselines in both dialectal expressiveness and
speaker similarity. We further validate the quality and utility of the
synthesized speech through a challenging speech-to-speech dialect conversion
task. Our contributions include: (1) a novel few-shot TTS system tailored for
Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale
synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source
evaluation toolkit for standardized assessment of speaker similarity, dialect
consistency, and audio quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Upgrading Democracies with Fairer Voting Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evangelos Pournaras, Srijoni Majumdar, Thomas Wellings, Joshua C. Yang, Fatemeh B. Heravan, Regula Hänggli Fricker, Dirk Helbing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voting methods are instrumental design element of democracies. Citizens use
them to express and aggregate their preferences to reach a collective decision.
However, voting outcomes can be as sensitive to voting rules as they are to
people's voting choices. Despite the significance and inter-disciplinary
scientific progress on voting methods, several democracies keep relying on
outdated voting methods that do not fit modern, pluralistic societies well,
while lacking social innovation. Here, we demonstrate how one can upgrade
real-world democracies, namely by using alternative preferential voting methods
such as cumulative voting and the method of equal shares designed for a
proportional representation of voters' preferences. By rigorously assessing a
new participatory budgeting approach applied in the city of Aarau, Switzerland,
we unravel the striking voting outcomes of fair voting methods: more winning
projects with the same budget and broader geographic and preference
representation of citizens by the elected projects, in particular for voters
who used to be under-represented, while promoting novel project ideas. We
provide profound causal evidence showing that citizens prefer proportional
voting methods, which possess strong legitimacy without the need of very
technical specialized explanations. We also reveal strong underlying democratic
values exhibited by citizens who support fair voting methods such as altruism
and compromise. These findings come with a global momentum to unleash a new and
long-awaited participation blueprint of how to upgrade democracies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Includes Supplementary Information</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Classification with Semi-Supervised Deep Learning Using
  Distance-Based Sample Weights <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aydin Abedinia, Shima Tabakhi, Vahid Seydi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in semi-supervised deep learning have introduced
effective strategies for leveraging both labeled and unlabeled data to improve
classification performance. This work proposes a semi-supervised framework that
utilizes a distance-based weighting mechanism to prioritize critical training
samples based on their proximity to test data. By focusing on the most
informative examples, the method enhances model generalization and robustness,
particularly in challenging scenarios with noisy or imbalanced datasets.
Building on techniques such as uncertainty consistency and graph-based
representations, the approach addresses key challenges of limited labeled data
while maintaining scalability. Experiments on twelve benchmark datasets
demonstrate significant improvements across key metrics, including accuracy,
precision, and recall, consistently outperforming existing methods. This
framework provides a robust and practical solution for semi-supervised
learning, with potential applications in domains such as healthcare and
security where data limitations pose significant challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures. This paper has been accepted for publication and
  oral presentation at the 2025 10th IEEE International Conference on Machine
  Learning Technologies (ICMLT 2025). The final authenticated version will be
  available in IEEE Xplore following the conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Replace in Translation: Boost Concept Alignment in Counterfactual
  Text-to-Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sifan Li, Ming Tao, Hao Zhao, Ling Shao, Hao Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Image (T2I) has been prevalent in recent years, with most common
condition tasks having been optimized nicely. Besides, counterfactual
Text-to-Image is obstructing us from a more versatile AIGC experience. For
those scenes that are impossible to happen in real world and anti-physics, we
should spare no efforts in increasing the factual feel, which means
synthesizing images that people think very likely to be happening, and concept
alignment, which means all the required objects should be in the same frame. In
this paper, we focus on concept alignment. As controllable T2I models have
achieved satisfactory performance for real applications, we utilize this
technology to replace the objects in a synthesized image in latent space
step-by-step to change the image from a common scene to a counterfactual scene
to meet the prompt. We propose a strategy to instruct this replacing process,
which is called as Explicit Logical Narrative Prompt (ELNP), by using the newly
SoTA language model DeepSeek to generate the instructions. Furthermore, to
evaluate models' performance in counterfactual T2I, we design a metric to
calculate how many required concepts in the prompt can be covered averagely in
the synthesized images. The extensive experiments and qualitative comparisons
demonstrate that our strategy can boost the concept alignment in counterfactual
T2I.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Handloom Design Generation Using Generative Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajat Kanti Bhattacharjee, Meghali Nandi, Amrit Jha, Gunajit Kalita, Ferdous Ahmed Barbhuiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes deep learning techniques of generating designs for
clothing, focused on handloom fabric and discusses the associated challenges
along with its application. The capability of generative neural network models
in understanding artistic designs and synthesizing those is not yet explored
well. In this work, multiple methods are employed incorporating the current
state of the art generative models and style transfer algorithms to study and
observe their performance for the task. The results are then evaluated through
user score. This work also provides a new dataset NeuralLoom for the task of
the design generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Jailbreak Attacks on LLMs through Intent Concealment and
  Diversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiehan Cui, Yanxu Mao, Peipei Liu, Congying Liu, Datao You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) have achieved remarkable advancements,
their security remains a pressing concern. One major threat is jailbreak
attacks, where adversarial prompts bypass model safeguards to generate harmful
or objectionable content. Researchers study jailbreak attacks to understand
security and robustness of LLMs. However, existing jailbreak attack methods
face two main challenges: (1) an excessive number of iterative queries, and (2)
poor generalization across models. In addition, recent jailbreak evaluation
datasets focus primarily on question-answering scenarios, lacking attention to
text generation tasks that require accurate regeneration of toxic content. To
tackle these challenges, we propose two contributions: (1) ICE, a novel
black-box jailbreak method that employs Intent Concealment and divErsion to
effectively circumvent security constraints. ICE achieves high attack success
rates (ASR) with a single query, significantly improving efficiency and
transferability across different models. (2) BiSceneEval, a comprehensive
dataset designed for assessing LLM robustness in question-answering and
text-generation tasks. Experimental results demonstrate that ICE outperforms
existing jailbreak techniques, revealing critical vulnerabilities in current
defense mechanisms. Our findings underscore the necessity of a hybrid security
strategy that integrates predefined security mechanisms with real-time semantic
decomposition to enhance the security of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional
  Evaluation in Tabular Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyungeun Lee, Moonjung Eo, Hye-Seung Cho, Dongmin Kim, Ye Seul Sim, Seoyoon Kim, Min-Kook Suh, Woohyung Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread use of tabular data in real-world applications, most
benchmarks rely on average-case metrics, which fail to reveal how model
behavior varies across diverse data regimes. To address this, we propose
MultiTab, a benchmark suite and evaluation framework for multi-dimensional,
data-aware analysis of tabular learning algorithms. Rather than comparing
models only in aggregate, MultiTab categorizes 196 publicly available datasets
along key data characteristics, including sample size, label imbalance, and
feature interaction, and evaluates 13 representative models spanning a range of
inductive biases. Our analysis shows that model performance is highly sensitive
to such regimes: for example, models using sample-level similarity excel on
datasets with large sample sizes or high inter-feature correlation, while
models encoding inter-feature dependencies perform best with weakly correlated
features. These findings reveal that inductive biases do not always behave as
intended, and that regime-aware evaluation is essential for understanding and
improving model behavior. MultiTab enables more principled model design and
offers practical guidance for selecting models tailored to specific data
characteristics. All datasets, code, and optimization logs are publicly
available at https://huggingface.co/datasets/LGAI-DILab/Multitab.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring
  Deceptive Behaviors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maheep Chaudhary, Fazl Barez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-risk industries like nuclear and aviation use real-time monitoring to
detect dangerous system conditions. Similarly, Large Language Models (LLMs)
need monitoring safeguards. We propose a real-time framework to predict harmful
AI outputs before they occur by using an unsupervised approach that treats
normal behavior as the baseline and harmful outputs as outliers. Our study
focuses specifically on backdoor-triggered responses -- where specific input
phrases activate hidden vulnerabilities causing the model to generate unsafe
content like violence, pornography, or hate speech. We address two key
challenges: (1) identifying true causal indicators rather than surface
correlations, and (2) preventing advanced models from deception -- deliberately
evading monitoring systems. Hence, we approach this problem from an
unsupervised lens by drawing parallels to human deception: just as humans
exhibit physical indicators while lying, we investigate whether LLMs display
distinct internal behavioral signatures when generating harmful content. Our
study addresses two critical challenges: 1) designing monitoring systems that
capture true causal indicators rather than superficial correlations; and
2)preventing intentional evasion by increasingly capable "Future models''. Our
findings show that models can produce harmful content through causal mechanisms
and can become deceptive by: (a) alternating between linear and non-linear
representations, and (b) modifying feature relationships. To counter this, we
developed Safety-Net -- a multi-detector framework that monitors different
representation dimensions, successfully detecting harmful behavior even when
information is shifted across representational spaces to evade individual
monitors. Our evaluation shows 96% accuracy in detecting harmful cases using
our unsupervised ensemble approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking data encoding methods in Quantum Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orlane Zang, Grégoire Barrué, Tony Quertier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data encoding plays a fundamental and distinctive role in Quantum Machine
Learning (QML). While classical approaches process data directly as vectors,
QML may require transforming classical data into quantum states through
encoding circuits, known as quantum feature maps or quantum embeddings. This
step leverages the inherently high-dimensional and non-linear nature of Hilbert
space, enabling more efficient data separation in complex feature spaces that
may be inaccessible to classical methods. This encoding part significantly
affects the performance of the QML model, so it is important to choose the
right encoding method for the dataset to be encoded. However, this choice is
generally arbitrary, since there is no "universal" rule for knowing which
encoding to choose based on a specific set of data. There are currently a
variety of encoding methods using different quantum logic gates. We studied the
most commonly used types of encoding methods and benchmarked them using
different datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EVA: Red-Teaming GUI Agents via Evolving Indirect <span class="highlight-title">Prompt</span> Injection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijie Lu, Tianjie Ju, Manman Zhao, Xinbei Ma, Yuan Guo, ZhuoSheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As multimodal agents are increasingly trained to operate graphical user
interfaces (GUIs) to complete user tasks, they face a growing threat from
indirect prompt injection, attacks in which misleading instructions are
embedded into the agent's visual environment, such as popups or chat messages,
and misinterpreted as part of the intended task. A typical example is
environmental injection, in which GUI elements are manipulated to influence
agent behavior without directly modifying the user prompt. To address these
emerging attacks, we propose EVA, a red teaming framework for indirect prompt
injection which transforms the attack into a closed loop optimization by
continuously monitoring an agent's attention distribution over the GUI and
updating adversarial cues, keywords, phrasing, and layout, in response.
Compared with prior one shot methods that generate fixed prompts without regard
for how the model allocates visual attention, EVA dynamically adapts to
emerging attention hotspots, yielding substantially higher attack success rates
and far greater transferability across diverse GUI scenarios. We evaluate EVA
on six widely used generalist and specialist GUI agents in realistic settings
such as popup manipulation, chat based phishing, payments, and email
composition. Experimental results show that EVA substantially improves success
rates over static baselines. Under goal agnostic constraints, where the
attacker does not know the agent's task intent, EVA still discovers effective
patterns. Notably, we find that injection styles transfer well across models,
revealing shared behavioral biases in GUI agents. These results suggest that
evolving indirect prompt injection is a powerful tool not only for red teaming
agents, but also for uncovering common vulnerabilities in their multimodal
decision making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AquaSignal: An Integrated Framework for Robust Underwater Acoustic
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eirini Panteli, Paulo E. Santos, Nabil Humphrey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents AquaSignal, a modular and scalable pipeline for
preprocessing, denoising, classification, and novelty detection of underwater
acoustic signals. Designed to operate effectively in noisy and dynamic marine
environments, AquaSignal integrates state-of-the-art deep learning
architectures to enhance the reliability and accuracy of acoustic signal
analysis. The system is evaluated on a combined dataset from the Deepship and
Ocean Networks Canada (ONC) benchmarks, providing a diverse set of real-world
underwater scenarios. AquaSignal employs a U-Net architecture for denoising, a
ResNet18 convolutional neural network for classifying known acoustic events,
and an AutoEncoder-based model for unsupervised detection of novel or anomalous
signals. To our knowledge, this is the first comprehensive study to apply and
evaluate this combination of techniques on maritime vessel acoustic data.
Experimental results show that AquaSignal improves signal clarity and task
performance, achieving 71% classification accuracy and 91% accuracy in novelty
detection. Despite slightly lower classification performance compared to some
state-of-the-art models, differences in data partitioning strategies limit
direct comparisons. Overall, AquaSignal demonstrates strong potential for
real-time underwater acoustic monitoring in scientific, environmental, and
maritime domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages; 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jennifer D'Souza, Hamed Babaei Giglou, Quentin Münch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) drive scientific question-answering on modern
search engines, yet their evaluation robustness remains underexplored. We
introduce YESciEval, an open-source framework that combines fine-grained
rubric-based assessment with reinforcement learning to mitigate optimism bias
in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including
adversarial variants, with evaluation scores from multiple LLMs. Independent of
proprietary models and human feedback, our approach enables scalable, cost-free
evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI
alignment and fosters robust, transparent evaluation essential for scientific
inquiry and artificial general intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, Accepted as a Long Paper at the 63rd Annual
  Meeting of the Association for Computational Linguistics (ACL 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary
  Rule-Based Machine Learning <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroki Shiraishi, Hisao Ishibuchi, Masaya Nakata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Function approximation is a critical task in various fields. However,
existing neural network approaches struggle with locally complex or
discontinuous functions due to their reliance on a single global model covering
the entire problem space. We propose X-KAN, a novel method that optimizes
multiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary
rule-based machine learning framework called XCSF. X-KAN combines KAN's high
expressiveness with XCSF's adaptive partitioning capability by implementing
local KAN models as rule consequents and defining local regions via rule
antecedents. Our experimental results on artificial test functions and
real-world datasets demonstrate that X-KAN significantly outperforms
conventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms
of approximation accuracy. Notably, X-KAN effectively handles functions with
locally complex or discontinuous structures that are challenging for
conventional KAN, using a compact set of rules (average 7.2 $\pm$ 2.3 rules).
These results validate the effectiveness of using KAN as a local model in XCSF,
which evaluates the rule fitness based on both accuracy and generality. Our
X-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 34th International Joint Conference on Artificial
  Intelligence (IJCAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Think-J: Learning to Think for Generative LLM-as-a-Judge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Huang, Yancheng He, Hongli Zhou, Rui Zhang, Wei Liu, Weixun Wang, Wenbo Su, Bo Zheng, Jiaheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-as-a-Judge refers to the automatic modeling of preferences for responses
generated by Large Language Models (LLMs), which is of significant importance
for both LLM evaluation and reward modeling. Although generative LLMs have made
substantial progress in various tasks, their performance as LLM-Judge still
falls short of expectations. In this work, we propose Think-J, which improves
generative LLM-as-a-Judge by learning how to think. We first utilized a small
amount of curated data to develop the model with initial judgment thinking
capabilities. Subsequently, we optimize the judgment thinking traces based on
reinforcement learning (RL). We propose two methods for judgment thinking
optimization, based on offline and online RL, respectively. The offline RL
requires training a critic model to construct positive and negative examples
for learning. The online method defines rule-based reward as feedback for
optimization. Experimental results showed that our approach can significantly
enhance the evaluation capability of generative LLM-Judge, surpassing both
generative and classifier-based LLM-Judge without requiring extra human
annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speculative Decoding Reimagined for Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luxi Lin, Zhihang Lin, Zhanpeng Zeng, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Multimodal Speculative Decoding (MSD) to accelerate
Multimodal Large Language Models (MLLMs) inference. Speculative decoding has
been shown to accelerate Large Language Models (LLMs) without sacrificing
accuracy. However, current speculative decoding methods for MLLMs fail to
achieve the same speedup as they do for LLMs. To address this, we reimagine
speculative decoding specifically for MLLMs. Our analysis of MLLM
characteristics reveals two key design principles for MSD: (1) Text and visual
tokens have fundamentally different characteristics and need to be processed
separately during drafting. (2) Both language modeling ability and visual
perception capability are crucial for the draft model. For the first principle,
MSD decouples text and visual tokens in the draft model, allowing each to be
handled based on its own characteristics. For the second principle, MSD uses a
two-stage training strategy: In stage one, the draft model is trained on
text-only instruction-tuning datasets to improve its language modeling ability.
In stage two, MSD gradually introduces multimodal data to enhance the visual
perception capability of the draft model. Experiments show that MSD boosts
inference speed by up to $2.29\times$ for LLaVA-1.5-7B and up to $2.46\times$
for LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.
Our code is available at https://github.com/Lyn-Lucy/MSD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FuxiMT: Sparsifying Large Language Models for Chinese-Centric
  Multilingual Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaolin Zhu, Tianyu Dong, Bo Li, Deyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present FuxiMT, a novel Chinese-centric multilingual
machine translation model powered by a sparsified large language model (LLM).
We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on
a massive Chinese corpus and then conduct multilingual fine-tuning on a large
parallel dataset encompassing 65 languages. FuxiMT incorporates
Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust
performance across various resource levels. Experimental results demonstrate
that FuxiMT significantly outperforms strong baselines, including
state-of-the-art LLMs and machine translation models, particularly under
low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot
translation capabilities for unseen language pairs, indicating its potential to
bridge communication gaps where parallel data are scarce or unavailable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence
  Encoders and Physics-Informed Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mouad Elaarabi, Domenico Borzacchiello, Philippe Le Bot, Nathan Lauzeral, Sebastien Comas-Cardona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we explore the integration of Sequence Encoding for Online
Parameter Identification with Physics-Informed Neural Networks to create a
model that, once trained, can be utilized for real time applications with
variable parameters, boundary conditions, and initial conditions. Recently, the
combination of PINNs with Sparse Regression has emerged as a method for
performing dynamical system identification through supervised learning and
sparse regression optimization, while also solving the dynamics using PINNs.
However, this approach can be limited by variations in parameters or boundary
and initial conditions, requiring retraining of the model whenever changes
occur. In this work, we introduce an architecture that employs Deep Sets or
Sequence Encoders to encode dynamic parameters, boundary conditions, and
initial conditions, using these encoded features as inputs for the PINN,
enabling the model to adapt to changes in parameters, BCs, and ICs. We apply
this approach to three different problems. First, we analyze the Rossler ODE
system, demonstrating the robustness of the model with respect to noise and its
ability to generalize. Next, we explore the model's capability in a 2D
Navier-Stokes PDE problem involving flow past a cylinder with a parametric
sinusoidal inlet velocity function, showing that the model can encode pressure
data from a few points to identify the inlet velocity profile and utilize
physics to compute velocity and pressure throughout the domain. Finally, we
address a 1D heat monitoring problem using real data from the heating of glass
fiber and thermoplastic composite plates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Agentic Reinforcement Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native
agentic ability to use external tools such as web browsers for searching and
writing/executing code for image manipulation to think with images. In the
open-source research community, while significant progress has been made in
language-only agentic abilities such as function calling and tool integration,
the development of multi-modal agentic capabilities that involve truly thinking
with images, and their corresponding benchmarks, are still less explored. This
work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning
(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large
Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the
ability to browse websites for real-time information updates and write code to
manipulate and analyze input images through cropping, rotation, and other image
processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)
with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'
agentic search and coding abilities. Our experimental results demonstrate that
Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and
+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT
also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks
such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.
Our findings suggest that Visual-ARFT offers a promising path toward building
robust and generalizable multimodal agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project url:
  https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ABBA: Highly Expressive Hadamard Product Adaptation for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Singhal, Kaustubh Ponkshe, Rohit Vartak, Praneeth Vepakomma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have demonstrated strong performance across a wide
range of tasks, but adapting them efficiently to new domains remains a key
challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by
introducing lightweight, trainable modules while keeping most pre-trained
weights fixed. The prevailing approach, LoRA, models updates using a low-rank
decomposition, but its expressivity is inherently constrained by the rank.
Recent methods like HiRA aim to increase expressivity by incorporating a
Hadamard product with the frozen weights, but still rely on the structure of
the pre-trained model. We introduce ABBA, a new PEFT architecture that
reparameterizes the update as a Hadamard product of two independently learnable
low-rank matrices. In contrast to prior work, ABBA fully decouples the update
from the pre-trained weights, enabling both components to be optimized freely.
This leads to significantly higher expressivity under the same parameter
budget. We formally analyze ABBA's expressive capacity and validate its
advantages through matrix reconstruction experiments. Empirically, ABBA
achieves state-of-the-art results on arithmetic and commonsense reasoning
benchmarks, consistently outperforming existing PEFT methods by a significant
margin across multiple models. Our code is publicly available at:
https://github.com/CERT-Lab/abba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Raghav Singhal, Kaustubh Ponkshe, and Rohit Vartak contributed
  equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Embodied AGI: A <span class="highlight-title">Review</span> of Embodied AI and the Road Ahead 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yequan Wang, Aixin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial General Intelligence (AGI) is often envisioned as inherently
embodied. With recent advances in robotics and foundational AI models, we stand
at the threshold of a new era-one marked by increasingly generalized embodied
AI systems. This paper contributes to the discourse by introducing a systematic
taxonomy of Embodied AGI spanning five levels (L1-L5). We review existing
research and challenges at the foundational stages (L1-L2) and outline the key
components required to achieve higher-level capabilities (L3-L5). Building on
these insights and existing technologies, we propose a conceptual framework for
an L3+ robotic brain, offering both a technical outlook and a foundation for
future exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast and close Shannon entropy approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Illia Horenko, Davide Bassetti, Lukáš Pospíšil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy
are key components in many tools used in physics, information theory, machine
learning (ML) and quantum computing. Besides of the significant amounts of SE
computations required in these fields, the singularity of the SE gradient is
one of the central mathematical reason inducing the high cost, frequently low
robustness and slow convergence of such tools. Here we propose the Fast Entropy
Approximation (FEA) - a non-singular rational approximation of Shannon entropy
and its gradient that achieves a mean absolute error of $10^{-3}$, which is
approximately $20$ times lower than comparable state-of-the-art methods. FEA
allows around $50\%$ faster computation, requiring only $5$ to $6$ elementary
computational operations, as compared to tens of elementary operations behind
the fastest entropy computation algorithms with table look-ups, bitshifts, or
series approximations. On a set of common benchmarks for the feature selection
problem in machine learning, we show that the combined effect of fewer
elementary operations, low approximation error, and a non-singular gradient
allows significantly better model quality and enables ML feature extraction
that is two to three orders of magnitude faster and computationally cheaper
when incorporating FEA into AI tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanistic Fine-tuning for In-context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hakaze Cho, Peng Luo, Mariko Kato, Rin Kaenbyou, Naoya Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context Learning (ICL) utilizes structured demonstration-query inputs to
induce few-shot learning on Language Models (LMs), which are not originally
pre-trained on ICL-style data. To bridge the gap between ICL and pre-training,
some approaches fine-tune LMs on large ICL-style datasets by an end-to-end
paradigm with massive computational costs. To reduce such costs, in this paper,
we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous
findings on the inner mechanism of ICL, building training objectives on the
attention scores instead of the final outputs, to force the attention scores to
focus on the correct label tokens presented in the context and mitigate
attention scores from the wrong label tokens. Our experiments on 9 modern LMs
and 8 datasets empirically find that ABFT outperforms in performance,
robustness, unbiasedness, and efficiency, with only around 0.01% data cost
compared to the previous methods. Moreover, our subsequent analysis finds that
the end-to-end training objective contains the ABFT objective, suggesting the
implicit bias of ICL-style data to the emergence of induction heads. Our work
demonstrates the possibility of controlling specific module sequences within
LMs to improve their behavior, opening up the future application of mechanistic
interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 31 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoQA: Visual-only Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyang Jiang, Jianing An, Jie Luo, Wenjun Wu, Lei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Visual-only Question Answering (VoQA), a novel multimodal task in
which questions are visually embedded within images, without any accompanying
textual input. This requires models to locate, recognize, and reason over
visually embedded textual questions, posing challenges for existing large
vision-language models (LVLMs), which show notable performance drops even with
carefully designed prompts. To bridge this gap, we introduce Guided Response
Triggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy
that guides the model to perform step-by-step reasoning purely based on visual
input, significantly improving model performance. Our work enhances models'
capacity for human-like visual understanding in complex multimodal scenarios,
where information, including language, is perceived visually.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed
  Hinglish to Red-Team LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darpan Aswal, Siddharth D Jaiswal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become increasingly powerful, with
multilingual and multimodal capabilities improving by the day. These models are
being evaluated through audits, alignment studies and red-teaming efforts to
expose model vulnerabilities towards generating harmful, biased and unfair
content. Existing red-teaming efforts have previously focused on the English
language, using fixed template-based attacks; thus, models continue to be
susceptible to multilingual jailbreaking strategies, especially in the
multimodal context. In this study, we introduce a novel strategy that leverages
code-mixing and phonetic perturbations to jailbreak LLMs for both text and
image generation tasks. We also introduce two new jailbreak strategies that
show higher effectiveness than baseline strategies. Our work presents a method
to effectively bypass safety filters in LLMs while maintaining interpretability
by applying phonetic misspellings to sensitive words in code-mixed prompts. Our
novel prompts achieve a 99% Attack Success Rate for text generation and 78% for
image generation, with Attack Relevance Rate of 100% for text generation and
95% for image generation when using the phonetically perturbed code-mixed
prompts. Our interpretability experiments reveal that phonetic perturbations
impact word tokenization, leading to jailbreak success. Our study motivates
increasing the focus towards more generalizable safety alignment for
multilingual multimodal models, especially in real-world settings wherein
prompts can have misspelt words.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated learning in low-resource settings: A chest imaging study in
  Africa -- Challenges and lessons learned 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Fabila, Lidia Garrucho, Víctor M. Campello, Carlos Martín-Isla, Karim Lekadir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the use of Federated Learning (FL) for tuberculosis (TB)
diagnosis using chest X-rays in low-resource settings across Africa. FL allows
hospitals to collaboratively train AI models without sharing raw patient data,
addressing privacy concerns and data scarcity that hinder traditional
centralized models. The research involved hospitals and research centers in
eight African countries. Most sites used local datasets, while Ghana and The
Gambia used public ones. The study compared locally trained models with a
federated model built across all institutions to evaluate FL's real-world
feasibility. Despite its promise, implementing FL in sub-Saharan Africa faces
challenges such as poor infrastructure, unreliable internet, limited digital
literacy, and weak AI regulations. Some institutions were also reluctant to
share model updates due to data control concerns. In conclusion, FL shows
strong potential for enabling AI-driven healthcare in underserved regions, but
broader adoption will require improvements in infrastructure, education, and
regulatory support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning vs. Distillation: Understanding Accuracy and
  Capability in LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minwu Kim, Anubhav Shrestha, Safal Shrestha, Aadim Nepal, Keith Ross
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that reinforcement learning with verifiable rewards
(RLVR) enhances overall accuracy but fails to improve capability, while
distillation can improve both. In this paper, we investigate the mechanisms
behind these phenomena. First, we demonstrate that RLVR does not improve
capability because it focuses on improving the accuracy of the less-difficult
questions to the detriment of the accuracy of the most difficult questions,
thereby leading to no improvement in capability. Second, we find that RLVR does
not merely increase the success probability for the less difficult questions,
but in our small model settings produces quality responses that were absent in
its output distribution before training. In addition, we show these responses
are neither noticeably longer nor feature more reflection-related keywords,
underscoring the need for more reliable indicators of response quality. Third,
we show that while distillation reliably improves accuracy by learning strong
reasoning patterns, it only improves capability when new knowledge is
introduced. Moreover, when distilling only with reasoning patterns and no new
knowledge, the accuracy of the less-difficult questions improves to the
detriment of the most difficult questions, similar to RLVR. Together, these
findings offer a clearer understanding of how RLVR and distillation shape
reasoning behavior in language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic <span class="highlight-title">Dataset</span> Generation for Knowledge Intensive Question Answering
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sizhe Yuen, Ting Su, Ziyang Wang, Yali Du, Adam J. Sobey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A question-answering (QA) system is to search suitable answers within a
knowledge base. Current QA systems struggle with queries requiring complex
reasoning or real-time knowledge integration. They are often supplemented with
retrieval techniques on a data source such as Retrieval-Augmented Generation
(RAG). However, RAG continues to face challenges in handling complex reasoning
and logical connections between multiple sources of information. A novel
approach for enhancing Large Language Models (LLMs) in knowledge-intensive QA
tasks is presented through the automated generation of context-based QA pairs.
This methodology leverages LLMs to create fine-tuning data, reducing reliance
on human labelling and improving model comprehension and reasoning
capabilities. The proposed system includes an automated QA generator and a
model fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.
Comprehensive experiments demonstrate improvements in logical coherence and
factual accuracy, with implications for developing adaptable Artificial
Intelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,
BLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA
pairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA
pairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedded Mean Field Reinforcement Learning for Perimeter-defense Game 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Wang, Xin Yu, Xuxin Lv, Gangzheng Ai, Wenjun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of unmanned aerial vehicles (UAVs) and missile
technologies, perimeter-defense game between attackers and defenders for the
protection of critical regions have become increasingly complex and
strategically significant across a wide range of domains. However, existing
studies predominantly focus on small-scale, simplified two-dimensional
scenarios, often overlooking realistic environmental perturbations, motion
dynamics, and inherent heterogeneity--factors that pose substantial challenges
to real-world applicability. To bridge this gap, we investigate large-scale
heterogeneous perimeter-defense game in a three-dimensional setting,
incorporating realistic elements such as motion dynamics and wind fields. We
derive the Nash equilibrium strategies for both attackers and defenders,
characterize the victory regions, and validate our theoretical findings through
extensive simulations. To tackle large-scale heterogeneous control challenges
in defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)
framework. EMFAC leverages representation learning to enable high-level action
aggregation in a mean-field manner, supporting scalable coordination among
defenders. Furthermore, we introduce a lightweight agent-level attention
mechanism based on reward representation, which selectively filters
observations and mean-field information to enhance decision-making efficiency
and accelerate convergence in large-scale tasks. Extensive simulations across
varying scales demonstrate the effectiveness and adaptability of EMFAC, which
outperforms established baselines in both convergence speed and overall
performance. To further validate practicality, we test EMFAC in small-scale
real-world experiments and conduct detailed analyses, offering deeper insights
into the framework's effectiveness in complex scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Challenges and Limitations in the Synthetic Generation of mHealth Sensor
  Data <span class="chip">ALT</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Flavio Di Martino, Franca Delmastro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of mobile sensors has the potential to provide
massive and heterogeneous time series data, driving Artificial Intelligence
applications in mHealth. However, data collection remains limited due to
stringent ethical regulations, privacy concerns, and other constraints,
hindering progress in the field. Synthetic data generation, particularly
through Generative Adversarial Networks and Diffusion Models, has emerged as a
promising solution to address both data scarcity and privacy issues. Yet, these
models are often limited to short-term, unimodal signal patterns. This paper
presents a systematic evaluation of state-of-the-art generative models for time
series synthesis, with a focus on their ability to jointly handle
multi-modality, long-range dependencies, and conditional generation-key
challenges in the mHealth domain. To ensure a fair comparison, we introduce a
novel evaluation framework designed to measure both the intrinsic quality of
synthetic data and its utility in downstream predictive tasks. Our findings
reveal critical limitations in the existing approaches, particularly in
maintaining cross-modal consistency, preserving temporal coherence, and
ensuring robust performance in train-on-synthetic, test-on-real, and data
augmentation scenarios. Finally, we present our future research directions to
enhance synthetic time series generation and improve the applicability of
generative models in mHealth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ACM Transactions on Computing for Healthcare (ACM
  HEALTH)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Early Risk Prediction of Pediatric Cardiac Arrest from Electronic Health
  Records via Multimodal Fused <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07158v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07158v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaying Lu, Stephanie R. Brown, Songyuan Liu, Shifan Zhao, Kejun Dong, Del Bold, Michael Fundora, Alaa Aljiffry, Alex Fedorov, Jocelyn Grunwell, Xiao Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early prediction of pediatric cardiac arrest (CA) is critical for timely
intervention in high-risk intensive care settings. We introduce PedCA-FT, a
novel transformer-based framework that fuses tabular view of EHR with the
derived textual view of EHR to fully unleash the interactions of
high-dimensional risk factors and their dynamics. By employing dedicated
transformer modules for each modality view, PedCA-FT captures complex temporal
and contextual patterns to produce robust CA risk estimates. Evaluated on a
curated pediatric cohort from the CHOA-CICU database, our approach outperforms
ten other artificial intelligence models across five key performance metrics
and identifies clinically meaningful risk factors. These findings underscore
the potential of multimodal fusion techniques to enhance early CA detection and
improve patient care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM
  Inference in Resource-Constrained Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.15364v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.15364v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyoung Park, Dalton Jones, Matthew J Morse, Raghavv Goel, Mingu Lee, Chris Lott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate that geometrically distinctive keys during LLM inference tend
to have high attention scores. Based on the phenomenon we propose KeyDiff, a
training-free KV cache eviction method based solely on key similarity. Unlike
other KV cache eviction methods, KeyDiff can process arbitrarily long prompts
within strict resource constraints and efficiently generate responses. We
provide a theoretical basis for KeyDiff by relating key diversity with
attention scores. These results imply KeyDiff can efficiently identify the most
important tokens to retain. Notably KeyDiff does not rely on attention scores,
allowing the use of optimized attention mechanisms like FlashAttention. Under a
strict memory allowance, we demonstrate the effectiveness of KeyDiff for the
Llama and Qwen model families by observing a performance gap of less than 0.04%
with 8K cache budget ($\sim$23% KV cache reduction) from the non-evicting
baseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near
baseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning
benchmark and decrease end-to-end inference latency by up to 30% compared to
the other token-eviction methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Medium Range Severe Weather Prediction through <span class="highlight-title">Transformer</span>
  Post-processing of AI Weather Forecasts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanxiang Hua, Ryan Sobash, David John Gagne II, Yingkai Sha, Alexandra Anderson-Frey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the skill of medium-range (1-8 day) severe weather prediction is
crucial for mitigating societal impacts. This study introduces a novel approach
leveraging decoder-only transformer networks to post-process AI-based weather
forecasts, specifically from the Pangu-Weather model, for improved severe
weather guidance. Unlike traditional post-processing methods that use a dense
neural network to predict the probability of severe weather using discrete
forecast samples, our method treats forecast lead times as sequential
``tokens'', enabling the transformer to learn complex temporal relationships
within the evolving atmospheric state. We compare this approach against
post-processing of the Global Forecast System (GFS) using both a traditional
dense neural network and our transformer, as well as configurations that
exclude convective parameters to fairly evaluate the impact of using the
Pangu-Weather AI model. Results demonstrate that the transformer-based
post-processing significantly enhances forecast skill compared to dense neural
networks. Furthermore, AI-driven forecasts, particularly Pangu-Weather
initialized from high resolution analysis, exhibit superior performance to GFS
in the medium-range, even without explicit convective parameters. Our approach
offers improved accuracy, and reliability, which also provides interpretability
through feature attribution analysis, advancing medium-range severe weather
prediction capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures; update fix issues with section reference number</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Effective Can Dropout Be in Multiple Instance Learning ? <span class="chip">ICML2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.14783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.14783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhui Zhu, Peijie Qiu, Xiwen Chen, Zhangsihao Yang, Aristeidis Sotiras, Abolfazl Razi, Yalin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple Instance Learning (MIL) is a popular weakly-supervised method for
various applications, with a particular interest in histological whole slide
image (WSI) classification. Due to the gigapixel resolution of WSI,
applications of MIL in WSI typically necessitate a two-stage training scheme:
first, extract features from the pre-trained backbone and then perform MIL
aggregation. However, it is well-known that this suboptimal training scheme
suffers from "noisy" feature embeddings from the backbone and inherent weak
supervision, hindering MIL from learning rich and generalizable features.
However, the most commonly used technique (i.e., dropout) for mitigating this
issue has yet to be explored in MIL. In this paper, we empirically explore how
effective the dropout can be in MIL. Interestingly, we observe that dropping
the top-k most important instances within a bag leads to better performance and
generalization even under noise attack. Based on this key observation, we
propose a novel MIL-specific dropout method, termed MIL-Dropout, which
systematically determines which instances to drop. Experiments on five MIL
benchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the
performance of current MIL methods with a negligible computational cost. The
code is available at https://github.com/ChongQingNoSubway/MILDropout.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion-Based Failure Sampling for Evaluating Safety-Critical
  Autonomous Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harrison Delecki, Marc R. Schlichting, Mansur Arief, Anthony Corso, Marcell Vazquez-Chanlatte, Mykel J. Kochenderfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Validating safety-critical autonomous systems in high-dimensional domains
such as robotics presents a significant challenge. Existing black-box
approaches based on Markov chain Monte Carlo may require an enormous number of
samples, while methods based on importance sampling often rely on simple
parametric families that may struggle to represent the distribution over
failures. We propose to sample the distribution over failures using a
conditional denoising diffusion model, which has shown success in complex
high-dimensional problems such as robotic task planning. We iteratively train a
diffusion model to produce state trajectories closer to failure. We demonstrate
the effectiveness of our approach on high-dimensional robotic validation tasks,
improving sample efficiency and mode coverage compared to existing black-box
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appears in IEEE International Conference on Engineering Reliable
  Autonomous Systems (ERAS) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TiEBe: Tracking Language Model Recall of Notable Worldwide Events
  Through Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thales Sales Almeida, Giovana Kerche Bonás, João Guilherme Alves Santos, Hugo Abonizio, Rodrigo Nogueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the knowledge landscape evolves and large language models (LLMs) become
increasingly widespread, there is a growing need to keep these models updated
with current events. While existing benchmarks assess general factual recall,
few studies explore how LLMs retain knowledge over time or across different
regions. To address these gaps, we present the Timely Events Benchmark (TiEBe),
a dataset of over 23,000 question-answer pairs centered on notable global and
regional events, spanning more than 10 years of events, 23 regions, and 13
languages. TiEBe leverages structured retrospective data from Wikipedia to
identify notable events through time. These events are then used to construct a
benchmark to evaluate LLMs' understanding of global and regional developments,
grounded in factual evidence beyond Wikipedia itself. Our results reveal
significant geographic disparities in factual recall, emphasizing the need for
more balanced global representation in LLM training. We also observe a Pearson
correlation of more than 0.7 between models' performance in TiEBe and various
countries' socioeconomic indicators, such as HDI. In addition, we examine the
impact of language on factual recall by posing questions in the native language
of the region where each event occurred, uncovering substantial performance
gaps for low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability
  of Large Reasoning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.10368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.10368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyuan Zhang, Shuaiyi Nie, Xinghua Zhang, Zefeng Zhang, Tingwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce S1-Bench, a novel benchmark designed to evaluate the performance
of Large Reasoning Models (LRMs) on simple tasks that favor intuitive system 1
thinking rather than deliberative system 2 reasoning. While LRMs have achieved
significant breakthroughs in complex reasoning tasks through explicit chains of
thought, their heavy reliance on system 2 thinking may limit their system 1
thinking capabilities. However, there is a lack of an appropriate benchmark for
evaluating LRM's system 1 thinking capabilities. To fill this gap, S1-Bench
introduces a suite of simple, diverse, and natural questions across multiple
domains and languages, specifically designed to assess LRMs' performance on
questions more suitable for system 1 . We conduct extensive evaluations across
28 LRMs, revealing their inefficiency, inadequate accuracy, and limited
robustness when handling simple questions. Additionally, we observe a gap
between their difficulty perception and generation length. Overall, this work
paves the way toward dual-system compatibility in the development of LRMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 9 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in
  Multilingual LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himanshu Beniwal, Sailesh Panda, Birudugadda Srivibhav, Mayank Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore \textbf{C}ross-lingual \textbf{B}ackdoor \textbf{AT}tacks (X-BAT)
in multilingual Large Language Models (mLLMs), revealing how backdoors inserted
in one language can automatically transfer to others through shared embedding
spaces. Using toxicity classification as a case study, we demonstrate that
attackers can compromise multilingual systems by poisoning data in a single
language, with rare and high-occurring tokens serving as specific, effective
triggers. Our findings expose a critical vulnerability that influences the
model's architecture, resulting in a concealed backdoor effect during the
information flow. Our code and data are publicly available
https://github.com/himanshubeniwal/X-BAT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Scheduling for LLM Inference with KV Cache Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07115v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07115v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Jaillet, Jiashuo Jiang, Konstantina Mellou, Marco Molinaro, Chara Podimata, Zijie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) inference, where a trained model generates text
one word at a time in response to user prompts, is a computationally intensive
process requiring efficient scheduling to optimize latency and resource
utilization. A key challenge in LLM inference is the management of the
Key-Value (KV) cache, which reduces redundant computations but introduces
memory constraints. In this work, we model LLM inference with KV cache
constraints theoretically and propose a novel batching and scheduling algorithm
that minimizes inference latency while effectively managing the KV cache's
memory.
  More specifically, we make the following contributions. First, to evaluate
the performance of online algorithms for scheduling in LLM inference, we
introduce a hindsight optimal benchmark, formulated as an integer program that
computes the minimum total inference latency under full future information.
Second, we prove that no deterministic online algorithm can achieve a constant
competitive ratio when the arrival process is arbitrary. Third, motivated by
the computational intractability of solving the integer program at scale, we
propose a polynomial-time online scheduling algorithm and show that under
certain conditions it can achieve a constant competitive ratio. We also
demonstrate our algorithm's strong empirical performance by comparing it to the
hindsight optimal in a synthetic dataset. Finally, we conduct empirical
evaluations on a real-world public LLM inference dataset, simulating the
Llama2-70B model on A100 GPUs, and show that our algorithm significantly
outperforms the benchmark algorithms. Overall, our results offer a path toward
more sustainable and cost-effective LLM deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Effect of Explanation Content and Format on User
  Comprehension and Trust in Healthcare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17401v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17401v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Rago, Bence Palfi, Purin Sukpanichnant, Hannibal Nabli, Kavyesh Vivek, Olga Kostopoulou, James Kinross, Francesca Toni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-driven tools for healthcare are widely acknowledged as potentially
beneficial to health practitioners and patients, e.g. the QCancer regression
tool for cancer risk prediction. However, for these tools to be trusted, they
need to be supplemented with explanations. We examine how explanations' content
and format affect user comprehension and trust when explaining QCancer's
predictions. Regarding content, we deploy SHAP and Occlusion-1. Regarding
format, we present SHAP explanations, conventionally, as charts (SC) and
Occlusion-1 explanations as charts (OC) as well as text (OT), to which their
simpler nature lends itself. We conduct experiments with two sets of
stakeholders: the general public (representing patients) and medical students
(representing healthcare practitioners). Our experiments showed higher
subjective comprehension and trust for Occlusion-1 over SHAP explanations based
on content. However, when controlling for format, only OT outperformed SC,
suggesting this trend is driven by preferences for text. Other findings
corroborated that explanation format, rather than content, is often the
critical factor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Synthetic Data and Fine-grained Adaptive Feature Alignment for
  Composed Person Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16515v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16515v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Liu, Haiwen Li, Zhaohui Hou, Zhicheng Zhao, Fei Su, Yuan Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person retrieval has attracted rising attention. Existing methods are mainly
divided into two retrieval modes, namely image-only and text-only. However,
they are unable to make full use of the available information and are difficult
to meet diverse application requirements. To address the above limitations, we
propose a new Composed Person Retrieval (CPR) task, which combines visual and
textual queries to identify individuals of interest from large-scale person
image databases. Nevertheless, the foremost difficulty of the CPR task is the
lack of available annotated datasets. Therefore, we first introduce a scalable
automatic data synthesis pipeline, which decomposes complex multimodal data
generation into the creation of textual quadruples followed by
identity-consistent image synthesis using fine-tuned generative models.
Meanwhile, a multimodal filtering method is designed to ensure the resulting
SynCPR dataset retains 1.15 million high-quality and fully synthetic triplets.
Additionally, to improve the representation of composed person queries, we
propose a novel Fine-grained Adaptive Feature Alignment (FAFA) framework
through fine-grained dynamic alignment and masked feature reasoning. Moreover,
for objective evaluation, we manually annotate the Image-Text Composed Person
Retrieval (ITCPR) test set. The extensive experiments demonstrate the
effectiveness of the SynCPR dataset and the superiority of the proposed FAFA
framework when compared with the state-of-the-art methods. All code and data
will be provided at
https://github.com/Delong-liu-bupt/Composed_Person_Retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoLA: Compute-Efficient <span class="highlight-title">Pre-Train</span>ing of LLMs via Low-Rank Activation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10940v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10940v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Liu, Ruijie Zhang, Zhengyang Wang, Zi Yang, Paul Hovland, Bogdan Nicolae, Franck Cappello, Zheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The full-size MLPs and the projection layers in attention introduce
tremendous model sizes of large language models (LLMs), imposing extremely
demanding needs of computational resources in the pre-training stage. However,
we empirically observe that the activations of pre-trained LLMs exhibit
low-rank property. Motivated by such observations, we propose CoLA and its
memory-efficient implementation, CoLA-M, to replace these full-size layers with
compute-efficient auto-encoders that naturally enforce low-rank activations
throughout training. This fundamental architectural change eliminates the
activation redundancy and significantly boosts model capacity and training
efficiency. Experiments on LLaMA models with 60 million to 7 billion parameters
show that CoLA reduces the computing cost by $\bf 2\pmb{\times}$ and improves
training throughput by $\bf 1.86\pmb{\times}$ while maintaining full-rank level
performance. CoLA-M further squeezes memory cost without sacrificing
throughput, offering a pre-training approach with collectively superior
parameter, computing, and memory efficiency. The LLMs produced are also $\bf
2\pmb{\times}$ smaller, enabling faster inference with lower memory cost on
resource-constrained platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can LLMs be Good Graph Judge for Knowledge Graph Construction? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17388v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17388v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Huang, Chong Chen, Zeang Sheng, Yang Li, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, most of the data obtained from the information
retrieval (IR) system is unstructured. Converting natural language sentences
into structured Knowledge Graphs (KGs) remains a critical challenge. We
identified three limitations with respect to existing KG construction methods:
(1) There could be a large amount of noise in real-world documents, which could
result in extracting messy information. (2) Naive LLMs usually extract
inaccurate knowledge from some domain-specific documents. (3) Hallucination
phenomenon cannot be overlooked when directly using LLMs to construct KGs. In
this paper, we propose \textbf{GraphJudge}, a KG construction framework to
address the aforementioned challenges. In this framework, we designed an
entity-centric strategy to eliminate the noise information in the documents.
And we fine-tuned a LLM as a graph judge to finally enhance the quality of
generated KGs. Experiments conducted on two general and one domain-specific
text-graph pair datasets demonstrate state-of-the-art performance against
various baseline methods with strong generalization abilities. Our code is
available at
\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EquiBench: Benchmarking Large Language Models' Understanding of Program
  Semantics via Equivalence Checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12466v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12466v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjiang Wei, Jiannan Cao, Ran Li, Hongyu Chen, Yuhui Zhang, Ziheng Wang, Yuan Liu, Thiago S. F. X. Teixeira, Diyi Yang, Ke Wang, Alex Aiken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become integral to code-related tasks, a
central question emerges: do LLMs truly understand program execution semantics?
We introduce EquiBench, a new benchmark for evaluating LLMs through equivalence
checking, i.e., determining whether two programs produce identical outputs for
all possible inputs. Unlike prior code generation benchmarks, this task
directly tests a model's understanding of code execution semantics. EquiBench
consists of 2400 program pairs across four languages and six categories. These
pairs are generated through program analysis, compiler scheduling, and
superoptimization, ensuring high-confidence labels, nontrivial difficulty, and
full automation. The transformations span syntactic edits, structural
modifications, and algorithmic changes, covering a broad spectrum of semantic
variation. We evaluate 19 state-of-the-art LLMs and find that in the most
challenging categories, the best accuracies are 63.8% and 76.2%, only modestly
above the 50% random baseline. Further analysis reveals that models often rely
on syntactic similarity rather than exhibiting robust reasoning over execution
semantics, highlighting fundamental limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KunServe: Efficient Parameter-centric Memory Management for LLM Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18169v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18169v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongxin Cheng, Yuxin Lai, Xingda Wei, Rong Chen, Haibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Serving LLMs with a cluster of GPUs is common nowadays, where the serving
system must meet strict latency SLOs required by applications. However, the
stateful nature of LLM serving requires maintaining huge states (i.e., KVCache)
in limited GPU memory. Under spikes in real-world workloads, GPU memory can be
easily throttled, leading to orders of magnitude higher response latency due to
queuing introduced by waiting for KVCache to be reclaimed. Prior
KVCache-centric approaches handle load throttling by dropping, migrating, or
swapping KVCache. These methods fail to release sufficient memory quickly with
requests still queued.
  This paper proposes the first parameter-centric approach to handling
throttling by selectively dropping replicated parameters to instantly free
memory for requests, based on an unnoticed observation that model parameters
are commonly replicated across GPUs for serving LLMs. With additional memory,
all requests can be served with a larger batch without queuing. To make the
parameter-centric approach correct and efficient, we cooperatively execute
requests on GPUs with a complete copy of parameters using pipeline parallelism,
and derive an appropriate drop plan without unnecessary cooperation. We also
design techniques to minimize the performance overhead due to pipeline
parallelism with the execution patterns of requests under drop. Evaluations
show that {\sys} reduces the tail TTFT of requests under throttling by up to
72.2 times compared to the state-of-the-art systems including Llumnix, vLLM and
InferCept.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sinusoidal Initialization, Time for a New Start 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Fernández-Hernández, Jose I. Mestre, Manuel F. Dolz, Jose Duato, Enrique S. Quintana-Ortí
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Initialization plays a critical role in Deep Neural Network training,
directly influencing convergence, stability, and generalization. Common
approaches such as Glorot and He initializations rely on randomness, which can
produce uneven weight distributions across layer connections. In this paper, we
introduce the Sinusoidal initialization, a novel deterministic method that
employs sinusoidal functions to construct structured weight matrices expressly
to improve the spread and balance of weights throughout the network while
simultaneously fostering a more uniform, well-conditioned distribution of
neuron activation states from the very first forward pass. Because Sinusoidal
initialization begins with weights and activations that are already evenly and
efficiently utilized, it delivers consistently faster convergence, greater
training stability, and higher final accuracy across a wide range of models,
including convolutional neural networks, vision transformers, and large
language models. On average, our experiments show an increase of 4.9% in final
validation accuracy and 20.9% in convergence speed. By replacing randomness
with structure, this initialization provides a stronger and more reliable
foundation for Deep Learning systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of
  Multimodal Large Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11051v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11051v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Huo, Yibo Yan, Xu Zheng, Yuanhuiyi Lyu, Xin Zou, Zhihua Wei, Xuming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in Machine Unlearning (MU) has introduced solutions for the
selective removal of private or sensitive information encoded within deep
neural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs)
remains in its nascent phase. Therefore, we propose to reformulate the task of
multimodal MU in the era of MLLMs, which aims to erase only the visual patterns
associated with a given entity while preserving the corresponding textual
knowledge encoded within the original parameters of the language model
backbone. Furthermore, we develop a novel geometry-constrained gradient ascent
method MMUnlearner. It updates the weights of MLLMs with a weight saliency map
jointly restricted by the remaining concepts and textual knowledge during
unlearning, thereby preserving parameters essential for non-target knowledge.
Extensive experiments demonstrate that MMUnlearner surpasses baselines that
finetuning MLLMs with VQA data directly through Gradient Ascent (GA) or
Negative Preference Optimization (NPO), across all evaluation dimensions. Our
code will be released upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the Correctness of Inference Patterns Used by LLMs for
  Judgment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09083v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09083v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Chen, Yuxuan Huang, Yixing Li, Dongrui Liu, Qihan Ren, Shuai Zhao, Kun Kuang, Zilong Zheng, Quanshi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a method to analyze the inference patterns used by Large
Language Models (LLMs) for judgment in a case study on legal LLMs, so as to
identify potential incorrect representations of the LLM, according to human
domain knowledge. Unlike traditional evaluations on language generation
results, we propose to evaluate the correctness of the detailed inference
patterns of an LLM behind its seemingly correct outputs. To this end, we
quantify the interactions between input phrases used by the LLM as primitive
inference patterns, because recent theoretical achievements have proven several
mathematical guarantees of the faithfulness of the interaction-based
explanation. We design a set of metrics to evaluate the detailed inference
patterns of LLMs. Experiments show that even when the language generation
results appear correct, a significant portion of the inference patterns used by
the LLM for the legal judgment may represent misleading or irrelevant logic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Technical Report: Quantifying and Analyzing the Generalization Power of
  a DNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan He, Junpeng Zhang, Lei Cheng, Hongyuan Zhang, Quanshi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new perspective for analyzing the generalization power
of deep neural networks (DNNs), i.e., directly disentangling and analyzing the
dynamics of generalizable and non-generalizable interaction encoded by a DNN
through the training process. Specifically, this work builds upon the recent
theoretical achievement in explainble AI, which proves that the detailed
inference logic of DNNs can be can be strictly rewritten as a small number of
AND-OR interaction patterns. Based on this, we propose an efficient method to
quantify the generalization power of each interaction, and we discover a
distinct three-phase dynamics of the generalization power of interactions
during training. In particular, the early phase of training typically removes
noisy and non-generalizable interactions and learns simple and generalizable
ones. The second and the third phases tend to capture increasingly complex
interactions that are harder to generalize. Experimental results verify that
the learning of non-generalizable interactions is the the direct cause for the
gap between the training and testing losses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RadCLIP: Enhancing Radiologic Image Analysis through Contrastive
  Language-Image <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09948v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09948v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixiu Lu, Hailong Li, Nehal A. Parikh, Jonathan R. Dillman, Lili He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of artificial intelligence (AI) with radiology marks a
transformative era in medicine. Vision foundation models have been adopted to
enhance radiologic imaging analysis. However, the distinct complexities of
radiologic 2D and 3D radiologic data pose unique challenges that existing
models, pre-trained on general non-medical images, fail to address adequately.
To bridge this gap and capitalize on the diagnostic precision required in
radiologic imaging, we introduce Radiologic Contrastive Language-Image
Pre-training (RadCLIP): a cross-modal vision-language foundational model that
harnesses Vision Language Pre-training (VLP) framework to improve radiologic
image analysis. Building upon Contrastive Language-Image Pre-training (CLIP),
RadCLIP incorporates a slice pooling mechanism tailored for volumetric image
analysis and is pre-trained using a large and diverse dataset of radiologic
image-text pairs. The RadCLIP was pre-trained to effectively align radiologic
images with their corresponding text annotations, creating a robust vision
backbone for radiologic images. Extensive experiments demonstrate RadCLIP's
superior performance in both uni-modal radiologic image classification and
cross-modal image-text matching, highlighting its significant promise for
improving diagnostic accuracy and efficiency in clinical settings. Our Key
contributions include curating a large dataset with diverse radiologic 2D/3D
radiologic image-text pairs, a slice pooling adapter using an attention
mechanism for integrating 2D images, and comprehensive evaluations of RadCLIP
on various radiologic downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Rich Emotions in 3D Avatars: A Text-to-3D Avatar Generation
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haidong Xu, Meishan Zhang, Hao Ju, Zhedong Zheng, Erik Cambria, Min Zhang, Hao Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Producing emotionally dynamic 3D facial avatars with text derived from spoken
words (Emo3D) has been a pivotal research topic in 3D avatar generation. While
progress has been made in general-purpose 3D avatar generation, the exploration
of generating emotional 3D avatars remains scarce, primarily due to the
complexities of identifying and rendering rich emotions from spoken words. This
paper reexamines Emo3D generation and draws inspiration from human processes,
breaking down Emo3D into two cascading steps: Text-to-3D Expression Mapping
(T3DEM) and 3D Avatar Rendering (3DAR). T3DEM is the most crucial step in
determining the quality of Emo3D generation and encompasses three key
challenges: Expression Diversity, Emotion-Content Consistency, and Expression
Fluidity. To address these challenges, we introduce a novel benchmark to
advance research in Emo3D generation. First, we present EmoAva, a large-scale,
high-quality dataset for T3DEM, comprising 15,000 text-to-3D expression
mappings that characterize the aforementioned three challenges in Emo3D
generation. Furthermore, we develop various metrics to effectively evaluate
models against these identified challenges. Next, to effectively model the
consistency, diversity, and fluidity of human expressions in the T3DEM step, we
propose the Continuous Text-to-Expression Generator, which employs an
autoregressive Conditional Variational Autoencoder for expression code
generation, enhanced with Latent Temporal Attention and Expression-wise
Attention mechanisms. Finally, to further enhance the 3DAR step on rendering
higher-quality subtle expressions, we present the Globally-informed Gaussian
Avatar (GiGA) model. GiGA incorporates a global information mechanism into 3D
Gaussian representations, enabling the capture of subtle micro-expressions and
seamless transitions between emotional states.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages. Project website: https://github.com/WalkerMitty/EmoAva</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CRoP: Context-wise Robust Static Human-Sensing Personalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17994v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17994v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sawinder Kaur, Avery Gump, Yi Xiao, Jingyu Xin, Harshit Sharma, Nina R Benway, Jonathan L Preston, Asif Salekin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement in deep learning and internet-of-things have led to diverse
human sensing applications. However, distinct patterns in human sensing,
influenced by various factors or contexts, challenge the generic neural network
model's performance due to natural distribution shifts. To address this,
personalization tailors models to individual users. Yet most personalization
studies overlook intra-user heterogeneity across contexts in sensory data,
limiting intra-user generalizability. This limitation is especially critical in
clinical applications, where limited data availability hampers both
generalizability and personalization. Notably, intra-user sensing attributes
are expected to change due to external factors such as treatment progression,
further complicating the challenges. To address the intra-user generalization
challenge, this work introduces CRoP, a novel static personalization approach.
CRoP leverages off-the-shelf pre-trained models as generic starting points and
captures user-specific traits through adaptive pruning on a minimal sub-network
while allowing generic knowledge to be incorporated in remaining parameters.
CRoP demonstrates superior personalization effectiveness and intra-user
robustness across four human-sensing datasets, including two from real-world
health domains, underscoring its practical and social impact. Additionally, to
support CRoP's generalization ability and design choices, we provide empirical
justification through gradient inner product analysis, ablation studies, and
comparisons against state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 6 figues and 15 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore
  Model-level Scaling Up in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.10657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.10657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongzhan Huang, Guoming Ling, Yupei Lin, Yandong Chen, Shanshan Zhong, Hefeng Wu, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Routing large language models (LLMs) is a new paradigm that uses a router to
recommend the best LLM from a pool of candidates for a given input. In this
paper, our comprehensive analysis with more than 8,500 LLMs reveals a novel
model-level scaling up phenomenon in Routing LLMs, i.e., a capable router can
significantly enhance the performance of this paradigm as the number of
candidates increases. This improvement can even surpass the performance of the
best single model in the pool and many existing strong LLMs, confirming it a
highly promising paradigm. However, the lack of comprehensive and open-source
benchmarks for Routing LLMs has hindered the development of routers. In this
paper, we introduce RouterEval, a benchmark tailored for router research, which
includes over 200,000,000 performance records for 12 popular LLM evaluations
across various areas such as commonsense reasoning, semantic understanding,
etc., based on over 8,500 various LLMs. Using RouterEval, extensive evaluations
of existing Routing LLM methods reveal that most still have significant room
for improvement. See https://github.com/MilkThink-Lab/RouterEval for all data,
code and tutorial.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ J4R: Learning to Judge with Equivalent Initial State Group Relative
  Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Austin Xu, Yilun Zhou, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To keep pace with the increasing pace of large language models (LLM)
development, model output evaluation has transitioned away from time-consuming
human evaluation to automatic evaluation, where LLMs themselves are tasked with
assessing and critiquing other model outputs. LLM-as-judge models are a class
of generative evaluators that excel in evaluating relatively simple domains,
like chat quality, but struggle in reasoning intensive domains where model
responses contain more substantive and challenging content. To remedy existing
judge shortcomings, we explore training judges with reinforcement learning
(RL). We make three key contributions: (1) We propose the Equivalent Initial
State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us
to train our judge to be robust to positional biases that arise in more complex
evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that
evaluates judges in diverse reasoning settings not covered by prior work. (3)
We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that
outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or
exceeding the performance of larger GRPO-trained judges on both JudgeBench and
ReasoningJudgeBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 4 figures, 6 tables. To be updated with links for
  code/benchmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hotspot-Driven Peptide Design via Multi-Fragment Autoregressive
  Extension <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18463v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18463v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahan Li, Tong Chen, Shitong Luo, Chaoran Cheng, Jiaqi Guan, Ruihan Guo, Sheng Wang, Ge Liu, Jian Peng, Jianzhu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Peptides, short chains of amino acids, interact with target proteins, making
them a unique class of protein-based therapeutics for treating human diseases.
Recently, deep generative models have shown great promise in peptide
generation. However, several challenges remain in designing effective peptide
binders. First, not all residues contribute equally to peptide-target
interactions. Second, the generated peptides must adopt valid geometries due to
the constraints of peptide bonds. Third, realistic tasks for peptide drug
development are still lacking. To address these challenges, we introduce
PepHAR, a hot-spot-driven autoregressive generative model for designing
peptides targeting specific proteins. Building on the observation that certain
hot spot residues have higher interaction potentials, we first use an
energy-based density model to fit and sample these key residues. Next, to
ensure proper peptide geometry, we autoregressively extend peptide fragments by
estimating dihedral angles between residue frames. Finally, we apply an
optimization process to iteratively refine fragment assembly, ensuring correct
peptide structures. By combining hot spot sampling with fragment-based
extension, our approach enables de novo peptide design tailored to a target
protein and allows the incorporation of key hot spot residues into peptide
scaffolds. Extensive experiments, including peptide design and peptide scaffold
generation, demonstrate the strong potential of PepHAR in computational peptide
binder design. Source code will be available at
https://github.com/Ced3-han/PepHAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can LLM-based Financial Investing Strategies Outperform the Market in
  Long Run? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixian Waylon Li, Hyeonjun Kim, Mihai Cucuringu, Tiejun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently been leveraged for asset pricing
tasks and stock trading applications, enabling AI agents to generate investment
decisions from unstructured financial data. However, most evaluations of LLM
timing-based investing strategies are conducted on narrow timeframes and
limited stock universes, overstating effectiveness due to survivorship and
data-snooping biases. We critically assess their generalizability and
robustness by proposing FINSABER, a backtesting framework evaluating
timing-based strategies across longer periods and a larger universe of symbols.
Systematic backtests over two decades and 100+ symbols reveal that previously
reported LLM advantages deteriorate significantly under broader cross-section
and over a longer-term evaluation. Our market regime analysis further
demonstrates that LLM strategies are overly conservative in bull markets,
underperforming passive benchmarks, and overly aggressive in bear markets,
incurring heavy losses. These findings highlight the need to develop LLM
strategies that are able to prioritise trend detection and regime-aware risk
controls over mere scaling of framework complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuicai Dong, Yujing Chang, Xin Deik Goh, Dexun Li, Ruiming Tang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal document retrieval aims to identify and retrieve various forms of
multimodal content, such as figures, tables, charts, and layout information
from extensive documents. Despite its increasing popularity, there is a notable
lack of a comprehensive and robust benchmark to effectively evaluate the
performance of systems in such tasks. To address this gap, this work introduces
a new benchmark, named MMDocIR, that encompasses two distinct tasks: page-level
and layout-level retrieval. The former evaluates the performance of identifying
the most relevant pages within a long document, while the later assesses the
ability of detecting specific layouts, providing a more fine-grained measure
than whole-page analysis. A layout refers to a variety of elements, including
textual paragraphs, equations, figures, tables, or charts. The MMDocIR
benchmark comprises a rich dataset featuring 1,685 questions annotated by
experts and 173,843 questions with bootstrapped labels, making it a valuable
resource in multimodal document retrieval for both training and evaluation.
Through rigorous experiments, we demonstrate that (i) visual retrievers
significantly outperform their text counterparts, (ii) MMDocIR training set
effectively enhances the performance of multimodal document retrieval and (iii)
text retrievers leveraging VLM-text significantly outperforms retrievers
relying on OCR-text. Our dataset is available at
https://mmdocrag.github.io/MMDocIR/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://huggingface.co/MMDocIR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Iterative Self-Alignment for Radiology Report Generation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11983v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11983v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting Xiao, Lei Shi, Yang Zhang, HaoFeng Yang, Zhe Wang, Chenjia Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiology Report Generation (RRG) is an important research topic for
relieving radiologist' heavy workload. Existing RRG models mainly rely on
supervised fine-tuning (SFT) based on different model architectures using data
pairs of radiological images and corresponding radiologist-annotated reports.
Recent research has shifted focus to post-training improvements, aligning RRG
model outputs with human preferences using reinforcement learning (RL).
However, the limited data coverage of high-quality annotated data poses risks
of overfitting and generalization. This paper proposes a novel Online Iterative
Self-Alignment (OISA) method for RRG that consists of four stages:
self-generation of diverse data, self-evaluation for multi-objective preference
data,self-alignment for multi-objective optimization and self-iteration for
further improvement. Our approach allows for generating varied reports tailored
to specific clinical objectives, enhancing the overall performance of the RRG
model iteratively. Unlike existing methods, our frame-work significantly
increases data quality and optimizes performance through iterative
multi-objective optimization. Experimental results demonstrate that our method
surpasses previous approaches, achieving state-of-the-art performance across
multiple evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Walk the Talk? Measuring the Faithfulness of Large Language Model
  Explanations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.14150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.14150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katie Matton, Robert Osazuwa Ness, John Guttag, Emre Kıcıman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are capable of generating plausible explanations
of how they arrived at an answer to a question. However, these explanations can
misrepresent the model's "reasoning" process, i.e., they can be unfaithful.
This, in turn, can lead to over-trust and misuse. We introduce a new approach
for measuring the faithfulness of LLM explanations. First, we provide a
rigorous definition of faithfulness. Since LLM explanations mimic human
explanations, they often reference high-level concepts in the input question
that purportedly influenced the model. We define faithfulness in terms of the
difference between the set of concepts that LLM explanations imply are
influential and the set that truly are. Second, we present a novel method for
estimating faithfulness that is based on: (1) using an auxiliary LLM to modify
the values of concepts within model inputs to create realistic counterfactuals,
and (2) using a Bayesian hierarchical model to quantify the causal effects of
concepts at both the example- and dataset-level. Our experiments show that our
method can be used to quantify and discover interpretable patterns of
unfaithfulness. On a social bias task, we uncover cases where LLM explanations
hide the influence of social bias. On a medical question answering task, we
uncover cases where LLM explanations provide misleading claims about which
pieces of evidence influenced the model's decisions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>66 pages, 14 figures, 40 tables; ICLR 2025 (spotlight) camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention Mechanism for LLM-based Agents Dynamic Diffusion under
  Information Asymmetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13160v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13160v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Zhang, Yifu Wu, Wenyue Hua, Xiang Lu, Xuming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have been used to simulate human society using
multi-agent systems. Most current social simulation research emphasizes
interactive behaviors in fixed environments, ignoring information opacity,
relationship variability, and diffusion diversity. In this paper, we first
propose a general framework for exploring multi-agent information diffusion. We
identified LLMs' deficiency in the perception and utilization of social
relationships, as well as diverse actions. Then, we designed a dynamic
attention mechanism to help agents allocate attention to different information,
addressing the limitations of the LLM attention mechanism. Agents start by
responding to external information stimuli within a five-agent group,
increasing group size and forming information circles while developing
relationships and sharing information. Additionally, we explore the information
diffusion features in the asymmetric open environment by observing the
evolution of information gaps, diffusion patterns, and the accumulation of
social capital, which are closely linked to psychological, sociological, and
communication theories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated
  Data Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13794v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13794v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhou, Shiyu Zhao, Yuxiao Chen, Zhenting Wang, Can Jin, Dimitris N. Metaxas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large foundation models trained on large-scale vision-language data can boost
Open-Vocabulary Object Detection (OVD) via synthetic training data, yet the
hand-crafted pipelines often introduce bias and overfit to specific prompts. We
sidestep this issue by directly fusing hidden states from Large Language Models
(LLMs) into detectors-an avenue surprisingly under-explored. This paper
presents a systematic method to enhance visual grounding by utilizing decoder
layers of the LLM of an MLLM. We introduce a zero-initialized cross-attention
adapter to enable efficient knowledge fusion from LLMs to object detectors, a
new approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We
find that intermediate LLM layers already encode rich spatial semantics;
adapting only the early layers yields most of the gain. With Swin-T as the
vision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at
just 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to
6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths
further corroborate our design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging LLM Inconsistency to Boost Pass@k Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12938v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12938v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Dalal, Meirav Segal, Zvika Ben-Haim, Dan Lahav, Omer Nevo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) achieve impressive abilities in numerous
domains, but exhibit inconsistent performance in response to minor input
changes. Rather than view this as a drawback, in this paper we introduce a
novel method for leveraging models' inconsistency to boost Pass@k performance.
Specifically, we present a "Variator" agent that generates k variants of a
given task and submits one candidate solution for each one. Our variant
generation approach is applicable to a wide range of domains as it is task
agnostic and compatible with free-form inputs. We demonstrate the efficacy of
our agent theoretically using a probabilistic model of the inconsistency
effect, and show empirically that it outperforms the baseline on the APPS
dataset. Furthermore, we establish that inconsistency persists even in frontier
reasoning models across coding and cybersecurity domains, suggesting our method
is likely to remain relevant for future model generations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRAML: Goal Recognition As Metric Learning <span class="chip">IJCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matan Shamir, Reuth Mirsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Goal Recognition (GR) is the problem of recognizing an agent's objectives
based on observed actions. Recent data-driven approaches for GR alleviate the
need for costly, manually crafted domain models. However, these approaches can
only reason about a pre-defined set of goals, and time-consuming training is
needed for new emerging goals. To keep this model-learning automated while
enabling quick adaptation to new goals, this paper introduces GRAML: Goal
Recognition As Metric Learning. GRAML uses a Siamese network to treat GR as a
deep metric learning task, employing an RNN that learns a metric over an
embedding space, where the embeddings for observation traces leading to
different goals are distant, and embeddings of traces leading to the same goals
are close. This metric is especially useful when adapting to new goals, even if
given just one example observation trace per goal. Evaluated on a versatile set
of environments, GRAML shows speed, flexibility, and runtime improvements over
the state-of-the-art GR while maintaining accurate recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in International Joint Conference on
  Artificial Intelligence (IJCAI) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Explainable Multi-player MCTS-minimax Hybrids in Board Game
  Using Process Mining <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.23326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.23326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyu Qian, Tim Miller, Zheng Qian, Liyuan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monte-Carlo Tree Search (MCTS) is a family of sampling-based search
algorithms widely used for online planning in sequential decision-making
domains and at the heart of many recent advances in artificial intelligence.
Understanding the behavior of MCTS agents is difficult for developers and users
due to the frequently large and complex search trees that result from the
simulation of many possible futures, their evaluations, and their
relationships. This paper presents our ongoing investigation into potential
explanations for the decision-making and behavior of MCTS. A weakness of MCTS
is that it constructs a highly selective tree and, as a result, can miss
crucial moves and fall into tactical traps. Full-width minimax search
constitutes the solution. We integrate shallow minimax search into the rollout
phase of multi-player MCTS and use process mining technique to explain agents'
strategies in 3v3 checkers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, AAAI 2025 PRL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-Step Offline Distillation of Diffusion-based Models via Koopman
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nimrod Berman, Ilan Naiman, Moshe Eliasof, Hedi Zisling, Omri Azencot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based generative models have demonstrated exceptional performance,
yet their iterative sampling procedures remain computationally expensive. A
prominent strategy to mitigate this cost is distillation, with offline
distillation offering particular advantages in terms of efficiency, modularity,
and flexibility. In this work, we identify two key observations that motivate a
principled distillation framework: (1) while diffusion models have been viewed
through the lens of dynamical systems theory, powerful and underexplored tools
can be further leveraged; and (2) diffusion models inherently impose
structured, semantically coherent trajectories in latent space. Building on
these observations, we introduce the Koopman Distillation Model KDM, a novel
offline distillation approach grounded in Koopman theory-a classical framework
for representing nonlinear dynamics linearly in a transformed space. KDM
encodes noisy inputs into an embedded space where a learned linear operator
propagates them forward, followed by a decoder that reconstructs clean samples.
This enables single-step generation while preserving semantic fidelity. We
provide theoretical justification for our approach: (1) under mild assumptions,
the learned diffusion dynamics admit a finite-dimensional Koopman
representation; and (2) proximity in the Koopman latent space correlates with
semantic similarity in the generated outputs, allowing for effective trajectory
alignment. Empirically, KDM achieves state-of-the-art performance across
standard offline distillation benchmarks, improving FID scores by up to 40% in
a single generation step. All implementation details and code for the
experimental setups are provided in our GitHub -
https://github.com/azencot-group/KDM, or in our project page -
https://sites.google.com/view/koopman-distillation-model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Iterative Formalization and Planning in Partially Observable
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liancheng Gong, Wang Zhu, Jesse Thomason, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using LLMs not to predict plans but to formalize an environment into the
Planning Domain Definition Language (PDDL) has been shown to improve
performance and control. Existing work focuses on fully observable
environments; we tackle the more realistic and challenging partially observable
environments that lack of complete, reliable information. We propose PDDLego+,
a framework to iteratively formalize, plan, grow, and refine PDDL
representations in a zero-shot manner, without needing access to any existing
trajectories. On two textual simulated environments, we show that PDDLego+
improves goal reaching success and exhibits robustness against problem
complexity. We also show that the domain knowledge captured after a successful
trial can benefit future tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Zhao, Xiaobao Wu, Cong-Duy Nguyen, Yanhao Jia, Meihuizi Jia, Yichao Feng, Luu Anh Tuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) can bridge the gap between large
language models (LLMs) and downstream tasks. However, PEFT has been proven
vulnerable to malicious attacks. Research indicates that poisoned LLMs, even
after PEFT, retain the capability to activate internalized backdoors when input
samples contain predefined triggers. In this paper, we introduce a novel
weak-to-strong unlearning algorithm to defend against backdoor attacks based on
feature alignment knowledge distillation, named W2SDefense. Specifically, we
first train a small-scale language model through full-parameter fine-tuning to
serve as the clean teacher model. Then, this teacher model guides the
large-scale poisoned student model in unlearning the backdoor, leveraging PEFT.
Theoretical analysis suggests that W2SDefense has the potential to enhance the
student model's ability to unlearn backdoor features, preventing the activation
of the backdoor. We conduct comprehensive experiments on three state-of-the-art
large language models and several different backdoor attack algorithms. Our
empirical results demonstrate the outstanding performance of W2SDefense in
defending against backdoor attacks without compromising model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Action Learning Requires Supervision in the Presence of
  Distractors <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00379v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00379v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Nikulin, Ilya Zisman, Denis Tarasov, Nikita Lyubaykin, Andrei Polubarov, Igor Kiselev, Vladislav Kurenkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, latent action learning, pioneered by Latent Action Policies (LAPO),
have shown remarkable pre-training efficiency on observation-only data,
offering potential for leveraging vast amounts of video available on the web
for embodied AI. However, prior work has focused on distractor-free data, where
changes between observations are primarily explained by ground-truth actions.
Unfortunately, real-world videos contain action-correlated distractors that may
hinder latent action learning. Using Distracting Control Suite (DCS) we
empirically investigate the effect of distractors on latent action learning and
demonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO
modification that improves the quality of latent actions by 8x, as measured by
linear probing. Importantly, we show that providing supervision with
ground-truth actions, as few as 2.5% of the full dataset, during latent action
learning improves downstream performance by 4.2x on average. Our findings
suggest that integrating supervision during Latent Action Models (LAM) training
is critical in the presence of distractors, challenging the conventional
pipeline of first learning LAM and only then decoding from latent to
ground-truth actions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025, Poster, Source code: https://github.com/dunnolab/laom</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BioCube: A Multimodal <span class="highlight-title">Dataset</span> for Biodiversity Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11568v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11568v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stylianos Stasinos, Martino Mensio, Elena Lazovik, Athanasios Trantas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biodiversity research requires complete and detailed information to study
ecosystem dynamics at different scales. Employing data-driven methods like
Machine Learning is getting traction in ecology and more specific biodiversity,
offering alternative modelling pathways. For these methods to deliver accurate
results there is the need for large, curated and multimodal datasets that offer
granular spatial and temporal resolutions. In this work, we introduce BioCube,
a multimodal, fine-grained global dataset for ecology and biodiversity
research. BioCube incorporates species observations through images, audio
recordings and descriptions, environmental DNA, vegetation indices,
agricultural, forest, land indicators, and high-resolution climate variables.
All observations are geospatially aligned under the WGS84 geodetic system,
spanning from 2000 to 2020. The dataset will become available at
https://huggingface.co/datasets/BioDT/BioCube while the acquisition and
processing code base at https://github.com/BioDT/bfm-data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to BiDS'25, 5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OATS: Outlier-Aware Pruning Through Sparse and Low Rank Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13652v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13652v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Zhang, Vardan Papyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent paradigm shift to large-scale foundation models has brought about
a new era for deep learning that, while has found great success in practice,
has also been plagued by prohibitively expensive costs in terms of high memory
consumption and compute. To mitigate these issues, there has been a concerted
effort in post-hoc neural network pruning techniques that do not require costly
retraining. Despite the considerable progress being made, existing methods
often exhibit a steady drop in model performance as the compression increases.
In this paper, we present a novel approach to compressing large transformers,
coined OATS, that utilizes the second moment information in the input
embeddings to decompose the model weights into a sum of sparse and low-rank
matrices. Without any retraining, OATS achieves state-of-the-art performance
when compressing models by up to $60\%$ on large language models such as
Llama-3 and Phi-3 and vision transformers such as ViT and DINOv2 while
delivering up to $1.37\times$ the CPU acceleration versus a model that was
comparably pruned.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Any-to-Any Learning in Computational Pathology via Triplet Multimodal
  <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qichen Sun, Zhengrui Guo, Rui Peng, Hao Chen, Jinzhuo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in computational pathology and artificial intelligence have
significantly enhanced the utilization of gigapixel whole-slide images and and
additional modalities (e.g., genomics) for pathological diagnosis. Although
deep learning has demonstrated strong potential in pathology, several key
challenges persist: (1) fusing heterogeneous data types requires sophisticated
strategies beyond simple concatenation due to high computational costs; (2)
common scenarios of missing modalities necessitate flexible strategies that
allow the model to learn robustly in the absence of certain modalities; (3) the
downstream tasks in CPath are diverse, ranging from unimodal to multimodal,
cnecessitating a unified model capable of handling all modalities. To address
these challenges, we propose ALTER, an any-to-any tri-modal pretraining
framework that integrates WSIs, genomics, and pathology reports. The term "any"
emphasizes ALTER's modality-adaptive design, enabling flexible pretraining with
any subset of modalities, and its capacity to learn robust, cross-modal
representations beyond WSI-centric approaches. We evaluate ALTER across
extensive clinical tasks including survival prediction, cancer subtyping, gene
mutation prediction, and report generation, achieving superior or comparable
performance to state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Stick-Breaking Attention: An Efficient Implementation and
  In-depth Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17980v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17980v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Tan, Songlin Yang, Aaron Courville, Rameswar Panda, Yikang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The self-attention mechanism traditionally relies on the softmax operator,
necessitating positional embeddings like RoPE, or position biases to account
for token order. But current methods using still face length generalisation
challenges. We investigate an alternative attention mechanism based on the
stick-breaking process in larger scale settings. The method works as follows:
For each token before the current, we determine a break point, which represents
the proportion of the stick, the weight of the attention, to allocate to the
current token. We repeat this on the remaining stick, until all tokens are
allocated a weight, resulting in a sequence of attention weights. This process
naturally incorporates recency bias, which has linguistic motivations for
grammar parsing. We study the implications of replacing the conventional
softmax-based attention mechanism with stick-breaking attention. We then
discuss implementation of numerically stable stick-breaking attention and adapt
Flash Attention to accommodate this mechanism. When used as a drop-in
replacement for current softmax+RoPE attention systems, we find that
stick-breaking attention performs competitively with current methods on length
generalisation and downstream tasks. Stick-breaking also performs well at
length generalisation, allowing a model trained with $2^{11}$ context window to
perform well at $2^{14}$ with perplexity improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Xue, Edward Moroshko, Feng Chen, Jingyu Sun, Steven McDonagh, Sotirios A. Tsaftaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Image diffusion models can produce undesirable content that
necessitates concept erasure. However, existing methods struggle with
under-erasure, leaving residual traces of targeted concepts, or over-erasure,
mistakenly eliminating unrelated but visually similar concepts. To address
these limitations, we introduce CRCE, a novel concept erasure framework that
leverages Large Language Models to identify both semantically related concepts
that should be erased alongside the target and distinct concepts that should be
preserved. By explicitly modelling coreferential and retained concepts
semantically, CRCE enables more precise concept removal, without unintended
erasure. Experiments demonstrate that CRCE outperforms existing methods on
diverse erasure tasks, including real-world object, person identities, and
abstract intellectual property characteristics. The constructed dataset
CorefConcept and the source code will be release upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>ing Large Language Models for Training-Free Non-Intrusive Load
  Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06330v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06330v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Xue, Xudong Wang, Xiaoling He, Shicheng Liu, Yi Wang, Guoming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-intrusive load monitoring (NILM) aims to disaggregate aggregate household
electricity consumption into individual appliance usage and thus enables more
effective energy management. While deep learning has advanced NILM, it remains
limited by its dependence on labeled data, restricted generalization, and lack
of explainability. This paper introduces the first prompt-based NILM framework
that leverages large language models (LLMs) with in-context learning. We design
and evaluate prompt strategies that integrate appliance features, timestamps
and contextual information, as well as representative time-series examples on
widely used open datasets. With optimized prompts, LLMs achieve competitive
state detection accuracy and demonstrate robust generalization without the need
for fine-tuning. LLMs also enhance explainability by providing clear,
human-readable explanations for their predictions. Our results show that LLMs
can reduce data requirements, improve adaptability, and provide transparent
energy disaggregation in NILM applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Hybrid Model Pruning through Loss Landscape Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10271v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10271v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Internò, Elena Raponi, Niki van Stein, Thomas Bäck, Markus Olhofer, Yaochu Jin, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the era of connectivity and unprecedented data generation expands,
collaborative intelligence emerges as a key driver for machine learning,
encouraging global-scale model development. Federated learning (FL) stands at
the heart of this transformation, enabling distributed systems to work
collectively on complex tasks while respecting strict constraints on privacy
and security. Despite its vast potential, specially in the age of complex
models, FL encounters challenges such as elevated communication costs,
computational constraints, and the heterogeneous data distributions. In this
context, we present AutoFLIP, a novel framework that optimizes FL through an
adaptive hybrid pruning approach, grounded in a federated loss exploration
phase. By jointly analyzing diverse non-IID client loss landscapes, AutoFLIP
efficiently identifies model substructures for pruning both at structured and
unstructured levels. This targeted optimization fosters a symbiotic
intelligence loop, reducing computational burdens and boosting model
performance on resource-limited devices for a more inclusive and democratized
model usage. Our extensive experiments across multiple datasets and FL tasks
show that AutoFLIP delivers quantifiable benefits: a 48.8% reduction in
computational overhead, a 35.5% decrease in communication costs, and a notable
improvement in global accuracy. By significantly reducing these overheads,
AutoFLIP offer the way for efficient FL deployment in real-world applications
for a scalable and broad applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08399v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08399v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Jou Huang, Rafik Hadfi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-report questionnaires have long been used to assess LLM personality
traits, yet they fail to capture behavioral nuances due to biases and
meta-knowledge contamination. This paper proposes a novel multi-observer
framework for personality trait assessments in LLM agents that draws on
informant-report methods in psychology. Instead of relying on self-assessments,
we employ multiple observer agents. Each observer is configured with a specific
relational context (e.g., family member, friend, or coworker) and engages the
subject LLM in dialogue before evaluating its behavior across the Big Five
dimensions. We show that these observer-report ratings align more closely with
human judgments than traditional self-reports and reveal systematic biases in
LLM self-assessments. We also found that aggregating responses from 5 to 7
observers reduces systematic biases and achieves optimal reliability. Our
results highlight the role of relationship context in perceiving personality
and demonstrate that a multi-observer paradigm offers a more reliable,
context-sensitive approach to evaluating LLM personality traits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanhao Jia, Xinyi Wu, Hao Li, Qinglin Zhang, Yuxiao Hu, Shuai Zhao, Wenqi Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In AI-facilitated teaching, leveraging various query styles to interpret
abstract text descriptions is crucial for ensuring high-quality teaching.
However, current retrieval models primarily focus on natural text-image
retrieval, making them insufficiently tailored to educational scenarios due to
the ambiguities in the retrieval process. In this paper, we propose a diverse
expression retrieval task tailored to educational scenarios, supporting
retrieval based on multiple query styles and expressions. We introduce the STEM
Education Retrieval Dataset (SER), which contains over 24,000 query pairs of
different styles, and the Uni-Retrieval, an efficient and style-diversified
retrieval vision-language model based on prompt tuning. Uni-Retrieval extracts
query style features as prototypes and builds a continuously updated Prompt
Bank containing prompt tokens for diverse queries. This bank can updated during
test time to represent domain-specific knowledge for different subject
retrieval scenarios. Our framework demonstrates scalability and robustness by
dynamically retrieving prompt tokens based on prototype similarity, effectively
facilitating learning for unknown queries. Experimental results indicate that
Uni-Retrieval outperforms existing retrieval models in most retrieval tasks.
This advancement provides a scalable and precise solution for diverse
educational needs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Continuous Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Sun, Yi Jiang, Tao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in continuous generative models, including multi-step
approaches like diffusion and flow-matching (typically requiring 8-1000
sampling steps) and few-step methods such as consistency models (typically 1-8
steps), have demonstrated impressive generative performance. However, existing
work often treats these approaches as distinct paradigms, resulting in separate
training and sampling methodologies. We introduce a unified framework for
training, sampling, and analyzing these models. Our implementation, the Unified
Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves
state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a
675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID
in 20 steps and a few-step model reaching 1.42 FID in just 2 steps.
Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at
250 steps) improves performance to 1.06 FID in only 40 steps. Code is available
at: https://github.com/LINs-lab/UCGM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/LINs-lab/UCGM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younghyun Kim, Jongheon Jeong, Sangkyung Kwak, Kyungmin Lee, Juho Lee, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning robust representations from data often requires scale, which has led
to the success of recent zero-shot models such as CLIP. However, the obtained
robustness can easily be deteriorated when these models are fine-tuned on other
downstream tasks (e.g., of smaller scales). Previous works often interpret this
phenomenon in the context of domain shift, developing fine-tuning methods that
aim to preserve the original domain as much as possible. However, in a
different context, fine-tuned models with limited data are also prone to
learning features that are spurious to humans, such as background or texture.
In this paper, we propose StarFT (Spurious Textual Alignment Regularization), a
novel framework for fine-tuning zero-shot models to enhance robustness by
preventing them from learning spuriosity. We introduce a regularization that
aligns the output distribution for spuriosity-injected labels with the original
zero-shot model, ensuring that the model is not induced to extract irrelevant
features further from these descriptions. We leverage recent language models to
get such spuriosity-injected labels by generating alternative textual
descriptions that highlight potentially confounding features. Extensive
experiments validate the robust generalization of StarFT and its emerging
properties: zero-shot group robustness and improved zero-shot classification.
Notably, StarFT boosts both worst-group and average accuracy by 14.30% and
3.02%, respectively, in the Waterbirds group shift scenario, where other robust
fine-tuning baselines show even degraded performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCAI 2025; Code is available at https://github.com/alinlab/StarFT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Debate Only When Necessary: Adaptive Multiagent Collaboration for
  Efficient LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sugyeong Eo, Hyeonseok Moon, Evelyn Hayoon Zi, Chanjun Park, Heuiseok Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiagent collaboration has emerged as a promising framework for enhancing
the reasoning capabilities of large language models (LLMs). Despite
improvements in reasoning, the approach introduces substantial computational
overhead resulting from iterative agent interactions. Furthermore, engaging in
unnecessary debates increases the risk of generating erroneous responses. To
address these challenges, we propose Debate Only When Necessary (DOWN), an
adaptive multiagent debate framework that selectively activates debate based on
the confidence score of the agent's initial response. Debate is activated only
for queries requiring further deliberation, during which agents refine their
outputs by referencing peer responses and associated confidence scores.
Evaluations on benchmarks show that DOWN improves efficiency by up to six times
while preserving or even outperforming the performance of existing methods.
Further analysis indicates that DOWN effectively mitigates the risk of error
propagation stemming from the unnecessary debate process. These findings
demonstrate the effectiveness of our approach in delivering high-performance
LLM solutions at a lower computational cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SQLong: Enhanced NL2SQL for Longer Contexts with LLMs <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dai Quoc Nguyen, Cong Duy Vu Hoang, Duy Vu, Gioacchino Tangari, Thanh Tien Vu, Don Dharmasiri, Yuan-Fang Li, Long Duong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-weight large language models (LLMs) have significantly advanced
performance in the Natural Language to SQL (NL2SQL) task. However, their
effectiveness diminishes when dealing with large database schemas, as the
context length increases. To address this limitation, we present SQLong, a
novel and efficient data augmentation framework designed to enhance LLM
performance in long-context scenarios for the NL2SQL task. SQLong generates
augmented datasets by extending existing database schemas with additional
synthetic CREATE TABLE commands and corresponding data rows, sampled from
diverse schemas in the training data. This approach effectively simulates
long-context scenarios during finetuning and evaluation. Through experiments on
the Spider and BIRD datasets, we demonstrate that LLMs finetuned with
SQLong-augmented data significantly outperform those trained on standard
datasets. These imply SQLong's practical implementation and its impact on
improving NL2SQL capabilities in real-world settings with complex database
schemas.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Table Representation Learning Workshop at ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient and Scalable Neural Symbolic Search for Knowledge Graph
  Complex Query Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08155v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08155v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhi Fei, Zihao Wang, hang Yin, Shukai Zhao, Wei Zhang, Yangqiu Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex Query Answering (CQA) aims to retrieve answer sets for complex
logical formulas from incomplete knowledge graphs, which is a crucial yet
challenging task in knowledge graph reasoning. While neuro-symbolic search
utilized neural link predictions achieve superior accuracy, they encounter
significant complexity bottlenecks: (i) Data complexity typically scales
quadratically with the number of entities in the knowledge graph, and (ii)
Query complexity becomes NP-hard for cyclic queries. Consequently, these
approaches struggle to effectively scale to larger knowledge graphs and more
complex queries. To address these challenges, we propose an efficient and
scalable symbolic search framework. First, we propose two constraint strategies
to compute neural logical indices to reduce the domain of variables, thereby
decreasing the data complexity of symbolic search. Additionally, we introduce
an approximate algorithm based on local search to tackle the NP query
complexity of cyclic queries. Experiments on various CQA benchmarks demonstrate
that our framework reduces the computational load of symbolic methods by 90\%
while maintaining nearly the same performance, thus alleviating both efficiency
and scalability issues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and
  Interpretability Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.12324v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.12324v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengying Yuan, Wenhao Wang, Zixuan Wang, Yujie Huang, Kangli Wei, Fei Li, Chong Teng, Donghong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Inference (NLI) is a fundamental task in natural language
processing. While NLI has developed many sub-directions such as sentence-level
NLI, document-level NLI and cross-lingual NLI, Cross-Document Cross-Lingual NLI
(CDCL-NLI) remains largely unexplored. In this paper, we propose a novel
paradigm: CDCL-NLI, which extends traditional NLI capabilities to
multi-document, multilingual scenarios. To support this task, we construct a
high-quality CDCL-NLI dataset including 25,410 instances and spanning 26
languages. To address the limitations of previous methods on CDCL-NLI task, we
further propose an innovative method that integrates RST-enhanced graph fusion
with interpretability-aware prediction. Our approach leverages RST (Rhetorical
Structure Theory) within heterogeneous graph neural networks for cross-document
context modeling, and employs a structure-aware semantic alignment based on
lexical chains for cross-lingual understanding. For NLI interpretability, we
develop an EDU (Elementary Discourse Unit)-level attribution framework that
produces extractive explanations. Extensive experiments demonstrate our
approach's superior performance, achieving significant improvements over both
conventional NLI models as well as large language models. Our work sheds light
on the study of NLI and will bring research interest on cross-document
cross-lingual context understanding, hallucination elimination and
interpretability inference. Our code and datasets are available at
\href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-link} for peer
review.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12442v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12442v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liwen Wang, Wenxuan Wang, Shuai Wang, Zongjie Li, Zhenlan Ji, Zongyi Lyu, Daoyuan Wu, Shing-Chi Cheung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has led to the
emergence of Multi-Agent Systems (MAS) to perform complex tasks through
collaboration. However, the intricate nature of MAS, including their
architecture and agent interactions, raises significant concerns regarding
intellectual property (IP) protection. In this paper, we introduce MASLEAK, a
novel attack framework designed to extract sensitive information from MAS
applications. MASLEAK targets a practical, black-box setting, where the
adversary has no prior knowledge of the MAS architecture or agent
configurations. The adversary can only interact with the MAS through its public
API, submitting attack query $q$ and observing outputs from the final agent.
Inspired by how computer worms propagate and infect vulnerable network hosts,
MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain
responses from each MAS agent that reveal a full set of proprietary components,
including the number of agents, system topology, system prompts, task
instructions, and tool usages. We construct the first synthetic dataset of MAS
applications with 810 applications and also evaluate MASLEAK against real-world
MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in
extracting MAS IP, with an average attack success rate of 87% for system
prompts and task instructions, and 92% for system architecture in most cases.
We conclude by discussing the implications of our findings and the potential
defenses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncovering Untapped Potential in Sample-Efficient World Model Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lior Cohen, Kaixin Wang, Bingyi Kang, Uri Gadot, Shie Mannor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World model (WM) agents enable sample-efficient reinforcement learning by
learning policies entirely from simulated experience. However, existing
token-based world models (TBWMs) are limited to visual inputs and discrete
actions, restricting their adoption and applicability. Moreover, although both
intrinsic motivation and prioritized WM replay have shown promise in improving
WM performance and generalization, they remain underexplored in this setting,
particularly in combination. We introduce Simulus, a highly modular TBWM agent
that integrates (1) a modular multi-modality tokenization framework, (2)
intrinsic motivation, (3) prioritized WM replay, and (4)
regression-as-classification for reward and return prediction. Simulus achieves
state-of-the-art sample efficiency for planning-free WMs across three diverse
benchmarks. Ablation studies reveal the individual contribution of each
component while highlighting their synergy. Our code and model weights are
publicly available at https://github.com/leor-c/Simulus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge-Guided <span class="highlight-title">Prompt</span> Learning for Request Quality Assurance in Public
  Code <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Li, Xinchun Yu, Xinyu Chen, Peng Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Public Code Review (PCR) is developed in the Software Question Answering
(SQA) community, assisting developers in exploring high-quality and efficient
review services. Current methods on PCR mainly focus on the reviewer's
perspective, including finding a capable reviewer, predicting comment quality,
and recommending/generating review comments. However, it is not well studied
that how to satisfy the review necessity requests posted by developers which
can increase their visibility, which in turn acts as a prerequisite for better
review responses. To this end, we propose Knowledge-guided Prompt learning for
Public Code Review (KP-PCR) to achieve developer-based code review request
quality assurance (i.e., predicting request necessity and recommending tags
subtask). Specifically, we reformulate the two subtasks via 1) text prompt
tuning which converts both of them into a Masked Language Model (MLM) by
constructing prompt templates using hard prompt; and 2) knowledge and code
prefix tuning which introduces knowledge guidance from fine-tuned large
language models by soft prompt, and uses program dependence graph to
characterize code snippets. Finally, both of the request necessity prediction
and tag recommendation subtasks output predicted results through an answer
engineering module. In addition, we further analysis the time complexity of our
KP-PCR that has lightweight prefix based the operation of introducing knowledge
guidance. Experimental results on the PCR dataset for the period 2011-2023
demonstrate that our KP-PCR outperforms baselines by 2.3%-8.4% in the request
necessity prediction and by 1.4%-6.9% in the tag recommendation. The code
implementation is released at https://github.com/WUT-IDEA/KP-PCR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 5 images, 12 tables, Manuscript revision submitted to a
  journal (2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-driven Personalized Privacy Assistants: a Systematic Literature
  <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07693v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07693v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Morel, Leonardo Iwaya, Simone Fischer-Hübner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, several personalized assistants based on AI have been
researched and developed to help users make privacy-related decisions. These
AI-driven Personalized Privacy Assistants (AI-driven PPAs) can provide
significant benefits for users, who might otherwise struggle with making
decisions about their personal data in online environments that often overload
them with different privacy decision requests. So far, no studies have
systematically investigated the emerging topic of AI-driven PPAs, classifying
their underlying technologies, architecture and features, including decision
types or the accuracy of their decisions. To fill this gap, we present a
Systematic Literature Review (SLR) to map the existing solutions found in the
scientific literature, which allows reasoning about existing approaches and
open challenges for this research field. We screened several hundred unique
research papers over the recent years (2013-2025), constructing a
classification from 41 included papers. As a result, this SLR reviews several
aspects of existing research on AI-driven PPAs in terms of types of
publications, contributions, methodological quality, and other quantitative
insights. Furthermore, we provide a comprehensive classification for AI-driven
PPAs, delving into their architectural choices, system contexts, types of AI
used, data sources, types of decisions, and control over decisions, among other
facets. Based on our SLR, we further underline the research gaps and challenges
and formulate recommendations for the design and development of AI-driven PPAs
as well as avenues for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on
  Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12767v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12767v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumin Jo, Junseong Choi, Jiho Kim, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have combined Large Language Models (LLMs) with Knowledge
Graphs (KGs) to enhance reasoning, improving inference accuracy without
additional training while mitigating hallucination. However, existing
frameworks still suffer two practical drawbacks: they must be re-tuned whenever
the KG or reasoning task changes, and they depend on a single, high-capacity
LLM for reliable (i.e., trustworthy) reasoning. To address this, we introduce
R2-KG, a plug-and-play, dual-agent framework that separates reasoning into two
roles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor
(a high-capacity LLM) that makes final judgments. This design is cost-efficient
for LLM inference while still maintaining strong reasoning accuracy.
Additionally, R2-KG employs an Abstention mechanism, generating answers only
when sufficient evidence is collected from KG, which significantly enhances
reliability. Experiments across five diverse benchmarks show that R2-KG
consistently outperforms baselines in both accuracy and reliability, regardless
of the inherent capability of LLMs used as the Operator. Further experiments
reveal that the single-agent version of R2-KG, equipped with a strict
self-consistency strategy, achieves significantly higher-than-baseline
reliability with reduced inference cost but increased abstention rate in
complex KGs. Our findings establish R2-KG as a flexible and cost-effective
solution for KG-based reasoning, reducing reliance on high-capacity LLMs while
ensuring trustworthy inference. The code is available at
https://github.com/ekrxjwh2009/R2-KG/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HICD: Hallucination-Inducing via Attention Dispersion for Contrastive
  Decoding to Mitigate Hallucinations in Large Language Models <span class="chip">ACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12908v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12908v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyan Jiang, Hang Ye, Yongxin Zhu, Xiaoying Zheng, Zikang Chen, Jun Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often generate hallucinations, producing outputs
that are contextually inaccurate or factually incorrect. We introduce HICD, a
novel method designed to induce hallucinations for contrastive decoding to
mitigate hallucinations. Unlike existing contrastive decoding methods, HICD
selects attention heads crucial to the model's prediction as inducing heads,
then induces hallucinations by dispersing attention of these inducing heads and
compares the hallucinated outputs with the original outputs to obtain the final
result. Our approach significantly improves performance on tasks requiring
contextual faithfulness, such as context completion, reading comprehension, and
question answering. It also improves factuality in tasks requiring accurate
knowledge recall. We demonstrate that our inducing heads selection and
attention dispersion method leads to more "contrast-effective" hallucinations
for contrastive decoding, outperforming other hallucination-inducing methods.
Our findings provide a promising strategy for reducing hallucinations by
inducing hallucinations in a controlled manner, enhancing the performance of
LLMs in a wide range of tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL2025 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02429v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02429v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuo An, Yunjiao Zhou, Han Zou, Jianfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel in textual and visual tasks but often
produce outputs that defy physical laws when dealing with physical-world
reasoning tasks. Inspired by human cognition, where perception is fundamental
to reasoning, we explore augmenting LLMs with enhanced perception abilities
using Internet of Things (IoT) sensor data and pertinent knowledge for
IoT-sensory task reasoning in the physical world. In this work, we
systematically study LLMs' capability to address real-world IoT-sensory tasks
by augmenting their perception and knowledge base, and then propose a unified
framework, IoT-LLM, to enhance such capability. In IoT-LLM, we customize three
steps for LLMs: preprocessing IoT data into formats amenable to LLMs, expanding
their understanding via IoT-oriented retrieval-augmented generation based on
in-context learning and activating their commonsense knowledge through
chain-of-thought prompting and specialized role definitions. We design a new
benchmark comprising five real-world tasks with varying data types and
reasoning complexities to evaluate the performance of IoT-LLM. Experimental
results on six LLMs reveal that IoT-LLM significantly improves the performance
of IoT-sensory task reasoning of LLMs, with models like GPT-4o-mini showing a
49.4% average improvement over previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 11 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grouping First, Attending Smartly: Training-Free Acceleration for
  Diffusion <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sucheng Ren, Qihang Yu, Ju He, Alan Yuille, Liang-Chieh Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based Transformers have demonstrated impressive generative
capabilities, but their high computational costs hinder practical deployment,
for example, generating an $8192\times 8192$ image can take over an hour on an
A100 GPU. In this work, we propose GRAT (\textbf{GR}ouping first,
\textbf{AT}tending smartly), a training-free attention acceleration strategy
for fast image and video generation without compromising output quality. The
key insight is to exploit the inherent sparsity in learned attention maps
(which tend to be locally focused) in pretrained Diffusion Transformers and
leverage better GPU parallelism. Specifically, GRAT first partitions contiguous
tokens into non-overlapping groups, aligning both with GPU execution patterns
and the local attention structures learned in pretrained generative
Transformers. It then accelerates attention by having all query tokens within
the same group share a common set of attendable key and value tokens. These key
and value tokens are further restricted to structured regions, such as
surrounding blocks or criss-cross regions, significantly reducing computational
overhead (e.g., attaining a \textbf{35.8$\times$} speedup over full attention
when generating $8192\times 8192$ images) while preserving essential attention
patterns and long-range context. We validate GRAT on pretrained Flux and
HunyuanVideo for image and video generation, respectively. In both cases, GRAT
achieves substantially faster inference without any fine-tuning, while
maintaining the performance of full attention. We hope GRAT will inspire future
research on accelerating Diffusion Transformers for scalable visual generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website at oliverrensu.github.io/project/GRAT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emerging Properties in Unified Multimodal <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, Haoqi Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unifying multimodal understanding and generation has shown impressive
capabilities in cutting-edge proprietary systems. In this work, we introduce
BAGEL, an open0source foundational model that natively supports multimodal
understanding and generation. BAGEL is a unified, decoder0only model pretrained
on trillions of tokens curated from large0scale interleaved text, image, video,
and web data. When scaled with such diverse multimodal interleaved data, BAGEL
exhibits emerging capabilities in complex multimodal reasoning. As a result, it
significantly outperforms open-source unified models in both multimodal
generation and understanding across standard benchmarks, while exhibiting
advanced multimodal reasoning abilities such as free-form image manipulation,
future frame prediction, 3D manipulation, and world navigation. In the hope of
facilitating further opportunities for multimodal research, we share the key
findings, pretraining details, data creation protocal, and release our code and
checkpoints to the community. The project page is at https://bagel-ai.org/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal
  Understanding and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Tian, Mingfei Gao, Mingze Xu, Jiaming Hu, Jiasen Lu, Zuxuan Wu, Yinfei Yang, Afshin Dehghan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce UniGen, a unified multimodal large language model (MLLM) capable
of image understanding and generation. We study the full training pipeline of
UniGen from a data-centric perspective, including multi-stage pre-training,
supervised fine-tuning, and direct preference optimization. More importantly,
we propose a new Chain-of-Thought Verification (CoT-V) strategy for test-time
scaling, which significantly boosts UniGen's image generation quality using a
simple Best-of-N test-time strategy. Specifically, CoT-V enables UniGen to act
as both image generator and verifier at test time, assessing the semantic
alignment between a text prompt and its generated image in a step-by-step CoT
manner. Trained entirely on open-source datasets across all stages, UniGen
achieves state-of-the-art performance on a range of image understanding and
generation benchmarks, with a final score of 0.78 on GenEval and 85.19 on
DPG-Bench. Through extensive ablation studies, our work provides actionable
insights and addresses key challenges in the full life cycle of building
unified MLLMs, contributing meaningful directions to the future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two Experts Are All You Need for Steering Thinking: Reinforcing
  Cognitive Effort in MoE Reasoning Models Without Additional Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengru Wang, Xingyu Chen, Yue Wang, Zhiwei He, Jiahao Xu, Tian Liang, Qiuzhi Liu, Yunzhi Yao, Wenxuan Wang, Ruotian Ma, Haitao Mi, Ningyu Zhang, Zhaopeng Tu, Xiaolong Li, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)
have achieved impressive reasoning capabilities by selectively activating
experts to facilitate structured cognitive processes. Despite notable advances,
existing reasoning models often suffer from cognitive inefficiencies like
overthinking and underthinking. To address these limitations, we introduce a
novel inference-time steering methodology called Reinforcing Cognitive Experts
(RICE), designed to improve reasoning performance without additional training
or complex heuristics. Leveraging normalized Pointwise Mutual Information
(nPMI), we systematically identify specialized experts, termed ''cognitive
experts'' that orchestrate meta-level reasoning operations characterized by
tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs
(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning
benchmarks demonstrate noticeable and consistent improvements in reasoning
accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our
lightweight approach substantially outperforms prevalent reasoning-steering
techniques, such as prompt design and decoding constraints, while preserving
the model's general instruction-following skills. These results highlight
reinforcing cognitive experts as a promising, practical, and interpretable
direction to enhance cognitive efficiency within advanced reasoning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visionary-R1: Mitigating Shortcuts in Visual Reasoning with
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaer Xia, Yuhang Zang, Peng Gao, Yixuan Li, Kaiyang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning general-purpose reasoning capabilities has long been a challenging
problem in AI. Recent research in large language models (LLMs), such as
DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can
enable pre-trained LLMs to develop reasoning capabilities using simple
question-answer pairs. In this paper, we aim to train visual language models
(VLMs) to perform reasoning on image data through reinforcement learning and
visual question-answer pairs, without any explicit chain-of-thought (CoT)
supervision. Our findings indicate that simply applying reinforcement learning
to a VLM -- by prompting the model to produce a reasoning chain before
providing an answer -- can lead the model to develop shortcuts from easy
questions, thereby reducing its ability to generalize across unseen data
distributions. We argue that the key to mitigating shortcut learning is to
encourage the model to interpret images prior to reasoning. Therefore, we train
the model to adhere to a caption-reason-answer output format: initially
generating a detailed caption for an image, followed by constructing an
extensive reasoning chain. When trained on 273K CoT-free visual question-answer
pairs and using only reinforcement learning, our model, named Visionary-R1,
outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and
Gemini-1.5-Pro, on multiple visual reasoning benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training-Free Watermarking for Autoregressive Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Tong, Zihao Pan, Shuai Yang, Kaiyang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Invisible image watermarking can protect image ownership and prevent
malicious misuse of visual generative models. However, existing generative
watermarking methods are mainly designed for diffusion models while
watermarking for autoregressive image generation models remains largely
underexplored. We propose IndexMark, a training-free watermarking framework for
autoregressive image generation models. IndexMark is inspired by the redundancy
property of the codebook: replacing autoregressively generated indices with
similar indices produces negligible visual differences. The core component in
IndexMark is a simple yet effective match-then-replace method, which carefully
selects watermark tokens from the codebook based on token similarity, and
promotes the use of watermark tokens through token replacement, thereby
embedding the watermark without affecting the image quality. Watermark
verification is achieved by calculating the proportion of watermark tokens in
generated images, with precision further improved by an Index Encoder.
Furthermore, we introduce an auxiliary validation scheme to enhance robustness
against cropping attacks. Experiments demonstrate that IndexMark achieves
state-of-the-art performance in terms of image quality and verification
accuracy, and exhibits robustness against various perturbations, including
cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG
compression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniCTokens: Boosting Personalized Understanding and Generation via
  Unified Concept Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichuan An, Sihan Yang, Renrui Zhang, Zijun Shen, Ming Lu, Gaole Dai, Hao Liang, Ziyu Guo, Shilin Yan, Yulin Luo, Bocheng Zou, Chaoqun Yang, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized models have demonstrated remarkable success in understanding and
generating concepts provided by users. However, existing methods use separate
concept tokens for understanding and generation, treating these tasks in
isolation. This may result in limitations for generating images with complex
prompts. For example, given the concept $\langle bo\rangle$, generating
"$\langle bo\rangle$ wearing its hat" without additional textual descriptions
of its hat. We call this kind of generation personalized knowledge-driven
generation. To address the limitation, we present UniCTokens, a novel framework
that effectively integrates personalized information into a unified vision
language model (VLM) for understanding and generation. UniCTokens trains a set
of unified concept tokens to leverage complementary semantics, boosting two
personalized tasks. Moreover, we propose a progressive training strategy with
three stages: understanding warm-up, bootstrapping generation from
understanding, and deepening understanding from generation to enhance mutual
benefits between both tasks. To quantitatively evaluate the unified VLM
personalization, we present UnifyBench, the first benchmark for assessing
concept understanding, concept generation, and knowledge-driven generation.
Experimental results on UnifyBench indicate that UniCTokens shows competitive
performance compared to leading methods in concept understanding, concept
generation, and achieving state-of-the-art results in personalized
knowledge-driven generation. Our research demonstrates that enhanced
understanding improves generation, and the generation process can yield
valuable insights into understanding. Our code and dataset will be released at:
\href{https://github.com/arctanxarc/UniCTokens}{https://github.com/arctanxarc/UniCTokens}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of
  Cross-Modal Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilin Ye, Junchao Huang, Xingchen Zeng, Jiazhi Xia, Wei Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal embeddings form the foundation for multi-modal models. However,
visualization methods for interpreting cross-modal embeddings have been
primarily confined to traditional dimensionality reduction (DR) techniques like
PCA and t-SNE. These DR methods primarily focus on feature distributions within
a single modality, whilst failing to incorporate metrics (e.g., CLIPScore)
across multiple modalities.This paper introduces AKRMap, a new DR technique
designed to visualize cross-modal embeddings metric with enhanced accuracy by
learning kernel regression of the metric landscape in the projection space.
Specifically, AKRMap constructs a supervised projection network guided by a
post-projection kernel regression loss, and employs adaptive generalized
kernels that can be jointly optimized with the projection. This approach
enables AKRMap to efficiently generate visualizations that capture complex
metric distributions, while also supporting interactive features such as zoom
and overlay for deeper exploration. Quantitative experiments demonstrate that
AKRMap outperforms existing DR methods in generating more accurate and
trustworthy visualizations. We further showcase the effectiveness of AKRMap in
visualizing and comparing cross-modal embeddings for text-to-image models. Code
and demo are available at https://github.com/yilinye/AKRMap.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmoGist: Efficient In-Context Learning for Visual Emotion Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronald Seoh, Dan Goldwasser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce EmoGist, a training-free, in-context learning
method for performing visual emotion classification with LVLMs. The key
intuition of our approach is that context-dependent definition of emotion
labels could allow more accurate predictions of emotions, as the ways in which
emotions manifest within images are highly context dependent and nuanced.
EmoGist pre-generates multiple explanations of emotion labels, by analyzing the
clusters of example images belonging to each category. At test time, we
retrieve a version of explanation based on embedding similarity, and feed it to
a fast VLM for classification. Through our experiments, we show that EmoGist
allows up to 13 points improvement in micro F1 scores with the multi-label
Memotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Words: Multimodal LLM Knows When to Speak 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikai Liao, Yi Ouyang, Yi-Lun Lee, Chen-Ping Yu, Yi-Hsuan Tsai, Zhaozheng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language model (LLM)-based chatbots have demonstrated strong
capabilities in generating coherent and contextually relevant responses, they
often struggle with understanding when to speak, particularly in delivering
brief, timely reactions during ongoing conversations. This limitation arises
largely from their reliance on text input, lacking the rich contextual cues in
real-world human dialogue. In this work, we focus on real-time prediction of
response types, with an emphasis on short, reactive utterances that depend on
subtle, multimodal signals across vision, audio, and text. To support this, we
introduce a new multimodal dataset constructed from real-world conversational
videos, containing temporally aligned visual, auditory, and textual streams.
This dataset enables fine-grained modeling of response timing in dyadic
interactions. Building on this dataset, we propose MM-When2Speak, a multimodal
LLM-based model that adaptively integrates visual, auditory, and textual
context to predict when a response should occur, and what type of response is
appropriate. Experiments show that MM-When2Speak significantly outperforms
state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x
improvement in response timing accuracy over leading commercial LLMs. These
results underscore the importance of multimodal inputs for producing timely,
natural, and engaging conversational AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/lzk901372/MM-When2Speak</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided
  Design Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna C. Doris, Md Ferdous Alam, Amin Heyrani Nobari, Faez Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient creation of accurate and editable 3D CAD models is critical in
engineering design, significantly impacting cost and time-to-market in product
innovation. Current manual workflows remain highly time-consuming and demand
extensive user expertise. While recent developments in AI-driven CAD generation
show promise, existing models are limited by incomplete representations of CAD
operations, inability to generalize to real-world images, and low output
accuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model
(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)
directly from visual input. Leveraging a novel dataset that we
created--GenCAD-Code, consisting of over 163k CAD-model image and code
pairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and
Qwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in
3D solid similarity. Notably, our VLM demonstrates some signs of
generalizability, successfully generating CAD code from real-world images and
executing CAD operations unseen during fine-tuning. The performance and
adaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code
to streamline CAD workflows for engineers and designers. CAD-Coder is publicly
available at: https://github.com/anniedoris/CAD-Coder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Ma, Weiming Ren, Yiming Jia, Zhuofeng Li, Ping Nie, Ge Zhang, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models (LMMs) have recently emerged as a powerful tool for
long video understanding (LVU), prompting the development of standardized LVU
benchmarks to evaluate their performance. However, our investigation reveals a
rather sober lesson for existing LVU benchmarks. First, most existing
benchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation
results are inflated due to the possibility of guessing the correct answer;
Second, a significant portion of questions in these benchmarks have strong
priors to allow models to answer directly without even reading the input video.
For example, Gemini-1.5-Pro can achieve over 50\% accuracy given a random frame
from a long video on Video-MME. We also observe that increasing the number of
frames does not necessarily lead to improvement on existing benchmarks, which
is counterintuitive. As a result, the validity and robustness of current LVU
benchmarks are undermined, impeding a faithful assessment of LMMs' long-video
understanding capability. To tackle this problem, we propose VideoEval-Pro, a
realistic LVU benchmark containing questions with open-ended short-answer,
which truly require understanding the entire video. VideoEval-Pro assesses both
segment-level and full-video understanding through perception and reasoning
tasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the
following findings: (1) video LMMs show drastic performance ($>$25\%) drops on
open-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do
not lead to higher open-ended scores on VideoEval-Pro; (3) compared to other
MCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input
frames. Our results show that VideoEval-Pro offers a more realistic and
reliable measure of long video understanding, providing a clearer view of
progress in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dataset: https://huggingface.co/datasets/TIGER-Lab/VideoEval-Pro,
  Project Webpage: https://tiger-ai-lab.github.io/VideoEval-Pro</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Precision Quantization for Efficient and Accurate Deep Neural
  Networks Inference <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomer Gafni, Asaf Karnieli, Yair Hanani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have achieved state-of-the-art results in a wide range
of applications, from natural language processing and computer vision to speech
recognition. However, as tasks become increasingly complex, model sizes
continue to grow, posing challenges in latency and memory efficiency. To meet
these constraints, post-training quantization has emerged as a promising
solution. In this paper, we propose a novel hardware-efficient quantization and
inference scheme that exploits hardware advantages with minimal accuracy
degradation. Specifically, we introduce a W4A8 scheme, where weights are
quantized and stored using 4-bit integer precision, and inference computations
are performed using 8-bit floating-point arithmetic, demonstrating significant
speedups and improved memory utilization compared to 16-bit operations,
applicable on various modern accelerators. To mitigate accuracy loss, we
develop a novel quantization algorithm, dubbed Dual Precision Quantization
(DPQ), that leverages the unique structure of our scheme without introducing
additional inference overhead. Experimental results demonstrate improved
performance (i.e., increased throughput) while maintaining tolerable accuracy
degradation relative to the full-precision model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at eLVM Workshop, CVPR, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A General Framework for Group Sparsity in Hyperspectral Unmixing Using
  Endmember Bundles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokul Bhusal, Yifei Lou, Cristina Garcia-Cardona, Ekaterina Merkurjev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to low spatial resolution, hyperspectral data often consists of mixtures
of contributions from multiple materials. This limitation motivates the task of
hyperspectral unmixing (HU), a fundamental problem in hyperspectral imaging. HU
aims to identify the spectral signatures (\textit{endmembers}) of the materials
present in an observed scene, along with their relative proportions
(\textit{fractional abundance}) in each pixel. A major challenge lies in the
class variability in materials, which hinders accurate representation by a
single spectral signature, as assumed in the conventional linear mixing model.
Moreover, To address this issue, we propose using group sparsity after
representing each material with a set of spectral signatures, known as
endmember bundles, where each group corresponds to a specific material. In
particular, we develop a bundle-based framework that can enforce either
inter-group sparsity or sparsity within and across groups (SWAG) on the
abundance coefficients. Furthermore, our framework offers the flexibility to
incorporate a variety of sparsity-promoting penalties, among which the
transformed $\ell_1$ (TL1) penalty is a novel regularization in the HU
literature. Extensive experiments conducted on both synthetic and real
hyperspectral data demonstrate the effectiveness and superiority of the
proposed approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large
  Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fnu Mohbat, Mohammed J Zaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) and the abundance of food
data have resulted in studies to improve food understanding using LLMs. Despite
several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there
has been limited research on integrating food related KGs with LLMs. We
introduce KERL, a unified system that leverages food KGs and LLMs to provide
personalized food recommendations and generates recipes with associated
micro-nutritional information. Given a natural language question, KERL extracts
entities, retrieves subgraphs from the KG, which are then fed into the LLM as
context to select the recipes that satisfy the constraints. Next, our system
generates the cooking steps and nutritional information for each recipe. To
evaluate our approach, we also develop a benchmark dataset by curating recipe
related questions, combined with constraints and personal preferences. Through
extensive experiments, we show that our proposed KG-augmented LLM significantly
outperforms existing approaches, offering a complete and coherent solution for
food recommendation, recipe generation, and nutritional analysis. Our code and
benchmark datasets are publicly available at
https://github.com/mohbattharani/KERL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Reconstruction from Sketches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhimanyu Talwar, Julien Laasri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of reconstructing a 3D scene from multiple sketches.
We propose a pipeline which involves (1) stitching together multiple sketches
through use of correspondence points, (2) converting the stitched sketch into a
realistic image using a CycleGAN, and (3) estimating that image's depth-map
using a pre-trained convolutional neural network based architecture called
MegaDepth. Our contribution includes constructing a dataset of image-sketch
pairs, the images for which are from the Zurich Building Database, and sketches
have been generated by us. We use this dataset to train a CycleGAN for our
pipeline's second step. We end up with a stitching process that does not
generalize well to real drawings, but the rest of the pipeline that creates a
3D reconstruction from a single sketch performs quite well on a wide variety of
drawings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 8 figures, paper dated December 12, 2018</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instance Segmentation for Point Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhimanyu Talwar, Julien Laasri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently proposed neural network architectures like PointNet [QSMG16] and
PointNet++ [QYSG17] have made it possible to apply Deep Learning to 3D point
sets. The feature representations of shapes learned by these two networks
enabled training classifiers for Semantic Segmentation, and more recently for
Instance Segmentation via the Similarity Group Proposal Network (SGPN)
[WYHN17]. One area of improvement which has been highlighted by SGPN's authors,
pertains to use of memory intensive similarity matrices which occupy memory
quadratic in the number of points. In this report, we attempt to tackle this
issue through use of two sampling based methods, which compute Instance
Segmentation on a sub-sampled Point Set, and then extrapolate labels to the
complete set using the nearest neigbhour approach. While both approaches
perform equally well on large sub-samples, the random-based strategy gives the
most improvements in terms of speed and memory usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 11 figures, paper dated 2019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Fetal Biometry Assessment with Deep Ensembles using
  Sparse-Sampling of 2D Intrapartum Ultrasound Images <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayroop Ramesh, Valentin Bacher, Mark C. Eid, Hoda Kalabizadeh, Christian Rupprecht, Ana IL Namburete, Pak-Hei Yeung, Madeleine K. Wyburd, Nicola K. Dinsdale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The International Society of Ultrasound advocates Intrapartum Ultrasound (US)
Imaging in Obstetrics and Gynecology (ISUOG) to monitor labour progression
through changes in fetal head position. Two reliable ultrasound-derived
parameters that are used to predict outcomes of instrumental vaginal delivery
are the angle of progression (AoP) and head-symphysis distance (HSD). In this
work, as part of the Intrapartum Ultrasounds Grand Challenge (IUGC) 2024, we
propose an automated fetal biometry measurement pipeline to reduce intra- and
inter-observer variability and improve measurement reliability. Our pipeline
consists of three key tasks: (i) classification of standard planes (SP) from US
videos, (ii) segmentation of fetal head and pubic symphysis from the detected
SPs, and (iii) computation of the AoP and HSD from the segmented regions. We
perform sparse sampling to mitigate class imbalances and reduce spurious
correlations in task (i), and utilize ensemble-based deep learning methods for
task (i) and (ii) to enhance generalizability under different US acquisition
settings. Finally, to promote robustness in task iii) with respect to the
structural fidelity of measurements, we retain the largest connected components
and apply ellipse fitting to the segmentations. Our solution achieved ACC:
0.9452, F1: 0.9225, AUC: 0.983, MCC: 0.8361, DSC: 0.918, HD: 19.73, ASD: 5.71,
$\Delta_{AoP}$: 8.90 and $\Delta_{HSD}$: 14.35 across an unseen hold-out set of
4 patients and 224 US frames. The results from the proposed automated pipeline
can improve the understanding of labour arrest causes and guide the development
of clinical risk stratification tools for efficient and effective prenatal
care.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Top 5 in MICCAI IUGC 2024: Intrapartum Ultrasound Grand Challenge &
  Runners up in Classification!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Inverse Scattering with Score-based Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gao, Wenhan Guo, Yu Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse scattering is a fundamental challenge in many imaging applications,
ranging from microscopy to remote sensing. Solving this problem often requires
jointly estimating two unknowns -- the image and the scattering field inside
the object -- necessitating effective image prior to regularize the inference.
In this paper, we propose a regularized neural field (NF) approach which
integrates the denoising score function used in score-based generative models.
The neural field formulation offers convenient flexibility to performing joint
estimation, while the denoising score function imposes the rich structural
prior of images. Our results on three high-contrast simulated objects show that
the proposed approach yields a better imaging quality compared to the
state-of-the-art NF approach, where regularization is based on total variation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynadiff: Single-stage Decoding of Images from Continuously Evolving
  fMRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marlène Careil, Yohann Benchetrit, Jean-Rémi King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain-to-image decoding has been recently propelled by the progress in
generative AI models and the availability of large ultra-high field functional
Magnetic Resonance Imaging (fMRI). However, current approaches depend on
complicated multi-stage pipelines and preprocessing steps that typically
collapse the temporal dimension of brain recordings, thereby limiting
time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural
Activity Diffusion for Image Reconstruction), a new single-stage diffusion
model designed for reconstructing images from dynamically evolving fMRI
recordings. Our approach offers three main contributions. First, Dynadiff
simplifies training as compared to existing approaches. Second, our model
outperforms state-of-the-art models on time-resolved fMRI signals, especially
on high-level semantic image reconstruction metrics, while remaining
competitive on preprocessed fMRI data that collapse time. Third, this approach
allows a precise characterization of the evolution of image representations in
brain activity. Overall, this work lays the foundation for time-resolved
brain-to-image decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Video Compression with Context Modulation <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanbo Tang, Zhuoyuan Li, Yifan Bian, Li Li, Dong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient video coding is highly dependent on exploiting the temporal
redundancy, which is usually achieved by extracting and leveraging the temporal
context in the emerging conditional coding-based neural video codec (NVC).
Although the latest NVC has achieved remarkable progress in improving the
compression performance, the inherent temporal context propagation mechanism
lacks the ability to sufficiently leverage the reference information, limiting
further improvement. In this paper, we address the limitation by modulating the
temporal context with the reference frame in two steps. Specifically, we first
propose the flow orientation to mine the inter-correlation between the
reference frame and prediction frame for generating the additional oriented
temporal context. Moreover, we introduce the context compensation to leverage
the oriented context to modulate the propagated temporal context generated from
the propagated reference feature. Through the synergy mechanism and decoupling
loss supervision, the irrelevant propagated information can be effectively
eliminated to ensure better context modeling. Experimental results demonstrate
that our codec achieves on average 22.7% bitrate reduction over the advanced
traditional video codec H.266/VVC, and offers an average 10.1% bitrate saving
over the previous state-of-the-art NVC DCVC-FM. The code is available at
https://github.com/Austin4USTC/DCMVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalize Your Gaussian: Consistent 3D Scene Personalization from a
  Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wang, Xuanyu Yi, Qingshan Xu, Yuan Zhou, Long Chen, Hanwang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalizing 3D scenes from a single reference image enables intuitive
user-guided editing, which requires achieving both multi-view consistency
across perspectives and referential consistency with the input image. However,
these goals are particularly challenging due to the viewpoint bias caused by
the limited perspective provided in a single image. Lacking the mechanisms to
effectively expand reference information beyond the original view, existing
methods of image-conditioned 3DGS personalization often suffer from this
viewpoint bias and struggle to produce consistent results. Therefore, in this
paper, we present Consistent Personalization for 3D Gaussian Splatting (CP-GS),
a framework that progressively propagates the single-view reference appearance
to novel perspectives. In particular, CP-GS integrates pre-trained image-to-3D
generation and iterative LoRA fine-tuning to extract and extend the reference
appearance, and finally produces faithful multi-view guidance images and the
personalized 3DGS outputs through a view-consistent generation process guided
by geometric cues. Extensive experiments on real-world scenes show that our
CP-GS effectively mitigates the viewpoint bias, achieving high-quality
personalization that significantly outperforms existing methods. The code will
be released at https://github.com/Yuxuan-W/CP-GS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ diffDemorph: Extending Reference-Free Demorphing to Unseen Faces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitish Shukla, Arun Ross
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A face morph is created by combining two (or more) face images corresponding
to two (or more) identities to produce a composite that successfully matches
the constituent identities. Reference-free (RF) demorphing reverses this
process using only the morph image, without the need for additional reference
images. Previous RF demorphing methods were overly constrained, as they rely on
assumptions about the distributions of training and testing morphs such as the
morphing technique used, face style, and images used to create the morph. In
this paper, we introduce a novel diffusion-based approach that effectively
disentangles component images from a composite morph image with high visual
fidelity. Our method is the first to generalize across morph techniques and
face styles, beating the current state of the art by $\geq 59.46\%$ under a
common training protocol across all datasets tested. We train our method on
morphs created using synthetically generated face images and test on real
morphs, thereby enhancing the practicality of the technique. Experiments on six
datasets and two face matchers establish the utility and efficacy of our
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SparC: Sparse Representation and Construction for High-Resolution 3D
  Shapes Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Li, Yufei Wang, Heliang Zheng, Yihao Luo, Bihan Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-fidelity 3D object synthesis remains significantly more challenging than
2D image generation due to the unstructured nature of mesh data and the cubic
complexity of dense volumetric grids. Existing two-stage pipelines-compressing
meshes with a VAE (using either 2D or 3D supervision), followed by latent
diffusion sampling-often suffer from severe detail loss caused by inefficient
representations and modality mismatches introduced in VAE. We introduce SparC,
a unified framework that combines a sparse deformable marching cubes
representation SparseCubes with a novel encoder SparConv-VAE. SparseCubes
converts raw meshes into high-resolution ($1024^3$) surfaces with arbitrary
topology by scattering signed distance and deformation fields onto a sparse
cube, allowing differentiable optimization. SparConv-VAE is the first
modality-consistent variational autoencoder built entirely upon sparse
convolutional networks, enabling efficient and near-lossless 3D reconstruction
suitable for high-resolution generative modeling through latent diffusion.
SparC achieves state-of-the-art reconstruction fidelity on challenging inputs,
including open surfaces, disconnected components, and intricate geometry. It
preserves fine-grained shape details, reduces training and inference cost, and
integrates naturally with latent diffusion models for scalable, high-resolution
3D generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://lizhihao6.github.io/SparC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReservoirTTA: Prolonged Test-time Adaptation for Evolving and Recurring
  Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillaume Vray, Devavrat Tomar, Xufeng Gao, Jean-Philippe Thiran, Evan Shelhamer, Behzad Bozorgtabar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces ReservoirTTA, a novel plug-in framework designed for
prolonged test-time adaptation (TTA) in scenarios where the test domain
continuously shifts over time, including cases where domains recur or evolve
gradually. At its core, ReservoirTTA maintains a reservoir of
domain-specialized models -- an adaptive test-time model ensemble -- that both
detects new domains via online clustering over style features of incoming
samples and routes each sample to the appropriate specialized model, and
thereby enables domain-specific adaptation. This multi-model strategy overcomes
key limitations of single model adaptation, such as catastrophic forgetting,
inter-domain interference, and error accumulation, ensuring robust and stable
performance on sustained non-stationary test distributions. Our theoretical
analysis reveals key components that bound parameter variance and prevent model
collapse, while our plug-in TTA module mitigates catastrophic forgetting of
previously encountered domains. Extensive experiments on the classification
corruption benchmarks, including ImageNet-C and CIFAR-10/100-C, as well as the
Cityscapes$\rightarrow$ACDC semantic segmentation task, covering recurring and
continuously evolving domain shifts, demonstrate that ReservoirTTA
significantly improves adaptation accuracy and maintains stable performance
across prolonged, recurring shifts, outperforming state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Interpretability of Sparse Latent Representations with Class
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farshad Sangari Abiz, Reshad Hosseini, Babak N. Araabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational Autoencoders (VAEs) are powerful generative models for learning
latent representations. Standard VAEs generate dispersed and unstructured
latent spaces by utilizing all dimensions, which limits their interpretability,
especially in high-dimensional spaces. To address this challenge, Variational
Sparse Coding (VSC) introduces a spike-and-slab prior distribution, resulting
in sparse latent representations for each input. These sparse representations,
characterized by a limited number of active dimensions, are inherently more
interpretable. Despite this advantage, VSC falls short in providing structured
interpretations across samples within the same class. Intuitively, samples from
the same class are expected to share similar attributes while allowing for
variations in those attributes. This expectation should manifest as consistent
patterns of active dimensions in their latent representations, but VSC does not
enforce such consistency.
  In this paper, we propose a novel approach to enhance the latent space
interpretability by ensuring that the active dimensions in the latent space are
consistent across samples within the same class. To achieve this, we introduce
a new loss function that encourages samples from the same class to share
similar active dimensions. This alignment creates a more structured and
interpretable latent space, where each shared dimension corresponds to a
high-level concept, or "factor." Unlike existing disentanglement-based methods
that primarily focus on global factors shared across all classes, our method
captures both global and class-specific factors, thereby enhancing the utility
and interpretability of latent representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaang Li, Yifei Yuan, Wenyan Li, Mohammad Aliannejadi, Daniel Hershcovich, Anders Søgaard, Ivan Vulić, Wenxuan Zhang, Paul Pu Liang, Yang Deng, Serge Belongie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As vision-language models (VLMs) become increasingly integrated into daily
life, the need for accurate visual culture understanding is becoming critical.
Yet, these models frequently fall short in interpreting cultural nuances
effectively. Prior work has demonstrated the effectiveness of
retrieval-augmented generation (RAG) in enhancing cultural understanding in
text-only settings, while its application in multimodal scenarios remains
underexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented
Visual culturE uNdErstAnding), a new benchmark designed to advance visual
culture understanding through retrieval, focusing on two tasks: culture-focused
visual question answering (cVQA) and culture-informed image captioning (cIC).
RAVENEA extends existing datasets by integrating over 10,000 Wikipedia
documents curated and ranked by human annotators. With RAVENEA, we train and
evaluate seven multimodal retrievers for each image query, and measure the
downstream impact of retrieval-augmented inputs across fourteen
state-of-the-art VLMs. Our results show that lightweight VLMs, when augmented
with culture-aware retrieval, outperform their non-augmented counterparts (by
at least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the
value of retrieval-augmented methods and culturally inclusive benchmarks for
multimodal understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VisualQuality-R1: Reasoning-Induced Image Quality Assessment via
  Reinforcement Learning to Rank 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhe Wu, Jian Zou, Jie Liang, Lei Zhang, Kede Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing
reasoning and generalization capabilities of large language models (LLMs)
through reinforcement learning. Nevertheless, the potential of
reasoning-induced computational modeling has not been thoroughly explored in
the context of image quality assessment (IQA), a task critically dependent on
visual reasoning. In this paper, we introduce VisualQuality-R1, a
reasoning-induced no-reference IQA (NR-IQA) model, and we train it with
reinforcement learning to rank, a learning algorithm tailored to the
intrinsically relative nature of visual quality. Specifically, for a pair of
images, we employ group relative policy optimization to generate multiple
quality scores for each image. These estimates are then used to compute
comparative probabilities of one image having higher quality than the other
under the Thurstone model. Rewards for each quality estimate are defined using
continuous fidelity measures rather than discretized binary labels. Extensive
experiments show that the proposed VisualQuality-R1 consistently outperforms
discriminative deep learning-based NR-IQA models as well as a recent
reasoning-induced quality regression method. Moreover, VisualQuality-R1 is
capable of generating contextually rich, human-aligned quality descriptions,
and supports multi-dataset training without requiring perceptual scale
realignment. These features make VisualQuality-R1 especially well-suited for
reliably measuring progress in a wide range of image processing tasks like
super-resolution and image generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Compression Commander: Plug-and-Play Inference Acceleration for
  Video Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuyang Liu, Yiyu Wang, Junpeng Ma, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video large language models (VideoLLM) excel at video understanding, but face
efficiency challenges due to the quadratic complexity of abundant visual
tokens. Our systematic analysis of token compression methods for VideoLLMs
reveals two critical issues: (i) overlooking distinctive visual signals across
frames, leading to information loss; (ii) suffering from implementation
constraints, causing incompatibility with modern architectures or efficient
operators. To address these challenges, we distill three design principles for
VideoLLM token compression and propose a plug-and-play inference acceleration
framework "Video Compression Commander" (VidCom2). By quantifying each frame's
uniqueness, VidCom2 adaptively adjusts compression intensity across frames,
effectively preserving essential information while reducing redundancy in video
sequences. Extensive experiments across various VideoLLMs and benchmarks
demonstrate the superior performance and efficiency of our VidCom2. With only
25% visual tokens, VidCom2 achieves 99.6% of the original performance on
LLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame
Compression Adjustment strategy is compatible with other token compression
methods to further improve their performance. Our code is available at
https://github.com/xuyang-liu16/VidCom2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/xuyang-liu16/VidCom2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diving into the Fusion of Monocular Priors for Generalized Stereo
  Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengtang Yao, Lidong Yu, Zhidan Liu, Jiaxi Zeng, Yuwei Wu, Yunde Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The matching formulation makes it naturally hard for the stereo matching to
handle ill-posed regions like occlusions and non-Lambertian surfaces. Fusing
monocular priors has been proven helpful for ill-posed matching, but the biased
monocular prior learned from small stereo datasets constrains the
generalization. Recently, stereo matching has progressed by leveraging the
unbiased monocular prior from the vision foundation model (VFM) to improve the
generalization in ill-posed regions. We dive into the fusion process and
observe three main problems limiting the fusion of the VFM monocular prior. The
first problem is the misalignment between affine-invariant relative monocular
depth and absolute depth of disparity. Besides, when we use the monocular
feature in an iterative update structure, the over-confidence in the disparity
update leads to local optima results. A direct fusion of a monocular depth map
could alleviate the local optima problem, but noisy disparity results computed
at the first several iterations will misguide the fusion. In this paper, we
propose a binary local ordering map to guide the fusion, which converts the
depth map into a binary relative format, unifying the relative and absolute
depth representation. The computed local ordering map is also used to re-weight
the initial disparity update, resolving the local optima and noisy problem. In
addition, we formulate the final direct fusion of monocular depth to the
disparity as a registration problem, where a pixel-wise linear regression
module can globally and adaptively align them. Our method fully exploits the
monocular prior to support stereo matching results effectively and efficiently.
We significantly improve the performance from the experiments when generalizing
from SceneFlow to Middlebury and Booster datasets while barely reducing the
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code:
  https://github.com/YaoChengTang/Diving-into-the-Fusion-of-Monocular-Priors-for-Generalized-Stereo-Matching</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating and Enhancing the Robustness of Large Multimodal Models
  Against Temporal Inconsistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiafeng Liang, Shixin Jiang, Xuan Dong, Ning Wang, Zheng Chu, Hui Su, Jinlan Fu, Ming Liu, See-Kiong Ng, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) have recently demonstrated impressive
performance on general video comprehension benchmarks. Nevertheless, for
broader applications, the robustness of their temporal analysis capability
needs to be thoroughly investigated yet predominantly ignored. Motivated by
this, we propose a novel temporal robustness benchmark (TemRobBench), which
introduces temporal inconsistency perturbations separately at the visual and
textual modalities to assess the robustness of models. We evaluate 16
mainstream LMMs and find that they exhibit over-reliance on prior knowledge and
textual context in adversarial environments, while ignoring the actual temporal
dynamics in the video. To mitigate this issue, we design panoramic direct
preference optimization (PanoDPO), which encourages LMMs to incorporate both
visual and linguistic feature preferences simultaneously. Experimental results
show that PanoDPO can effectively enhance the model's robustness and
reliability in temporal analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViC-Bench: Benchmarking Visual-Interleaved Chain-of-Thought Capability
  in MLLMs with Free-Style Intermediate State Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuecheng Wu, Jiaxing Liu, Danlei Huang, Xiaoyu Li, Yifan Wang, Chen Chen, Liya Ma, Xuezhi Cao, Junxiao Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-Interleaved Chain-of-Thought (VI-CoT) enables MLLMs to continually
update their understanding and decisions based on step-wise intermediate visual
states (IVS), much like a human would, which demonstrates impressive success in
various tasks, thereby leading to emerged advancements in related benchmarks.
Despite promising progress, current benchmarks provide models with relatively
fixed IVS, rather than free-style IVS, whch might forcibly distort the original
thinking trajectories, failing to evaluate their intrinsic reasoning
capabilities. More importantly, existing benchmarks neglect to systematically
explore the impact factors that IVS would impart to untamed reasoning
performance. To tackle above gaps, we introduce a specialized benchmark termed
ViC-Bench, consisting of four representive tasks: maze navigation, jigsaw
puzzle, embodied long-horizon planning, and complex counting, where each task
has dedicated free-style IVS generation pipeline supporting function calls. To
systematically examine VI-CoT capability, we propose a thorough evaluation
suite incorporating a progressive three-stage strategy with targeted new
metrics. Besides, we establish Incremental Prompting Information Injection
(IPII) strategy to ablatively explore the prompting factors for VI-CoT. We
extensively conduct evaluations for 18 advanced MLLMs, revealing key insights
into their VI-CoT capability. Our proposed benchmark is publicly open at
Huggingface.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepEyes: Incentivizing "Thinking with Images" via Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, Xing Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (VLMs) have shown strong capabilities in
multimodal understanding and reasoning, yet they are primarily constrained by
text-based reasoning processes. However, achieving seamless integration of
visual and textual reasoning which mirrors human cognitive processes remains a
significant challenge. In particular, effectively incorporating advanced visual
input processing into reasoning mechanisms is still an open question. Thus, in
this paper, we explore the interleaved multimodal reasoning paradigm and
introduce DeepEyes, a model with "thinking with images" capabilities
incentivized through end-to-end reinforcement learning without the need for
cold-start SFT. Notably, this ability emerges natively within the model itself,
leveraging its inherent grounding ability as a tool instead of depending on
separate specialized models. Specifically, we propose a tool-use-oriented data
selection mechanism and a reward strategy to encourage successful tool-assisted
reasoning trajectories. DeepEyes achieves significant performance gains on
fine-grained perception and reasoning benchmarks and also demonstrates
improvement in grounding, hallucination, and mathematical reasoning tasks.
Interestingly, we observe the distinct evolution of tool-calling behavior from
initial exploration to efficient and accurate exploitation, and diverse
thinking patterns that closely mirror human visual reasoning processes. Code is
available at https://github.com/Visual-Agent/DeepEyes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Language Modeling Meets Remote Sensing: Models, <span class="highlight-title">Dataset</span>s and
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingxing Weng, Chao Pang, Gui-Song Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language modeling (VLM) aims to bridge the information gap between
images and natural language. Under the new paradigm of first pre-training on
massive image-text pairs and then fine-tuning on task-specific data, VLM in the
remote sensing domain has made significant progress. The resulting models
benefit from the absorption of extensive general knowledge and demonstrate
strong performance across a variety of remote sensing data analysis tasks.
Moreover, they are capable of interacting with users in a conversational
manner. In this paper, we aim to provide the remote sensing community with a
timely and comprehensive review of the developments in VLM using the two-stage
paradigm. Specifically, we first cover a taxonomy of VLM in remote sensing:
contrastive learning, visual instruction tuning, and text-conditioned image
generation. For each category, we detail the commonly used network architecture
and pre-training objectives. Second, we conduct a thorough review of existing
works, examining foundation models and task-specific adaptation methods in
contrastive-based VLM, architectural upgrades, training strategies and model
capabilities in instruction-based VLM, as well as generative foundation models
with their representative downstream applications. Third, we summarize datasets
used for VLM pre-training, fine-tuning, and evaluation, with an analysis of
their construction methodologies (including image sources and caption
generation) and key properties, such as scale and task adaptability. Finally,
we conclude this survey with insights and discussions on future research
directions: cross-modal representation alignment, vague requirement
comprehension, explanation-driven model reliability, continually scalable model
capabilities, and large-scale datasets featuring richer modalities and greater
challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Geoscience and Remote Sensing Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Data Alignment Makes AI-Generated Image Detector Easier
  Generalizable 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoxin Chen, Junwei Xi, Zhiyuan Yan, Ke-Yue Zhang, Shuang Wu, Jingyi Xie, Xu Chen, Lei Xu, Isabel Guan, Taiping Yao, Shouhong Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing detectors are often trained on biased datasets, leading to the
possibility of overfitting on non-causal image attributes that are spuriously
correlated with real/synthetic labels. While these biased features enhance
performance on the training data, they result in substantial performance
degradation when applied to unbiased datasets. One common solution is to
perform dataset alignment through generative reconstruction, matching the
semantic content between real and synthetic images. However, we revisit this
approach and show that pixel-level alignment alone is insufficient. The
reconstructed images still suffer from frequency-level misalignment, which can
perpetuate spurious correlations. To illustrate, we observe that reconstruction
models tend to restore the high-frequency details lost in real images (possibly
due to JPEG compression), inadvertently creating a frequency-level
misalignment, where synthetic images appear to have richer high-frequency
content than real ones. This misalignment leads to models associating
high-frequency features with synthetic labels, further reinforcing biased cues.
To resolve this, we propose Dual Data Alignment (DDA), which aligns both the
pixel and frequency domains. Moreover, we introduce two new test sets:
DDA-COCO, containing DDA-aligned synthetic images for testing detector
performance on the most aligned dataset, and EvalGEN, featuring the latest
generative models for assessing detectors under new generative architectures
such as visual auto-regressive generators. Finally, our extensive evaluations
demonstrate that a detector trained exclusively on DDA-aligned MSCOCO could
improve across 8 diverse benchmarks by a non-trivial margin, showing a +7.2% on
in-the-wild benchmarks, highlighting the improved generalizability of unbiased
detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 Pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vid2World: Crafting Video Diffusion Models to Interactive World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqiao Huang, Jialong Wu, Qixing Zhou, Shangchen Miao, Mingsheng Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models, which predict transitions based on history observation and
action sequences, have shown great promise in improving data efficiency for
sequential decision making. However, existing world models often require
extensive domain-specific training and still produce low-fidelity, coarse
predictions, limiting their applicability in complex environments. In contrast,
video diffusion models trained on large, internet-scale datasets have
demonstrated impressive capabilities in generating high-quality videos that
capture diverse real-world dynamics. In this work, we present Vid2World, a
general approach for leveraging and transferring pre-trained video diffusion
models into interactive world models. To bridge the gap, Vid2World performs
casualization of a pre-trained video diffusion model by crafting its
architecture and training objective to enable autoregressive generation.
Furthermore, it introduces a causal action guidance mechanism to enhance action
controllability in the resulting interactive world model. Extensive experiments
in robot manipulation and game simulation domains show that our method offers a
scalable and effective approach for repurposing highly capable video diffusion
models to interactive world models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: http://knightnemo.github.io/vid2world/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Egocentric Action-aware Inertial Localization in Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingfang Zhang, Ryo Yonetani, Yifei Huang, Liangyang Ouyang, Ruicong Liu, Yoichi Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel inertial localization framework named Egocentric
Action-aware Inertial Localization (EAIL), which leverages egocentric action
cues from head-mounted IMU signals to localize the target individual within a
3D point cloud. Human inertial localization is challenging due to IMU sensor
noise that causes trajectory drift over time. The diversity of human actions
further complicates IMU signal processing by introducing various motion
patterns. Nevertheless, we observe that some actions observed through the
head-mounted IMU correlate with spatial environmental structures (e.g., bending
down to look inside an oven, washing dishes next to a sink), thereby serving as
spatial anchors to compensate for the localization drift. The proposed EAIL
framework learns such correlations via hierarchical multi-modal alignment. By
assuming that the 3D point cloud of the environment is available, it
contrastively learns modality encoders that align short-term egocentric action
cues in IMU signals with local environmental features in the point cloud. These
encoders are then used in reasoning the IMU data and the point cloud over time
and space to perform inertial localization. Interestingly, these encoders can
further be utilized to recognize the corresponding sequence of actions as a
by-product. Extensive experiments demonstrate the effectiveness of the proposed
framework over state-of-the-art inertial localization and inertial action
recognition baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Replace in Translation: Boost Concept Alignment in Counterfactual
  Text-to-Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sifan Li, Ming Tao, Hao Zhao, Ling Shao, Hao Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Image (T2I) has been prevalent in recent years, with most common
condition tasks having been optimized nicely. Besides, counterfactual
Text-to-Image is obstructing us from a more versatile AIGC experience. For
those scenes that are impossible to happen in real world and anti-physics, we
should spare no efforts in increasing the factual feel, which means
synthesizing images that people think very likely to be happening, and concept
alignment, which means all the required objects should be in the same frame. In
this paper, we focus on concept alignment. As controllable T2I models have
achieved satisfactory performance for real applications, we utilize this
technology to replace the objects in a synthesized image in latent space
step-by-step to change the image from a common scene to a counterfactual scene
to meet the prompt. We propose a strategy to instruct this replacing process,
which is called as Explicit Logical Narrative Prompt (ELNP), by using the newly
SoTA language model DeepSeek to generate the instructions. Furthermore, to
evaluate models' performance in counterfactual T2I, we design a metric to
calculate how many required concepts in the prompt can be covered averagely in
the synthesized images. The extensive experiments and qualitative comparisons
demonstrate that our strategy can boost the concept alignment in counterfactual
T2I.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plane Geometry Problem Solving with Multi-modal Reasoning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunghyuk Cho, Zhenyue Qin, Yang Liu, Youngbin Choi, Seungbeom Lee, Dongwoo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Plane geometry problem solving (PGPS) has recently gained significant
attention as a benchmark to assess the multi-modal reasoning capabilities of
large vision-language models. Despite the growing interest in PGPS, the
research community still lacks a comprehensive overview that systematically
synthesizes recent work in PGPS. To fill this gap, we present a survey of
existing PGPS studies. We first categorize PGPS methods into an encoder-decoder
framework and summarize the corresponding output formats used by their encoders
and decoders. Subsequently, we classify and analyze these encoders and decoders
according to their architectural designs. Finally, we outline major challenges
and promising directions for future research. In particular, we discuss the
hallucination issues arising during the encoding phase within encoder-decoder
architectures, as well as the problem of data leakage in current PGPS
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umberto Cappellazzo, Minsu Kim, Stavros Petridis, Daniele Falavigna, Alessio Brutti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy
environments by integrating visual cues. While recent advances integrate Large
Language Models (LLMs) into AVSR, their high computational cost hinders
deployment in resource-constrained settings. To address this, we propose
Llama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of
Projectors (SMoP) module to scale model capacity without increasing inference
costs. By incorporating sparsely-gated mixture-of-experts (MoE) projectors,
Llama-SMoP enables the use of smaller LLMs while maintaining strong
performance. We explore three SMoP configurations and show that Llama-SMoP DEDR
(Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and
experts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation
studies confirm its effectiveness in expert activation, scalability, and noise
robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Adaptation for Multi-label Image Classification: a
  Discriminator-free Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inder Pal Singh, Enjie Ghorbel, Anis Kacem, Djamila Aouada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a discriminator-free adversarial-based approach termed
DDA-MLIC for Unsupervised Domain Adaptation (UDA) in the context of Multi-Label
Image Classification (MLIC). While recent efforts have explored
adversarial-based UDA methods for MLIC, they typically include an additional
discriminator subnet. Nevertheless, decoupling the classification and the
discrimination tasks may harm their task-specific discriminative power. Herein,
we address this challenge by presenting a novel adversarial critic directly
derived from the task-specific classifier. Specifically, we employ a
two-component Gaussian Mixture Model (GMM) to model both source and target
predictions, distinguishing between two distinct clusters. Instead of using the
traditional Expectation Maximization (EM) algorithm, our approach utilizes a
Deep Neural Network (DNN) to estimate the parameters of each GMM component.
Subsequently, the source and target GMM parameters are leveraged to formulate
an adversarial loss using the Fr\'echet distance. The proposed framework is
therefore not only fully differentiable but is also cost-effective as it avoids
the expensive iterative process usually induced by the standard EM method. The
proposed method is evaluated on several multi-label image datasets covering
three different types of domain shift. The obtained results demonstrate that
DDA-MLIC outperforms existing state-of-the-art methods in terms of precision
while requiring a lower number of parameters. The code is made publicly
available at github.com/cvi2snt/DDA-MLIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is under consideration at Computer Vision and Image
  Understanding. arXiv admin note: text overlap with arXiv:2301.10611</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Handloom Design Generation Using Generative Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajat Kanti Bhattacharjee, Meghali Nandi, Amrit Jha, Gunajit Kalita, Ferdous Ahmed Barbhuiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes deep learning techniques of generating designs for
clothing, focused on handloom fabric and discusses the associated challenges
along with its application. The capability of generative neural network models
in understanding artistic designs and synthesizing those is not yet explored
well. In this work, multiple methods are employed incorporating the current
state of the art generative models and style transfer algorithms to study and
observe their performance for the task. The results are then evaluated through
user score. This work also provides a new dataset NeuralLoom for the task of
the design generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or
  True Temporal Understanding? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Feng, Zhengfeng Lai, Shiyu Li, Zizhen Wang, Simon Wang, Ping Huang, Meng Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing video understanding benchmarks often conflate knowledge-based and
purely image-based questions, rather than clearly isolating a model's temporal
reasoning ability, which is the key aspect that distinguishes video
understanding from other modalities. We identify two major limitations that
obscure whether higher scores truly indicate stronger understanding of the
dynamic content in videos: (1) strong language priors, where models can answer
questions without watching the video; and (2) shuffling invariance, where
models maintain similar performance on certain questions even when video frames
are temporally shuffled. To alleviate these issues, we propose VBenchComp, an
automated pipeline that categorizes questions into different domains:
LLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions
can be answered without viewing the video; Semantic questions remain answerable
even when the video frames are shuffled; and Temporal questions require
understanding the correct temporal order of frames. The rest of the questions
are labeled as Others. This can enable fine-grained evaluation of different
capabilities of a video LLM. Our analysis reveals nuanced model weaknesses that
are hidden by traditional overall scores, and we offer insights and
recommendations for designing future benchmarks that more accurately assess
video LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accuracy and Fairness of Facial Recognition Technology in Low-Quality
  Police Images: An Experiment With Synthetic Faces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Cuellar, Hon Kiu,  To, Arush Mehrotra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial recognition technology (FRT) is increasingly used in criminal
investigations, yet most evaluations of its accuracy rely on high-quality
images, unlike those often encountered by law enforcement. This study examines
how five common forms of image degradation--contrast, brightness, motion blur,
pose shift, and resolution--affect FRT accuracy and fairness across demographic
groups. Using synthetic faces generated by StyleGAN3 and labeled with FairFace,
we simulate degraded images and evaluate performance using Deepface with
ArcFace loss in 1:n identification tasks. We perform an experiment and find
that false positive rates peak near baseline image quality, while false
negatives increase as degradation intensifies--especially with blur and low
resolution. Error rates are consistently higher for women and Black
individuals, with Black females most affected. These disparities raise concerns
about fairness and reliability when FRT is used in real-world investigative
contexts. Nevertheless, even under the most challenging conditions and for the
most affected subgroups, FRT accuracy remains substantially higher than that of
many traditional forensic methods. This suggests that, if appropriately
validated and regulated, FRT should be considered a valuable investigative
tool. However, algorithmic accuracy alone is not sufficient: we must also
evaluate how FRT is used in practice, including user-driven data manipulation.
Such cases underscore the need for transparency and oversight in FRT deployment
to ensure both fairness and forensic validity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RETRO: REthinking Tactile Representation Learning with Material PriOrs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Xia, Chenliang Zhou, Cengiz Oztireli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile perception is profoundly influenced by the surface properties of
objects in contact. However, despite their crucial role in shaping tactile
experiences, these material characteristics have been largely neglected in
existing tactile representation learning methods. Most approaches primarily
focus on aligning tactile data with visual or textual information, overlooking
the richness of tactile feedback that comes from understanding the materials'
inherent properties. In this work, we address this gap by revisiting the
tactile representation learning framework and incorporating material-aware
priors into the learning process. These priors, which represent pre-learned
characteristics specific to different materials, allow tactile models to better
capture and generalize the nuances of surface texture. Our method enables more
accurate, contextually rich tactile feedback across diverse materials and
textures, improving performance in real-world applications such as robotics,
haptic feedback systems, and material editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/weihaox/RETRO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RADAR: Enhancing Radiology Report Generation with Supplementary
  Knowledge Injection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjun Hou, Yi Cheng, Kaishuai Xu, Heng Li, Yan Hu, Wenjie Li, Jiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities in
various domains, including radiology report generation. Previous approaches
have attempted to utilize multimodal LLMs for this task, enhancing their
performance through the integration of domain-specific knowledge retrieval.
However, these approaches often overlook the knowledge already embedded within
the LLMs, leading to redundant information integration and inefficient
utilization of learned representations. To address this limitation, we propose
RADAR, a framework for enhancing radiology report generation with supplementary
knowledge injection. RADAR improves report generation by systematically
leveraging both the internal knowledge of an LLM and externally retrieved
information. Specifically, it first extracts the model's acquired knowledge
that aligns with expert image-based classification outputs. It then retrieves
relevant supplementary knowledge to further enrich this information. Finally,
by aggregating both sources, RADAR generates more accurate and informative
radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU
X-ray demonstrate that our model outperforms state-of-the-art LLMs in both
language quality and clinical accuracy
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Review</span> of Vision-Based Assistive Systems for Visually Impaired People:
  Technologies, Applications, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fulong Yao, Wenju Zhou, Huosheng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visually impaired individuals rely heavily on accurate and timely information
about obstacles and their surrounding environments to achieve independent
living. In recent years, significant progress has been made in the development
of assistive technologies, particularly vision-based systems, that enhance
mobility and facilitate interaction with the external world in both indoor and
outdoor settings. This paper presents a comprehensive review of recent advances
in assistive systems designed for the visually impaired, with a focus on
state-of-the-art technologies in obstacle detection, navigation, and user
interaction. In addition, emerging trends and future directions in visual
guidance systems are discussed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Generating Realistic Underwater Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul-Kazeem Shamba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the use of contrastive learning and generative
adversarial networks for generating realistic underwater images from synthetic
images with uniform lighting. We investigate the performance of image
translation models for generating realistic underwater images using the VAROS
dataset. Two key evaluation metrics, Fr\'echet Inception Distance (FID) and
Structural Similarity Index Measure (SSIM), provide insights into the
trade-offs between perceptual quality and structural preservation. For paired
image translation, pix2pix achieves the best FID scores due to its paired
supervision and PatchGAN discriminator, while the autoencoder model attains the
highest SSIM, suggesting better structural fidelity despite producing blurrier
outputs. Among unpaired methods, CycleGAN achieves a competitive FID score by
leveraging cycle-consistency loss, whereas CUT, which replaces
cycle-consistency with contrastive learning, attains higher SSIM, indicating
improved spatial similarity retention. Notably, incorporating depth information
into CUT results in the lowest overall FID score, demonstrating that depth cues
enhance realism. However, the slight decrease in SSIM suggests that depth-aware
learning may introduce structural variations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RA-Touch: Retrieval-Augmented Touch Understanding with Enriched Visual
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoorhim Cho, Hongyeob Kim, Semin Kim, Youjia Zhang, Yunseok Choi, Sungeun Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visuo-tactile perception aims to understand an object's tactile properties,
such as texture, softness, and rigidity. However, the field remains
underexplored because collecting tactile data is costly and labor-intensive. We
observe that visually distinct objects can exhibit similar surface textures or
material properties. For example, a leather sofa and a leather jacket have
different appearances but share similar tactile properties. This implies that
tactile understanding can be guided by material cues in visual data, even
without direct tactile supervision. In this paper, we introduce RA-Touch, a
retrieval-augmented framework that improves visuo-tactile perception by
leveraging visual data enriched with tactile semantics. We carefully recaption
a large-scale visual dataset with tactile-focused descriptions, enabling the
model to access tactile semantics typically absent from conventional visual
datasets. A key challenge remains in effectively utilizing these tactile-aware
external descriptions. RA-Touch addresses this by retrieving visual-textual
representations aligned with tactile inputs and integrating them to focus on
relevant textural and material properties. By outperforming prior methods on
the TVL benchmark, our method demonstrates the potential of retrieval-based
visual reuse for tactile understanding. Code is available at
https://aim-skku.github.io/RA-Touch
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speculative Decoding Reimagined for Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luxi Lin, Zhihang Lin, Zhanpeng Zeng, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Multimodal Speculative Decoding (MSD) to accelerate
Multimodal Large Language Models (MLLMs) inference. Speculative decoding has
been shown to accelerate Large Language Models (LLMs) without sacrificing
accuracy. However, current speculative decoding methods for MLLMs fail to
achieve the same speedup as they do for LLMs. To address this, we reimagine
speculative decoding specifically for MLLMs. Our analysis of MLLM
characteristics reveals two key design principles for MSD: (1) Text and visual
tokens have fundamentally different characteristics and need to be processed
separately during drafting. (2) Both language modeling ability and visual
perception capability are crucial for the draft model. For the first principle,
MSD decouples text and visual tokens in the draft model, allowing each to be
handled based on its own characteristics. For the second principle, MSD uses a
two-stage training strategy: In stage one, the draft model is trained on
text-only instruction-tuning datasets to improve its language modeling ability.
In stage two, MSD gradually introduces multimodal data to enhance the visual
perception capability of the draft model. Experiments show that MSD boosts
inference speed by up to $2.29\times$ for LLaVA-1.5-7B and up to $2.46\times$
for LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.
Our code is available at https://github.com/Lyn-Lucy/MSD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Attention Distribution to Information Flow for Hallucination
  Mitigation in Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfei Zhao, Feng Zhang, Xin Sun, Chong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the unidirectional masking mechanism, Decoder-Only models propagate
information from left to right. LVLMs (Large Vision-Language Models) follow the
same architecture, with visual information gradually integrated into semantic
representations during forward propagation. Through systematic analysis, we
observe that over 80\% of the visual information is absorbed into the semantic
representations. However, the model's attention still predominantly focuses on
the visual representations. This misalignment between the attention
distribution and the actual information flow undermines the model's visual
understanding ability and contributes to hallucinations. To address this issue,
we enhance the model's visual understanding by leveraging the core information
embedded in semantic representations. Specifically, we identify attention heads
that focus on core semantic representations based on their attention
distributions. Then, through a two-stage optimization paradigm, we propagate
the advantages of these attention heads across the entire model, aligning the
attention distribution with the actual information flow. We evaluate our method
on three image captioning benchmarks using five different LVLMs, demonstrating
its effectiveness in significantly reducing hallucinations. Further experiments
reveal a trade-off between reduced hallucinations and richer details. Notably,
our method allows for manual adjustment of the model's conservativeness,
enabling flexible control to meet diverse real-world requirements. Code will be
released once accepted.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instructing Text-to-Image Diffusion Models via Classifier-Guided
  Semantic Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanyuan Chang, Yinghua Yao, Tao Qin, Mengmeng Wang, Ivor Tsang, Guang Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have emerged as powerful tools for
high-quality image generation and editing. Many existing approaches rely on
text prompts as editing guidance. However, these methods are constrained by the
need for manual prompt crafting, which can be time-consuming, introduce
irrelevant details, and significantly limit editing performance. In this work,
we propose optimizing semantic embeddings guided by attribute classifiers to
steer text-to-image models toward desired edits, without relying on text
prompts or requiring any training or fine-tuning of the diffusion model. We
utilize classifiers to learn precise semantic embeddings at the dataset level.
The learned embeddings are theoretically justified as the optimal
representation of attribute semantics, enabling disentangled and accurate
edits. Experiments further demonstrate that our method achieves high levels of
disentanglement and strong generalization across different domains of data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Agentic Reinforcement Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native
agentic ability to use external tools such as web browsers for searching and
writing/executing code for image manipulation to think with images. In the
open-source research community, while significant progress has been made in
language-only agentic abilities such as function calling and tool integration,
the development of multi-modal agentic capabilities that involve truly thinking
with images, and their corresponding benchmarks, are still less explored. This
work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning
(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large
Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the
ability to browse websites for real-time information updates and write code to
manipulate and analyze input images through cropping, rotation, and other image
processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)
with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'
agentic search and coding abilities. Our experimental results demonstrate that
Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and
+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT
also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks
such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.
Our findings suggest that Visual-ARFT offers a promising path toward building
robust and generalizable multimodal agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project url:
  https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupling Classifier for Boosting Few-shot Object Detection and
  Instance Segmentation <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin-Bin Gao, Xiaochen Chen, Zhongyi Huang, Congchong Nie, Jun Liu, Jinxiang Lai, Guannan Jiang, Xi Wang, Chengjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focus on few-shot object detection~(FSOD) and instance
segmentation~(FSIS), which requires a model to quickly adapt to novel classes
with a few labeled instances. The existing methods severely suffer from bias
classification because of the missing label issue which naturally exists in an
instance-level few-shot scenario and is first formally proposed by us. Our
analysis suggests that the standard classification head of most FSOD or FSIS
models needs to be decoupled to mitigate the bias classification. Therefore, we
propose an embarrassingly simple but effective method that decouples the
standard classifier into two heads. Then, these two individual heads are
capable of independently addressing clear positive samples and noisy negative
samples which are caused by the missing label. In this way, the model can
effectively learn novel classes while mitigating the effects of noisy negative
samples. Without bells and whistles, our model without any additional
computation cost and parameters consistently outperforms its baseline and
state-of-the-art by a large margin on PASCAL VOC and MS-COCO benchmarks for
FSOD and FSIS tasks. The Code is available at
https://csgaobb.github.io/Projects/DCFS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sule Bai, Mingxing Li, Yong Liu, Jing Tang, Haoji Zhang, Lei Sun, Xiangxiang Chu, Yansong Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional visual grounding methods primarily focus on single-image
scenarios with simple textual references. However, extending these methods to
real-world scenarios that involve implicit and complex instructions,
particularly in conjunction with multiple images, poses significant challenges,
which is mainly due to the lack of advanced reasoning ability across diverse
multi-modal contexts. In this work, we aim to address the more practical
universal grounding task, and propose UniVG-R1, a reasoning guided multimodal
large language model (MLLM) for universal visual grounding, which enhances
reasoning capabilities through reinforcement learning (RL) combined with
cold-start data. Specifically, we first construct a high-quality
Chain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning
chains, to guide the model towards correct reasoning paths via supervised
fine-tuning. Subsequently, we perform rule-based reinforcement learning to
encourage the model to identify correct reasoning chains, thereby incentivizing
its reasoning capabilities. In addition, we identify a difficulty bias arising
from the prevalence of easy samples as RL training progresses, and we propose a
difficulty-aware weight adjustment strategy to further strengthen the
performance. Experimental results demonstrate the effectiveness of UniVG-R1,
which achieves state-of-the-art performance on MIG-Bench with a 9.1%
improvement over the previous method. Furthermore, our model exhibits strong
generalizability, achieving an average improvement of 23.4% in zero-shot
performance across four image and video reasoning grounding benchmarks. The
project page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoQA: Visual-only Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyang Jiang, Jianing An, Jie Luo, Wenjun Wu, Lei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Visual-only Question Answering (VoQA), a novel multimodal task in
which questions are visually embedded within images, without any accompanying
textual input. This requires models to locate, recognize, and reason over
visually embedded textual questions, posing challenges for existing large
vision-language models (LVLMs), which show notable performance drops even with
carefully designed prompts. To bridge this gap, we introduce Guided Response
Triggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy
that guides the model to perform step-by-step reasoning purely based on visual
input, significantly improving model performance. Our work enhances models'
capacity for human-like visual understanding in complex multimodal scenarios,
where information, including language, is perceived visually.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flexible-weighted Chamfer Distance: Enhanced Objective Function for
  Point Cloud Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Li, Shengwei Tian, Long Yu, Xin Ning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chamfer Distance (CD) comprises two components that can evaluate the global
distribution and local performance of generated point clouds, making it widely
utilized as a similarity measure between generated and target point clouds in
point cloud completion tasks. Additionally, CD's computational efficiency has
led to its frequent application as an objective function for guiding point
cloud generation. However, using CD directly as an objective function with
fixed equal weights for its two components can often result in seemingly high
overall performance (i.e., low CD score), while failing to achieve a good
global distribution. This is typically reflected in high Earth Mover's Distance
(EMD) and Decomposed Chamfer Distance (DCD) scores, alongside poor human
assessments. To address this issue, we propose a Flexible-Weighted Chamfer
Distance (FCD) to guide point cloud generation. FCD assigns a higher weight to
the global distribution component of CD and incorporates a flexible weighting
strategy to adjust the balance between the two components, aiming to improve
global distribution while maintaining robust overall performance. Experimental
results on two state-of-the-art networks demonstrate that our method achieves
superior results across multiple evaluation metrics, including CD, EMD, DCD,
and F-Score, as well as in human evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beginning with You: Perceptual-Initialization Improves Vision-Language
  Representation and Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Hu, Runchen Wang, Stephen Chong Zhao, Xuhui Zhan, Do Hun Kim, Mark Wallace, David A. Tovar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Perceptual-Initialization (PI), a paradigm shift in visual
representation learning that incorporates human perceptual structure during the
initialization phase rather than as a downstream fine-tuning step. By
integrating human-derived triplet embeddings from the NIGHTS dataset to
initialize a CLIP vision encoder, followed by self-supervised learning on
YFCC15M, our approach demonstrates significant zero-shot performance
improvements, without any task-specific fine-tuning, across 29 zero shot
classification and 2 retrieval benchmarks. On ImageNet-1K, zero-shot gains
emerge after approximately 15 epochs of pretraining. Benefits are observed
across datasets of various scales, with improvements manifesting at different
stages of the pretraining process depending on dataset characteristics. Our
approach consistently enhances zero-shot top-1 accuracy, top-5 accuracy, and
retrieval recall (e.g., R@1, R@5) across these diverse evaluation tasks,
without requiring any adaptation to target domains. These findings challenge
the conventional wisdom of using human-perceptual data primarily for
fine-tuning and demonstrate that embedding human perceptual structure during
early representation learning yields more capable and vision-language aligned
systems that generalize immediately to unseen tasks. Our work shows that
"beginning with you", starting with human perception, provides a stronger
foundation for general-purpose vision-language intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Omnidirectional Reasoning with 360-R1: A <span class="highlight-title">Dataset</span>, Benchmark, and
  GRPO-based Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinshen Zhang, Zhen Ye, Xu Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Omnidirectional images (ODIs), with their 360{\deg} field of view, provide
unparalleled spatial awareness for immersive applications like augmented
reality and embodied AI. However, the capability of existing multi-modal large
language models (MLLMs) to comprehend and reason about such panoramic scenes
remains underexplored. This paper addresses this gap by introducing OmniVQA,
the first dataset and conducting the first benchmark for omnidirectional visual
question answering. Our evaluation of state-of-the-art MLLMs reveals
significant limitations in handling omnidirectional visual question answering,
highlighting persistent challenges in object localization, feature extraction,
and hallucination suppression within panoramic contexts. These results
underscore the disconnect between current MLLM capabilities and the demands of
omnidirectional visual understanding, which calls for dedicated architectural
or training innovations tailored to 360{\deg} imagery. Building on the OmniVQA
dataset and benchmark, we further introduce a rule-based reinforcement learning
method, 360-R1, based on Qwen2.5-VL-Instruct. Concretely, we modify the group
relative policy optimization (GRPO) by proposing three novel reward functions:
(1) reasoning process similarity reward, (2) answer semantic accuracy reward,
and (3) structured format compliance reward. Extensive experiments on our
OmniVQA demonstrate the superiority of our proposed method in omnidirectional
space (+6% improvement).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridge the Gap between Past and Future: Siamese Model Optimization for
  Context-Aware Document Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songhao Wu, Quan Tu, Mingjie Zhong, Hong Liu, Jia Xu, Jinjie Gu, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of information retrieval, users often engage in multi-turn
interactions with search engines to acquire information, leading to the
formation of sequences of user feedback behaviors. Leveraging the session
context has proven to be beneficial for inferring user search intent and
document ranking. A multitude of approaches have been proposed to exploit
in-session context for improved document ranking. Despite these advances, the
limitation of historical session data for capturing evolving user intent
remains a challenge. In this work, we explore the integration of future
contextual information into the session context to enhance document ranking. We
present the siamese model optimization framework, comprising a
history-conditioned model and a future-aware model. The former processes only
the historical behavior sequence, while the latter integrates both historical
and anticipated future behaviors. Both models are trained collaboratively using
the supervised labels and pseudo labels predicted by the other. The
history-conditioned model, referred to as ForeRanker, progressively learns
future-relevant information to enhance ranking, while it singly uses historical
session at inference time. To mitigate inconsistencies during training, we
introduce the peer knowledge distillation method with a dynamic gating
mechanism, allowing models to selectively incorporate contextual information.
Experimental results on benchmark datasets demonstrate the effectiveness of our
ForeRanker, showcasing its superior performance compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From stability of Langevin diffusion to convergence of proximal MCMC for
  non-log-concave sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marien Renaud, Valentin De Bortoli, Arthur Leclaire, Nicolas Papadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of sampling distributions stemming from non-convex
potentials with Unadjusted Langevin Algorithm (ULA). We prove the stability of
the discrete-time ULA to drift approximations under the assumption that the
potential is strongly convex at infinity. In many context, e.g. imaging inverse
problems, potentials are non-convex and non-smooth. Proximal Stochastic
Gradient Langevin Algorithm (PSGLA) is a popular algorithm to handle such
potentials. It combines the forward-backward optimization algorithm with a ULA
step. Our main stability result combined with properties of the Moreau envelope
allows us to derive the first proof of convergence of the PSGLA for non-convex
potentials. We empirically validate our methodology on synthetic data and in
the context of imaging inverse problems. In particular, we observe that PSGLA
exhibits faster convergence rates than Stochastic Gradient Langevin Algorithm
for posterior sampling while preserving its restoration properties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LMP: Leveraging Motion Prior in Zero-Shot Video Generation with
  Diffusion <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changgu Chen, Xiaoyan Yang, Junwei Shu, Changbo Wang, Yang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, large-scale pre-trained diffusion transformer models have
made significant progress in video generation. While current DiT models can
produce high-definition, high-frame-rate, and highly diverse videos, there is a
lack of fine-grained control over the video content. Controlling the motion of
subjects in videos using only prompts is challenging, especially when it comes
to describing complex movements. Further, existing methods fail to control the
motion in image-to-video generation, as the subject in the reference image
often differs from the subject in the reference video in terms of initial
position, size, and shape. To address this, we propose the Leveraging Motion
Prior (LMP) framework for zero-shot video generation. Our framework harnesses
the powerful generative capabilities of pre-trained diffusion transformers to
enable motion in the generated videos to reference user-provided motion videos
in both text-to-video and image-to-video generation. To this end, we first
introduce a foreground-background disentangle module to distinguish between
moving subjects and backgrounds in the reference video, preventing interference
in the target video generation. A reweighted motion transfer module is designed
to allow the target video to reference the motion from the reference video. To
avoid interference from the subject in the reference video, we propose an
appearance separation module to suppress the appearance of the reference
subject in the target video. We annotate the DAVIS dataset with detailed
prompts for our experiments and design evaluation metrics to validate the
effectiveness of our method. Extensive experiments demonstrate that our
approach achieves state-of-the-art performance in generation quality,
prompt-video consistency, and control capability. Our homepage is available at
https://vpx-ecnu.github.io/LMP-Website/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M3Depth: Wavelet-Enhanced Depth Estimation on Mars via Mutual Boosting
  of Dual-Modal Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Li, Jiawei Wang, Miyu Li, Yu Liu, Yumei Wang, Haitao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation plays a great potential role in obstacle avoidance and
navigation for further Mars exploration missions. Compared to traditional
stereo matching, learning-based stereo depth estimation provides a data-driven
approach to infer dense and precise depth maps from stereo image pairs.
However, these methods always suffer performance degradation in environments
with sparse textures and lacking geometric constraints, such as the
unstructured terrain of Mars. To address these challenges, we propose M3Depth,
a depth estimation model tailored for Mars rovers. Considering the sparse and
smooth texture of Martian terrain, which is primarily composed of low-frequency
features, our model incorporates a convolutional kernel based on wavelet
transform that effectively captures low-frequency response and expands the
receptive field. Additionally, we introduce a consistency loss that explicitly
models the complementary relationship between depth map and surface normal map,
utilizing the surface normal as a geometric constraint to enhance the accuracy
of depth estimation. Besides, a pixel-wise refinement module with mutual
boosting mechanism is designed to iteratively refine both depth and surface
normal predictions. Experimental results on synthetic Mars datasets with depth
annotations show that M3Depth achieves a significant 16% improvement in depth
estimation accuracy compared to other state-of-the-art methods in depth
estimation. Furthermore, the model demonstrates strong applicability in
real-world Martian scenarios, offering a promising solution for future Mars
exploration missions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unify Graph Learning with Text: Unleashing LLM Potentials for Session
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songhao Wu, Quan Tu, Hong Liu, Jia Xu, Zhongyi Liu, Guannan Zhang, Ran Wang, Xiuying Chen, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Session search involves a series of interactive queries and actions to
fulfill user's complex information need. Current strategies typically
prioritize sequential modeling for deep semantic understanding, overlooking the
graph structure in interactions. While some approaches focus on capturing
structural information, they use a generalized representation for documents,
neglecting the word-level semantic modeling. In this paper, we propose Symbolic
Graph Ranker (SGR), which aims to take advantage of both text-based and
graph-based approaches by leveraging the power of recent Large Language Models
(LLMs). Concretely, we first introduce a set of symbolic grammar rules to
convert session graph into text. This allows integrating session history,
interaction process, and task instruction seamlessly as inputs for the LLM.
Moreover, given the natural discrepancy between LLMs pre-trained on textual
corpora, and the symbolic language we produce using our graph-to-text grammar,
our objective is to enhance LLMs' ability to capture graph structures within a
textual format. To achieve this, we introduce a set of self-supervised symbolic
learning tasks including link prediction, node content generation, and
generative contrastive learning, to enable LLMs to capture the topological
information from coarse-grained to fine-grained. Experiment results and
comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm
the superiority of our approach. Our paradigm also offers a novel and effective
methodology that bridges the gap between traditional search strategies and
modern LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReactDiff: Latent Diffusion for Facial Reaction Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Li, Sheng Wang, Xin Wang, Yitao Zhu, Honglin Xiong, Zixu Zhuang, Qian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the audio-visual clip of the speaker, facial reaction generation aims
to predict the listener's facial reactions. The challenge lies in capturing the
relevance between video and audio while balancing appropriateness, realism, and
diversity. While prior works have mostly focused on uni-modal inputs or
simplified reaction mappings, recent approaches such as PerFRDiff have explored
multi-modal inputs and the one-to-many nature of appropriate reaction mappings.
In this work, we propose the Facial Reaction Diffusion (ReactDiff) framework
that uniquely integrates a Multi-Modality Transformer with conditional
diffusion in the latent space for enhanced reaction generation. Unlike existing
methods, ReactDiff leverages intra- and inter-class attention for fine-grained
multi-modal interaction, while the latent diffusion process between the encoder
and decoder enables diverse yet contextually appropriate outputs. Experimental
results demonstrate that ReactDiff significantly outperforms existing
approaches, achieving a facial reaction correlation of 0.26 and diversity score
of 0.094 while maintaining competitive realism. The code is open-sourced at
\href{https://github.com/Hunan-Tiger/ReactDiff}{github}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hunyuan-Game: Industrial-grade Intelligent Game Creation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihuang Li, Caijin Zhou, Shoujian Zheng, Jianxiang Lu, Jiabin Huang, Comi Chen, Junshu Tang, Guangzheng Xu, Jiale Tao, Hongmei Wang, Donghao Li, Wenqing Yu, Senbo Wang, Zhimin Li, Yetshuan Shi, Haoyu Yang, Yukun Wang, Wenxun Dai, Jiaqi Li, Linqing Wang, Qixun Wang, Zhiyong Xu, Yingfang Zhang, Jiangfeng Xiong, Weijie Kong, Chao Zhang, Hongxin Zhang, Qiaoling Zheng, Weiting Guo, Xinchi Deng, Yixuan Li, Renjia Wei, Yulin Jian, Duojun Huang, Xuhua Ren, Sihuan Lin, Yifu Sun, Yuan Zhou, Joey Wang, Qin Lin, Jingmiao Yu, Jihong Zhang, Caesar Zhong, Di Wang, Yuhong Liu,  Linus, Jie Jiang, Longhuang Wu, Shuai Shao, Qinglin Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent game creation represents a transformative advancement in game
development, utilizing generative artificial intelligence to dynamically
generate and enhance game content. Despite notable progress in generative
models, the comprehensive synthesis of high-quality game assets, including both
images and videos, remains a challenging frontier. To create high-fidelity game
content that simultaneously aligns with player preferences and significantly
boosts designer efficiency, we present Hunyuan-Game, an innovative project
designed to revolutionize intelligent game production. Hunyuan-Game encompasses
two primary branches: image generation and video generation. The image
generation component is built upon a vast dataset comprising billions of game
images, leading to the development of a group of customized image generation
models tailored for game scenarios: (1) General Text-to-Image Generation. (2)
Game Visual Effects Generation, involving text-to-effect and reference
image-based game visual effect generation. (3) Transparent Image Generation for
characters, scenes, and game visual effects. (4) Game Character Generation
based on sketches, black-and-white images, and white models. The video
generation component is built upon a comprehensive dataset of millions of game
and anime videos, leading to the development of five core algorithmic models,
each targeting critical pain points in game development and having robust
adaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2)
360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4)
Generative Video Super-Resolution. (5) Interactive Game Video Generation. These
image and video generation models not only exhibit high-level aesthetic
expression but also deeply integrate domain-specific knowledge, establishing a
systematic understanding of diverse game and anime art styles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intra-class Patch Swap for Self-Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjun Choi, Eun Som Jeon, Ankita Shukla, Pavan Turaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) is a valuable technique for compressing large
deep learning models into smaller, edge-suitable networks. However,
conventional KD frameworks rely on pre-trained high-capacity teacher networks,
which introduce significant challenges such as increased memory/storage
requirements, additional training costs, and ambiguity in selecting an
appropriate teacher for a given student model. Although a teacher-free
distillation (self-distillation) has emerged as a promising alternative, many
existing approaches still rely on architectural modifications or complex
training procedures, which limit their generality and efficiency.
  To address these limitations, we propose a novel framework based on
teacher-free distillation that operates using a single student network without
any auxiliary components, architectural modifications, or additional learnable
parameters. Our approach is built on a simple yet highly effective
augmentation, called intra-class patch swap augmentation. This augmentation
simulates a teacher-student dynamic within a single model by generating pairs
of intra-class samples with varying confidence levels, and then applying
instance-to-instance distillation to align their predictive distributions. Our
method is conceptually simple, model-agnostic, and easy to implement, requiring
only a single augmentation function. Extensive experiments across image
classification, semantic segmentation, and object detection show that our
method consistently outperforms both existing self-distillation baselines and
conventional teacher-based KD approaches. These results suggest that the
success of self-distillation could hinge on the design of the augmentation
itself. Our codes are available at
https://github.com/hchoi71/Intra-class-Patch-Swap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Neurocomputing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CONSIGN: Conformal Segmentation Informed by Spatial Groupings via
  Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Viti, Elias Karabelas, Martin Holler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most machine learning-based image segmentation models produce pixel-wise
confidence scores - typically derived from softmax outputs - that represent the
model's predicted probability for each class label at every pixel. While this
information can be particularly valuable in high-stakes domains such as medical
imaging, these (uncalibrated) scores are heuristic in nature and do not
constitute rigorous quantitative uncertainty estimates. Conformal prediction
(CP) provides a principled framework for transforming heuristic confidence
scores into statistically valid uncertainty estimates. However, applying CP
directly to image segmentation ignores the spatial correlations between pixels,
a fundamental characteristic of image data. This can result in overly
conservative and less interpretable uncertainty estimates. To address this, we
propose CONSIGN (Conformal Segmentation Informed by Spatial Groupings via
Decomposition), a CP-based method that incorporates spatial correlations to
improve uncertainty quantification in image segmentation. Our method generates
meaningful prediction sets that come with user-specified, high-probability
error guarantees. It is compatible with any pre-trained segmentation model
capable of generating multiple sample outputs - such as those using dropout,
Bayesian modeling, or ensembles. We evaluate CONSIGN against a standard
pixel-wise CP approach across three medical imaging datasets and two COCO
dataset subsets, using three different pre-trained segmentation models. Results
demonstrate that accounting for spatial structure significantly improves
performance across multiple metrics and enhances the quality of uncertainty
estimates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unintended Bias in 2D+ Image Segmentation and Its Effect on Attention
  Asymmetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zsófia Molnár, Gergely Szabó, András Horváth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised pretrained models have become widely used in deep learning,
especially for image segmentation tasks. However, when applied to specialized
datasets such as biomedical imaging, pretrained weights often introduce
unintended biases. These biases cause models to assign different levels of
importance to different slices, leading to inconsistencies in feature
utilization, which can be observed as asymmetries in saliency map
distributions. This transfer of color distributions from natural images to
non-natural datasets can compromise model performance and reduce the
reliability of results. In this study, we investigate the effects of these
biases and propose strategies to mitigate them. Through a series of
experiments, we test both pretrained and randomly initialized models, comparing
their performance and saliency map distributions. Our proposed methods, which
aim to neutralize the bias introduced by pretrained color channel weights,
demonstrate promising results, offering a practical approach to improving model
explainability while maintaining the benefits of pretrained models. This
publication presents our findings, providing insights into addressing
pretrained weight biases across various deep learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking the Power of SAM 2 for Few-Shot Segmentation <span class="chip">ICML'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianxiong Xu, Lanyun Zhu, Xuanyi Liu, Guosheng Lin, Cheng Long, Ziyue Li, Rui Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few
classes to segment arbitrary classes, but at the risk of overfitting. To
address this, some methods use the well-learned knowledge of foundation models
(e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM
by supporting video segmentation, whose class-agnostic matching ability is
useful to FSS. A simple idea is to encode support foreground (FG) features as
memory, with which query FG features are matched and fused. Unfortunately, the
FG objects in different frames of SAM 2's video data are always the same
identity, while those in FSS are different identities, i.e., the matching step
is incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo
query memory, matching with query features in a compatible way. However, the
memories can never be as accurate as the real ones, i.e., they are likely to
contain incomplete query FG, and some unexpected query background (BG)
features, leading to wrong segmentation. Hence, we further design Iterative
Memory Refinement to fuse more query FG features into the memory, and devise a
Support-Calibrated Memory Attention to suppress the unexpected query BG
features in memory. Extensive experiments have been conducted on PASCAL-5$^i$
and COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot
mIoU can be 4.2\% better than the best baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by ICML'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizable Multispectral Land Cover Classification via
  Frequency-Aware Mixture of Low-Rank Token Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Chen, Shen Yan, Juelin Zhu, Chen Chen, Yu Liu, Maojun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Land-MoE, a novel approach for multispectral land cover
classification (MLCC). Spectral shift, which emerges from disparities in
sensors and geospatial conditions, poses a significant challenge in this
domain. Existing methods predominantly rely on domain adaptation and
generalization strategies, often utilizing small-scale models that exhibit
limited performance. In contrast, Land-MoE addresses these issues by
hierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts,
to fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner.
Specifically, Land-MoE comprises two key modules: the mixture of low-rank token
experts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages
rank-differentiated tokens to generate diverse feature adjustments for
individual instances within multispectral images. By dynamically combining
learnable low-rank token experts of varying ranks, it enhances the robustness
against spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on
the refined features. This process enables the model to effectively capture
frequency band information that is strongly correlated with semantic essence,
while simultaneously suppressing frequency noise irrelevant to the task.
Comprehensive experiments on MLCC tasks involving cross-sensor and
cross-geospatial setups demonstrate that Land-MoE outperforms existing methods
by a large margin. Additionally, the proposed approach has also achieved
state-of-the-art performance in domain generalization semantic segmentation
tasks of RGB remote sensing images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large-Scale Multi-Character Interaction Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Chang, He Wang, George Alex Koulieris, Hubert P. H. Shum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating large-scale multi-character interactions is a challenging and
important task in character animation. Multi-character interactions involve not
only natural interactive motions but also characters coordinated with each
other for transition. For example, a dance scenario involves characters dancing
with partners and also characters coordinated to new partners based on spatial
and temporal observations. We term such transitions as coordinated interactions
and decompose them into interaction synthesis and transition planning. Previous
methods of single-character animation do not consider interactions that are
critical for multiple characters. Deep-learning-based interaction synthesis
usually focuses on two characters and does not consider transition planning.
Optimization-based interaction synthesis relies on manually designing objective
functions that may not generalize well. While crowd simulation involves more
characters, their interactions are sparse and passive. We identify two
challenges to multi-character interaction synthesis, including the lack of data
and the planning of transitions among close and dense interactions. Existing
datasets either do not have multiple characters or do not have close and dense
interactions. The planning of transitions for multi-character close and dense
interactions needs both spatial and temporal considerations. We propose a
conditional generative pipeline comprising a coordinatable multi-character
interaction space for interaction synthesis and a transition planning network
for coordinations. Our experiments demonstrate the effectiveness of our
proposed pipeline for multicharacter interaction synthesis and the applications
facilitated by our method show the scalability and transferability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Textual Steering Vectors Can Improve Visual Understanding in Multimodal
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Woody Haosheng Gan, Deqing Fu, Julian Asilis, Ollie Liu, Dani Yogatama, Vatsal Sharan, Robin Jia, Willie Neiswanger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steering methods have emerged as effective and targeted tools for guiding
large language models' (LLMs) behavior without modifying their parameters.
Multimodal large language models (MLLMs), however, do not currently enjoy the
same suite of techniques, due in part to their recency and architectural
diversity. Inspired by this gap, we investigate whether MLLMs can be steered
using vectors derived from their text-only LLM backbone, via sparse
autoencoders (SAEs), mean shift, and linear probing. We find that text-derived
steering consistently enhances multimodal accuracy across diverse MLLM
architectures and visual tasks. In particular, mean shift boosts spatial
relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to
+3.3%, outperforming prompting and exhibiting strong generalization to
out-of-distribution datasets. These results highlight textual steering vectors
as a powerful, efficient mechanism for enhancing grounding in MLLMs with
minimal additional data collection and computational overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Place Recognition: A Comprehensive <span class="highlight-title">Review</span>, Current Challenges and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Li, Tianyi Shang, Pengjie Xu, Zhaojun Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Place recognition is a cornerstone of vehicle navigation and mapping, which
is pivotal in enabling systems to determine whether a location has been
previously visited. This capability is critical for tasks such as loop closure
in Simultaneous Localization and Mapping (SLAM) and long-term navigation under
varying environmental conditions. In this survey, we comprehensively review
recent advancements in place recognition, emphasizing three representative
methodological paradigms: Convolutional Neural Network (CNN)-based approaches,
Transformer-based frameworks, and cross-modal strategies. We begin by
elucidating the significance of place recognition within the broader context of
autonomous systems. Subsequently, we trace the evolution of CNN-based methods,
highlighting their contributions to robust visual descriptor learning and
scalability in large-scale environments. We then examine the emerging class of
Transformer-based models, which leverage self-attention mechanisms to capture
global dependencies and offer improved generalization across diverse scenes.
Furthermore, we discuss cross-modal approaches that integrate heterogeneous
data sources such as Lidar, vision, and text description, thereby enhancing
resilience to viewpoint, illumination, and seasonal variations. We also
summarize standard datasets and evaluation metrics widely adopted in the
literature. Finally, we identify current research challenges and outline
prospective directions, including domain adaptation, real-time performance, and
lifelong learning, to inspire future advancements in this domain. The unified
framework of leading-edge place recognition methods, i.e., code library, and
the results of their experimental evaluations are available at
https://github.com/CV4RA/SOTA-Place-Recognitioner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in
  Brain MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cosmin I. Bercea, Jun Li, Philipp Raffler, Evamaria O. Riedel, Lena Schmitzer, Angela Kurz, Felix Bitzer, Paula Roßmüller, Julian Canisius, Mirjam L. Beyrle, Che Liu, Wenjia Bai, Bernhard Kainz, Julia A. Schnabel, Benedikt Wiestler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world applications, deployed models encounter inputs that differ
from the data seen during training. Out-of-distribution detection identifies
whether an input stems from an unseen distribution, while open-world
recognition flags such inputs to ensure the system remains robust as
ever-emerging, previously $unknown$ categories appear and must be addressed
without retraining. Foundation and vision-language models are pre-trained on
large and diverse datasets with the expectation of broad generalization across
domains, including medical imaging. However, benchmarking these models on test
sets with only a few common outlier types silently collapses the evaluation
back to a closed-set problem, masking failures on rare or truly novel
conditions encountered in clinical use.
  We therefore present $NOVA$, a challenging, real-life $evaluation-only$
benchmark of $\sim$900 brain MRI scans that span 281 rare pathologies and
heterogeneous acquisition protocols. Each case includes rich clinical
narratives and double-blinded expert bounding-box annotations. Together, these
enable joint assessment of anomaly localisation, visual captioning, and
diagnostic reasoning. Because NOVA is never used for training, it serves as an
$extreme$ stress-test of out-of-distribution generalisation: models must bridge
a distribution gap both in sample appearance and in semantic space. Baseline
results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and
Qwen2.5-VL-72B) reveal substantial performance drops across all tasks,
establishing NOVA as a rigorous testbed for advancing models that can detect,
localize, and reason about truly unknown anomalies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Vision Mamba Across Resolutions via Fractal Traversal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Li, Haoke Xiao, Lv Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Mamba has recently emerged as a promising alternative to
Transformer-based architectures, offering linear complexity in sequence length
while maintaining strong modeling capacity. However, its adaptation to visual
inputs is hindered by challenges in 2D-to-1D patch serialization and weak
scalability across input resolutions. Existing serialization strategies such as
raster scanning disrupt local spatial continuity and limit the model's ability
to generalize across scales. In this paper, we propose FractalMamba++, a robust
vision backbone that leverages fractal-based patch serialization via Hilbert
curves to preserve spatial locality and enable seamless resolution
adaptability. To address long-range dependency fading in high-resolution
inputs, we further introduce a Cross-State Routing (CSR) mechanism that
enhances global context propagation through selective state reuse.
Additionally, we propose a Positional-Relation Capture (PRC) module to recover
local adjacency disrupted by curve inflection points. Extensive experiments on
image classification, semantic segmentation, object detection, and change
detection demonstrate that FractalMamba++ consistently outperforms previous
Mamba-based backbones, particularly under high-resolution settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progressing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dolphin: Document Image Parsing via Heterogeneous Anchor <span class="highlight-title">Prompt</span>ing <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Feng, Shu Wei, Xiang Fei, Wei Shi, Yingdong Han, Lei Liao, Jinghui Lu, Binghong Wu, Qi Liu, Chunhui Lin, Jingqun Tang, Hao Liu, Can Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document image parsing is challenging due to its complexly intertwined
elements such as text paragraphs, figures, formulas, and tables. Current
approaches either assemble specialized expert models or directly generate
page-level content autoregressively, facing integration overhead, efficiency
bottlenecks, and layout structure degradation despite their decent performance.
To address these limitations, we present \textit{Dolphin}
(\textit{\textbf{Do}cument Image \textbf{P}arsing via \textbf{H}eterogeneous
Anchor Prompt\textbf{in}g}), a novel multimodal document image parsing model
following an analyze-then-parse paradigm. In the first stage, Dolphin generates
a sequence of layout elements in reading order. These heterogeneous elements,
serving as anchors and coupled with task-specific prompts, are fed back to
Dolphin for parallel content parsing in the second stage. To train Dolphin, we
construct a large-scale dataset of over 30 million samples, covering
multi-granularity parsing tasks. Through comprehensive evaluations on both
prevalent benchmarks and self-constructed ones, Dolphin achieves
state-of-the-art performance across diverse page-level and element-level
settings, while ensuring superior efficiency through its lightweight
architecture and parallel parsing mechanism. The code and pre-trained models
are publicly available at https://github.com/ByteDance/Dolphin
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Concept-Driven Logical Rules for Interpretable and
  Generalizable Medical Image Classification <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Gao, Hangqi Zhou, Zheyao Gao, Bomin Wang, Shangqi Gao, Sihan Wang, Xiahai Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pursuit of decision safety in clinical applications highlights the
potential of concept-based methods in medical imaging. While these models offer
active interpretability, they often suffer from concept leakages, where
unintended information within soft concept representations undermines both
interpretability and generalizability. Moreover, most concept-based models
focus solely on local explanations (instance-level), neglecting the global
decision logic (dataset-level). To address these limitations, we propose
Concept Rule Learner (CRL), a novel framework to learn Boolean logical rules
from binarized visual concepts. CRL employs logical layers to capture concept
correlations and extract clinically meaningful rules, thereby providing both
local and global interpretability. Experiments on two medical image
classification tasks show that CRL achieves competitive performance with
existing methods while significantly improving generalizability to
out-of-distribution data. The code of our work is available at
https://github.com/obiyoag/crl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>early accepted by MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Structured State Space for Multispectral-fused Small Target
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianqian Zhang, WeiJun Wang, Yunxing Liu, Li Zhou, Hao Zhao, Junshe An, Zihan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Target detection in high-resolution remote sensing imagery faces challenges
due to the low recognition accuracy of small targets and high computational
costs. The computational complexity of the Transformer architecture increases
quadratically with image resolution, while Convolutional Neural Networks (CNN)
architectures are forced to stack deeper convolutional layers to expand their
receptive fields, leading to an explosive growth in computational demands. To
address these computational constraints, we leverage Mamba's linear complexity
for efficiency. However, Mamba's performance declines for small targets,
primarily because small targets occupy a limited area in the image and have
limited semantic information. Accurate identification of these small targets
necessitates not only Mamba's global attention capabilities but also the
precise capture of fine local details. To this end, we enhance Mamba by
developing the Enhanced Small Target Detection (ESTD) module and the
Convolutional Attention Residual Gate (CARG) module. The ESTD module bolsters
local attention to capture fine-grained details, while the CARG module, built
upon Mamba, emphasizes spatial and channel-wise information, collectively
improving the model's ability to capture distinctive representations of small
targets. Additionally, to highlight the semantic representation of small
targets, we design a Mask Enhanced Pixel-level Fusion (MEPF) module for
multispectral fusion, which enhances target features by effectively fusing
visible and infrared multimodal information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarially <span class="highlight-title">Pretrain</span>ed <span class="highlight-title">Transformer</span>s may be Universally Robust
  In-Context Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training is one of the most effective adversarial defenses, but
it incurs a high computational cost. In this study, we show that transformers
adversarially pretrained on diverse tasks can serve as robust foundation models
and eliminate the need for adversarial training in downstream tasks.
Specifically, we theoretically demonstrate that through in-context learning, a
single adversarially pretrained transformer can robustly generalize to multiple
unseen tasks without any additional training, i.e., without any parameter
updates. This robustness stems from the model's focus on robust features and
its resistance to attacks that exploit non-predictive features. Besides these
positive findings, we also identify several limitations. Under certain
conditions (though unrealistic), no universally robust single-layer
transformers exist. Moreover, robust transformers exhibit an
accuracy--robustness trade-off and require a large number of in-context
demonstrations. The code is available at
https://github.com/s-kumano/universally-robust-in-context-learner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AppleGrowthVision: A large-scale stereo <span class="highlight-title">dataset</span> for phenological
  analysis, fruit detection, and 3D reconstruction in apple orchards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura-Sophia von Hirschhausen, Jannes S. Magnusson, Mykyta Kovalenko, Fredrik Boye, Tanay Rawat, Peter Eisert, Anna Hilsmann, Sebastian Pretzsch, Sebastian Bosse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has transformed computer vision for precision agriculture, yet
apple orchard monitoring remains limited by dataset constraints. The lack of
diverse, realistic datasets and the difficulty of annotating dense,
heterogeneous scenes. Existing datasets overlook different growth stages and
stereo imagery, both essential for realistic 3D modeling of orchards and tasks
like fruit localization, yield estimation, and structural analysis. To address
these gaps, we present AppleGrowthVision, a large-scale dataset comprising two
subsets. The first includes 9,317 high resolution stereo images collected from
a farm in Brandenburg (Germany), covering six agriculturally validated growth
stages over a full growth cycle. The second subset consists of 1,125 densely
annotated images from the same farm in Brandenburg and one in Pillnitz
(Germany), containing a total of 31,084 apple labels. AppleGrowthVision
provides stereo-image data with agriculturally validated growth stages,
enabling precise phenological analysis and 3D reconstructions. Extending
MinneApple with our data improves YOLOv8 performance by 7.69 % in terms of
F1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by
31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy
using VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges
the gap between agricultural science and computer vision, by enabling the
development of robust models for fruit detection, growth modeling, and 3D
analysis in precision agriculture. Future work includes improving annotation,
enhancing 3D reconstruction, and extending multimodal analysis across all
growth stages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniStyle: Filtering High Quality Style Transfer Data at Scale <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Wang, Ruiqi Liu, Jiang Lin, Fei Liu, Zili Yi, Yilin Wang, Rui Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce OmniStyle-1M, a large-scale paired style transfer
dataset comprising over one million content-style-stylized image triplets
across 1,000 diverse style categories, each enhanced with textual descriptions
and instruction prompts. We show that OmniStyle-1M can not only enable
efficient and scalable of style transfer models through supervised training but
also facilitate precise control over target stylization. Especially, to ensure
the quality of the dataset, we introduce OmniFilter, a comprehensive style
transfer quality assessment framework, which filters high-quality triplets
based on content preservation, style consistency, and aesthetic appeal.
Building upon this foundation, we propose OmniStyle, a framework based on the
Diffusion Transformer (DiT) architecture designed for high-quality and
efficient style transfer. This framework supports both instruction-guided and
image-guided style transfer, generating high resolution outputs with
exceptional detail. Extensive qualitative and quantitative evaluations
demonstrate OmniStyle's superior performance compared to existing approaches,
highlighting its efficiency and versatility. OmniStyle-1M and its accompanying
methodologies provide a significant contribution to advancing high-quality
style transfer, offering a valuable resource for the research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Efficient Multi-Scale Deformable Attention on NPU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenghuan Huang, Zhigeng Xu, Chong Sun, Chen Li, Ziyang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-scale deformable attention (MSDA) is a flexible and powerful feature
extraction mechanism for visual tasks, but its random-access grid sampling
strategy poses significant optimization challenges, especially on
domain-specific accelerators such as NPUs. In this work, we present a co-design
approach that systematically rethinks memory access and computation strategies
for MSDA on the Ascend NPU architecture. With this co-design approach, our
implementation supports both efficient forward and backward computation, is
fully adapted for training workloads, and incorporates a suite of
hardware-aware optimizations. Extensive experiments show that our solution
achieves up to $5.9\times$ (forward), $8.9\times$ (backward), and $7.3\times$
(end-to-end training) speedup over the grid sample-based baseline, and
$1.9\times$, $2.4\times$, and $2.0\times$ acceleration over the latest vendor
library, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Training from Mean Field Perspective <span class="chip">NeurIPS23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although adversarial training is known to be effective against adversarial
examples, training dynamics are not well understood. In this study, we present
the first theoretical analysis of adversarial training in random deep neural
networks without any assumptions on data distributions. We introduce a new
theoretical framework based on mean field theory, which addresses the
limitations of existing mean field-based approaches. Based on this framework,
we derive (empirically tight) upper bounds of $\ell_q$ norm-based adversarial
loss with $\ell_p$ norm-based adversarial examples for various values of $p$
and $q$. Moreover, we prove that networks without shortcuts are generally not
adversarially trainable and that adversarial training reduces network capacity.
We also show that network width alleviates these issues. Furthermore, we
present the various impacts of the input and output dimensions on the upper
bounds and time evolution of the weight variance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-end Cortical Surface Reconstruction from Clinical Magnetic
  Resonance Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesper Duemose Nielsen, Karthik Gopinath, Andrew Hoopes, Adrian Dalca, Colin Magdamo, Steven Arnold, Sudeshna Das, Axel Thielscher, Juan Eugenio Iglesias, Oula Puonti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surface-based cortical analysis is valuable for a variety of neuroimaging
tasks, such as spatial normalization, parcellation, and gray matter (GM)
thickness estimation. However, most tools for estimating cortical surfaces work
exclusively on scans with at least 1 mm isotropic resolution and are tuned to a
specific magnetic resonance (MR) contrast, often T1-weighted (T1w). This
precludes application using most clinical MR scans, which are very
heterogeneous in terms of contrast and resolution. Here, we use synthetic
domain-randomized data to train the first neural network for explicit
estimation of cortical surfaces from scans of any contrast and resolution,
without retraining. Our method deforms a template mesh to the white matter (WM)
surface, which guarantees topological correctness. This mesh is further
deformed to estimate the GM surface. We compare our method to
recon-all-clinical (RAC), an implicit surface reconstruction method which is
currently the only other tool capable of processing heterogeneous clinical MR
scans, on ADNI and a large clinical dataset (n=1,332). We show a approximately
50 % reduction in cortical thickness error (from 0.50 to 0.24 mm) with respect
to RAC and better recovery of the aging-related cortical thinning patterns
detected by FreeSurfer on high-resolution T1w scans. Our method enables fast
and accurate surface reconstruction of clinical scans, allowing studies (1)
with sample sizes far beyond what is feasible in a research setting, and (2) of
clinical populations that are difficult to enroll in research studies. The code
is publicly available at https://github.com/simnibs/brainnet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EGFormer: Towards Efficient and Generalizable Multimodal Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelin Zhang, Tao Zhang,  KediLI, Xu Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent efforts have explored multimodal semantic segmentation using various
backbone architectures. However, while most methods aim to improve accuracy,
their computational efficiency remains underexplored. To address this, we
propose EGFormer, an efficient multimodal semantic segmentation framework that
flexibly integrates an arbitrary number of modalities while significantly
reducing model parameters and inference time without sacrificing performance.
Our framework introduces two novel modules. First, the Any-modal Scoring Module
(ASM) assigns importance scores to each modality independently, enabling
dynamic ranking based on their feature maps. Second, the Modal Dropping Module
(MDM) filters out less informative modalities at each stage, selectively
preserving and aggregating only the most valuable features. This design allows
the model to leverage useful information from all available modalities while
discarding redundancy, thus ensuring high segmentation quality. In addition to
efficiency, we evaluate EGFormer on a synthetic-to-real transfer task to
demonstrate its generalizability. Extensive experiments show that EGFormer
achieves competitive performance with up to 88 percent reduction in parameters
and 50 percent fewer GFLOPs. Under unsupervised domain adaptation settings, it
further achieves state-of-the-art transfer performance compared to existing
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models <span class="chip">ICRA-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15250v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15250v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sombit Dey, Jan-Nico Zaech, Nikolay Nikolov, Luc Van Gool, Danda Pani Paudel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in large language models and access to large-scale robotic
datasets has sparked a paradigm shift in robotics models transforming them into
generalists able to adapt to various tasks, scenes, and robot modalities. A
large step for the community are open Vision Language Action models which
showcase strong performance in a wide variety of tasks. In this work, we study
the visual generalization capabilities of three existing robotic foundation
models, and propose a corresponding evaluation framework. Our study shows that
the existing models do not exhibit robustness to visual out-of-domain
scenarios. This is potentially caused by limited variations in the training
data and/or catastrophic forgetting, leading to domain limitations in the
vision foundation models. We further explore OpenVLA, which uses two
pre-trained vision foundation models and is, therefore, expected to generalize
to out-of-domain experiments. However, we showcase catastrophic forgetting by
DINO-v2 in OpenVLA through its failure to fulfill the task of depth regression.
To overcome the aforementioned issue of visual catastrophic forgetting, we
propose a gradual backbone reversal approach founded on model merging. This
enables OpenVLA -- which requires the adaptation of the visual backbones during
initial training -- to regain its visual generalization ability. Regaining this
capability enables our ReVLA model to improve over OpenVLA by a factor of 77\%
and 66\% for grasping and lifting in visual OOD tasks. Comprehensive
evaluations, episode rollouts and model weights are available on the ReVLA Page
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA-2025, Atlanta</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Effective Can Dropout Be in Multiple Instance Learning ? <span class="chip">ICML2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.14783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.14783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhui Zhu, Peijie Qiu, Xiwen Chen, Zhangsihao Yang, Aristeidis Sotiras, Abolfazl Razi, Yalin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple Instance Learning (MIL) is a popular weakly-supervised method for
various applications, with a particular interest in histological whole slide
image (WSI) classification. Due to the gigapixel resolution of WSI,
applications of MIL in WSI typically necessitate a two-stage training scheme:
first, extract features from the pre-trained backbone and then perform MIL
aggregation. However, it is well-known that this suboptimal training scheme
suffers from "noisy" feature embeddings from the backbone and inherent weak
supervision, hindering MIL from learning rich and generalizable features.
However, the most commonly used technique (i.e., dropout) for mitigating this
issue has yet to be explored in MIL. In this paper, we empirically explore how
effective the dropout can be in MIL. Interestingly, we observe that dropping
the top-k most important instances within a bag leads to better performance and
generalization even under noise attack. Based on this key observation, we
propose a novel MIL-specific dropout method, termed MIL-Dropout, which
systematically determines which instances to drop. Experiments on five MIL
benchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the
performance of current MIL methods with a negligible computational cost. The
code is available at https://github.com/ChongQingNoSubway/MILDropout.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Generalizability of Foundation Models for Crop Type Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09451v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09451v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Chia Chang, Adam J. Stewart, Favyen Bastani, Piper Wolters, Shreya Kannan, George R. Huber, Jingtong Wang, Arindam Banerjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models pre-trained using self-supervised learning have shown
powerful transfer learning capabilities on various downstream tasks, including
language understanding, text generation, and image recognition. The Earth
observation (EO) field has produced several foundation models pre-trained
directly on multispectral satellite imagery for applications like precision
agriculture, wildfire and drought monitoring, and natural disaster response.
However, few studies have investigated the ability of these models to
generalize to new geographic locations, and potential concerns of geospatial
bias -- models trained on data-rich developed nations not transferring well to
data-scarce developing nations -- remain. We evaluate three popular EO
foundation models, SSL4EO-S12, SatlasPretrain, and ImageNet, on five crop
classification datasets across five continents. Results show that pre-trained
weights designed explicitly for Sentinel-2, such as SSL4EO-S12, outperform
general pre-trained weights like ImageNet. While only 100 labeled images are
sufficient for achieving high overall accuracy, 900 images are required to
mitigate class imbalance and improve average accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE IGARSS 2025. The final version will appear in the
  Proceedings of the IEEE International Geoscience and Remote Sensing Symposium
  (IGARSS) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KIND: Knowledge Integration and Diversion for Training Decomposable
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07337v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07337v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Xie, Fu Feng, Ruixiao Shi, Jing Wang, Yong Rui, Xin Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained models have become the preferred backbone due to the increasing
complexity of model parameters. However, traditional pre-trained models often
face deployment challenges due to their fixed sizes, and are prone to negative
transfer when discrepancies arise between training tasks and target tasks. To
address this, we propose KIND, a novel pre-training method designed to
construct decomposable models. KIND integrates knowledge by incorporating
Singular Value Decomposition (SVD) as a structural constraint, with each basic
component represented as a combination of a column vector, singular value, and
row vector from U, \Sigma, and V^\top matrices. These components are
categorized into learngenes for encapsulating class-agnostic knowledge and
tailors for capturing class-specific knowledge, with knowledge diversion
facilitated by a class gate mechanism during training. Extensive experiments
demonstrate that models pre-trained with KIND can be decomposed into learngenes
and tailors, which can be adaptively recombined for diverse
resource-constrained deployments. Moreover, for tasks with large domain shifts,
transferring only learngenes with task-agnostic knowledge, when combined with
randomly initialized tailors, effectively mitigates domain shifts. Code will be
made available at https://github.com/Te4P0t/KIND.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ActiveSSF: An Active-Learning-Guided <span class="highlight-title">Self-Supervised</span> Framework for
  Long-Tailed Megakaryocyte Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linghao Zhuang, Ying Zhang, Gege Yuan, Xingyue Zhao, Zhiping Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise classification of megakaryocytes is crucial for diagnosing
myelodysplastic syndromes. Although self-supervised learning has shown promise
in medical image analysis, its application to classifying megakaryocytes in
stained slides faces three main challenges: (1) pervasive background noise that
obscures cellular details, (2) a long-tailed distribution that limits data for
rare subtypes, and (3) complex morphological variations leading to high
intra-class variability. To address these issues, we propose the ActiveSSF
framework, which integrates active learning with self-supervised pretraining.
Specifically, our approach employs Gaussian filtering combined with K-means
clustering and HSV analysis (augmented by clinical prior knowledge) for
accurate region-of-interest extraction; an adaptive sample selection mechanism
that dynamically adjusts similarity thresholds to mitigate class imbalance; and
prototype clustering on labeled samples to overcome morphological complexity.
Experimental results on clinical megakaryocyte datasets demonstrate that
ActiveSSF not only achieves state-of-the-art performance but also significantly
improves recognition accuracy for rare subtypes. Moreover, the integration of
these advanced techniques further underscores the practical potential of
ActiveSSF in clinical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StainDiffuser: MultiTask Dual Diffusion Model for Virtual Staining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11340v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11340v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tushar Kataria, Beatrice Knudsen, Shireen Y. Elhabian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hematoxylin and Eosin (H&E) staining is widely regarded as the standard in
pathology for diagnosing diseases and tracking tumor recurrence. While H&E
staining shows tissue structures, it lacks the ability to reveal specific
proteins that are associated with disease severity and treatment response.
Immunohistochemical (IHC) stains use antibodies to highlight the expression of
these proteins on their respective cell types, improving diagnostic accuracy,
and assisting with drug selection for treatment. Despite their value, IHC
stains require additional time and resources, limiting their utilization in
some clinical settings. Recent advances in deep learning have positioned
Image-to-Image (I2I) translation as a computational, cost-effective alternative
for IHC. I2I generates high fidelity stain transformations digitally,
potentially replacing manual staining in IHC. Diffusion models, the current
state of the art in image generation and conditional tasks, are particularly
well suited for virtual IHC due to their ability to produce high quality images
and resilience to mode collapse. However, these models require extensive and
diverse datasets (often millions of samples) to achieve a robust performance, a
challenge in virtual staining applications where only thousands of samples are
typically available. Inspired by the success of multitask deep learning models
in scenarios with limited data, we introduce STAINDIFFUSER, a novel multitask
diffusion architecture tailored to virtual staining that achieves convergence
with smaller datasets. STAINDIFFUSER simultaneously trains two diffusion
processes: (a) generating cell specific IHC stains from H&E images and (b)
performing H&E based cell segmentation, utilizing coarse segmentation labels
exclusively during training. STAINDIFFUSER generates high-quality virtual
stains for two markers, outperforming over twenty I2I baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Synthetic Data and Fine-grained Adaptive Feature Alignment for
  Composed Person Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16515v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16515v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Liu, Haiwen Li, Zhaohui Hou, Zhicheng Zhao, Fei Su, Yuan Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person retrieval has attracted rising attention. Existing methods are mainly
divided into two retrieval modes, namely image-only and text-only. However,
they are unable to make full use of the available information and are difficult
to meet diverse application requirements. To address the above limitations, we
propose a new Composed Person Retrieval (CPR) task, which combines visual and
textual queries to identify individuals of interest from large-scale person
image databases. Nevertheless, the foremost difficulty of the CPR task is the
lack of available annotated datasets. Therefore, we first introduce a scalable
automatic data synthesis pipeline, which decomposes complex multimodal data
generation into the creation of textual quadruples followed by
identity-consistent image synthesis using fine-tuned generative models.
Meanwhile, a multimodal filtering method is designed to ensure the resulting
SynCPR dataset retains 1.15 million high-quality and fully synthetic triplets.
Additionally, to improve the representation of composed person queries, we
propose a novel Fine-grained Adaptive Feature Alignment (FAFA) framework
through fine-grained dynamic alignment and masked feature reasoning. Moreover,
for objective evaluation, we manually annotate the Image-Text Composed Person
Retrieval (ITCPR) test set. The extensive experiments demonstrate the
effectiveness of the SynCPR dataset and the superiority of the proposed FAFA
framework when compared with the state-of-the-art methods. All code and data
will be provided at
https://github.com/Delong-liu-bupt/Composed_Person_Retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal Incremental Learning: Mitigating Confusion from Inter- and
  Intra-task Distribution Randomness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07035v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07035v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Luo, Yi Zhou, Tao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incremental learning (IL) aims to overcome catastrophic forgetting of
previous tasks while learning new ones. Existing IL methods make strong
assumptions that the incoming task type will either only increases new classes
or domains (i.e. Class IL, Domain IL), or increase by a static scale in a
class- and domain-agnostic manner (i.e. Versatile IL (VIL)), which greatly
limit their applicability in the unpredictable and dynamic wild. In this work,
we investigate $\textbf{Universal Incremental Learning (UIL)}$, where a model
neither knows which new classes or domains will increase along sequential
tasks, nor the scale of the increments within each task. This uncertainty
prevents the model from confidently learning knowledge from all task
distributions and symmetrically focusing on the diverse knowledge within each
task distribution. Consequently, UIL presents a more general and realistic IL
scenario, making the model face confusion arising from inter-task and
intra-task distribution randomness. To $\textbf{Mi}$tigate both
$\textbf{Co}$nfusion, we propose a simple yet effective framework for UIL,
named $\textbf{MiCo}$. At the inter-task distribution level, we employ a
multi-objective learning scheme to enforce accurate and deterministic
predictions, and its effectiveness is further enhanced by a direction
recalibration module that reduces conflicting gradients. Moreover, at the
intra-task distribution level, we introduce a magnitude recalibration module to
alleviate asymmetrical optimization towards imbalanced class distribution.
Extensive experiments on three benchmarks demonstrate the effectiveness of our
method, outperforming existing state-of-the-art methods in both the UIL
scenario and the VIL scenario. Our code will be available at
$\href{https://github.com/rolsheng/UIL}{here}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Customized SAM 2 for Referring Remote Sensing Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07266v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07266v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu Rong, Meng Lan, Qian Zhang, Lefei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring Remote Sensing Image Segmentation (RRSIS) aims to segment target
objects in remote sensing (RS) images based on textual descriptions. Although
Segment Anything Model 2 (SAM 2) has shown remarkable performance in various
segmentation tasks, its application to RRSIS presents several challenges,
including understanding the text-described RS scenes and generating effective
prompts from text descriptions. To address these issues, we propose RS2-SAM 2,
a novel framework that adapts SAM 2 to RRSIS by aligning the adapted RS
features and textual features, providing pseudo-mask-based dense prompts, and
enforcing boundary constraints. Specifically, we first employ a union encoder
to jointly encode the visual and textual inputs, generating aligned visual and
text embeddings as well as multimodal class tokens. Then, we design a
bidirectional hierarchical fusion module to adapt SAM 2 to RS scenes and align
adapted visual features with the visually enhanced text embeddings, improving
the model's interpretation of text-described RS scenes. Additionally, a mask
prompt generator is introduced to take the visual embeddings and class tokens
as input and produce a pseudo-mask as the dense prompt of SAM 2. To further
refine segmentation, we introduce a text-guided boundary loss to optimize
segmentation boundaries by computing text-weighted gradient differences.
Experimental results on several RRSIS benchmarks demonstrate that RS2-SAM 2
achieves state-of-the-art performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the Correctness of Inference Patterns Used by LLMs for
  Judgment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09083v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09083v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Chen, Yuxuan Huang, Yixing Li, Dongrui Liu, Qihan Ren, Shuai Zhao, Kun Kuang, Zilong Zheng, Quanshi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a method to analyze the inference patterns used by Large
Language Models (LLMs) for judgment in a case study on legal LLMs, so as to
identify potential incorrect representations of the LLM, according to human
domain knowledge. Unlike traditional evaluations on language generation
results, we propose to evaluate the correctness of the detailed inference
patterns of an LLM behind its seemingly correct outputs. To this end, we
quantify the interactions between input phrases used by the LLM as primitive
inference patterns, because recent theoretical achievements have proven several
mathematical guarantees of the faithfulness of the interaction-based
explanation. We design a set of metrics to evaluate the detailed inference
patterns of LLMs. Experiments show that even when the language generation
results appear correct, a significant portion of the inference patterns used by
the LLM for the legal judgment may represent misleading or irrelevant logic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral-Spatial <span class="highlight-title">Self-Supervised</span> Learning for Few-Shot Hyperspectral
  Image Classification <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenchen Chen, Yanmei Zhang, Zhongwei Xiao, Jianping Chu, Xingbo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot classification of hyperspectral images (HSI) faces the challenge of
scarce labeled samples. Self-Supervised learning (SSL) and Few-Shot Learning
(FSL) offer promising avenues to address this issue. However, existing methods
often struggle to adapt to the spatial geometric diversity of HSIs and lack
sufficient spectral prior knowledge. To tackle these challenges, we propose a
method, Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral
Image Classification (S4L-FSC), aimed at improving the performance of few-shot
HSI classification. Specifically, we first leverage heterogeneous datasets to
pretrain a spatial feature extractor using a designed Rotation-Mirror
Self-Supervised Learning (RM-SSL) method, combined with FSL. This approach
enables the model to learn the spatial geometric diversity of HSIs using
rotation and mirroring labels as supervisory signals, while acquiring
transferable spatial meta-knowledge through few-shot learning. Subsequently,
homogeneous datasets are utilized to pretrain a spectral feature extractor via
a combination of FSL and Masked Reconstruction Self-Supervised Learning
(MR-SSL). The model learns to reconstruct original spectral information from
randomly masked spectral vectors, inferring spectral dependencies. In parallel,
FSL guides the model to extract pixel-level discriminative features, thereby
embedding rich spectral priors into the model. This spectral-spatial
pretraining method, along with the integration of knowledge from heterogeneous
and homogeneous sources, significantly enhances model performance. Extensive
experiments on four HSI datasets demonstrate the effectiveness and superiority
of the proposed S4L-FSC approach for few-shot HSI classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/Wenchen-Chen/S4L-FSC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric
  Content Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adarsh Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective dietary monitoring is critical for managing Type 2 diabetes, yet
accurately estimating caloric intake remains a major challenge. While
continuous glucose monitors (CGMs) offer valuable physiological data, they
often fall short in capturing the full nutritional profile of meals due to
inter-individual and meal-specific variability. In this work, we introduce a
multimodal deep learning framework that jointly leverages CGM time-series data,
Demographic/Microbiome, and pre-meal food images to enhance caloric estimation.
Our model utilizes attention based encoding and a convolutional feature
extraction for meal imagery, multi-layer perceptrons for CGM and Microbiome
data followed by a late fusion strategy for joint reasoning. We evaluate our
approach on a curated dataset of over 40 participants, incorporating
synchronized CGM, Demographic and Microbiome data and meal photographs with
standardized caloric labels. Our model achieves a Root Mean Squared Relative
Error (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These
findings demonstrate the potential of multimodal sensing to improve automated
dietary assessment tools for chronic disease management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The manuscript was submitted without proper consideration of
  institutional policies. Upon review with professor, it was found that the
  content is subject to licensing restrictions which prohibit public
  dissemination in its current form. Therefore, I am withdrawing the paper to
  comply with these requirements</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Technical Report: Quantifying and Analyzing the Generalization Power of
  a DNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan He, Junpeng Zhang, Lei Cheng, Hongyuan Zhang, Quanshi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new perspective for analyzing the generalization power
of deep neural networks (DNNs), i.e., directly disentangling and analyzing the
dynamics of generalizable and non-generalizable interaction encoded by a DNN
through the training process. Specifically, this work builds upon the recent
theoretical achievement in explainble AI, which proves that the detailed
inference logic of DNNs can be can be strictly rewritten as a small number of
AND-OR interaction patterns. Based on this, we propose an efficient method to
quantify the generalization power of each interaction, and we discover a
distinct three-phase dynamics of the generalization power of interactions
during training. In particular, the early phase of training typically removes
noisy and non-generalizable interactions and learns simple and generalizable
ones. The second and the third phases tend to capture increasingly complex
interactions that are harder to generalize. Experimental results verify that
the learning of non-generalizable interactions is the the direct cause for the
gap between the training and testing losses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RadCLIP: Enhancing Radiologic Image Analysis through Contrastive
  Language-Image <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09948v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09948v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixiu Lu, Hailong Li, Nehal A. Parikh, Jonathan R. Dillman, Lili He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of artificial intelligence (AI) with radiology marks a
transformative era in medicine. Vision foundation models have been adopted to
enhance radiologic imaging analysis. However, the distinct complexities of
radiologic 2D and 3D radiologic data pose unique challenges that existing
models, pre-trained on general non-medical images, fail to address adequately.
To bridge this gap and capitalize on the diagnostic precision required in
radiologic imaging, we introduce Radiologic Contrastive Language-Image
Pre-training (RadCLIP): a cross-modal vision-language foundational model that
harnesses Vision Language Pre-training (VLP) framework to improve radiologic
image analysis. Building upon Contrastive Language-Image Pre-training (CLIP),
RadCLIP incorporates a slice pooling mechanism tailored for volumetric image
analysis and is pre-trained using a large and diverse dataset of radiologic
image-text pairs. The RadCLIP was pre-trained to effectively align radiologic
images with their corresponding text annotations, creating a robust vision
backbone for radiologic images. Extensive experiments demonstrate RadCLIP's
superior performance in both uni-modal radiologic image classification and
cross-modal image-text matching, highlighting its significant promise for
improving diagnostic accuracy and efficiency in clinical settings. Our Key
contributions include curating a large dataset with diverse radiologic 2D/3D
radiologic image-text pairs, a slice pooling adapter using an attention
mechanism for integrating 2D images, and comprehensive evaluations of RadCLIP
on various radiologic downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Rich Emotions in 3D Avatars: A Text-to-3D Avatar Generation
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haidong Xu, Meishan Zhang, Hao Ju, Zhedong Zheng, Erik Cambria, Min Zhang, Hao Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Producing emotionally dynamic 3D facial avatars with text derived from spoken
words (Emo3D) has been a pivotal research topic in 3D avatar generation. While
progress has been made in general-purpose 3D avatar generation, the exploration
of generating emotional 3D avatars remains scarce, primarily due to the
complexities of identifying and rendering rich emotions from spoken words. This
paper reexamines Emo3D generation and draws inspiration from human processes,
breaking down Emo3D into two cascading steps: Text-to-3D Expression Mapping
(T3DEM) and 3D Avatar Rendering (3DAR). T3DEM is the most crucial step in
determining the quality of Emo3D generation and encompasses three key
challenges: Expression Diversity, Emotion-Content Consistency, and Expression
Fluidity. To address these challenges, we introduce a novel benchmark to
advance research in Emo3D generation. First, we present EmoAva, a large-scale,
high-quality dataset for T3DEM, comprising 15,000 text-to-3D expression
mappings that characterize the aforementioned three challenges in Emo3D
generation. Furthermore, we develop various metrics to effectively evaluate
models against these identified challenges. Next, to effectively model the
consistency, diversity, and fluidity of human expressions in the T3DEM step, we
propose the Continuous Text-to-Expression Generator, which employs an
autoregressive Conditional Variational Autoencoder for expression code
generation, enhanced with Latent Temporal Attention and Expression-wise
Attention mechanisms. Finally, to further enhance the 3DAR step on rendering
higher-quality subtle expressions, we present the Globally-informed Gaussian
Avatar (GiGA) model. GiGA incorporates a global information mechanism into 3D
Gaussian representations, enabling the capture of subtle micro-expressions and
seamless transitions between emotional states.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages. Project website: https://github.com/WalkerMitty/EmoAva</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SG-Reg: Generalizable and Efficient Scene Graph Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.14440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.14440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhao Liu, Zhijian Qiao, Jieqi Shi, Ke Wang, Peize Liu, Shaojie Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenges of registering two rigid semantic scene
graphs, an essential capability when an autonomous agent needs to register its
map against a remote agent, or against a prior map. The hand-crafted
descriptors in classical semantic-aided registration, or the ground-truth
annotation reliance in learning-based scene graph registration, impede their
application in practical real-world environments. To address the challenges, we
design a scene graph network to encode multiple modalities of semantic nodes:
open-set semantic feature, local topology with spatial awareness, and shape
feature. These modalities are fused to create compact semantic node features.
The matching layers then search for correspondences in a coarse-to-fine manner.
In the back-end, we employ a robust pose estimator to decide transformation
according to the correspondences. We manage to maintain a sparse and
hierarchical scene representation. Our approach demands fewer GPU resources and
fewer communication bandwidth in multi-agent tasks. Moreover, we design a new
data generation approach using vision foundation models and a semantic mapping
module to reconstruct semantic scene graphs. It differs significantly from
previous works, which rely on ground-truth semantic annotations to generate
data. We validate our method in a two-agent SLAM benchmark. It significantly
outperforms the hand-crafted baseline in terms of registration success rate.
Compared to visual loop closure networks, our method achieves a slightly higher
registration recall while requiring only 52 KB of communication bandwidth for
each query frame. Code available at:
\href{http://github.com/HKUST-Aerial-Robotics/SG-Reg}{http://github.com/HKUST-Aerial-Robotics/SG-Reg}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions Robotics Regular Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuicai Dong, Yujing Chang, Xin Deik Goh, Dexun Li, Ruiming Tang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal document retrieval aims to identify and retrieve various forms of
multimodal content, such as figures, tables, charts, and layout information
from extensive documents. Despite its increasing popularity, there is a notable
lack of a comprehensive and robust benchmark to effectively evaluate the
performance of systems in such tasks. To address this gap, this work introduces
a new benchmark, named MMDocIR, that encompasses two distinct tasks: page-level
and layout-level retrieval. The former evaluates the performance of identifying
the most relevant pages within a long document, while the later assesses the
ability of detecting specific layouts, providing a more fine-grained measure
than whole-page analysis. A layout refers to a variety of elements, including
textual paragraphs, equations, figures, tables, or charts. The MMDocIR
benchmark comprises a rich dataset featuring 1,685 questions annotated by
experts and 173,843 questions with bootstrapped labels, making it a valuable
resource in multimodal document retrieval for both training and evaluation.
Through rigorous experiments, we demonstrate that (i) visual retrievers
significantly outperform their text counterparts, (ii) MMDocIR training set
effectively enhances the performance of multimodal document retrieval and (iii)
text retrievers leveraging VLM-text significantly outperforms retrievers
relying on OCR-text. Our dataset is available at
https://mmdocrag.github.io/MMDocIR/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://huggingface.co/MMDocIR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Iterative Self-Alignment for Radiology Report Generation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11983v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11983v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting Xiao, Lei Shi, Yang Zhang, HaoFeng Yang, Zhe Wang, Chenjia Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiology Report Generation (RRG) is an important research topic for
relieving radiologist' heavy workload. Existing RRG models mainly rely on
supervised fine-tuning (SFT) based on different model architectures using data
pairs of radiological images and corresponding radiologist-annotated reports.
Recent research has shifted focus to post-training improvements, aligning RRG
model outputs with human preferences using reinforcement learning (RL).
However, the limited data coverage of high-quality annotated data poses risks
of overfitting and generalization. This paper proposes a novel Online Iterative
Self-Alignment (OISA) method for RRG that consists of four stages:
self-generation of diverse data, self-evaluation for multi-objective preference
data,self-alignment for multi-objective optimization and self-iteration for
further improvement. Our approach allows for generating varied reports tailored
to specific clinical objectives, enhancing the overall performance of the RRG
model iteratively. Unlike existing methods, our frame-work significantly
increases data quality and optimizes performance through iterative
multi-objective optimization. Experimental results demonstrate that our method
surpasses previous approaches, achieving state-of-the-art performance across
multiple evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language
  Model <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Junlin Han, Ender Konukoglu, Serge Belongie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to
new classes with few support samples while retaining base class segmentation.
Existing GFS-PCS methods enhance prototypes via interacting with support or
query features but remain limited by sparse knowledge from few-shot samples.
Meanwhile, 3D vision-language models (3D VLMs), generalizing across open-world
novel classes, contain rich but noisy novel class knowledge. In this work, we
introduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels
from 3D VLMs with precise yet sparse few-shot samples to maximize the strengths
of both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label
selection to filter low-quality regions, followed by an adaptive infilling
strategy that combines knowledge from pseudo-label contexts and few-shot
samples to adaptively label the filtered, unlabeled areas. Additionally, we
design a novel-base mix strategy to embed few-shot samples into training
scenes, preserving essential context for improved novel class learning.
Moreover, recognizing the limited diversity in current GFS-PCS benchmarks, we
introduce two challenging benchmarks with diverse novel classes for
comprehensive generalization evaluation. Experiments validate the effectiveness
of our framework across models and datasets. Our approach and benchmarks
provide a solid foundation for advancing GFS-PCS in the real world. The code is
at https://github.com/ZhaochongAn/GFS-VL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IP-<span class="highlight-title">Prompt</span>er: Training-Free Theme-Specific Image Generation via Dynamic
  Visual <span class="highlight-title">Prompt</span>ing <span class="chip">SIGGRAPH 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zhang, Minyan Luo, Weiming Dong, Xiao Yang, Haibin Huang, Chongyang Ma, Oliver Deussen, Tong-Yee Lee, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The stories and characters that captivate us as we grow up shape unique
fantasy worlds, with images serving as the primary medium for visually
experiencing these realms. Personalizing generative models through fine-tuning
with theme-specific data has become a prevalent approach in text-to-image
generation. However, unlike object customization, which focuses on learning
specific objects, theme-specific generation encompasses diverse elements such
as characters, scenes, and objects. Such diversity also introduces a key
challenge: how to adaptively generate multi-character, multi-concept, and
continuous theme-specific images (TSI). Moreover, fine-tuning approaches often
come with significant computational overhead, time costs, and risks of
overfitting. This paper explores a fundamental question: Can image generation
models directly leverage images as contextual input, similarly to how large
language models use text as context? To address this, we present IP-Prompter, a
novel training-free TSI generation method. IP-Prompter introduces visual
prompting, a mechanism that integrates reference images into generative models,
allowing users to seamlessly specify the target theme without requiring
additional training. To further enhance this process, we propose a Dynamic
Visual Prompting (DVP) mechanism, which iteratively optimizes visual prompts to
improve the accuracy and quality of generated images. Our approach enables
diverse applications, including consistent story generation, character design,
realistic character generation, and style-guided image generation. Comparative
evaluations against state-of-the-art personalization methods demonstrate that
IP-Prompter achieves significantly better results and excels in maintaining
character identity preserving, style consistency and text alignment, offering a
robust and flexible solution for theme-specific image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM SIGGRAPH 2025. Project page:
  https://ip-prompter.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does Acceleration Cause Hidden Instability in Vision Language Models?
  Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06794v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06794v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizheng Sun, Hao Li, Chang Xu, Hongpeng Zhou, Chenghua Lin, Riza Batista-Navarro, Jingyuan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) are powerful yet computationally intensive for
widespread practical deployments. To address such challenge without costly
re-training, post-training acceleration techniques like quantization and token
reduction are extensively explored. However, current acceleration evaluations
primarily target minimal overall performance degradation, overlooking a crucial
question: does the accelerated model still give the same answers to the same
questions as it did before acceleration? This is vital for stability-centered
industrial applications where consistently correct answers for specific, known
situations are paramount, such as in AI-based disease diagnosis. We
systematically investigate this for accelerated VLMs, testing four leading
models (LLaVA-1.5, LLaVA-Next, Qwen2-VL, Qwen2.5-VL) with eight acceleration
methods on ten multi-modal benchmarks. Our findings are stark: despite minimal
aggregate performance drops, accelerated models changed original answers up to
20% of the time. Critically, up to 6.5% of these changes converted correct
answers to incorrect. Input perturbations magnified these inconsistencies, and
the trend is confirmed by case studies with the medical VLM LLaVA-Med. This
research reveals a significant oversight in VLM acceleration, stressing an
urgent need for instance-level stability checks to ensure trustworthy
real-world deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point2Primitive: CAD Reconstruction from Point Cloud by Direct Primitive
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.02043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.02043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Wang, Xinzhu Ma, Bin Wang, Shixiang Tang, Yuan Meng, Ping Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering CAD models from point clouds, especially the sketch-extrusion
process, can be seen as the process of rebuilding the topology and extrusion
primitives. Previous methods utilize implicit fields for sketch representation,
leading to shape reconstruction of curved edges. In this paper, we proposed a
CAD reconstruction network that produces editable CAD models from input point
clouds (Point2Primitive) by directly predicting every element of the extrusion
primitives. Point2Primitive can directly detect and predict sketch curves (type
and parameter) from point clouds based on an improved transformer. The sketch
curve parameters are formulated as position queries and optimized in an
autoregressive way, leading to high parameter accuracy. The topology is rebuilt
by extrusion segmentation, and each extrusion parameter (sketch and extrusion
operation) is recovered by combining the predicted curves and the computed
extrusion operation. Extensive experiments demonstrate that our method is
superior in primitive prediction accuracy and CAD reconstruction. The
reconstructed shapes are of high geometrical fidelity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated
  Data Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13794v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13794v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhou, Shiyu Zhao, Yuxiao Chen, Zhenting Wang, Can Jin, Dimitris N. Metaxas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large foundation models trained on large-scale vision-language data can boost
Open-Vocabulary Object Detection (OVD) via synthetic training data, yet the
hand-crafted pipelines often introduce bias and overfit to specific prompts. We
sidestep this issue by directly fusing hidden states from Large Language Models
(LLMs) into detectors-an avenue surprisingly under-explored. This paper
presents a systematic method to enhance visual grounding by utilizing decoder
layers of the LLM of an MLLM. We introduce a zero-initialized cross-attention
adapter to enable efficient knowledge fusion from LLMs to object detectors, a
new approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We
find that intermediate LLM layers already encode rich spatial semantics;
adapting only the early layers yields most of the gain. With Swin-T as the
vision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at
just 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to
6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths
further corroborate our design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Framework for Event-based Frame Interpolation with Ad-hoc
  Deblurring in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05191v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05191v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Sun, Daniel Gehrig, Christos Sakaridis, Mathias Gehrig, Jingyun Liang, Peng Sun, Zhijie Xu, Kaiwei Wang, Luc Van Gool, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective video frame interpolation hinges on the adept handling of motion in
the input scene. Prior work acknowledges asynchronous event information for
this, but often overlooks whether motion induces blur in the video, limiting
its scope to sharp frame interpolation. We instead propose a unified framework
for event-based frame interpolation that performs deblurring ad-hoc and thus
works both on sharp and blurry input videos. Our model consists in a
bidirectional recurrent network that incorporates the temporal dimension of
interpolation and fuses information from the input frames and the events
adaptively based on their temporal proximity. To enhance the generalization
from synthetic data to real event cameras, we integrate self-supervised
framework with the proposed model to enhance the generalization on real-world
datasets in the wild. At the dataset level, we introduce a novel real-world
high-resolution dataset with events and color videos named HighREV, which
provides a challenging evaluation setting for the examined task. Extensive
experiments show that our network consistently outperforms previous
state-of-the-art methods on frame interpolation, single image deblurring, and
the joint task of both. Experiments on domain transfer reveal that
self-supervised training effectively mitigates the performance degradation
observed when transitioning from synthetic data to real-world data. Code and
datasets are available at https://github.com/AHupuJR/REFID.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to T-PAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level
  Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01051v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01051v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Zhang, Cheng Da, Kun Ding, Huan Yang, Kun Jin, Yan Li, Tingting Gao, Di Zhang, Shiming Xiang, Chunhong Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference optimization for diffusion models aims to align them with human
preferences for images. Previous methods typically use Vision-Language Models
(VLMs) as pixel-level reward models to approximate human preferences. However,
when used for step-level preference optimization, these models face challenges
in handling noisy images of different timesteps and require complex
transformations into pixel space. In this work, we show that pre-trained
diffusion models are naturally suited for step-level reward modeling in the
noisy latent space, as they are explicitly designed to process latent images at
various noise levels. Accordingly, we propose the Latent Reward Model (LRM),
which repurposes components of the diffusion model to predict preferences of
latent images at arbitrary timesteps. Building on LRM, we introduce Latent
Preference Optimization (LPO), a step-level preference optimization method
conducted directly in the noisy latent space. Experimental results indicate
that LPO significantly improves the model's alignment with general, aesthetic,
and text-image alignment preferences, while achieving a 2.5-28x training
speedup over existing preference optimization methods. Our code and models are
available at https://github.com/Kwai-Kolors/LPO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 26 tables, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep activity propagation via weight initialization in spiking neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00580v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00580v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aurora Micheli, Olaf Booij, Jan van Gemert, Nergis Tömen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) and neuromorphic computing offer bio-inspired
advantages such as sparsity and ultra-low power consumption, providing a
promising alternative to conventional networks. However, training deep SNNs
from scratch remains a challenge, as SNNs process and transmit information by
quantizing the real-valued membrane potentials into binary spikes. This can
lead to information loss and vanishing spikes in deeper layers, impeding
effective training. While weight initialization is known to be critical for
training deep neural networks, what constitutes an effective initial state for
a deep SNN is not well-understood. Existing weight initialization methods
designed for conventional networks (ANNs) are often applied to SNNs without
accounting for their distinct computational properties. In this work we derive
an optimal weight initialization method specifically tailored for SNNs, taking
into account the quantization operation. We show theoretically that, unlike
standard approaches, this method enables the propagation of activity in deep
SNNs without loss of spikes. We demonstrate this behavior in numerical
simulations of SNNs with up to 100 layers across multiple time steps. We
present an in-depth analysis of the numerical conditions, regarding layer width
and neuron hyperparameters, which are necessary to accurately apply our
theoretical findings. Furthermore, our experiments on MNIST demonstrate higher
accuracy and faster convergence when using the proposed weight initialization
scheme. Finally, we show that the newly introduced weight initialization is
robust against variations in several network and neuron hyperparameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Action Learning Requires Supervision in the Presence of
  Distractors <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00379v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00379v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Nikulin, Ilya Zisman, Denis Tarasov, Nikita Lyubaykin, Andrei Polubarov, Igor Kiselev, Vladislav Kurenkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, latent action learning, pioneered by Latent Action Policies (LAPO),
have shown remarkable pre-training efficiency on observation-only data,
offering potential for leveraging vast amounts of video available on the web
for embodied AI. However, prior work has focused on distractor-free data, where
changes between observations are primarily explained by ground-truth actions.
Unfortunately, real-world videos contain action-correlated distractors that may
hinder latent action learning. Using Distracting Control Suite (DCS) we
empirically investigate the effect of distractors on latent action learning and
demonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO
modification that improves the quality of latent actions by 8x, as measured by
linear probing. Importantly, we show that providing supervision with
ground-truth actions, as few as 2.5% of the full dataset, during latent action
learning improves downstream performance by 4.2x on average. Our findings
suggest that integrating supervision during Latent Action Models (LAM) training
is critical in the presence of distractors, challenging the conventional
pipeline of first learning LAM and only then decoding from latent to
ground-truth actions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025, Poster, Source code: https://github.com/dunnolab/laom</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Any-to-Any Learning in Computational Pathology via Triplet Multimodal
  <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qichen Sun, Zhengrui Guo, Rui Peng, Hao Chen, Jinzhuo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in computational pathology and artificial intelligence have
significantly enhanced the utilization of gigapixel whole-slide images and and
additional modalities (e.g., genomics) for pathological diagnosis. Although
deep learning has demonstrated strong potential in pathology, several key
challenges persist: (1) fusing heterogeneous data types requires sophisticated
strategies beyond simple concatenation due to high computational costs; (2)
common scenarios of missing modalities necessitate flexible strategies that
allow the model to learn robustly in the absence of certain modalities; (3) the
downstream tasks in CPath are diverse, ranging from unimodal to multimodal,
cnecessitating a unified model capable of handling all modalities. To address
these challenges, we propose ALTER, an any-to-any tri-modal pretraining
framework that integrates WSIs, genomics, and pathology reports. The term "any"
emphasizes ALTER's modality-adaptive design, enabling flexible pretraining with
any subset of modalities, and its capacity to learn robust, cross-modal
representations beyond WSI-centric approaches. We evaluate ALTER across
extensive clinical tasks including survival prediction, cancer subtyping, gene
mutation prediction, and report generation, achieving superior or comparable
performance to state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpaceJAM: a Lightweight and Regularization-free Method for Fast Joint
  Alignment of Images <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nir Barel, Ron Shapira Weber, Nir Mualem, Shahaf E. Finder, Oren Freifeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The unsupervised task of Joint Alignment (JA) of images is beset by
challenges such as high complexity, geometric distortions, and convergence to
poor local or even global optima. Although Vision Transformers (ViT) have
recently provided valuable features for JA, they fall short of fully addressing
these issues. Consequently, researchers frequently depend on expensive models
and numerous regularization terms, resulting in long training times and
challenging hyperparameter tuning. We introduce the Spatial Joint Alignment
Model (SpaceJAM), a novel approach that addresses the JA task with efficiency
and simplicity. SpaceJAM leverages a compact architecture with only 16K
trainable parameters and uniquely operates without the need for regularization
or atlas maintenance. Evaluations on SPair-71K and CUB datasets demonstrate
that SpaceJAM matches the alignment capabilities of existing methods while
significantly reducing computational demands and achieving at least a 10x
speedup. SpaceJAM sets a new standard for rapid and effective image alignment,
making the process more accessible and efficient. Our code is available at:
https://bgu-cs-vil.github.io/SpaceJAM/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Xue, Edward Moroshko, Feng Chen, Jingyu Sun, Steven McDonagh, Sotirios A. Tsaftaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Image diffusion models can produce undesirable content that
necessitates concept erasure. However, existing methods struggle with
under-erasure, leaving residual traces of targeted concepts, or over-erasure,
mistakenly eliminating unrelated but visually similar concepts. To address
these limitations, we introduce CRCE, a novel concept erasure framework that
leverages Large Language Models to identify both semantically related concepts
that should be erased alongside the target and distinct concepts that should be
preserved. By explicitly modelling coreferential and retained concepts
semantically, CRCE enables more precise concept removal, without unintended
erasure. Experiments demonstrate that CRCE outperforms existing methods on
diverse erasure tasks, including real-world object, person identities, and
abstract intellectual property characteristics. The constructed dataset
CorefConcept and the source code will be release upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Deep Learning Approaches for Uncertainty-Aware Retinal OCT
  Image Segmentation for Multiple Sclerosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel T. M. Ball
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical Coherence Tomography (OCT) provides valuable insights in
ophthalmology, cardiology, and neurology due to high-resolution,
cross-sectional images of the retina. One critical task for ophthalmologists
using OCT is delineation of retinal layers within scans. This process is
time-consuming and prone to human bias, affecting the accuracy and reliability
of diagnoses. Previous efforts to automate delineation using deep learning face
challenges in uptake from clinicians and statisticians due to the absence of
uncertainty estimation, leading to "confidently wrong" models via
hallucinations. In this study, we address these challenges by applying Bayesian
convolutional neural networks (BCNNs) to segment an openly available OCT
imaging dataset containing 35 human retina OCTs split between healthy controls
and patients with multiple sclerosis. Our findings demonstrate that Bayesian
models can be used to provide uncertainty maps of the segmentation, which can
further be used to identify highly uncertain samples that exhibit recording
artefacts such as noise or miscalibration at inference time. Our method also
allows for uncertainty-estimation for important secondary measurements such as
layer thicknesses, that are medically relevant for patients. We show that these
features come in addition to greater performance compared to similar work over
all delineations; with an overall Dice score of 95.65%. Our work brings greater
clinical applicability, statistical robustness, and performance to retinal OCT
segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Continuous Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Sun, Yi Jiang, Tao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in continuous generative models, including multi-step
approaches like diffusion and flow-matching (typically requiring 8-1000
sampling steps) and few-step methods such as consistency models (typically 1-8
steps), have demonstrated impressive generative performance. However, existing
work often treats these approaches as distinct paradigms, resulting in separate
training and sampling methodologies. We introduce a unified framework for
training, sampling, and analyzing these models. Our implementation, the Unified
Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves
state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a
675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID
in 20 steps and a few-step model reaching 1.42 FID in just 2 steps.
Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at
250 steps) improves performance to 1.06 FID in only 40 steps. Code is available
at: https://github.com/LINs-lab/UCGM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/LINs-lab/UCGM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younghyun Kim, Jongheon Jeong, Sangkyung Kwak, Kyungmin Lee, Juho Lee, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning robust representations from data often requires scale, which has led
to the success of recent zero-shot models such as CLIP. However, the obtained
robustness can easily be deteriorated when these models are fine-tuned on other
downstream tasks (e.g., of smaller scales). Previous works often interpret this
phenomenon in the context of domain shift, developing fine-tuning methods that
aim to preserve the original domain as much as possible. However, in a
different context, fine-tuned models with limited data are also prone to
learning features that are spurious to humans, such as background or texture.
In this paper, we propose StarFT (Spurious Textual Alignment Regularization), a
novel framework for fine-tuning zero-shot models to enhance robustness by
preventing them from learning spuriosity. We introduce a regularization that
aligns the output distribution for spuriosity-injected labels with the original
zero-shot model, ensuring that the model is not induced to extract irrelevant
features further from these descriptions. We leverage recent language models to
get such spuriosity-injected labels by generating alternative textual
descriptions that highlight potentially confounding features. Extensive
experiments validate the robust generalization of StarFT and its emerging
properties: zero-shot group robustness and improved zero-shot classification.
Notably, StarFT boosts both worst-group and average accuracy by 14.30% and
3.02%, respectively, in the Waterbirds group shift scenario, where other robust
fine-tuning baselines show even degraded performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCAI 2025; Code is available at https://github.com/alinlab/StarFT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Orthogonal Subspace Decomposition for Generalizable AI-Generated Image
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15633v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15633v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Yan, Jiangming Wang, Peng Jin, Ke-Yue Zhang, Chengchun Liu, Shen Chen, Taiping Yao, Shouhong Ding, Baoyuan Wu, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-generated images (AIGIs), such as natural or face images, have become
increasingly important yet challenging. In this paper, we start from a new
perspective to excavate the reason behind the failure generalization in AIGI
detection, named the \textit{asymmetry phenomenon}, where a naively trained
detector tends to favor overfitting to the limited and monotonous fake
patterns, causing the feature space to become highly constrained and
low-ranked, which is proved seriously limiting the expressivity and
generalization. One potential remedy is incorporating the pre-trained knowledge
within the vision foundation models (higher-ranked) to expand the feature
space, alleviating the model's overfitting to fake. To this end, we employ
Singular Value Decomposition (SVD) to decompose the original feature space into
\textit{two orthogonal subspaces}. By freezing the principal components and
adapting only the remained components, we preserve the pre-trained knowledge
while learning fake patterns. Compared to existing full-parameters and
LoRA-based tuning methods, we explicitly ensure orthogonality, enabling the
higher rank of the whole feature space, effectively minimizing overfitting and
enhancing generalization. We finally identify a crucial insight: our method
implicitly learns \textit{a vital prior that fakes are actually derived from
the real}, indicating a hierarchical relationship rather than independence.
Modeling this prior, we believe, is essential for achieving superior
generalization. Our codes are publicly available at
\href{https://github.com/YZY-stack/Effort-AIGI-Detection}{GitHub}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepForest: Sensing Into Self-Occluding Volumes of Vegetation With
  Aerial Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Youssef, Jian Peng, Oliver Bimber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Access to below-canopy volumetric vegetation data is crucial for
understanding ecosystem dynamics. We address the long-standing limitation of
remote sensing to penetrate deep into dense canopy layers. LiDAR and radar are
currently considered the primary options for measuring 3D vegetation
structures, while cameras can only extract the reflectance and depth of top
layers. Using conventional, high-resolution aerial images, our approach allows
sensing deep into self-occluding vegetation volumes, such as forests. It is
similar in spirit to the imaging process of wide-field microscopy, but can
handle much larger scales and strong occlusion. We scan focal stacks by
synthetic-aperture imaging with drones and reduce outof-focus signal
contributions using pre-trained 3D convolutional neural networks with mean
squared error (MSE) as the loss function. The resulting volumetric reflectance
stacks contain low-frequency representations of the vegetation volume.
Combining multiple reflectance stacks from various spectral channels provides
insights into plant health, growth, and environmental conditions throughout the
entire vegetation volume. Compared with simulated ground truth, our correction
leads to ~x7 average improvements (min: ~x2, max: ~x12) for forest densities of
200 trees/ha - 1680 trees/ha. In our field experiment, we achieved an MSE of
0.05 when comparing with the top-vegetation layer that was measured with
classical multispectral aerial imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BigReg: An Efficient Registration Pipeline for High-Resolution X-Ray and
  Light-Sheet Fluorescence Microscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Mei, Fuxin Fan, Mareike Thies, Mingxuan Gu, Fabian Wagner, Oliver Aust, Ina Erceg, Zeynab Mirzaei, Georgiana Neag, Yipeng Sun, Yixing Huang, Andreas Maier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, X-ray microscopy (XRM) and light-sheet fluorescence microscopy
(LSFM) have emerged as pivotal tools in preclinical research, particularly for
studying bone remodeling diseases such as osteoporosis. These modalities offer
micrometer-level resolution, and their integration allows for a complementary
examination of bone microstructures which is essential for analyzing functional
changes. However, registering high-resolution volumes from these independently
scanned modalities poses substantial challenges, especially in real-world and
reference-free scenarios. This paper presents BigReg, a fast, two-stage
pipeline designed for large-volume registration of XRM and LSFM data. The first
stage involves extracting surface features and applying two successive point
cloud-based methods for coarse alignment. The subsequent stage refines this
alignment using a modified cross-correlation technique, achieving precise
volumetric registration. Evaluations using expert-annotated landmarks and
augmented test data demonstrate that BigReg approaches the accuracy of
landmark-based registration with a landmark distance (LMD) of 8.36\,\textmu
m\,$\pm$\,0.12\,\textmu m and a landmark fitness (LM fitness) of
85.71\%\,$\pm$\,1.02\%. Moreover, BigReg can provide an optimal initialization
for mutual information-based methods which otherwise fail independently,
further reducing LMD to 7.24\,\textmu m\,$\pm$\,0.11\,\textmu m and increasing
LM fitness to 93.90\%\,$\pm$\,0.77\%. Ultimately, key microstructures, notably
lacunae in XRM and bone cells in LSFM, are accurately aligned, enabling
unprecedented insights into the pathology of osteoporosis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Image Contrastive Decoding: Precise, Lossless Suppression of
  Language Priors in Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10634v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10634v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfei Zhao, Feng Zhang, Xin Sun, Chong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language priors are a major cause of hallucinations in Large Vision-Language
Models (LVLMs), often leading to text that is linguistically plausible but
visually inconsistent. Recent work explores contrastive decoding as a
training-free solution, but these methods typically construct negative contexts
from the original image, resulting in visual information loss and distorted
distribution. Motivated by the observation that language priors stem from the
LLM backbone and remain consistent across images, we propose Cross-Images
Contrastive Decoding (CICD), a simple yet effective training-free method that
uses different images to construct negative contexts. We further analyze the
cross-image behavior of language priors and introduce a distinction between
essential priors (supporting fluency) and detrimental priors (causing
hallucinations). By selectively preserving essential priors and suppressing
detrimental ones, our method reduces hallucinations while maintaining coherent
and fluent language generation. Experiments on 4 benchmarks and 6 LVLMs across
three model families confirm the effectiveness and generalizability of CICD,
especially in image captioning, where language priors are particularly
pronounced. Code will be released once accepted.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward
  Model <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, Kai Chen, Dahua Lin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the promising performance of Large Vision Language Models (LVLMs) in
visual understanding, they occasionally generate incorrect outputs. While
reward models (RMs) with reinforcement learning or test-time scaling offer the
potential for improving generation quality, a critical gap remains: publicly
available multi-modal RMs for LVLMs are scarce, and the implementation details
of proprietary models are often unclear. We bridge this gap with
InternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective
multi-modal reward model that aligns LVLMs with human preferences. To ensure
the robustness and versatility of IXC-2.5-Reward, we set up a high-quality
multi-modal preference corpus spanning text, image, and video inputs across
diverse domains, such as instruction following, general understanding,
text-rich documents, mathematical reasoning, and video understanding.
IXC-2.5-Reward achieves excellent results on the latest multi-modal reward
model benchmark and shows competitive performance on text-only reward model
benchmarks. We further demonstrate three key applications of IXC-2.5-Reward:
(1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward
with Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows
consistent improvements in instruction following and multi-modal open-ended
dialogue; (2) Selecting the best response from candidate responses for
test-time scaling; and (3) Filtering outlier or noisy samples from existing
image and video instruction tuning training data. To ensure reproducibility and
facilitate further research, we have open-sourced all model weights and
training recipes at
https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-Reward
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CompBench: Benchmarking Complex Instruction-guided Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Jia, Wenxuan Huang, Yuntian Tang, Junbo Qiao, Jincheng Liao, Shaosheng Cao, Fei Zhao, Zhaopeng Feng, Zhouhong Gu, Zhenfei Yin, Lei Bai, Wanli Ouyang, Lin Chen, Fei Zhao, Zihan Wang, Yuan Xie, Shaohui Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While real-world applications increasingly demand intricate scene
manipulation, existing instruction-guided image editing benchmarks often
oversimplify task complexity and lack comprehensive, fine-grained instructions.
To bridge this gap, we introduce, a large-scale benchmark specifically designed
for complex instruction-guided image editing. CompBench features challenging
editing scenarios that incorporate fine-grained instruction following, spatial
and contextual reasoning, thereby enabling comprehensive evaluation of image
editing models' precise manipulation capabilities. To construct CompBench, We
propose an MLLM-human collaborative framework with tailored task pipelines.
Furthermore, we propose an instruction decoupling strategy that disentangles
editing intents into four key dimensions: location, appearance, dynamics, and
objects, ensuring closer alignment between instructions and complex editing
requirements. Extensive evaluations reveal that CompBench exposes fundamental
limitations of current image editing models and provides critical insights for
the development of next-generation instruction-guided image editing systems.
The dataset, code, and models are available in https://comp-bench.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Cancer Survival Analysis via Hypergraph Learning with
  Cross-Modality Rebalance <span class="chip">IJCAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingcheng Qu, Guang Yang, Donglin Di, Tonghua Su, Yue Gao, Yang Song, Lei Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal pathology-genomic analysis has become increasingly prominent in
cancer survival prediction. However, existing studies mainly utilize
multi-instance learning to aggregate patch-level features, neglecting the
information loss of contextual and hierarchical details within pathology
images. Furthermore, the disparity in data granularity and dimensionality
between pathology and genomics leads to a significant modality imbalance. The
high spatial resolution inherent in pathology data renders it a dominant role
while overshadowing genomics in multimodal integration. In this paper, we
propose a multimodal survival prediction framework that incorporates hypergraph
learning to effectively capture both contextual and hierarchical details from
pathology images. Moreover, it employs a modality rebalance mechanism and an
interactive alignment fusion strategy to dynamically reweight the contributions
of the two modalities, thereby mitigating the pathology-genomics imbalance.
Quantitative and qualitative experiments are conducted on five TCGA datasets,
demonstrating that our model outperforms advanced methods by over 3.4\% in
C-Index performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by IJCAI2025 Code: https://github.com/MCPathology/MRePath</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Social Media Image Categorization Using Large Models with
  Different Adaptation Methods: A Case Study on Cultural Nature's Contributions
  to People 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00275v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00275v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohaifa Khaldi, Domingo Alcaraz-Segura, Ignacio Sánchez-Herrera, Javier Martinez-Lopez, Carlos Javier Navarro, Siham Tabik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media images provide valuable insights for modeling, mapping, and
understanding human interactions with natural and cultural heritage. However,
categorizing these images into semantically meaningful groups remains highly
complex due to the vast diversity and heterogeneity of their visual content as
they contain an open-world human and nature elements. This challenge becomes
greater when categories involve abstract concepts and lack consistent visual
patterns. Related studies involve human supervision in the categorization
process and the lack of public benchmark datasets make comparisons between
these works unfeasible. On the other hand, the continuous advances in large
models, including Large Language Models (LLMs), Large Visual Models (LVMs), and
Large Visual Language Models (LVLMs), provide a large space of unexplored
solutions. In this work 1) we introduce FLIPS a dataset of Flickr images that
capture the interaction between human and nature, and 2) evaluate various
solutions based on different types and combinations of large models using
various adaptation methods. We assess and report their performance in terms of
cost, productivity, scalability, and result quality to address the challenges
of social media image categorization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Future Frame Synthesis: Bridging Deterministic and
  Generative Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14718v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14718v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruibo Ming, Zhewei Huang, Jingwei Wu, Zhuoxuan Ju, Jianming Hu, Lihui Peng, Shuchang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Future Frame Synthesis (FFS, aka Video Frame Prediction) focuses on
generating future frame sequences conditioned on existing content. This survey
provides a comprehensive review of existing research on FFS, covering commonly
used datasets and representative algorithms. We discuss key challenges and
trace the evolution of FFS in computer vision, particularly the shift from
deterministic to generative approaches. Our taxonomy outlines major advances
and methodological shifts, emphasizing the rising significance of generative
models in producing realistic and diverse predictions. This survey offers a
comprehensive analysis of current research and, moreover, suggests promising
avenues for future exploration in this ever-changing domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review, 22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Breaking Language Barriers in Visual Language Models via Multilingual
  Textual Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.22577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.22577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iñigo Pikabea, Iñaki Lacunza, Oriol Pareras, Carlos Escolano, Aitor Gonzalez-Agirre, Javier Hernando, Marta Villegas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid advancements in Visual Language Models (VLMs) have transformed
multimodal understanding but are often constrained by generating English
responses regardless of the input language. This phenomenon has been termed as
Image-induced Fidelity Loss (IFL) and stems from limited multimodal
multilingual training data. To address this, we propose a continuous
multilingual integration strategy that injects text-only multilingual data
during visual instruction tuning, preserving the language model's original
multilingual capabilities. Extensive evaluations demonstrate that our approach
significantly improves linguistic fidelity across languages without degradation
in visual performance. We also explore model merging, which improves language
fidelity but comes at the cost of visual performance. In contrast, our core
method achieves robust multilingual alignment without trade-offs, offering a
scalable and effective path to mitigating IFL for global VLM adoption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: Expanded model merging experiments. Fix duplicated subsection on
  limitations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-granular body modeling with Redundancy-Free Spatiotemporal Fusion
  for Text-Driven Motion Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingzu Zhan, Chen Xie, Honghang Chen, Haoran Sun, Xiaochun Mai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-motion generation sits at the intersection of multimodal learning and
computer graphics and is gaining momentum because it can simplify content
creation for games, animation, robotics and virtual reality. Most current
methods stack spatial and temporal features in a straightforward way, which
adds redundancy and still misses subtle joint-level cues. We introduce HiSTF
Mamba, a framework with three parts: Dual-Spatial Mamba, Bi-Temporal Mamba and
a Dynamic Spatiotemporal Fusion Module (DSFM). The Dual-Spatial module runs
part-based and whole-body models in parallel, capturing both overall
coordination and fine-grained joint motion. The Bi-Temporal module scans
sequences forward and backward to encode short-term details and long-term
dependencies. DSFM removes redundant temporal information, extracts
complementary cues and fuses them with spatial features to build a richer
spatiotemporal representation. Experiments on the HumanML3D benchmark show that
HiSTF Mamba performs well across several metrics, achieving high fidelity and
tight semantic alignment between text and motion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15pages,5figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient Leakage Defense with Key-Lock Module for Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04095v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04095v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanchi Ren, Jingjing Deng, Xianghua Xie, Xiaoke Ma, Jianfeng Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a widely adopted privacy-preserving machine
learning approach where private data remains local, enabling secure
computations and the exchange of local model gradients between local clients
and third-party parameter servers. However, recent findings reveal that privacy
may be compromised and sensitive information potentially recovered from shared
gradients. In this study, we offer detailed analysis and a novel perspective on
understanding the gradient leakage problem. These theoretical works lead to a
new gradient leakage defense technique that secures arbitrary model
architectures using a private key-lock module. Only the locked gradient is
transmitted to the parameter server for global model aggregation. Our proposed
learning method is resistant to gradient leakage attacks, and the key-lock
module is designed and trained to ensure that, without the private information
of the key-lock module: a) reconstructing private training data from the shared
gradient is infeasible; and b) the global model's inference performance is
significantly compromised. We discuss the theoretical underpinnings of why
gradients can leak private information and provide theoretical proof of our
method's effectiveness. We conducted extensive empirical evaluations with many
models on several popular benchmarks, demonstrating the robustness of our
proposed approach in both maintaining model performance and defending against
gradient leakage attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code can be found at https://github.com/Rand2AI/FedKL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and
  Interactive Geometry Refiner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, Xiaoxiao Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel generative 3D modeling system, coined CraftsMan, which can
generate high-fidelity 3D geometries with highly varied shapes, regular mesh
topologies, and detailed surfaces, and, notably, allows for refining the
geometry in an interactive manner. Despite the significant advancements in 3D
generation, existing methods still struggle with lengthy optimization
processes, irregular mesh topologies, noisy surfaces, and difficulties in
accommodating user edits, consequently impeding their widespread adoption and
implementation in 3D modeling software. Our work is inspired by the craftsman,
who usually roughs out the holistic figure of the work first and elaborates the
surface details subsequently. Specifically, we employ a 3D native diffusion
model, which operates on latent space learned from latent set-based 3D
representations, to generate coarse geometries with regular mesh topology in
seconds. In particular, this process takes as input a text prompt or a
reference image and leverages a powerful multi-view (MV) diffusion model to
generate multiple views of the coarse geometry, which are fed into our
MV-conditioned 3D diffusion model for generating the 3D geometry, significantly
improving robustness and generalizability. Following that, a normal-based
geometry refiner is used to significantly enhance the surface details. This
refinement can be performed automatically, or interactively with user-supplied
edits. Extensive experiments demonstrate that our method achieves high efficacy
in producing superior-quality 3D assets compared to existing methods. HomePage:
https://craftsman3d.github.io/, Code: https://github.com/wyysf-98/CraftsMan
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>HomePage: https://craftsman3d.github.io/, Code:
  https://github.com/wyysf-98/CraftsMan3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DragLoRA: Online Optimization of LoRA Adapters for Drag-based Image
  Editing in Diffusion Model <span class="chip">ICML2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siwei Xia, Li Sun, Tiantian Sun, Qingli Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drag-based editing within pretrained diffusion model provides a precise and
flexible way to manipulate foreground objects. Traditional methods optimize the
input feature obtained from DDIM inversion directly, adjusting them iteratively
to guide handle points towards target locations. However, these approaches
often suffer from limited accuracy due to the low representation ability of the
feature in motion supervision, as well as inefficiencies caused by the large
search space required for point tracking. To address these limitations, we
present DragLoRA, a novel framework that integrates LoRA (Low-Rank Adaptation)
adapters into the drag-based editing pipeline. To enhance the training of LoRA
adapters, we introduce an additional denoising score distillation loss which
regularizes the online model by aligning its output with that of the original
model. Additionally, we improve the consistency of motion supervision by
adapting the input features using the updated LoRA, giving a more stable and
accurate input feature for subsequent operations. Building on this, we design
an adaptive optimization scheme that dynamically toggles between two modes,
prioritizing efficiency without compromising precision. Extensive experiments
demonstrate that DragLoRA significantly enhances the control precision and
computational efficiency for drag-based image editing. The Codes of DragLoRA
are available at: https://github.com/Sylvie-X/DragLoRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterative Tool Usage Exploration for Multimodal Agents via Step-wise
  Preference Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.21561v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.21561v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengxiang Li, Zhi Gao, Bofei Zhang, Yapeng Mi, Xiaojian Ma, Chenrui Shi, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal agents, which integrate a controller e.g., a vision language
model) with external tools, have demonstrated remarkable capabilities in
tackling complex multimodal tasks. Existing approaches for training these
agents, both supervised fine-tuning and reinforcement learning, depend on
extensive human-annotated task-answer pairs and tool trajectories. However, for
complex multimodal tasks, such annotations are prohibitively expensive or
impractical to obtain. In this paper, we propose an iterative tool usage
exploration method for multimodal agents without any pre-collected data, namely
SPORT, via step-wise preference optimization to refine the trajectories of tool
usage. Our method enables multimodal agents to autonomously discover effective
tool usage strategies through self-exploration and optimization, eliminating
the bottleneck of human annotation. SPORT has four iterative components: task
synthesis, step sampling, step verification, and preference tuning. We first
synthesize multimodal tasks using language models. Then, we introduce a novel
trajectory exploration scheme, where step sampling and step verification are
executed alternately to solve synthesized tasks. In step sampling, the agent
tries different tools and obtains corresponding results. In step verification,
we employ a verifier to provide AI feedback to construct step-wise preference
data. The data is subsequently used to update the controller for tool usage
through preference tuning, producing a SPORT agent. By interacting with real
environments, the SPORT agent gradually evolves into a more refined and capable
system. Evaluation in the GTA and GAIA benchmarks shows that the SPORT agent
achieves 6.41% and 3.64% improvements, underscoring the generalization and
effectiveness introduced by our method. The project page is
https://SPORT-Agents.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Distillation Learning: Knowledge Distillation in <span class="highlight-title">Prompt</span>-based
  Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13911v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13911v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qifan Zhang, Yunhui Guo, Yu Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the problem of continual distillation learning (CDL) in order to
use knowledge distillation (KD) to improve prompt-based continual learning (CL)
models. The CDL problem is valuable to study since the use of a larger vision
transformer (ViT) leads to better performance in prompt-based continual
learning. The distillation of knowledge from a large ViT to a small ViT
improves the inference efficiency for prompt-based CL models. We empirically
found that existing KD methods such as logit distillation and feature
distillation cannot effectively improve the student model in the CDL setup. To
address this issue, we introduce a novel method named Knowledge Distillation
based on Prompts (KDP), in which globally accessible prompts specifically
designed for knowledge distillation are inserted into the frozen ViT backbone
of the student model. We demonstrate that our KDP method effectively enhances
the distillation performance in comparison to existing KD methods in the CDL
setup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AS3D: 2D-Assisted Cross-Modal Understanding with Semantic-Spatial Scene
  Graphs for 3D Visual Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.04058v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.04058v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Xiao, Hongbin Xu, Guocan Zhao, Wenxiong Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding aims to localize the unique target described by natural
languages in 3D scenes. The significant gap between 3D and language modalities
makes it a notable challenge to distinguish multiple similar objects through
the described spatial relationships. Current methods attempt to achieve
cross-modal understanding in complex scenes via a target-centered learning
mechanism, ignoring the perception of referred objects. We propose a novel
2D-assisted 3D visual grounding framework that constructs semantic-spatial
scene graphs with referred object discrimination for relationship perception.
The framework incorporates a dual-branch visual encoder that utilizes 2D
pre-trained attributes to guide the multi-modal object encoding. Furthermore,
our cross-modal interaction module uses graph attention to facilitate
relationship-oriented information fusion. The enhanced object representation
and iterative relational learning enable the model to establish effective
alignment between 3D vision and referential descriptions. Experimental results
on the popular benchmarks demonstrate our superior performance compared to
state-of-the-art methods, especially in addressing the challenges of multiple
similar distractors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures,
  Languages, and Domains in Video Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17821v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17821v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Chen, Yunxin Li, Haoyuan Shi, Baotian Hu, Wenhan Luo, Yaowei Wang, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing the video comprehension capabilities of multimodal AI systems can
effectively measure their understanding and reasoning abilities. Most video
evaluation benchmarks are limited to a single language, typically English, and
predominantly feature videos rooted in Western cultural contexts. In this
paper, we present VideoVista-CulturalLingo, the first video evaluation
benchmark designed to bridge cultural, linguistic, and domain divide in video
comprehension. Our work differs from existing benchmarks in the following ways:
1) Cultural diversity, incorporating cultures from China, North America, and
Europe; 2) Multi-linguistics, with questions presented in Chinese and
English-two of the most widely spoken languages; and 3) Broad domain, featuring
videos sourced from hundreds of human-created domains. VideoVista-CulturalLingo
contains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent
open-source or proprietary video large models. From the experiment results, we
observe that: 1) Existing models perform worse on Chinese-centric questions
than Western-centric ones, particularly those related to Chinese history; 2)
Current open-source models still exhibit limitations in temporal understanding,
especially in the Event Localization task, achieving a maximum score of only
45.2%; 3) Mainstream models demonstrate strong performance in general
scientific questions, while open-source models demonstrate weak performance in
mathematics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pyramid Sparse <span class="highlight-title">Transformer</span>: Enhancing Multi-Scale Feature Fusion with
  Dynamic Token Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Hu, Tian Bai, Fengyi Wu, Zhenming Peng, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature fusion is critical for high-performance vision models but often
incurs prohibitive complexity. However, prevailing attention-based fusion
methods often involve significant computational complexity and implementation
challenges, limiting their efficiency in resource-constrained environments. To
address these issues, we introduce the Pyramid Sparse Transformer (PST), a
lightweight, plug-and-play module that integrates coarse-to-fine token
selection and shared attention parameters to reduce computation while
preserving spatial detail. PST can be trained using only coarse attention and
seamlessly activated at inference for further accuracy gains without
retraining. When added to state-of-the-art real-time detection models, such as
YOLOv11-N/S/M, PST yields mAP improvements of 0.9%, 0.5%, and 0.4% on MS COCO
with minimal latency impact. Likewise, embedding PST into ResNet-18/50/101 as
backbones, boosts ImageNet top-1 accuracy by 6.5%, 1.7%, and 1.0%,
respectively. These results demonstrate PST's effectiveness as a simple,
hardware-friendly enhancement for both detection and classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffDesign: Controllable Diffusion with Meta Prior for Efficient
  Interior Design Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Yang, Tao Geng, Jingyao Wang, Changwen Zheng, Fuchun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interior design is a complex and creative discipline involving aesthetics,
functionality, ergonomics, and materials science. Effective solutions must meet
diverse requirements, typically producing multiple deliverables such as
renderings and design drawings from various perspectives. Consequently,
interior design processes are often inefficient and demand significant
creativity. With advances in machine learning, generative models have emerged
as a promising means of improving efficiency by creating designs from text
descriptions or sketches. However, few generative works focus on interior
design, leading to substantial discrepancies between outputs and practical
needs, such as differences in size, spatial scope, and the lack of controllable
generation quality. To address these challenges, we propose DiffDesign, a
controllable diffusion model with meta priors for efficient interior design
generation. Specifically, we utilize the generative priors of a 2D diffusion
model pre-trained on a large image dataset as our rendering backbone. We
further guide the denoising process by disentangling cross-attention control
over design attributes, such as appearance, pose, and size, and introduce an
optimal transfer-based alignment module to enforce view consistency.
Simultaneously, we construct an interior design-specific dataset, DesignHelper,
consisting of over 400 solutions across more than 15 spatial types and 15
design styles. This dataset helps fine-tune DiffDesign. Extensive experiments
conducted on various benchmark datasets demonstrate the effectiveness and
robustness of DiffDesign.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Customizing Visual-Language Foundation Models for Multi-modal Anomaly
  Detection and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11083v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11083v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohao Xu, Yunkang Cao, Huaxin Zhang, Nong Sang, Xiaonan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection is vital in various industrial scenarios, including the
identification of unusual patterns in production lines and the detection of
manufacturing defects for quality control. Existing techniques tend to be
specialized in individual scenarios and lack generalization capacities. In this
study, our objective is to develop a generic anomaly detection model that can
be applied in multiple scenarios. To achieve this, we custom-build generic
visual language foundation models that possess extensive knowledge and robust
reasoning abilities as anomaly detectors and reasoners. Specifically, we
introduce a multi-modal prompting strategy that incorporates domain knowledge
from experts as conditions to guide the models. Our approach considers diverse
prompt types, including task descriptions, class context, normality rules, and
reference images. In addition, we unify the input representation of
multi-modality into a 2D image format, enabling multi-modal anomaly detection
and reasoning. Our preliminary studies demonstrate that combining visual and
language prompts as conditions for customizing the models enhances anomaly
detection performance. The customized models showcase the ability to detect
anomalies across different data modalities such as images, point clouds, and
videos. Qualitative case studies further highlight the anomaly detection and
reasoning capabilities, particularly for multi-object scenes and temporal data.
Our code is publicly available at
https://github.com/Xiaohao-Xu/Customizable-VLM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Best Student Paper Award at IEEE International Conference on Computer
  Supported Cooperative Work in Design, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GranQ: Granular Zero-Shot Quantization with Channel-Wise Activation
  Scaling in QAT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18339v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18339v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inpyo Hong, Youngwan Jo, Hyojeong Lee, Sunghyun Ahn, Sanghyun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot quantization (ZSQ) enables neural network compression without
original training data, making it a promising solution for restricted data
access scenarios. To compensate for the lack of data, recent ZSQ methods
typically rely on synthetic inputs generated from the full-precision model.
However, these synthetic inputs often lead to activation distortion, especially
under low-bit settings. As a result, existing methods struggle to mitigate this
issue due to coarse activation scaling. To address this issue, we propose
GranQ, a novel activation quantization framework that efficiently applies
per-channel scaling through vectorized computation. In contrast to conventional
channel-wise methods, which apply vectorization only to the quantization step,
GranQ improves efficiency by vectorizing the scaling operation. This design
allows GranQ to maintain fine-grained quantization granularity with minimal
computational overhead, even in low-bit environments. Extensive experiments
under quantization-aware training (QAT) settings demonstrate that GranQ
consistently outperforms state-of-the-art ZSQ methods across CIFAR and
ImageNet. In particular, our method achieves up to 5.45% higher accuracy in the
3-bit setting on CIFAR-100 and even surpasses the full-precision baseline on
CIFAR-10. Furthermore, GranQ achieves significant speedup in quantization
latency over conventional per-channel methods, demonstrating improved
efficiency. With these findings, we anticipate that GranQ will inspire future
research beyond conventional ZSQ approaches centered on data generation and
model fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conjuring Positive Pairs for Efficient Unification of Representation
  Learning and Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.15060v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.15060v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imanol G. Estepa, Jesús M. Rodríguez-de-Vera, Ignacio Sarasúa, Bhalaji Nagarajan, Petia Radeva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While representation learning and generative modeling seek to understand
visual data, unifying both domains remains unexplored. Recent Unified
Self-Supervised Learning (SSL) methods have started to bridge the gap between
both paradigms. However, they rely solely on semantic token reconstruction,
which requires an external tokenizer during training -- introducing a
significant overhead. In this work, we introduce Sorcen, a novel unified SSL
framework, incorporating a synergic Contrastive-Reconstruction objective. Our
Contrastive objective, "Echo Contrast", leverages the generative capabilities
of Sorcen, eliminating the need for additional image crops or augmentations
during training. Sorcen "generates" an echo sample in the semantic token space,
forming the contrastive positive pair. Sorcen operates exclusively on
precomputed tokens, eliminating the need for an online token transformation
during training, thereby significantly reducing computational overhead.
Extensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the
previous Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear
probing, unconditional image generation, few-shot learning, and transfer
learning, respectively, while being 60.8% more efficient. Additionally, Sorcen
surpasses previous single-crop MIM SoTA in linear probing and achieves SoTA
performance in unconditional image generation, highlighting significant
improvements and breakthroughs in Unified SSL models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code is available in https://github.com/ImaGonEs/Sorcen</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10238v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10238v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbo Ding, Xirui Hu, Zhizhi Guo, Yali Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human image animation has gained increasing attention and developed rapidly
due to its broad applications in digital humans. However, existing methods rely
largely on 2D-rendered pose images for motion guidance, which limits
generalization and discards essential 3D information for open-world animation.
To tackle this problem, we propose MTVCrafter (Motion Tokenization Video
Crafter), the first framework that directly models raw 3D motion sequences
(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT
(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.
Compared to 2D-rendered pose images, 4D motion tokens offer more robust
spatio-temporal cues and avoid strict pixel-level alignment between pose image
and character, enabling more flexible and disentangled control. Then, we
introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention
with 4D positional encodings, MV-DiT can effectively leverage motion tokens as
4D compact yet expressive context for human image animation in the complex 3D
world. Hence, it marks a significant step forward in this field and opens a new
direction for pose-guided human video generation. Experiments show that our
MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,
surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter
also generalizes well to diverse open-world characters (single/multiple,
full/half-body) across various styles and scenarios. Our video demos and code
are on: https://github.com/DINGYANB/MTVCrafter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal Collaborative Optimization and Expansion Network for
  Event-assisted Single-eye Expression Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12007v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12007v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runduo Han, Xiuping Liu, Shangxuan Yi, Yi Zhang, Hongchen Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we proposed a Multi-modal Collaborative Optimization and
Expansion Network (MCO-E Net), to use event modalities to resist challenges
such as low light, high exposure, and high dynamic range in single-eye
expression recognition tasks. The MCO-E Net introduces two innovative designs:
Multi-modal Collaborative Optimization Mamba (MCO-Mamba) and Heterogeneous
Collaborative and Expansion Mixture-of-Experts (HCE-MoE). MCO-Mamba, building
upon Mamba, leverages dual-modal information to jointly optimize the model,
facilitating collaborative interaction and fusion of modal semantics. This
approach encourages the model to balance the learning of both modalities and
harness their respective strengths. HCE-MoE, on the other hand, employs a
dynamic routing mechanism to distribute structurally varied experts (deep,
attention, and focal), fostering collaborative learning of complementary
semantics. This heterogeneous architecture systematically integrates diverse
feature extraction paradigms to comprehensively capture expression semantics.
Extensive experiments demonstrate that our proposed network achieves
competitive performance in the task of single-eye expression recognition,
especially under poor lighting conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uni4D: A Unified <span class="highlight-title">Self-Supervised</span> Learning Framework for Point Cloud
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Zuo, Chenyi Zhuang, Pan Gao, Jie Qin, Hao Feng, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised representation learning for point cloud videos remains a
challenging problem with two key limitations: (1) existing methods rely on
explicit knowledge to learn motion, resulting in suboptimal representations;
(2) prior Masked AutoEncoder (MAE) frameworks struggle to bridge the gap
between low-level geometry and high-level dynamics in 4D data. In this work, we
propose a novel self-disentangled MAE for learning expressive, discriminative,
and transferable 4D representations. To overcome the first limitation, we learn
motion by aligning high-level semantics in the latent space \textit{without any
explicit knowledge}. To tackle the second, we introduce a
\textit{self-disentangled learning} strategy that incorporates the latent token
with the geometry token within a shared decoder, effectively disentangling
low-level geometry and high-level semantics. In addition to the reconstruction
objective, we employ three alignment objectives to enhance temporal
understanding, including frame-level motion and video-level global information.
We show that our pre-trained encoder surprisingly discriminates spatio-temporal
representation without further fine-tuning. Extensive experiments on
MSR-Action3D, NTU-RGBD, HOI4D, NvGesture, and SHREC'17 demonstrate the
superiority of our approach in both coarse-grained and fine-grained 4D
downstream tasks. Notably, Uni4D improves action segmentation accuracy on HOI4D
by $+3.8\%$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Event-Driven Dynamic Scene Depth Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13279v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13279v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Yan, Jianhao Jiao, Zhengxue Wang, Gim Hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth completion in dynamic scenes poses significant challenges due to rapid
ego-motion and object motion, which can severely degrade the quality of input
modalities such as RGB images and LiDAR measurements. Conventional RGB-D
sensors often struggle to align precisely and capture reliable depth under such
conditions. In contrast, event cameras with their high temporal resolution and
sensitivity to motion at the pixel level provide complementary cues that are
%particularly beneficial in dynamic environments.To this end, we propose
EventDC, the first event-driven depth completion framework. It consists of two
key components: Event-Modulated Alignment (EMA) and Local Depth Filtering
(LDF). Both modules adaptively learn the two fundamental components of
convolution operations: offsets and weights conditioned on motion-sensitive
event streams. In the encoder, EMA leverages events to modulate the sampling
positions of RGB-D features to achieve pixel redistribution for improved
alignment and fusion. In the decoder, LDF refines depth estimations around
moving objects by learning motion-aware masks from events. Additionally,
EventDC incorporates two loss terms to further benefit global alignment and
enhance local depth recovery. Moreover, we establish the first benchmark for
event-based depth completion comprising one real-world and two synthetic
datasets to facilitate future research. Extensive experiments on this benchmark
demonstrate the superiority of our EventDC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Text-<span class="highlight-title">Prompt</span>able Surgical Instrument Segmentation with Robust
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12199v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12199v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tae-Min Choi, Juyoun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical instrument segmentation is an essential component of
computer-assisted and robotic surgery systems. Vision-based segmentation models
typically produce outputs limited to a predefined set of instrument categories,
which restricts their applicability in interactive systems and robotic task
automation. Promptable segmentation methods allow selective predictions based
on textual prompts. However, they often rely on the assumption that the
instruments present in the scene are already known, and prompts are generated
accordingly, limiting their ability to generalize to unseen or dynamically
emerging instruments. In practical surgical environments, where instrument
existence information is not provided, this assumption does not hold
consistently, resulting in false-positive segmentation. To address these
limitations, we formulate a new task called Robust text-promptable Surgical
Instrument Segmentation (R-SIS). Under this setting, prompts are issued for all
candidate categories without access to instrument presence information. R-SIS
requires distinguishing which prompts refer to visible instruments and
generating masks only when such instruments are explicitly present in the
scene. This setting reflects practical conditions where uncertainty in
instrument presence is inherent. We evaluate existing segmentation methods
under the R-SIS protocol using surgical video datasets and observe substantial
false-positive predictions in the absence of ground-truth instruments. These
findings demonstrate a mismatch between current evaluation protocols and
real-world use cases, and support the need for benchmarks that explicitly
account for prompt uncertainty and instrument absence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Alignment with Semantic Gap-Aware Corrections in Text-Video
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xiao, Zijie Song, Jialong Hu, Hao Cheng, Zhenzhen Hu, Jia Li, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-video retrieval have been largely driven by
contrastive learning frameworks. However, existing methods overlook a key
source of optimization tension: the separation between text and video
distributions in the representation space (referred to as the modality gap),
and the prevalence of false negatives in batch sampling. These factors lead to
conflicting gradients under the InfoNCE loss, impeding stable alignment. To
mitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces
a learnable, pair-specific increment Delta_ij between text t_i and video v_j to
offload the tension from the global anchor representation. We first derive the
ideal form of Delta_ij via a coupled multivariate first-order Taylor
approximation of the InfoNCE loss under a trust-region constraint, revealing it
as a mechanism for resolving gradient conflicts by guiding updates along a
locally optimal descent direction. Due to the high cost of directly computing
Delta_ij, we introduce a lightweight neural module conditioned on the semantic
gap between each video-text pair, enabling structure-aware correction guided by
gradient supervision. To further stabilize learning and promote
interpretability, we regularize Delta using three components: a trust-region
constraint to prevent oscillation, a directional diversity term to promote
semantic coverage, and an information bottleneck to limit redundancy.
Experiments across four retrieval benchmarks show that GARE consistently
improves alignment accuracy and robustness to noisy supervision, confirming the
effectiveness of gap-aware tension mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interactive Rendering of Relightable and Animatable Gaussian Avatars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10707v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10707v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youyi Zhan, Tianjia Shao, He Wang, Yin Yang, Kun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating relightable and animatable avatars from multi-view or monocular
videos is a challenging task for digital human creation and virtual reality
applications. Previous methods rely on neural radiance fields or ray tracing,
resulting in slow training and rendering processes. By utilizing Gaussian
Splatting, we propose a simple and efficient method to decouple body materials
and lighting from sparse-view or monocular avatar videos, so that the avatar
can be rendered simultaneously under novel viewpoints, poses, and lightings at
interactive frame rates (6.9 fps). Specifically, we first obtain the canonical
body mesh using a signed distance function and assign attributes to each mesh
vertex. The Gaussians in the canonical space then interpolate from nearby body
mesh vertices to obtain the attributes. We subsequently deform the Gaussians to
the posed space using forward skinning, and combine the learnable environment
light with the Gaussian attributes for shading computation. To achieve fast
shadow modeling, we rasterize the posed body mesh from dense viewpoints to
obtain the visibility. Our approach is not only simple but also fast enough to
allow interactive rendering of avatar animation under environmental light
changes. Experiments demonstrate that, compared to previous works, our method
can render higher quality results at a faster speed on both synthetic and real
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Visualization and Computer Graphics. Project
  page https://gapszju.github.io/InteractRAGA . Code
  https://github.com/1231234zhan/InteractRAGA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Coherent Matrixized Representation in Latent Space for
  Volumetric 4D Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qitong Yang, Mingtao Feng, Zijie Wu, Shijie Sun, Weisheng Dong, Yaonan Wang, Ajmal Mian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Directly learning to model 4D content, including shape, color, and motion, is
challenging. Existing methods rely on pose priors for motion control, resulting
in limited motion diversity and continuity in details. To address this, we
propose a framework that generates volumetric 4D sequences, where 3D shapes are
animated under given conditions (text-image guidance) with dynamic evolution in
shape and color across spatial and temporal dimensions, allowing for free
navigation and rendering from any direction. We first use a coherent 3D shape
and color modeling to encode the shape and color of each detailed 3D geometry
frame into a latent space. Then we propose a matrixized 4D sequence
representation allowing efficient diffusion model operation. Finally, we
introduce spatio-temporal diffusion for 4D volumetric generation under given
images and text prompts. Extensive experiments on the ShapeNet, 3DBiCar,
DeformingThings4D and Objaverse datasets for several tasks demonstrate that our
method effectively learns to generate high quality 3D shapes with consistent
color and coherent mesh animations, improving over the current methods. Our
code will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">30</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two Experts Are All You Need for Steering Thinking: Reinforcing
  Cognitive Effort in MoE Reasoning Models Without Additional Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengru Wang, Xingyu Chen, Yue Wang, Zhiwei He, Jiahao Xu, Tian Liang, Qiuzhi Liu, Yunzhi Yao, Wenxuan Wang, Ruotian Ma, Haitao Mi, Ningyu Zhang, Zhaopeng Tu, Xiaolong Li, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)
have achieved impressive reasoning capabilities by selectively activating
experts to facilitate structured cognitive processes. Despite notable advances,
existing reasoning models often suffer from cognitive inefficiencies like
overthinking and underthinking. To address these limitations, we introduce a
novel inference-time steering methodology called Reinforcing Cognitive Experts
(RICE), designed to improve reasoning performance without additional training
or complex heuristics. Leveraging normalized Pointwise Mutual Information
(nPMI), we systematically identify specialized experts, termed ''cognitive
experts'' that orchestrate meta-level reasoning operations characterized by
tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs
(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning
benchmarks demonstrate noticeable and consistent improvements in reasoning
accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our
lightweight approach substantially outperforms prevalent reasoning-steering
techniques, such as prompt design and decoding constraints, while preserving
the model's general instruction-following skills. These results highlight
reinforcing cognitive experts as a promising, practical, and interpretable
direction to enhance cognitive efficiency within advanced reasoning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunhao Dai, Wenjie Wang, Liang Pang, Jun Xu, See-Kiong Ng, Ji-Rong Wen, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI search is reshaping information retrieval by offering
end-to-end answers to complex queries, reducing users' reliance on manually
browsing and summarizing multiple web pages. However, while this paradigm
enhances convenience, it disrupts the feedback-driven improvement loop that has
historically powered the evolution of traditional Web search. Web search can
continuously improve their ranking models by collecting large-scale,
fine-grained user feedback (e.g., clicks, dwell time) at the document level. In
contrast, generative AI search operates through a much longer search pipeline,
spanning query decomposition, document retrieval, and answer generation, yet
typically receives only coarse-grained feedback on the final answer. This
introduces a feedback loop disconnect, where user feedback for the final output
cannot be effectively mapped back to specific system components, making it
difficult to improve each intermediate stage and sustain the feedback loop. In
this paper, we envision NExT-Search, a next-generation paradigm designed to
reintroduce fine-grained, process-level feedback into generative AI search.
NExT-Search integrates two complementary modes: User Debug Mode, which allows
engaged users to intervene at key stages; and Shadow User Mode, where a
personalized user agent simulates user preferences and provides AI-assisted
feedback for less interactive users. Furthermore, we envision how these
feedback signals can be leveraged through online adaptation, which refines
current search outputs in real-time, and offline update, which aggregates
interaction logs to periodically fine-tune query decomposition, retrieval, and
generation models. By restoring human control over key stages of the generative
AI search pipeline, we believe NExT-Search offers a promising direction for
building feedback-rich AI search systems that can evolve continuously alongside
human feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2025 Perspective Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R2MED: A Benchmark for Reasoning-Driven Medical Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Xiao Zhou, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current medical retrieval benchmarks primarily emphasize lexical or shallow
semantic similarity, overlooking the reasoning-intensive demands that are
central to clinical decision-making. In practice, physicians often retrieve
authoritative medical evidence to support diagnostic hypotheses. Such evidence
typically aligns with an inferred diagnosis rather than the surface form of a
patient's symptoms, leading to low lexical or semantic overlap between queries
and relevant documents. To address this gap, we introduce R2MED, the first
benchmark explicitly designed for reasoning-driven medical retrieval. It
comprises 876 queries spanning three tasks: Q&A reference retrieval, clinical
evidence retrieval, and clinical case retrieval. These tasks are drawn from
five representative medical scenarios and twelve body systems, capturing the
complexity and diversity of real-world medical information needs. We evaluate
15 widely-used retrieval systems on R2MED and find that even the best model
achieves only 31.4 nDCG@10, demonstrating the benchmark's difficulty. Classical
re-ranking and generation-augmented retrieval methods offer only modest
improvements. Although large reasoning models improve performance via
intermediate inference generation, the best results still peak at 41.4 nDCG@10.
These findings underscore a substantial gap between current retrieval
techniques and the reasoning demands of real clinical tasks. We release R2MED
as a challenging benchmark to foster the development of next-generation medical
retrieval systems with enhanced reasoning capabilities. Data and code are
available at https://github.com/R2MED/R2MED
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rank-K: Test-Time Reasoning for Listwise Reranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eugene Yang, Andrew Yates, Kathryn Ricci, Orion Weller, Vivek Chari, Benjamin Van Durme, Dawn Lawrie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieve-and-rerank is a popular retrieval pipeline because of its ability to
make slow but effective rerankers efficient enough at query time by reducing
the number of comparisons. Recent works in neural rerankers take advantage of
large language models for their capability in reasoning between queries and
passages and have achieved state-of-the-art retrieval effectiveness. However,
such rerankers are resource-intensive, even after heavy optimization. In this
work, we introduce Rank-K, a listwise passage reranking model that leverages
the reasoning capability of the reasoning language model at query time that
provides test time scalability to serve hard queries. We show that Rank-K
improves retrieval effectiveness by 23\% over the RankZephyr, the
state-of-the-art listwise reranker, when reranking a BM25 initial ranked list
and 19\% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is
inherently a multilingual model, we found that it ranks passages based on
queries in different languages as effectively as it does in monolingual
retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming Recommendation Bias with Causal Intervention on Evolving Personal
  Popularity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyin Tan, Dongyuan Li, Renhe Jiang, Zhen Wang, Xingtong Yu, Manabu Okumura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Popularity bias occurs when popular items are recommended far more frequently
than they should be, negatively impacting both user experience and
recommendation accuracy. Existing debiasing methods mitigate popularity bias
often uniformly across all users and only partially consider the time evolution
of users or items. However, users have different levels of preference for item
popularity, and this preference is evolving over time. To address these issues,
we propose a novel method called CausalEPP (Causal Intervention on Evolving
Personal Popularity) for taming recommendation bias, which accounts for the
evolving personal popularity of users. Specifically, we first introduce a
metric called {Evolving Personal Popularity} to quantify each user's preference
for popular items. Then, we design a causal graph that integrates evolving
personal popularity into the conformity effect, and apply deconfounded training
to mitigate the popularity bias of the causal graph. During inference, we
consider the evolution consistency between users and items to achieve a better
recommendation. Empirical studies demonstrate that CausalEPP outperforms
baseline methods in reducing popularity bias while improving recommendation
accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Technical Report on classification of literature related to children
  speech disorder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziang Wang, Amir Aryani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical report presents a natural language processing (NLP)-based
approach for systematically classifying scientific literature on childhood
speech disorders. We retrieved and filtered 4,804 relevant articles published
after 2015 from the PubMed database using domain-specific keywords. After
cleaning and pre-processing the abstracts, we applied two topic modeling
techniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify
latent thematic structures in the corpus. Our models uncovered 14 clinically
meaningful clusters, such as infantile hyperactivity and abnormal epileptic
behavior. To improve relevance and precision, we incorporated a custom stop
word list tailored to speech pathology. Evaluation results showed that the LDA
model achieved a coherence score of 0.42 and a perplexity of -7.5, indicating
strong topic coherence and predictive performance. The BERTopic model exhibited
a low proportion of outlier topics (less than 20%), demonstrating its capacity
to classify heterogeneous literature effectively. These results provide a
foundation for automating literature reviews in speech-language pathology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Limits of Graph Samplers for Training Inductive Recommender Systems:
  Extended results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theis E. Jendal, Matteo Lissandrini, Peter Dolog, Katja Hose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inductive Recommender Systems are capable of recommending for new users and
with new items thus avoiding the need to retrain after new data reaches the
system. However, these methods are still trained on all the data available,
requiring multiple days to train a single model, without counting
hyperparameter tuning. In this work we focus on graph-based recommender
systems, i.e., systems that model the data as a heterogeneous network. In other
applications, graph sampling allows to study a subgraph and generalize the
findings to the original graph. Thus, we investigate the applicability of
sampling techniques for this task. We test on three real world datasets, with
three state-of-the-art inductive methods, and using six different sampling
methods. We find that its possible to maintain performance using only 50% of
the training data with up to 86% percent decrease in training time; however,
using less training data leads to far worse performance. Further, we find that
when it comes to data for recommendations, graph sampling should also account
for the temporal dimension. Therefore, we find that if higher data reduction is
needed, new graph based sampling techniques should be studied and new inductive
methods should be designed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridge the Gap between Past and Future: Siamese Model Optimization for
  Context-Aware Document Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songhao Wu, Quan Tu, Mingjie Zhong, Hong Liu, Jia Xu, Jinjie Gu, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of information retrieval, users often engage in multi-turn
interactions with search engines to acquire information, leading to the
formation of sequences of user feedback behaviors. Leveraging the session
context has proven to be beneficial for inferring user search intent and
document ranking. A multitude of approaches have been proposed to exploit
in-session context for improved document ranking. Despite these advances, the
limitation of historical session data for capturing evolving user intent
remains a challenge. In this work, we explore the integration of future
contextual information into the session context to enhance document ranking. We
present the siamese model optimization framework, comprising a
history-conditioned model and a future-aware model. The former processes only
the historical behavior sequence, while the latter integrates both historical
and anticipated future behaviors. Both models are trained collaboratively using
the supervised labels and pseudo labels predicted by the other. The
history-conditioned model, referred to as ForeRanker, progressively learns
future-relevant information to enhance ranking, while it singly uses historical
session at inference time. To mitigate inconsistencies during training, we
introduce the peer knowledge distillation method with a dynamic gating
mechanism, allowing models to selectively incorporate contextual information.
Experimental results on benchmark datasets demonstrate the effectiveness of our
ForeRanker, showcasing its superior performance compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Abstractive Summarization of Scientific Papers Using Structure
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Bao, Heng Zhang, Chengzhi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstractive summarization of scientific papers has always been a research
focus, yet existing methods face two main challenges. First, most summarization
models rely on Encoder-Decoder architectures that treat papers as sequences of
words, thus fail to fully capture the structured information inherent in
scientific papers. Second, existing research often use keyword mapping or
feature engineering to identify the structural information, but these methods
struggle with the structural flexibility of scientific papers and lack
robustness across different disciplines. To address these challenges, we
propose a two-stage abstractive summarization framework that leverages
automatic recognition of structural functions within scientific papers. In the
first stage, we standardize chapter titles from numerous scientific papers and
construct a large-scale dataset for structural function recognition. A
classifier is then trained to automatically identify the key structural
components (e.g., Background, Methods, Results, Discussion), which provides a
foundation for generating more balanced summaries. In the second stage, we
employ Longformer to capture rich contextual relationships across sections and
generating context-aware summaries. Experiments conducted on two
domain-specific scientific paper summarization datasets demonstrate that our
method outperforms advanced baselines, and generates more comprehensive
summaries. The code and dataset can be accessed at
https://github.com/tongbao96/code-for-SFR-AS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unify Graph Learning with Text: Unleashing LLM Potentials for Session
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songhao Wu, Quan Tu, Hong Liu, Jia Xu, Zhongyi Liu, Guannan Zhang, Ran Wang, Xiuying Chen, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Session search involves a series of interactive queries and actions to
fulfill user's complex information need. Current strategies typically
prioritize sequential modeling for deep semantic understanding, overlooking the
graph structure in interactions. While some approaches focus on capturing
structural information, they use a generalized representation for documents,
neglecting the word-level semantic modeling. In this paper, we propose Symbolic
Graph Ranker (SGR), which aims to take advantage of both text-based and
graph-based approaches by leveraging the power of recent Large Language Models
(LLMs). Concretely, we first introduce a set of symbolic grammar rules to
convert session graph into text. This allows integrating session history,
interaction process, and task instruction seamlessly as inputs for the LLM.
Moreover, given the natural discrepancy between LLMs pre-trained on textual
corpora, and the symbolic language we produce using our graph-to-text grammar,
our objective is to enhance LLMs' ability to capture graph structures within a
textual format. To achieve this, we introduce a set of self-supervised symbolic
learning tasks including link prediction, node content generation, and
generative contrastive learning, to enable LLMs to capture the topological
information from coarse-grained to fine-grained. Experiment results and
comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm
the superiority of our approach. Our paradigm also offers a novel and effective
methodology that bridges the gap between traditional search strategies and
modern LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Keyphrase Extraction from Academic Articles Using Section
  Structure Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengzhi Zhang, Xinyi Yan, Lei Zhao, Yingyi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential increase in academic papers has significantly increased the
time required for researchers to access relevant literature. Keyphrase
Extraction (KPE) offers a solution to this situation by enabling researchers to
efficiently retrieve relevant literature. The current study on KPE from
academic articles aims to improve the performance of extraction models through
innovative approaches using Title and Abstract as input corpora. However, the
semantic richness of keywords is significantly constrained by the length of the
abstract. While full-text-based KPE can address this issue, it simultaneously
introduces noise, which significantly diminishes KPE performance. To address
this issue, this paper utilized the structural features and section texts
obtained from the section structure information of academic articles to extract
keyphrase from academic papers. The approach consists of two main parts: (1)
exploring the effect of seven structural features on KPE models, and (2)
integrating the extraction results from all section texts used as input corpora
for KPE models via a keyphrase integration algorithm to obtain the keyphrase
integration result. Furthermore, this paper also examined the effect of the
classification quality of section structure on the KPE performance. The results
show that incorporating structural features improves KPE performance, though
different features have varying effects on model efficacy. The keyphrase
integration approach yields the best performance, and the classification
quality of section structure can affect KPE performance. These findings
indicate that using the section structure information of academic articles
contributes to effective KPE from academic articles. The code and dataset
supporting this study are available at https://github.com/yan-xinyi/SSB_KPE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Chains: Bridging Large Language Models and Knowledge Bases in
  Complex Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihua Zhu, Qianying Liu, Akiko Aizawa, Hidetoshi Shimodaira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Base Question Answering (KBQA) aims to answer natural language
questions using structured knowledge from KBs. While LLM-only approaches offer
generalization, they suffer from outdated knowledge, hallucinations, and lack
of transparency. Chain-based KG-RAG methods address these issues by
incorporating external KBs, but are limited to simple chain-structured
questions due to the absence of planning and logical structuring. Inspired by
semantic parsing methods, we propose PDRR: a four-stage framework consisting of
Predict, Decompose, Retrieve, and Reason. Our method first predicts the
question type and decomposes the question into structured triples. Then
retrieves relevant information from KBs and guides the LLM as an agent to
reason over and complete the decomposed triples. Experimental results
demonstrate that PDRR consistently outperforms existing methods across various
LLM backbones and achieves superior performance on both chain-structured and
non-chain complex questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Process vs. Outcome Reward: Which is Better for Agentic RAG
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Derong Xu, Zhaocheng Du, Huifeng Guo, Ruiming Tang, Xiangyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) enhances the text generation
capabilities of large language models (LLMs) by integrating external knowledge
and up-to-date information. However, traditional RAG systems are limited by
static workflows and lack the adaptability required for multistep reasoning and
complex task management. To address these limitations, agentic RAG systems
(e.g., DeepResearch) have been proposed, enabling dynamic retrieval strategies,
iterative context refinement, and adaptive workflows for handling complex
search queries beyond the capabilities of conventional RAG. Recent advances,
such as Search-R1, have demonstrated promising gains using outcome-based
reinforcement learning, where the correctness of the final answer serves as the
reward signal. Nevertheless, such outcome-supervised agentic RAG methods face
challenges including low exploration efficiency, gradient conflict, and sparse
reward signals. To overcome these challenges, we propose to utilize
fine-grained, process-level rewards to improve training stability, reduce
computational costs, and enhance efficiency. Specifically, we introduce a novel
method ReasonRAG that automatically constructs RAG-ProGuide, a high-quality
dataset providing process-level rewards for (i) query generation, (ii) evidence
extraction, and (iii) answer generation, thereby enhancing model inherent
capabilities via process-supervised reinforcement learning. With the
process-level policy optimization, the proposed framework empowers LLMs to
autonomously invoke search, generate queries, extract relevant evidence, and
produce final answers. Compared to existing approaches such as Search-R1 and
traditional RAG systems, ReasonRAG, leveraging RAG-ProGuide, achieves superior
performance on five benchmark datasets using only 5k training instances,
significantly fewer than the 90k training instances required by Search-R1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Field Matters: A lightweight LLM-enhanced Method for CTR Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Cui, Feng Liu, Jiawei Chen, Xingyu Lou, Changwang Zhang, Jun Wang, Yuegang Sun, Xiaohu Yang, Can Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through rate (CTR) prediction is a fundamental task in modern
recommender systems. In recent years, the integration of large language models
(LLMs) has been shown to effectively enhance the performance of traditional CTR
methods. However, existing LLM-enhanced methods often require extensive
processing of detailed textual descriptions for large-scale instances or
user/item entities, leading to substantial computational overhead. To address
this challenge, this work introduces LLaCTR, a novel and lightweight
LLM-enhanced CTR method that employs a field-level enhancement paradigm.
Specifically, LLaCTR first utilizes LLMs to distill crucial and lightweight
semantic knowledge from small-scale feature fields through self-supervised
field-feature fine-tuning. Subsequently, it leverages this field-level semantic
knowledge to enhance both feature representation and feature interactions. In
our experiments, we integrate LLaCTR with six representative CTR models across
four datasets, demonstrating its superior performance in terms of both
effectiveness and efficiency compared to existing LLM-enhanced methods. Our
code is available at https://anonymous.4open.science/r/LLaCTR-EC46.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangled Multi-span Evolutionary Network against Temporal Knowledge
  Graph Reasoning <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Dong, Ziyue Qiao, Zhiyuan Ning, Qi Hao, Yi Du, Pengyang Wang, Yuanchun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs
(KGs), incorporate the temporal feature to express the transience of knowledge
by describing when facts occur. TKG extrapolation aims to infer possible future
facts based on known history, which has garnered significant attention in
recent years. Some existing methods treat TKG as a sequence of independent
subgraphs to model temporal evolution patterns, demonstrating impressive
reasoning performance. However, they still have limitations: 1) In modeling
subgraph semantic evolution, they usually neglect the internal structural
interactions between subgraphs, which are actually crucial for encoding TKGs.
2) They overlook the potential smooth features that do not lead to semantic
changes, which should be distinguished from the semantic evolution process.
Therefore, we propose a novel Disentangled Multi-span Evolutionary Network
(DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution
strategy that captures local neighbor features while perceiving historical
neighbor semantic information, thus enabling internal interactions between
subgraphs during the evolution process. To maximize the capture of semantic
change patterns, we design a disentangle component that adaptively separates
nodes' active and stable features, used to dynamically control the influence of
historical semantics on future evolution. Extensive experiments conducted on
four real-world TKG datasets show that DiMNet demonstrates substantial
performance in TKG reasoning, and outperforms the state-of-the-art up to 22.7%
in MRR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven
  Graph Partitioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems empower large language models
(LLMs) with external knowledge, yet struggle with efficiency-accuracy
trade-offs when scaling to large knowledge graphs. Existing approaches often
rely on monolithic graph retrieval, incurring unnecessary latency for simple
queries and fragmented reasoning for complex multi-hop questions. To address
these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework
that addresses these limitations with question-driven semantic graph
partitioning and collaborative subgraph retrieval. The innovative framework
first create Semantic Partitioning of Linked Information, then use the
Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware
graph segmentation manages to divide knowledge graphs into semantically
coherent subgraphs, ensuring subgraphs align with different query types, while
lightweight LLM agents are assigned to partitioned subgraphs, and only relevant
partitions are activated during retrieval, thus reduce search space while
enhancing efficiency. Finally, a hierarchical merging module resolves
inconsistencies across subgraph-derived answers through logical verifications.
Extensive experimental validation demonstrates considerable improvements
compared to existing approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIFF: Dual Side-Information Filtering and Fusion for Sequential
  Recommendation <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hye-young Kim, Minjin Choi, Sunkyung Lee, Ilwoong Baek, Jongwuk Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Side-information Integrated Sequential Recommendation (SISR) benefits from
auxiliary item information to infer hidden user preferences, which is
particularly effective for sparse interactions and cold-start scenarios.
However, existing studies face two main challenges. (i) They fail to remove
noisy signals in item sequence and (ii) they underutilize the potential of
side-information integration. To tackle these issues, we propose a novel SISR
model, Dual Side-Information Filtering and Fusion (DIFF), which employs
frequency-based noise filtering and dual multi-sequence fusion. Specifically,
we convert the item sequence to the frequency domain to filter out noisy
short-term fluctuations in user interests. We then combine early and
intermediate fusion to capture diverse relationships across item IDs and
attributes. Thanks to our innovative filtering and fusion strategy, DIFF is
more robust in learning subtle and complex item correlations in the sequence.
DIFF outperforms state-of-the-art SISR models, achieving improvements of up to
14.1% and 12.5% in Recall@20 and NDCG@20 across four benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2025. 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking the Myopic Trap: Positional Bias in Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Zeng, Dun Zhang, Jiacheng Li, Panxiang Zou, Yuqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates a specific form of positional bias, termed the Myopic
Trap, where retrieval models disproportionately attend to the early parts of
documents while overlooking relevant information that appears later. To
systematically quantify this phenomenon, we propose a semantics-preserving
evaluation framework that repurposes the existing NLP datasets into
position-aware retrieval benchmarks. By evaluating the SOTA models of full
retrieval pipeline, including BM25, embedding models, ColBERT-style
late-interaction models, and reranker models, we offer a broader empirical
perspective on positional bias than prior work. Experimental results show that
embedding models and ColBERT-style models exhibit significant performance
degradation when query-related content is shifted toward later positions,
indicating a pronounced head bias. Notably, under the same training
configuration, ColBERT-style approach show greater potential for mitigating
positional bias compared to the traditional single-vector approach. In
contrast, BM25 and reranker models remain largely unaffected by such
perturbations, underscoring their robustness to positional bias. Code and data
are publicly available at: www.github.com/NovaSearch-Team/RAG-Retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, 4 tables. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qifeng Cai, Hao Liang, Hejun Dong, Meiyi Qiang, Ruichuan An, Zhaoyang Han, Zhengzhou Zhu, Bin Cui, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long videos contain a vast amount of information, making video-text retrieval
an essential and challenging task in multimodal learning. However, existing
benchmarks suffer from limited video duration, low-quality captions, and coarse
annotation granularity, which hinder the evaluation of advanced video-text
retrieval methods. To address these limitations, we introduce LoVR, a benchmark
specifically designed for long video-text retrieval. LoVR contains 467 long
videos and over 40,804 fine-grained clips with high-quality captions. To
overcome the issue of poor machine-generated annotations, we propose an
efficient caption generation framework that integrates VLM automatic
generation, caption quality scoring, and dynamic refinement. This pipeline
improves annotation accuracy while maintaining scalability. Furthermore, we
introduce a semantic fusion method to generate coherent full-video captions
without losing important contextual information. Our benchmark introduces
longer videos, more detailed captions, and a larger-scale dataset, presenting
new challenges for video understanding and retrieval. Extensive experiments on
various advanced embedding models demonstrate that LoVR is a challenging
benchmark, revealing the limitations of current approaches and providing
valuable insights for future research. We release the code and dataset link at
https://github.com/TechNomad-ds/LoVR-benchmark
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VulCPE: Context-Aware Cybersecurity Vulnerability Retrieval and
  Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuning Jiang, Feiyang Shang, Freedy Tan Wei You, Huilin Wang, Chia Ren Cong, Qiaoran Meng, Nay Oo, Hoon Wei Lim, Biplab Sikdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dynamic landscape of cybersecurity demands precise and scalable solutions
for vulnerability management in heterogeneous systems, where
configuration-specific vulnerabilities are often misidentified due to
inconsistent data in databases like the National Vulnerability Database (NVD).
Inaccurate Common Platform Enumeration (CPE) data in NVD further leads to false
positives and incomplete vulnerability retrieval. Informed by our systematic
analysis of CPE and CVEdeails data, revealing more than 50% vendor name
inconsistencies, we propose VulCPE, a framework that standardizes data and
models configuration dependencies using a unified CPE schema (uCPE), entity
recognition, relation extraction, and graph-based modeling. VulCPE achieves
superior retrieval precision (0.766) and coverage (0.926) over existing tools.
VulCPE ensures precise, context-aware vulnerability management, enhancing cyber
resilience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TranSUN: A Preemptive Paradigm to Eradicate Retransformation Bias
  Intrinsically from Regression Models in Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Yu, Haozhuang Liu, Yeqiu Yang, Lu Chen, Wu Jian, Yuning Jiang, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regression models are crucial in recommender systems. However,
retransformation bias problem has been conspicuously neglected within the
community. While many works in other fields have devised effective bias
correction methods, all of them are post-hoc cures externally to the model,
facing practical challenges when applied to real-world recommender systems.
Hence, we propose a preemptive paradigm to eradicate the bias intrinsically
from the models via minor model refinement. Specifically, a novel TranSUN
method is proposed with a joint bias learning manner to offer theoretically
guaranteed unbiasedness under empirical superior convergence. It is further
generalized into a novel generic regression model family, termed Generalized
TranSUN (GTS), which not only offers more theoretical insights but also serves
as a generic framework for flexibly developing various bias-free models.
Comprehensive experimental results demonstrate the superiority of our methods
across data from various domains, which have been successfully deployed in two
real-world industrial recommendation scenarios, i.e. product and short video
recommendation scenarios in Guess What You Like business domain in the homepage
of Taobao App (a leading e-commerce platform), to serve the major online
traffic. Codes will be released after this paper is published.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Synthetic Data and Fine-grained Adaptive Feature Alignment for
  Composed Person Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16515v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16515v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Liu, Haiwen Li, Zhaohui Hou, Zhicheng Zhao, Fei Su, Yuan Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person retrieval has attracted rising attention. Existing methods are mainly
divided into two retrieval modes, namely image-only and text-only. However,
they are unable to make full use of the available information and are difficult
to meet diverse application requirements. To address the above limitations, we
propose a new Composed Person Retrieval (CPR) task, which combines visual and
textual queries to identify individuals of interest from large-scale person
image databases. Nevertheless, the foremost difficulty of the CPR task is the
lack of available annotated datasets. Therefore, we first introduce a scalable
automatic data synthesis pipeline, which decomposes complex multimodal data
generation into the creation of textual quadruples followed by
identity-consistent image synthesis using fine-tuned generative models.
Meanwhile, a multimodal filtering method is designed to ensure the resulting
SynCPR dataset retains 1.15 million high-quality and fully synthetic triplets.
Additionally, to improve the representation of composed person queries, we
propose a novel Fine-grained Adaptive Feature Alignment (FAFA) framework
through fine-grained dynamic alignment and masked feature reasoning. Moreover,
for objective evaluation, we manually annotate the Image-Text Composed Person
Retrieval (ITCPR) test set. The extensive experiments demonstrate the
effectiveness of the SynCPR dataset and the superiority of the proposed FAFA
framework when compared with the state-of-the-art methods. All code and data
will be provided at
https://github.com/Delong-liu-bupt/Composed_Person_Retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuicai Dong, Yujing Chang, Xin Deik Goh, Dexun Li, Ruiming Tang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal document retrieval aims to identify and retrieve various forms of
multimodal content, such as figures, tables, charts, and layout information
from extensive documents. Despite its increasing popularity, there is a notable
lack of a comprehensive and robust benchmark to effectively evaluate the
performance of systems in such tasks. To address this gap, this work introduces
a new benchmark, named MMDocIR, that encompasses two distinct tasks: page-level
and layout-level retrieval. The former evaluates the performance of identifying
the most relevant pages within a long document, while the later assesses the
ability of detecting specific layouts, providing a more fine-grained measure
than whole-page analysis. A layout refers to a variety of elements, including
textual paragraphs, equations, figures, tables, or charts. The MMDocIR
benchmark comprises a rich dataset featuring 1,685 questions annotated by
experts and 173,843 questions with bootstrapped labels, making it a valuable
resource in multimodal document retrieval for both training and evaluation.
Through rigorous experiments, we demonstrate that (i) visual retrievers
significantly outperform their text counterparts, (ii) MMDocIR training set
effectively enhances the performance of multimodal document retrieval and (iii)
text retrievers leveraging VLM-text significantly outperforms retrievers
relying on OCR-text. Our dataset is available at
https://mmdocrag.github.io/MMDocIR/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://huggingface.co/MMDocIR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanhao Jia, Xinyi Wu, Hao Li, Qinglin Zhang, Yuxiao Hu, Shuai Zhao, Wenqi Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In AI-facilitated teaching, leveraging various query styles to interpret
abstract text descriptions is crucial for ensuring high-quality teaching.
However, current retrieval models primarily focus on natural text-image
retrieval, making them insufficiently tailored to educational scenarios due to
the ambiguities in the retrieval process. In this paper, we propose a diverse
expression retrieval task tailored to educational scenarios, supporting
retrieval based on multiple query styles and expressions. We introduce the STEM
Education Retrieval Dataset (SER), which contains over 24,000 query pairs of
different styles, and the Uni-Retrieval, an efficient and style-diversified
retrieval vision-language model based on prompt tuning. Uni-Retrieval extracts
query style features as prototypes and builds a continuously updated Prompt
Bank containing prompt tokens for diverse queries. This bank can updated during
test time to represent domain-specific knowledge for different subject
retrieval scenarios. Our framework demonstrates scalability and robustness by
dynamically retrieving prompt tokens based on prototype similarity, effectively
facilitating learning for unknown queries. Experimental results indicate that
Uni-Retrieval outperforms existing retrieval models in most retrieval tasks.
This advancement provides a scalable and precise solution for diverse
educational needs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MCiteBench: A Multimodal Benchmark for Generating Text with Citations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02589v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02589v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caiyu Hu, Yikai Zhang, Tinghui Zhu, Yiwei Ye, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have advanced in integrating diverse
modalities but frequently suffer from hallucination. A promising solution to
mitigate this issue is to generate text with citations, providing a transparent
chain for verification. However, existing work primarily focuses on generating
citations for text-only content, leaving the challenges of multimodal scenarios
largely unexplored. In this paper, we introduce MCiteBench, the first benchmark
designed to assess the ability of MLLMs to generate text with citations in
multimodal contexts. Our benchmark comprises data derived from academic papers
and review-rebuttal interactions, featuring diverse information sources and
multimodal content. Experimental results reveal that MLLMs struggle to ground
their outputs reliably when handling multimodal input. Further analysis
uncovers a systematic modality bias and reveals how models internally rely on
different sources when generating citations, offering insights into model
behavior and guiding future directions for multimodal citation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://caiyuhu.github.io/MCiteBench/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Alignment with Semantic Gap-Aware Corrections in Text-Video
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xiao, Zijie Song, Jialong Hu, Hao Cheng, Zhenzhen Hu, Jia Li, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-video retrieval have been largely driven by
contrastive learning frameworks. However, existing methods overlook a key
source of optimization tension: the separation between text and video
distributions in the representation space (referred to as the modality gap),
and the prevalence of false negatives in batch sampling. These factors lead to
conflicting gradients under the InfoNCE loss, impeding stable alignment. To
mitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces
a learnable, pair-specific increment Delta_ij between text t_i and video v_j to
offload the tension from the global anchor representation. We first derive the
ideal form of Delta_ij via a coupled multivariate first-order Taylor
approximation of the InfoNCE loss under a trust-region constraint, revealing it
as a mechanism for resolving gradient conflicts by guiding updates along a
locally optimal descent direction. Due to the high cost of directly computing
Delta_ij, we introduce a lightweight neural module conditioned on the semantic
gap between each video-text pair, enabling structure-aware correction guided by
gradient supervision. To further stabilize learning and promote
interpretability, we regularize Delta using three components: a trust-region
constraint to prevent oscillation, a directional diversity term to promote
semantic coverage, and an information bottleneck to limit redundancy.
Experiments across four retrieval benchmarks show that GARE consistently
improves alignment accuracy and robustness to noisy supervision, confirming the
effectiveness of gap-aware tension mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced
  Logical Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10940v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10940v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Yu, Xiaobei Wang, Shuchang Liu, Yandong Bai, Xiaoyu Yang, Xueliang Wang, Chang Meng, Shanshan Wu, Hailan Yang, Huihui Xiao, Xiang Li, Fan Yang, Xiaoqiang Feng, Lantao Hu, Han Li, Kun Gai, Lixin Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems filter contents/items valuable to users by inferring
preferences from user features and historical behaviors. Mainstream approaches
follow the learning-to-rank paradigm, which focus on discovering and modeling
item topics (e.g., categories), and capturing user preferences on these topics
based on historical interactions. However, this paradigm often neglects the
modeling of user characteristics and their social roles, which are logical
confounders influencing the correlated interest and user preference transition.
To bridge this gap, we introduce the user role identification task and the
behavioral logic modeling task that aim to explicitly model user roles and
learn the logical relations between item topics and user social roles. We show
that it is possible to explicitly solve these tasks through an efficient
integration framework of Large Language Model (LLM) and recommendation systems,
for which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal)
LLM's world knowledge and logic inference ability to extract realistic
tag-based virtual logic graphs that reveal dynamic and expressive knowledge of
users, refining our understanding of user behaviors. On the other hand, TagCF
presents empirically effective integration modules that take advantage of the
extracted tag-logic information, augmenting the recommendation performance. We
conduct both online experiments and offline experiments with industrial and
public datasets as verification of TagCF's effectiveness, and we empirically
show that the user role modeling strategy is potentially a better choice than
the modeling of item topics. Additionally, we provide evidence that the
extracted logic graphs are empirically a general and transferable knowledge
that can benefit a wide range of recommendation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Does Topology Bias Distort Message Passing? A Dirichlet Energy
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbiao Ji, Yue Ding, Dan Luo, Chang Liu, Yuxiang Lu, Xin Xin, Hongtao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based recommender systems have achieved remarkable effectiveness by
modeling high-order interactions between users and items. However, such
approaches are significantly undermined by popularity bias, which distorts the
interaction graph's structure, referred to as topology bias. This leads to
overrepresentation of popular items, thereby reinforcing biases and fairness
issues through the user-system feedback loop. Despite attempts to study this
effect, most prior work focuses on the embedding or gradient level bias,
overlooking how topology bias fundamentally distorts the message passing
process itself. We bridge this gap by providing an empirical and theoretical
analysis from a Dirichlet energy perspective, revealing that graph message
passing inherently amplifies topology bias and consistently benefits highly
connected nodes. To address these limitations, we propose Test-time Simplicial
Propagation (TSP), which extends message passing to higher-order simplicial
complexes. By incorporating richer structures beyond pairwise connections, TSP
mitigates harmful topology bias and substantially improves the representation
and recommendation of long-tail items during inference. Extensive experiments
across five real-world datasets demonstrate the superiority of our approach in
mitigating topology bias and enhancing recommendation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Test-Time Inference with Policy-Optimized, Dynamic
  Retrieval-Augmented Generation via KV Caching and Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.01281v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.01281v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sakhinana Sagar Srinivas, Akash Das, Shivam Gupta, Venkataramana Runkana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive framework for enhancing Retrieval-Augmented
Generation (RAG) systems through dynamic retrieval strategies and reinforcement
fine-tuning. This approach significantly improves large language models on
knowledge-intensive tasks, including opendomain question answering and complex
reasoning. Our framework integrates two complementary techniques:
Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use
of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),
which dynamically determines retrieval timing and content based on contextual
needs. Together, these techniques enhance both the utilization and relevance of
retrieved content, improving factual accuracy and response quality. Designed as
a lightweight solution compatible with any Transformer-based LLM without
requiring additional training, our framework excels in knowledge-intensive
tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a
novel method to selectively compress key-value caches by token importance,
mitigating memory bottlenecks in long-context applications. The framework also
incorporates test-time scaling techniques to dynamically balance reasoning
depth and computational resources, alongside optimized decoding strategies for
faster inference. Experiments on benchmark datasets show that our framework
reduces hallucinations, strengthens domain-specific reasoning, and achieves
significant efficiency and scalability gains over traditional RAG systems. This
integrated approach advances the development of robust, efficient, and scalable
RAG systems across diverse applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation <span class="chip">ICML2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01776v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01776v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiansheng Wen, Yifei Wang, Zequn Zeng, Zhong Peng, Yudi Su, Xinyang Liu, Bo Chen, Hongwei Liu, Stefanie Jegelka, Chenyu You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many large-scale systems rely on high-quality deep representations
(embeddings) to facilitate tasks like retrieval, search, and generative
modeling. Matryoshka Representation Learning (MRL) recently emerged as a
solution for adaptive embedding lengths, but it requires full model retraining
and suffers from noticeable performance degradations at short lengths. In this
paper, we show that sparse coding offers a compelling alternative for achieving
adaptive representation with minimal overhead and higher fidelity. We propose
Contrastive Sparse Representation (CSR), a method that sparsifies pre-trained
embeddings into a high-dimensional but selectively activated feature space. By
leveraging lightweight autoencoding and task-aware contrastive objectives, CSR
preserves semantic quality while allowing flexible, cost-effective inference at
different sparsity levels. Extensive experiments on image, text, and multimodal
benchmarks demonstrate that CSR consistently outperforms MRL in terms of both
accuracy and retrieval speed-often by large margins-while also cutting training
time to a fraction of that required by MRL. Our results establish sparse coding
as a powerful paradigm for adaptive representation learning in real-world
applications where efficiency and fidelity are both paramount. Code is
available at https://github.com/neilwen987/CSR_Adaptive_Rep
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-19T00:00:00Z">2025-05-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">57</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Structural Design to Dynamics Modeling: Control-Oriented
  Development of a 3-RRR Parallel Ankle Rehabilitation Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Zhang, Yufei Zhang, Junlin Lyu, Sunil K. Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the development of a wearable ankle rehabilitation robot
based on a 3-RRR spherical parallel mechanism (SPM) to support multi-DOF
recovery through pitch, roll, and yaw motions. The system features a compact,
ergonomic structure designed for comfort, safety, and compatibility with ankle
biomechanics. A complete design-to-dynamics pipeline has been implemented,
including structural design, kinematic modeling for motion planning, and
Lagrangian-based dynamic modeling for torque estimation and simulation
analysis. Preliminary simulations verify stable joint coordination and smooth
motion tracking under representative rehabilitation trajectories. The control
framework is currently being developed to enhance responsiveness across the
workspace. Future work will focus on integrating personalized modeling and
adaptive strategies to address kinematic singularities through model based
control. This work establishes a foundational platform for intelligent,
personalized ankle rehabilitation, enabling both static training and potential
extension to gait-phase-timed assistance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SayCoNav: Utilizing Large Language Models for Adaptive Collaboration in
  Decentralized Multi-Robot Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Rajvanshi, Pritish Sahu, Tixiao Shan, Karan Sikka, Han-Pang Chiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive collaboration is critical to a team of autonomous robots to perform
complicated navigation tasks in large-scale unknown environments. An effective
collaboration strategy should be determined and adapted according to each
robot's skills and current status to successfully achieve the shared goal. We
present SayCoNav, a new approach that leverages large language models (LLMs)
for automatically generating this collaboration strategy among a team of
robots. Building on the collaboration strategy, each robot uses the LLM to
generate its plans and actions in a decentralized way. By sharing information
to each other during navigation, each robot also continuously updates its
step-by-step plans accordingly. We evaluate SayCoNav on Multi-Object Navigation
(MultiON) tasks, that require the team of the robots to utilize their
complementary strengths to efficiently search multiple different objects in
unknown environments. By validating SayCoNav with varied team compositions and
conditions against baseline methods, our experimental results show that
SayCoNav can improve search efficiency by at most 44.28% through effective
collaboration among heterogeneous robots. It can also dynamically adapt to the
changing conditions during task execution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practice Makes Perfect: A Study of Digital Twin Technology for Assembly
  and Problem-solving using Lunar Surface Telerobotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier O'Keefe, Katy McCutchan, Alexis Muniz, Jack Burns, Daniel Szafir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic systems that can traverse planetary or lunar surfaces to collect
environmental data and perform physical manipulation tasks, such as assembling
equipment or conducting mining operations, are envisioned to form the backbone
of future human activities in space. However, the environmental conditions in
which these robots, or "rovers," operate present challenges toward achieving
fully autonomous solutions, meaning that rover missions will require some
degree of human teleoperation or supervision for the foreseeable future. As a
result, human operators require training to successfully direct rovers and
avoid costly errors or mission failures, as well as the ability to recover from
any issues that arise on the fly during mission activities. While analog
environments, such as JPL's Mars Yard, can help with such training by
simulating surface environments in the real world, access to such resources may
be rare and expensive. As an alternative or supplement to such physical
analogs, we explore the design and evaluation of a virtual reality digital twin
system to train human teleoperation of robotic rovers with mechanical arms for
space mission activities. We conducted an experiment with 24 human operators to
investigate how our digital twin system can support human teleoperation of
rovers in both pre-mission training and in real-time problem solving in a mock
lunar mission in which users directed a physical rover in the context of
deploying dipole radio antennas. We found that operators who first trained with
the digital twin showed a 28% decrease in mission completion time, an 85%
decrease in unrecoverable errors, as well as improved mental markers, including
decreased cognitive load and increased situation awareness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Bipedal MPC with Foot-level Obstacle Avoidance and Adjustable
  Step Timing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianze Wang, Christian Hubicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collision-free planning is essential for bipedal robots operating within
unstructured environments. This paper presents a real-time Model Predictive
Control (MPC) framework that addresses both body and foot avoidance for dynamic
bipedal robots. Our contribution is two-fold: we introduce (1) a novel
formulation for adjusting step timing to facilitate faster body avoidance and
(2) a novel 3D foot-avoidance formulation that implicitly selects swing
trajectories and footholds that either steps over or navigate around obstacles
with awareness of Center of Mass (COM) dynamics. We achieve body avoidance by
applying a half-space relaxation of the safe region but introduce a switching
heuristic based on tracking error to detect a need to change foot-timing
schedules. To enable foot avoidance and viable landing footholds on all sides
of foot-level obstacles, we decompose the non-convex safe region on the ground
into several convex polygons and use Mixed-Integer Quadratic Programming to
determine the optimal candidate. We found that introducing a soft
minimum-travel-distance constraint is effective in preventing the MPC from
being trapped in local minima that can stall half-space relaxation methods
behind obstacles. We demonstrated the proposed algorithms on multibody
simulations on the bipedal robot platforms, Cassie and Digit, as well as
hardware experiments on Digit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Risk-Averse Traversal of Graphs with Stochastic and Correlated Edge
  Costs for Safe Global Planetary Mobility 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Lamarre, Jonathan Kelly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In robotic planetary surface exploration, strategic mobility planning is an
important task that involves finding candidate long-distance routes on orbital
maps and identifying segments with uncertain traversability. Then, expert human
operators establish safe, adaptive traverse plans based on the actual
navigation difficulties encountered in these uncertain areas. In this paper, we
formalize this challenge as a new, risk-averse variant of the Canadian
Traveller Problem (CTP) tailored to global planetary mobility. The objective is
to find a traverse policy minimizing a conditional value-at-risk (CVaR)
criterion, which is a risk measure with an intuitive interpretation. We propose
a novel search algorithm that finds exact CVaR-optimal policies. Our approach
leverages well-established optimal AND-OR search techniques intended for
(risk-agnostic) expectation minimization and extends these methods to the
risk-averse domain. We validate our approach through simulated long-distance
planetary surface traverses; we employ real orbital maps of the Martian surface
to construct problem instances and use terrain maps to express traversal
probabilities in uncertain regions. Our results illustrate different adaptive
decision-making schemes depending on the level of risk aversion. Additionally,
our problem setup allows accounting for traversability correlations between
similar areas of the environment. In such a case, we empirically demonstrate
how information-seeking detours can mitigate risk.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the Autonomous Robots journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoVLM: Improving Automated Vehicle Geolocalisation Using
  Vision-Language Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barkin Dagda, Muhammad Awais, Saber Fallah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-view geo-localisation identifies coarse geographical position of an
automated vehicle by matching a ground-level image to a geo-tagged satellite
image from a database. Despite the advancements in Cross-view geo-localisation,
significant challenges still persist such as similar looking scenes which makes
it challenging to find the correct match as the top match. Existing approaches
reach high recall rates but they still fail to rank the correct image as the
top match. To address this challenge, this paper proposes GeoVLM, a novel
approach which uses the zero-shot capabilities of vision language models to
enable cross-view geo-localisation using interpretable cross-view language
descriptions. GeoVLM is a trainable reranking approach which improves the best
match accuracy of cross-view geo-localisation. GeoVLM is evaluated on standard
benchmark VIGOR and University-1652 and also through real-life driving
environments using Cross-View United Kingdom, a new benchmark dataset
introduced in this paper. The results of the paper show that GeoVLM improves
retrieval performance of cross-view geo-localisation compared to the
state-of-the-art methods with the help of explainable natural language
descriptions. The code is available at
https://github.com/CAV-Research-Lab/GeoVLM
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Diffusion Constrained Sampling for Bimanual Robot Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolei Tong, Yuezhe Zhang, Sophie Lueth, Georgia Chalvatzaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coordinated multi-arm manipulation requires satisfying multiple simultaneous
geometric constraints across high-dimensional configuration spaces, which poses
a significant challenge for traditional planning and control methods. In this
work, we propose Adaptive Diffusion Constrained Sampling (ADCS), a generative
framework that flexibly integrates both equality (e.g., relative and absolute
pose constraints) and structured inequality constraints (e.g., proximity to
object surfaces) into an energy-based diffusion model. Equality constraints are
modeled using dedicated energy networks trained on pose differences in Lie
algebra space, while inequality constraints are represented via Signed Distance
Functions (SDFs) and encoded into learned constraint embeddings, allowing the
model to reason about complex spatial regions. A key innovation of our method
is a Transformer-based architecture that learns to weight constraint-specific
energy functions at inference time, enabling flexible and context-aware
constraint integration. Moreover, we adopt a two-phase sampling strategy that
improves precision and sample diversity by combining Langevin dynamics with
resampling and density-aware re-weighting. Experimental results on dual-arm
manipulation tasks show that ADCS significantly improves sample diversity and
generalization across settings demanding precise coordination and adaptive
constraint handling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale
  Synthetic Data Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhay Deshpande, Yuquan Deng, Arijit Ray, Jordi Salvador, Winson Han, Jiafei Duan, Kuo-Hao Zeng, Yuke Zhu, Ranjay Krishna, Rose Hendrix
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GrasMolmo, a generalizable open-vocabulary task-oriented grasping
(TOG) model. GraspMolmo predicts semantically appropriate, stable grasps
conditioned on a natural language instruction and a single RGB-D frame. For
instance, given "pour me some tea", GraspMolmo selects a grasp on a teapot
handle rather than its body. Unlike prior TOG methods, which are limited by
small datasets, simplistic language, and uncluttered scenes, GraspMolmo learns
from PRISM, a novel large-scale synthetic dataset of 379k samples featuring
cluttered environments and diverse, realistic task descriptions. We fine-tune
the Molmo visual-language model on this data, enabling GraspMolmo to generalize
to novel open-vocabulary instructions and objects. In challenging real-world
evaluations, GraspMolmo achieves state-of-the-art results, with a 70%
prediction success on complex tasks, compared to the 35% achieved by the next
best alternative. GraspMolmo also successfully demonstrates the ability to
predict semantically correct bimanual grasps zero-shot. We release our
synthetic dataset, code, model, and benchmarks to accelerate research in
task-semantic robotic manipulation, which, along with videos, are available at
https://abhaybd.github.io/GraspMolmo/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing, Saying, Solving: An LLM-to-TL Framework for Cooperative Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan BW Choe, Sundhar Vinodh Sangeetha, Steven Emanuel, Chih-Yuan Chiu, Samuel Coogan, Shreyas Kousik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Increased robot deployment, such as in warehousing, has revealed a need for
seamless collaboration among heterogeneous robot teams to resolve unforeseen
conflicts. To address this challenge, we propose a novel, decentralized
framework for robots to request and provide help. The framework begins with
robots detecting conflicts using a Vision Language Model (VLM), then reasoning
over whether help is needed. If so, it crafts and broadcasts a natural language
(NL) help request using a Large Language Model (LLM). Potential helper robots
reason over the request and offer help (if able), along with information about
impact to their current tasks. Helper reasoning is implemented via an LLM
grounded in Signal Temporal Logic (STL) using a Backus-Naur Form (BNF) grammar
to guarantee syntactically valid NL-to-STL translations, which are then solved
as a Mixed Integer Linear Program (MILP). Finally, the requester robot chooses
a helper by reasoning over impact on the overall system. We evaluate our system
via experiments considering different strategies for choosing a helper, and
find that a requester robot can minimize overall time impact on the system by
considering multiple help offers versus simple heuristics (e.g., selecting the
nearest robot to help).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximating Global Contact-Implicit MPC via Sampling and Local
  Complementarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharanya Venkatesh, Bibit Bianchini, Alp Aydinoglu, William Yang, Michael Posa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve general-purpose dexterous manipulation, robots must rapidly devise
and execute contact-rich behaviors. Existing model-based controllers are
incapable of globally optimizing in real-time over the exponential number of
possible contact sequences. Instead, recent progress in contact-implicit
control has leveraged simpler models that, while still hybrid, make local
approximations. However, the use of local models inherently limits the
controller to only exploit nearby interactions, potentially requiring
intervention to richly explore the space of possible contacts. We present a
novel approach which leverages the strengths of local complementarity-based
control in combination with low-dimensional, but global, sampling of possible
end-effector locations. Our key insight is to consider a contact-free stage
preceding a contact-rich stage at every control loop. Our algorithm, in
parallel, samples end effector locations to which the contact-free stage can
move the robot, then considers the cost predicted by contact-rich MPC local to
each sampled location. The result is a globally-informed, contact-implicit
controller capable of real-time dexterous manipulation. We demonstrate our
controller on precise, non-prehensile manipulation of non-convex objects using
a Franka Panda arm. Project page: https://approximating-global-ci-mpc.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>S.V. and B.B. contributed equally to this work. Project page:
  https://approximating-global-ci-mpc.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OPA-Pack: Object-Property-Aware Robotic Bin Packing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Hui Pan, Yeok Tatt Cheah, Zhengzhe Liu, Ka-Hei Hui, Xiaojie Gao, Pheng-Ann Heng, Yun-Hui Liu, Chi-Wing Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic bin packing aids in a wide range of real-world scenarios such as
e-commerce and warehouses. Yet, existing works focus mainly on considering the
shape of objects to optimize packing compactness and neglect object properties
such as fragility, edibility, and chemistry that humans typically consider when
packing objects. This paper presents OPA-Pack (Object-Property-Aware Packing
framework), the first framework that equips the robot with object property
considerations in planning the object packing. Technical-wise, we develop a
novel object property recognition scheme with retrieval-augmented generation
and chain-of-thought reasoning, and build a dataset with object property
annotations for 1,032 everyday objects. Also, we formulate OPA-Net, aiming to
jointly separate incompatible object pairs and reduce pressure on fragile
objects, while compacting the packing. Further, OPA-Net consists of a property
embedding layer to encode the property of candidate objects to be packed,
together with a fragility heightmap and an avoidance heightmap to keep track of
the packed objects. Then, we design a reward function and adopt a deep
Q-learning scheme to train OPA-Net. Experimental results manifest that OPA-Pack
greatly improves the accuracy of separating incompatible object pairs (from 52%
to 95%) and largely reduces pressure on fragile objects (by 29.4%), while
maintaining good packing compactness. Besides, we demonstrate the effectiveness
of OPA-Pack on a real packing platform, showcasing its practicality in
real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Robotics (TRO) on Feb. 10, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Importance Sampling in High Dimensions with Low-Rank Mixture
  Proposals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liam A. Kruse, Marc R. Schlichting, Mykel J. Kochenderfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Importance sampling is a Monte Carlo technique for efficiently estimating the
likelihood of rare events by biasing the sampling distribution towards the rare
event of interest. By drawing weighted samples from a learned proposal
distribution, importance sampling allows for more sample-efficient estimation
of rare events or tails of distributions. A common choice of proposal density
is a Gaussian mixture model (GMM). However, estimating full-rank GMM covariance
matrices in high dimensions is a challenging task due to numerical
instabilities. In this work, we propose using mixtures of probabilistic
principal component analyzers (MPPCA) as the parametric proposal density for
importance sampling methods. MPPCA models are a type of low-rank mixture model
that can be fit quickly using expectation-maximization, even in
high-dimensional spaces. We validate our method on three simulated systems,
demonstrating consistent gains in sample efficiency and quality of failure
distribution characterization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CoDIT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Voting-Based Task Assignment in Modular Construction Scenarios <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Weiner, Raj Korpan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modular construction, involving off-site prefabrication and on-site assembly,
offers significant advantages but presents complex coordination challenges for
robotic automation. Effective task allocation is critical for leveraging
multi-agent systems (MAS) in these structured environments. This paper
introduces the Hybrid Voting-Based Task Assignment (HVBTA) framework, a novel
approach to optimizing collaboration between heterogeneous multi-agent
construction teams. Inspired by human reasoning in task delegation, HVBTA
uniquely integrates multiple voting mechanisms with the capabilities of a Large
Language Model (LLM) for nuanced suitability assessment between agent
capabilities and task requirements. The framework operates by assigning
Capability Profiles to agents and detailed requirement lists called Task
Descriptions to construction tasks, subsequently generating a quantitative
Suitability Matrix. Six distinct voting methods, augmented by a pre-trained
LLM, analyze this matrix to robustly identify the optimal agent for each task.
Conflict-Based Search (CBS) is integrated for decentralized, collision-free
path planning, ensuring efficient and safe spatio-temporal coordination of the
robotic team during assembly operations. HVBTA enables efficient, conflict-free
assignment and coordination, facilitating potentially faster and more accurate
modular assembly. Current work is evaluating HVBTA's performance across various
simulated construction scenarios involving diverse robotic platforms and task
complexities. While designed as a generalizable framework for any domain with
clearly definable tasks and capabilities, HVBTA will be particularly effective
for addressing the demanding coordination requirements of multi-agent
collaborative robotics in modular construction due to the predetermined
construction planning involved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Block by Block workshop at ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Policy Contrastive Decoding for Robotic Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihan Wu, Ji Zhang, Xu Luo, Junlin Xie, Jingkuan Song, Heng Tao Shen, Lianli Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic foundation models, or generalist robot policies, hold immense
potential to enable flexible, general-purpose and dexterous robotic systems.
Despite their advancements, our empirical experiments reveal that existing
robot policies are prone to learning spurious correlations from pre-training
trajectories, adversely affecting their generalization capabilities beyond the
training data. To tackle this, we propose a novel Policy Contrastive Decoding
(PCD) approach, which redirects the robot policy's focus toward object-relevant
visual clues by contrasting action probability distributions derived from
original and object-masked visual inputs. As a training-free method, our PCD
can be used as a plugin to improve different types of robot policies without
needing to finetune or access model weights. We conduct extensive experiments
on top of three open-source robot policies, including the autoregressive policy
OpenVLA and the diffusion-based policies Octo and $\pi_0$. The obtained results
in both simulation and real-world environments prove PCD's flexibility and
effectiveness, e.g., PCD enhances the state-of-the-art policy $\pi_0$ by 8% in
the simulation environment and by 108% in the real-world environment. Code and
demos are publicly available at: https://Koorye.github.io/proj/PCD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Composing Dextrous Grasping and In-hand Manipulation via Scoring with a
  Reinforcement Learning Critic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Röstel, Dominik Winkelbauer, Johannes Pitz, Leon Sievers, Berthold Bäuml
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-hand manipulation and grasping are fundamental yet often separately
addressed tasks in robotics. For deriving in-hand manipulation policies,
reinforcement learning has recently shown great success. However, the derived
controllers are not yet useful in real-world scenarios because they often
require a human operator to place the objects in suitable initial (grasping)
states. Finding stable grasps that also promote the desired in-hand
manipulation goal is an open problem. In this work, we propose a method for
bridging this gap by leveraging the critic network of a reinforcement learning
agent trained for in-hand manipulation to score and select initial grasps. Our
experiments show that this method significantly increases the success rate of
in-hand manipulation without requiring additional training. We also present an
implementation of a full grasp manipulation pipeline on a real-world system,
enabling autonomous grasping and reorientation even of unwieldy objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Active Sampling for Hardness Classification with
  Vision-Based Tactile Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Chen, Alap Kshirsagar, Frederik Heller, Mario Gómez Andreu, Boris Belousov, Tim Schneider, Lisa P. Y. Lin, Katja Doerschner, Knut Drewing, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most important object properties that humans and robots perceive
through touch is hardness. This paper investigates information-theoretic active
sampling strategies for sample-efficient hardness classification with
vision-based tactile sensors. We evaluate three probabilistic classifier models
and two model-uncertainty-based sampling strategies on a robotic setup as well
as on a previously published dataset of samples collected by human testers. Our
findings indicate that the active sampling approaches, driven by uncertainty
metrics, surpass a random sampling baseline in terms of accuracy and stability.
Additionally, while in our human study, the participants achieve an average
accuracy of 48.00%, our best approach achieves an average accuracy of 88.78% on
the same set of objects, demonstrating the effectiveness of vision-based
tactile sensors for object hardness classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Robotic Friction Learning via Symbolic Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Scholl, Alexander Dietrich, Sebastian Wolf, Jinoh Lee, Alin-Albu Schäffer, Gitta Kutyniok, Maged Iskandar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately modeling the friction torque in robotic joints has long been
challenging due to the request for a robust mathematical description.
Traditional model-based approaches are often labor-intensive, requiring
extensive experiments and expert knowledge, and they are difficult to adapt to
new scenarios and dependencies. On the other hand, data-driven methods based on
neural networks are easier to implement but often lack robustness,
interpretability, and trustworthiness--key considerations for robotic hardware
and safety-critical applications such as human-robot interaction. To address
the limitations of both approaches, we propose the use of symbolic regression
(SR) to estimate the friction torque. SR generates interpretable symbolic
formulas similar to those produced by model-based methods while being flexible
to accommodate various dynamic effects and dependencies. In this work, we apply
SR algorithms to approximate the friction torque using collected data from a
KUKA LWR-IV+ robot. Our results show that SR not only yields formulas with
comparable complexity to model-based approaches but also achieves higher
accuracy. Moreover, SR-derived formulas can be seamlessly extended to include
load dependencies and other dynamic factors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Distance-aware Transition Augmentation for Offline Model-based
  Reinforcement Learning <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongsu Lee, Minhae Kwon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of offline reinforcement learning (RL) is to extract a
high-performance policy from the fixed datasets, minimizing performance
degradation due to out-of-distribution (OOD) samples. Offline model-based RL
(MBRL) is a promising approach that ameliorates OOD issues by enriching
state-action transitions with augmentations synthesized via a learned dynamics
model. Unfortunately, seminal offline MBRL methods often struggle in
sparse-reward, long-horizon tasks. In this work, we introduce a novel MBRL
framework, dubbed Temporal Distance-Aware Transition Augmentation (TempDATA),
that generates augmented transitions in a temporally structured latent space
rather than in raw state space. To model long-horizon behavior, TempDATA learns
a latent abstraction that captures a temporal distance from both trajectory and
transition levels of state space. Our experiments confirm that TempDATA
outperforms previous offline MBRL methods and achieves matching or surpassing
the performance of diffusion-based trajectory augmentation and goal-conditioned
RL on the D4RL AntMaze, FrankaKitchen, CALVIN, and pixel-based FrankaKitchen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 ICML</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constraint-Aware Diffusion Guidance for Robotics: Real-Time Obstacle
  Avoidance for Autonomous Racing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Ma, Sabrina Bodmer, Andrea Carron, Melanie Zeilinger, Michael Muehlebach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models hold great potential in robotics due to their ability to
capture complex, high-dimensional data distributions. However, their lack of
constraint-awareness limits their deployment in safety-critical applications.
We propose Constraint-Aware Diffusion Guidance (CoDiG), a data-efficient and
general-purpose framework that integrates barrier functions into the denoising
process, guiding diffusion sampling toward constraint-satisfying outputs. CoDiG
enables constraint satisfaction even with limited training data and generalizes
across tasks. We evaluate our framework in the challenging setting of miniature
autonomous racing, where real-time obstacle avoidance is essential. Real-world
experiments show that CoDiG generates safe outputs efficiently under dynamic
conditions, highlighting its potential for broader robotic applications. A
demonstration video is available at https://youtu.be/KNYsTdtdxOU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When do Lyapunov Subcenter Manifolds become Eigenmanifolds? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannik P. Wotte, Arne Sachtler, Alin Albu-Schäffer, Stefano Stramigioli, Cosimo Della Santina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-body mechanical systems have rich internal dynamics, which can be
exploited to formulate efficient control targets. For periodic regulation tasks
in robotics applications, this motivated the extension of the theory on
nonlinear normal modes to Riemannian manifolds, and led to the definition of
Eigenmanifolds. This definition is geometric, which is advantageous for
generality within robotics but also obscures the connection of Eigenmanifolds
to a large body of results from the literature on nonlinear dynamics. We bridge
this gap, showing that Eigenmanifolds are instances of Lyapunov subcenter
manifolds (LSMs), and that their stronger geometric properties with respect to
LSMs follow from a time-symmetry of conservative mechanical systems. This
directly leads to local existence and uniqueness results for Eigenmanifolds.
Furthermore, we show that an additional spatial symmetry provides
Eigenmanifolds with yet stronger properties of Rosenberg manifolds, which can
be favorable for control applications, and we present a sufficient condition
for their existence and uniqueness. These theoretical results are numerically
confirmed on two mechanical systems with a non-constant inertia tensor: a
double pendulum and a 5-link pendulum.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 24 figures, submitted to Automatica</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Coordiante Frames for Task Specific Motion Retargeting in
  Teleoperation using Shared Control and VR Controllers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Grobbel, Daniel Flögel, Philipp Rigoll, Sören Hohmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task performance in terms of task completion time in teleoperation is still
far behind compared to humans conducting tasks directly. One large identified
impact on this is the human capability to perform transformations and
alignments, which is directly influenced by the point of view and the motion
retargeting strategy. In modern teleoperation systems, motion retargeting is
usually implemented through a one time calibration or switching modes. Complex
tasks, like concatenated screwing, might be difficult, because the operator has
to align (e.g. mirror) rotational and translational input commands. Recent
research has shown, that the separation of translation and rotation leads to
increased task performance. This work proposes a formal motion retargeting
method, which separates translational and rotational input commands. This
method is then included in a optimal control based trajectory planner and shown
to work on a UR5e manipulator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Granular Loco-Manipulation: Repositioning Rocks Through Strategic Sand
  Avalanche 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodi Hu, Yue Wu, Feifei Qian, Daniel Seita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legged robots have the potential to leverage obstacles to climb steep sand
slopes. However, efficiently repositioning these obstacles to desired locations
is challenging. Here we present DiffusiveGRAIN, a learning-based method that
enables a multi-legged robot to strategically induce localized sand avalanches
during locomotion and indirectly manipulate obstacles. We conducted 375 trials,
systematically varying obstacle spacing, robot orientation, and leg actions in
75 of them. Results show that the movement of closely-spaced obstacles exhibits
significant interference, requiring joint modeling. In addition, different
multi-leg excavation actions could cause distinct robot state changes,
necessitating integrated planning of manipulation and locomotion. To address
these challenges, DiffusiveGRAIN includes a diffusion-based environment
predictor to capture multi-obstacle movements under granular flow interferences
and a robot state predictor to estimate changes in robot state from multi-leg
action patterns. Deployment experiments (90 trials) demonstrate that by
integrating the environment and robot state predictors, the robot can
autonomously plan its movements based on loco-manipulation goals, successfully
shifting closely located rocks to desired locations in over 65% of trials. Our
study showcases the potential for a locomoting robot to strategically
manipulate obstacles to achieve improved mobility on challenging terrains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AGI-Elo: How Far Are We From Mastering A Task? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Sun, Yimin Zhao, Christina Dao Wen Lee, Jiawei Sun, Chengran Yuan, Zefan Huang, Dongen Li, Justin KW Yeoh, Alok Prakash, Thomas W. Malone, Marcelo H. Ang Jr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the field progresses toward Artificial General Intelligence (AGI), there
is a pressing need for more comprehensive and insightful evaluation frameworks
that go beyond aggregate performance metrics. This paper introduces a unified
rating system that jointly models the difficulty of individual test cases and
the competency of AI models (or humans) across vision, language, and action
domains. Unlike existing metrics that focus solely on models, our approach
allows for fine-grained, difficulty-aware evaluations through competitive
interactions between models and tasks, capturing both the long-tail
distribution of real-world challenges and the competency gap between current
models and full task mastery. We validate the generalizability and robustness
of our system through extensive experiments on multiple established datasets
and models across distinct AGI domains. The resulting rating distributions
offer novel perspectives and interpretable insights into task difficulty, model
progression, and the outstanding challenges that remain on the path to
achieving full AGI task mastery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Collision Risk from Naturalistic Driving with Generalised
  Surrogate Safety Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiru Jiao, Simeon C. Calvert, Sander van Cranenburgh, Hans van Lint
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and timely alerts for drivers or automated systems to unfolding
collisions remains a challenge in road safety, particularly in highly
interactive urban traffic. Existing approaches require labour-intensive
annotation of sparse risk, struggle to consider varying interaction context, or
are useful only in the scenarios they are designed for. To address these
limits, this study introduces the generalised surrogate safety measure (GSSM),
a new approach that learns exclusively from naturalistic driving without crash
or risk labels. GSSM captures the patterns of normal driving and estimates the
extent to which a traffic interaction deviates from the norm towards unsafe
extreme. Utilising neural networks, normal interactions are characterised by
context-conditioned distributions of multi-directional spacing between road
users. In the same interaction context, a spacing closer than normal entails
higher risk of potential collision. Then a context-adaptive risk score and its
associated probability can be calculated based on the theory of extreme values.
Any measurable factors, such as motion kinematics, weather, lighting, can serve
as part of the context, allowing for diverse coverage of safety-critical
interactions. Multiple public driving datasets are used to train GSSMs, which
are tested with 4,875 real-world crashes and near-crashes reconstructed from
the SHRP2 NDS. A vanilla GSSM using only instantaneous states achieves AUPRC of
0.9 and secures a median time advance of 2.6 seconds to prevent potential
collisions. Additional data and contextual factors provide further performance
gains. Across various interaction types such as rear-end, merging, and
crossing, the accuracy and timeliness of GSSM consistently outperforms existing
baselines. GSSM therefore establishes a scalable, context-aware, and
generalisable foundation to proactively quantify collision risk in traffic
interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOON: Multi-Objective Optimization-Driven Object-Goal Navigation Using a
  Variable-Horizon Set-Orienteering Planner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daigo Nakajima, Kanji Tanaka, Daiki Iwata, Kouki Terashima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-goal navigation (ON) enables autonomous robots to locate and reach
user-specified objects in previously unknown environments, offering promising
applications in domains such as assistive care and disaster response. Existing
ON methods -- including training-free approaches, reinforcement learning, and
zero-shot planners -- generally depend on active exploration to identify
landmark objects (e.g., kitchens or desks), followed by navigation toward
semantically related targets (e.g., a specific mug). However, these methods
often lack strategic planning and do not adequately address trade-offs among
multiple objectives. To overcome these challenges, we propose a novel framework
that formulates ON as a multi-objective optimization problem (MOO), balancing
frontier-based knowledge exploration with knowledge exploitation over
previously observed landmarks; we call this framework MOON (MOO-driven ON). We
implement a prototype MOON system that integrates three key components: (1)
building on QOM [IROS05], a classical ON system that compactly and
discriminatively encodes landmarks based on their semantic relevance to the
target; (2) integrating StructNav [RSS23], a recently proposed training-free
planner, to enhance the navigation pipeline; and (3) introducing a
variable-horizon set orienteering problem formulation to enable global
optimization over both exploration and exploitation strategies. This work
represents an important first step toward developing globally optimized,
next-generation object-goal navigation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous
  Teleoperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hangyu Li, Qin Zhao, Haoran Xu, Xinyu Jiang, Qingwei Ben, Feiyu Jia, Haoyu Zhao, Liang Xu, Jia Zeng, Hanqing Wang, Bo Dai, Junting Dong, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teleoperation is a cornerstone of embodied-robot learning, and bimanual
dexterous teleoperation in particular provides rich demonstrations that are
difficult to obtain with fully autonomous systems. While recent studies have
proposed diverse hardware pipelines-ranging from inertial motion-capture gloves
to exoskeletons and vision-based interfaces-there is still no unified benchmark
that enables fair, reproducible comparison of these systems. In this paper, we
introduce TeleOpBench, a simulator-centric benchmark tailored to bimanual
dexterous teleoperation. TeleOpBench contains 30 high-fidelity task
environments that span pick-and-place, tool use, and collaborative
manipulation, covering a broad spectrum of kinematic and force-interaction
difficulty. Within this benchmark we implement four representative
teleoperation modalities-(i) MoCap, (ii) VR device, (iii) arm-hand
exoskeletons, and (iv) monocular vision tracking-and evaluate them with a
common protocol and metric suite. To validate that performance in simulation is
predictive of real-world behavior, we conduct mirrored experiments on a
physical dual-arm platform equipped with two 6-DoF dexterous hands. Across 10
held-out tasks we observe a strong correlation between simulator and hardware
performance, confirming the external validity of TeleOpBench. TeleOpBench
establishes a common yardstick for teleoperation research and provides an
extensible platform for future algorithmic and hardware innovation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamGen: Unlocking Generalization in Robot Learning through Neural
  Trajectories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, Loic Magne, Ajay Mandlekar, Avnish Narayan, You Liang Tan, Guanzhi Wang, Jing Wang, Qi Wang, Yinzhen Xu, Xiaohui Zeng, Kaiyuan Zheng, Ruijie Zheng, Ming-Yu Liu, Luke Zettlemoyer, Dieter Fox, Jan Kautz, Scott Reed, Yuke Zhu, Linxi Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce DreamGen, a simple yet highly effective 4-stage pipeline for
training robot policies that generalize across behaviors and environments
through neural trajectories - synthetic robot data generated from video world
models. DreamGen leverages state-of-the-art image-to-video generative models,
adapting them to the target robot embodiment to produce photorealistic
synthetic videos of familiar or novel tasks in diverse environments. Since
these models generate only videos, we recover pseudo-action sequences using
either a latent action model or an inverse-dynamics model (IDM). Despite its
simplicity, DreamGen unlocks strong behavior and environment generalization: a
humanoid robot can perform 22 new behaviors in both seen and unseen
environments, while requiring teleoperation data from only a single
pick-and-place task in one environment. To evaluate the pipeline
systematically, we introduce DreamGen Bench, a video generation benchmark that
shows a strong correlation between benchmark performance and downstream policy
success. Our work establishes a promising new axis for scaling robot learning
well beyond manual data collection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See website for videos:
  https://research.nvidia.com/labs/gear/dreamgen</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TD-GRPC: Temporal Difference Learning with Group Relative Policy
  Constraint for Humanoid Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khang Nguyen, Khai Nguyen, An T. Le, Jan Peters, Manfred Huber, Ngo Anh Vien, Minh Nhat Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot learning in high-dimensional control settings, such as humanoid
locomotion, presents persistent challenges for reinforcement learning (RL)
algorithms due to unstable dynamics, complex contact interactions, and
sensitivity to distributional shifts during training. Model-based methods,
\textit{e.g.}, Temporal-Difference Model Predictive Control (TD-MPC), have
demonstrated promising results by combining short-horizon planning with
value-based learning, enabling efficient solutions for basic locomotion tasks.
However, these approaches remain ineffective in addressing policy mismatch and
instability introduced by off-policy updates. Thus, in this work, we introduce
Temporal-Difference Group Relative Policy Constraint (TD-GRPC), an extension of
the TD-MPC framework that unifies Group Relative Policy Optimization (GRPO)
with explicit Policy Constraints (PC). TD-GRPC applies a trust-region
constraint in the latent policy space to maintain consistency between the
planning priors and learned rollouts, while leveraging group-relative ranking
to assess and preserve the physical feasibility of candidate trajectories.
Unlike prior methods, TD-GRPC achieves robust motions without modifying the
underlying planner, enabling flexible planning and policy learning. We validate
our method across a locomotion task suite ranging from basic walking to highly
dynamic movements on the 26-DoF Unitree H1-2 humanoid robot. Through simulation
results, TD-GRPC demonstrates its improvements in stability and policy
robustness with sampling efficiency while training for complex humanoid control
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dribble Master: Learning Agile Humanoid Dribbling Through Legged
  Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoheng Wang, Jinyin Zhou, Qi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanoid soccer dribbling is a highly challenging task that demands dexterous
ball manipulation while maintaining dynamic balance. Traditional rule-based
methods often struggle to achieve accurate ball control due to their reliance
on fixed walking patterns and limited adaptability to real-time ball dynamics.
To address these challenges, we propose a two-stage curriculum learning
framework that enables a humanoid robot to acquire dribbling skills without
explicit dynamics or predefined trajectories. In the first stage, the robot
learns basic locomotion skills; in the second stage, we fine-tune the policy
for agile dribbling maneuvers. We further introduce a virtual camera model in
simulation and design heuristic rewards to encourage active sensing, promoting
a broader visual range for continuous ball perception. The policy is trained in
simulation and successfully transferred to a physical humanoid robot.
Experimental results demonstrate that our method enables effective ball
manipulation, achieving flexible and visually appealing dribbling behaviors
across multiple environments. This work highlights the potential of
reinforcement learning in developing agile humanoid soccer robots. Additional
details, video demonstrations, and code are available at
https://zhuoheng0910.github.io/dribble-master/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-Visual Contact Classification for Tree Structures in Agriculture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Spears, Moonyoung Lee, George Kantor, Oliver Kroemer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contact-rich manipulation tasks in agriculture, such as pruning and
harvesting, require robots to physically interact with tree structures to
maneuver through cluttered foliage. Identifying whether the robot is contacting
rigid or soft materials is critical for the downstream manipulation policy to
be safe, yet vision alone is often insufficient due to occlusion and limited
viewpoints in this unstructured environment. To address this, we propose a
multi-modal classification framework that fuses vibrotactile (audio) and visual
inputs to identify the contact class: leaf, twig, trunk, or ambient. Our key
insight is that contact-induced vibrations carry material-specific signals,
making audio effective for detecting contact events and distinguishing material
types, while visual features add complementary semantic cues that support more
fine-grained classification. We collect training data using a hand-held sensor
probe and demonstrate zero-shot generalization to a robot-mounted probe
embodiment, achieving an F1 score of 0.82. These results underscore the
potential of audio-visual learning for manipulation in unstructured,
contact-rich environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Twins in the Cloud: A Modular, Scalable and Interoperable
  Framework for Accelerating Verification and Validation of Autonomous Driving
  Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanmay Vilas Samak, Chinmay Vilas Samak, Giovanni Martino, Pranav Nair, Venkat Krovi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verification and validation (V&V) of autonomous vehicles (AVs) typically
requires exhaustive testing across a variety of operating environments and
driving scenarios including rare, extreme, or hazardous situations that might
be difficult or impossible to capture in reality. Additionally, physical V&V
methods such as track-based evaluations or public-road testing are often
constrained by time, cost, and safety, which motivates the need for virtual
proving grounds. However, the fidelity and scalability of simulation-based V&V
methods can quickly turn into a bottleneck. In such a milieu, this work
proposes a virtual proving ground that flexibly scales digital twins within
high-performance computing clusters (HPCCs) and automates the V&V process.
Here, digital twins enable high-fidelity virtual representation of the AV and
its operating environments, allowing extensive scenario-based testing.
Meanwhile, HPCC infrastructure brings substantial advantages in terms of
computational power and scalability, enabling rapid iterations of simulations,
processing and storage of massive amounts of data, and deployment of
large-scale test campaigns, thereby reducing the time and cost associated with
the V&V process. We demonstrate the efficacy of this approach through a case
study that focuses on the variability analysis of a candidate autonomy
algorithm to identify potential vulnerabilities in its perception, planning,
and control sub-systems. The modularity, scalability, and interoperability of
the proposed framework are demonstrated by deploying a test campaign comprising
256 test cases on two different HPCC architectures to ensure continuous
operation in a publicly shared resource setting. The findings highlight the
ability of the proposed framework to accelerate and streamline the V&V process,
thereby significantly compressing (~30x) the timeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ASME International Design Engineering Technical
  Conferences & Computers and Information in Engineering Conference (IDETC-CIE)
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Robot of Theseus: A modular robotic testbed for legged locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karthik Urs, Jessica Carlson, Aditya Srinivas Manohar, Michael Rakowiecki, Abdulhadi Alkayyali, John E. Saunders, Faris Tulbah, Talia Y. Moore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic models are useful for independently varying specific features, but
most quadrupedal robots differ so greatly from animal morphologies that they
have minimal biomechanical relevance. Commercially available quadrupedal robots
are also prohibitively expensive for biological research programs and difficult
to customize. Here, we present a low-cost quadrupedal robot with modular legs
that can match a wide range of animal morphologies for biomechanical hypothesis
testing. The Robot Of Theseus (TROT) costs approximately $4000 to build out of
3D printed parts and standard off-the-shelf supplies. Each limb consists of 2
or 3 rigid links; the proximal joint can be rotated to become a knee or elbow.
Telescoping mechanisms vary the length of each limb link. The open-source
software accommodates user-defined gaits and morphology changes. Effective leg
length, or crouch, is determined by the four-bar linkage actuating each joint.
The backdrivable motors can vary virtual spring stiffness and range of motion.
Full descriptions of the TROT hardware and software are freely available
online. We demonstrate the use of TROT to compare locomotion among extant,
extinct, and theoretical morphologies. In addition to biomechanical hypothesis
testing, we envision a variety of different applications for this low-cost,
modular, legged robotic platform, including developing novel control
strategies, clearing land mines, or remote exploration. All CAD and code is
available for download on the TROT project page.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafeMove-RL: A Certifiable Reinforcement Learning Framework for Dynamic
  Motion Constraints in Trajectory Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengfei Liu, Haoyang Zhong, Jiazheng Hu, Tan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a dynamic safety margin-based reinforcement learning
framework for local motion planning in dynamic and uncertain environments. The
proposed planner integrates real-time trajectory optimization with adaptive gap
analysis, enabling effective feasibility assessment under partial observability
constraints. To address safety-critical computations in unknown scenarios, an
enhanced online learning mechanism is introduced, which dynamically corrects
spatial trajectories by forming dynamic safety margins while maintaining
control invariance. Extensive evaluations, including ablation studies and
comparisons with state-of-the-art algorithms, demonstrate superior success
rates and computational efficiency. The framework's effectiveness is further
validated on both simulated and physical robotic platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EndoForce: Development of an Intuitive Axial Force Measurement Device
  for Endoscopic Robotic Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansoul Kim, Dong-Ho Lee, Dukyoo Kong, Dong-Soo Kwon, Byungsik Cheon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic endoscopic systems provide intuitive control and eliminate radiation
exposure, making them a promising alternative to conventional methods. However,
the lack of axial force measurement from the robot remains a major challenge,
as it can lead to excessive colonic elongation, perforation, or ureteral
complications. Although various methods have been proposed in previous studies,
limitations such as model dependency, bulkiness, and environmental sensitivity
remain challenges that should be addressed before clinical application. In this
study, we propose EndoForce, a device designed for intuitive and accurate axial
force measurement in endoscopic robotic systems. Inspired by the insertion
motion performed by medical doctors during ureteroscopy and gastrointestinal
(GI) endoscopy, EndoForce ensures precise force measuring while maintaining
compatibility with clinical environments. The device features a streamlined
design, allowing for the easy attachment and detachment of a sterile cover, and
incorporates a commercial load cell to enhance cost-effectiveness and
facilitate practical implementation in real medical applications. To validate
the effectiveness of the proposed EndoForce, physical experiments were
performed using a testbed that simulates the ureter. We show that the axial
force generated during insertion was measured with high accuracy, regardless of
whether the pathway was straight or curved, in a testbed simulating the human
ureter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Survey</span> on Physical Risk Control in the Era of Foundation
  Model-enabled Robotics <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takeshi Kojima, Yaonan Zhu, Yusuke Iwasawa, Toshinori Kitamura, Gang Yan, Shu Morikuni, Ryosuke Takanami, Alfredo Solano, Tatsuya Matsushima, Akiko Murakami, Yutaka Matsuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Foundation Model-enabled robotics (FMRs) display greatly improved
general-purpose skills, enabling more adaptable automation than conventional
robotics. Their ability to handle diverse tasks thus creates new opportunities
to replace human labor. However, unlike general foundation models, FMRs
interact with the physical world, where their actions directly affect the
safety of humans and surrounding objects, requiring careful deployment and
control. Based on this proposition, our survey comprehensively summarizes robot
control approaches to mitigate physical risks by covering all the lifespan of
FMRs ranging from pre-deployment to post-accident stage. Specifically, we
broadly divide the timeline into the following three phases: (1) pre-deployment
phase, (2) pre-incident phase, and (3) post-incident phase. Throughout this
survey, we find that there is much room to study (i) pre-incident risk
mitigation strategies, (ii) research that assumes physical interaction with
humans, and (iii) essential issues of foundation models themselves. We hope
that this survey will be a milestone in providing a high-resolution analysis of
the physical risks of FMRs and their control, contributing to the realization
of a good human-robot relationship.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCAI 2025 Survey Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building reliable sim driving agents by scaling self-play 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14706v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14706v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daphne Cornelisse, Aarav Pandya, Kevin Joseph, Joseph Suárez, Eugene Vinitsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation agents are essential for designing and testing systems that
interact with humans, such as autonomous vehicles (AVs). These agents serve
various purposes, from benchmarking AV performance to stress-testing system
limits, but all applications share one key requirement: reliability. To enable
sound experimentation, a simulation agent must behave as intended. It should
minimize actions that may lead to undesired outcomes, such as collisions, which
can distort the signal-to-noise ratio in analyses. As a foundation for reliable
sim agents, we propose scaling self-play to thousands of scenarios on the Waymo
Open Motion Dataset under semi-realistic limits on human perception and
control. Training from scratch on a single GPU, our agents solve almost the
full training set within a day. They generalize to unseen test scenes,
achieving a 99.8% goal completion rate with less than 0.8% combined collision
and off-road incidents across 10,000 held-out scenarios. Beyond in-distribution
generalization, our agents show partial robustness to out-of-distribution
scenes and can be fine-tuned in minutes to reach near-perfect performance in
such cases. We open-source the pre-trained agents and integrate them with a
batched multi-agent simulator. Demonstrations of agent behaviors can be viewed
at https://sites.google.com/view/reliable-sim-agents, and we open-source our
agents at https://github.com/Emerge-Lab/gpudrive.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Words to Collisions: LLM-Guided Evaluation and Adversarial
  Generation of Safety-Critical Driving Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gao, Mattia Piccinini, Korbinian Moller, Johannes Betz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the safety of autonomous vehicles requires virtual scenario-based
testing, which depends on the robust evaluation and generation of
safety-critical scenarios. So far, researchers have used scenario-based testing
frameworks that rely heavily on handcrafted scenarios as safety metrics. To
reduce the effort of human interpretation and overcome the limited scalability
of these approaches, we combine Large Language Models (LLMs) with structured
scenario parsing and prompt engineering to automatically evaluate and generate
safety-critical driving scenarios. We introduce Cartesian and Ego-centric
prompt strategies for scenario evaluation, and an adversarial generation module
that modifies trajectories of risk-inducing vehicles (ego-attackers) to create
critical scenarios. We validate our approach using a 2D simulation framework
and multiple pre-trained LLMs. The results show that the evaluation module
effectively detects collision scenarios and infers scenario safety. Meanwhile,
the new generation module identifies high-risk agents and synthesizes
realistic, safety-critical scenarios. We conclude that an LLM equipped with
domain-informed prompting techniques can effectively evaluate and generate
safety-critical driving scenarios, reducing dependence on handcrafted metrics.
We release our open-source code and scenarios at:
https://github.com/TUM-AVS/From-Words-to-Collisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Probabilistic Collision Detection for Motion Planning Under
  Sensing Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoli Wang, Sipu Ruan, Xin Meng, Gregory Chirikjian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic collision detection (PCD) is essential in motion planning for
robots operating in unstructured environments, where considering sensing
uncertainty helps prevent damage. Existing PCD methods mainly used simplified
geometric models and addressed only position estimation errors. This paper
presents an enhanced PCD method with two key advancements: (a) using
superquadrics for more accurate shape approximation and (b) accounting for both
position and orientation estimation errors to improve robustness under sensing
uncertainty. Our method first computes an enlarged surface for each object that
encapsulates its observed rotated copies, thereby addressing the orientation
estimation errors. Then, the collision probability under the position
estimation errors is formulated as a chance-constraint problem that is solved
with a tight upper bound. Both the two steps leverage the recently developed
normal parameterization of superquadric surfaces. Results show that our PCD
method is twice as close to the Monte-Carlo sampled baseline as the best
existing PCD method and reduces path length by 30% and planning time by 37%,
respectively. A Real2Sim2Real pipeline further validates the importance of
considering orientation estimation errors, showing that the collision
probability of executing the planned path in simulation is only 2%, compared to
9% and 29% when considering only position estimation errors or none at all.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Long-Context Diffusion Policies via Past-Token Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Torne, Andy Tang, Yuejiang Liu, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning over long sequences of observations and actions is essential for
many robotic tasks. Yet, learning effective long-context policies from
demonstrations remains challenging. As context length increases, training
becomes increasingly expensive due to rising memory demands, and policy
performance often degrades as a result of spurious correlations. Recent methods
typically sidestep these issues by truncating context length, discarding
historical information that may be critical for subsequent decisions. In this
paper, we propose an alternative approach that explicitly regularizes the
retention of past information. We first revisit the copycat problem in
imitation learning and identify an opposite challenge in recent diffusion
policies: rather than over-relying on prior actions, they often fail to capture
essential dependencies between past and future actions. To address this, we
introduce Past-Token Prediction (PTP), an auxiliary task in which the policy
learns to predict past action tokens alongside future ones. This regularization
significantly improves temporal modeling in the policy head, with minimal
reliance on visual representations. Building on this observation, we further
introduce a multistage training strategy: pre-train the visual encoder with
short contexts, and fine-tune the policy head using cached long-context
embeddings. This strategy preserves the benefits of PTP while greatly reducing
memory and computational overhead. Finally, we extend PTP into a
self-verification mechanism at test time, enabling the policy to score and
select candidates consistent with past actions during inference. Experiments
across four real-world and six simulated tasks demonstrate that our proposed
method improves the performance of long-context diffusion policies by 3x and
accelerates policy training by more than 10x.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Videos are available at https://long-context-dp.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TCAFF: Temporal Consistency for Robot Frame Alignment <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05210v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05210v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mason B. Peterson, Parker C. Lusk, Antonio Avila, Jonathan P. How
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of collaborative robotics, the ability to communicate spatial
information like planned trajectories and shared environment information is
crucial. When no global position information is available (e.g., indoor or
GPS-denied environments), agents must align their coordinate frames before
shared spatial information can be properly expressed and interpreted.
Coordinate frame alignment is particularly difficult when robots have no
initial alignment and are affected by odometry drift. To this end, we develop a
novel multiple hypothesis algorithm, called TCAFF, for aligning the coordinate
frames of neighboring robots. TCAFF considers potential alignments from
associating sparse open-set object maps and leverages temporal consistency to
determine an initial alignment and correct for drift, all without any initial
knowledge of neighboring robot poses. We demonstrate TCAFF being used for frame
alignment in a collaborative object tracking application on a team of four
robots tracking six pedestrians and show that TCAFF enables robots to achieve a
tracking accuracy similar to that of a system with ground truth localization.
The code and hardware dataset are available at
https://github.com/mit-acl/tcaff.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, accepted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Proximity and Visuotactile Point Cloud Fusion for Contact Patches in
  Extreme Deformation <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica Yin, Paarth Shah, Naveen Kuppuswamy, Andrew Beaulieu, Avinash Uttamchandani, Alejandro Castro, James Pikul, Russ Tedrake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visuotactile sensors are a popular tactile sensing strategy due to
high-fidelity estimates of local object geometry. However, existing algorithms
for processing raw sensor inputs to useful intermediate signals such as contact
patches struggle in high-deformation regimes. This is due to physical
constraints imposed by sensor hardware and small-deformation assumptions used
by mechanics-based models. In this work, we propose a fusion algorithm for
proximity and visuotactile point clouds for contact patch segmentation,
entirely independent from membrane mechanics. This algorithm exploits the
synchronous, high spatial resolution proximity and visuotactile modalities
enabled by an extremely deformable, selectively transmissive soft membrane,
which uses visible light for visuotactile sensing and infrared light for
proximity depth. We evaluate our contact patch algorithm in low (10%), medium
(60%), and high (100%+) strain states. We compare our method against three
baselines: proximity-only, tactile-only, and a first principles mechanics
model. Our approach outperforms all baselines with an average RMSE under 2.8 mm
of the contact patch geometry across all strain ranges. We demonstrate our
contact patch algorithm in four applications: varied stiffness membranes,
torque and shear-induced wrinkling, closed loop control, and pose estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning In-Hand Translation Using Tactile Skin With Shear and Normal
  Force Sensing <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica Yin, Haozhi Qi, Jitendra Malik, James Pikul, Mark Yim, Tess Hellebrekers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in reinforcement learning (RL) and tactile sensing has
significantly advanced dexterous manipulation. However, these methods often
utilize simplified tactile signals due to the gap between tactile simulation
and the real world. We introduce a sensor model for tactile skin that enables
zero-shot sim-to-real transfer of ternary shear and binary normal forces. Using
this model, we develop an RL policy that leverages sliding contact for
dexterous in-hand translation. We conduct extensive real-world experiments to
assess how tactile sensing facilitates policy adaptation to various unseen
object properties and robot hand orientations. We demonstrate that our 3-axis
tactile policies consistently outperform baselines that use only shear forces,
only normal forces, or only proprioception. Website:
https://jessicayin.github.io/tactile-skin-rl/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://jessicayin.github.io/tactile-skin-rl/. Accepted to
  ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.15558v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.15558v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         NVIDIA,  :, Alisson Azzolini, Junjie Bai, Hannah Brandon, Jiaxin Cao, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Liang Feng, Francesco Ferroni, Rama Govindaraju, Jinwei Gu, Siddharth Gururani, Imad El Hanafi, Zekun Hao, Jacob Huffman, Jingyi Jin, Brendan Johnson, Rizwan Khan, George Kurian, Elena Lantz, Nayeon Lee, Zhaoshuo Li, Xuan Li, Maosheng Liao, Tsung-Yi Lin, Yen-Chen Lin, Ming-Yu Liu, Xiangyu Lu, Alice Luo, Andrew Mathau, Yun Ni, Lindsey Pavao, Wei Ping, David W. Romero, Misha Smelyanskiy, Shuran Song, Lyne Tchapmi, Andrew Z. Wang, Boxin Wang, Haoxiang Wang, Fangyin Wei, Jiashu Xu, Yao Xu, Dinghao Yang, Xiaodong Yang, Zhuolin Yang, Jingxu Zhang, Xiaohui Zeng, Zhe Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physical AI systems need to perceive, understand, and perform complex actions
in the physical world. In this paper, we present the Cosmos-Reason1 models that
can understand the physical world and generate appropriate embodied decisions
(e.g., next step action) in natural language through long chain-of-thought
reasoning processes. We begin by defining key capabilities for Physical AI
reasoning, with a focus on physical common sense and embodied reasoning. To
represent physical common sense, we use a hierarchical ontology that captures
fundamental knowledge about space, time, and physics. For embodied reasoning,
we rely on a two-dimensional ontology that generalizes across different
physical embodiments. Building on these capabilities, we develop two multimodal
large language models, Cosmos-Reason1-7B and Cosmos-Reason1-56B. We curate data
and train our models in two stages: Physical AI supervised fine-tuning (SFT)
and Physical AI reinforcement learning (RL). To evaluate our models, we build
comprehensive benchmarks for physical common sense and embodied reasoning
according to our ontologies. Evaluation results show that Physical AI SFT and
RL bring significant improvements. To facilitate the development of Physical
AI, we make our code and pre-trained models available under the NVIDIA Open
Model License at https://github.com/nvidia-cosmos/cosmos-reason1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REI-Bench: Can Embodied Agents Understand Vague Human Instructions in
  Task Planning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Jiang, Chuhao Zhou, Jianfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot task planning decomposes human instructions into executable action
sequences that enable robots to complete a series of complex tasks. Although
recent large language model (LLM)-based task planners achieve amazing
performance, they assume that human instructions are clear and straightforward.
However, real-world users are not experts, and their instructions to robots
often contain significant vagueness. Linguists suggest that such vagueness
frequently arises from referring expressions (REs), whose meanings depend
heavily on dialogue context and environment. This vagueness is even more
prevalent among the elderly and children, who robots should serve more. This
paper studies how such vagueness in REs within human instructions affects
LLM-based robot task planning and how to overcome this issue. To this end, we
propose the first robot task planning benchmark with vague REs (REI-Bench),
where we discover that the vagueness of REs can severely degrade robot planning
performance, leading to success rate drops of up to 77.9%. We also observe that
most failure cases stem from missing objects in planners. To mitigate the REs
issue, we propose a simple yet effective approach: task-oriented context
cognition, which generates clear instructions for robots, achieving
state-of-the-art performance compared to aware prompt and chains of thought.
This work contributes to the research community of human-robot interaction
(HRI) by making robot task planning more practical, particularly for non-expert
users, e.g., the elderly and children.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Certifying Stability of Reinforcement Learning Policies using
  Generalized Lyapunov Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10947v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10947v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kehan Long, Jorge Cortés, Nikolay Atanasov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of certifying the stability of closed-loop systems under
control policies derived from optimal control or reinforcement learning (RL).
Classical Lyapunov methods require a strict step-wise decrease in the Lyapunov
function but such a certificate is difficult to construct for a learned control
policy. The value function associated with an RL policy is a natural Lyapunov
function candidate but it is not clear how it should be modified. To gain
intuition, we first study the linear quadratic regulator (LQR) problem and make
two key observations. First, a Lyapunov function can be obtained from the value
function of an LQR policy by augmenting it with a residual term related to the
system dynamics and stage cost. Second, the classical Lyapunov decrease
requirement can be relaxed to a generalized Lyapunov condition requiring only
decrease on average over multiple time steps. Using this intuition, we consider
the nonlinear setting and formulate an approach to learn generalized Lyapunov
functions by augmenting RL value functions with neural network residual terms.
Our approach successfully certifies the stability of RL policies trained on
Gymnasium and DeepMind Control benchmarks. We also extend our method to jointly
train neural controllers and stability certificates using a multi-step Lyapunov
loss, resulting in larger certified inner approximations of the region of
attraction compared to the classical Lyapunov approach. Overall, our
formulation enables stability certification for a broad class of systems with
learned policies by making certificates easier to construct, thereby bridging
classical control theory and modern learning-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rapid and Inexpensive Inertia Tensor Estimation from a Single Object
  Throw 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13137v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13137v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Till M. Blaha, Mike M. Kuijper, Radu Pop, Ewoud J. J. Smeur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inertia tensor is an important parameter in many engineering fields, but
measuring it can be cumbersome and involve multiple experiments or accurate and
expensive equipment. We propose a method to measure the moment of inertia
tensor of a rigid body from a single spinning throw, by attaching a small and
inexpensive stand-alone measurement device consisting of a gyroscope,
accelerometer and a reaction wheel. The method includes a compensation for the
increase of moment of inertia due to adding the measurement device to the body,
and additionally obtains the location of the centre of gravity of the body as
an intermediate result. Experiments performed with known rigid bodies show that
the mean accuracy is around 2%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2025 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying Context Bias in Domain Adaptation for Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14679v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14679v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hojun Son, Asma Almutairi, Arpan Kusari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptation for object detection (DAOD) seeks to transfer a trained
model from a source to a target domain. Various DAOD methods exist, some of
which aim to minimize context bias between foreground-background associations
in various domains. However, no prior work has studied context bias in DAOD by
analyzing changes in background features during adaptation and how context bias
is represented in different domains. Our research experiment highlights the
potential usability of context bias in DAOD. We address the problem by varying
activation values over different layers of two different trained models,
Detectron2 and YOLOv11, and by masking the background, both of which impact the
number and quality of detections. We use two synthetic datasets, CARLA and
Virtual KITTI, and two different versions of real open-source data, Cityscapes
and KITTI semantic, as separate domains to represent and quantify context bias.
We utilize different metrics such as Maximum Mean Discrepancy (MMD) and Maximum
Variance Discrepancy (MVD) to find the layer-specific conditional probability
estimates of foreground given manipulated background regions for separate
domains. We further analyze foreground-background associations across various
dataset combinations. We find that state-of-the-art domain adaptation methods
exhibit some form of context bias and apply a potentially simple way to
alleviate the context bias achieving improved accuracy (from 51.189 to 53.646
mAP on Cityscapes foggy validation with 63.207 mAP and 64.233 mAP on Cityscapes
validation respectively). We demonstrate through detailed analysis that
understanding of the context bias can affect DAOD approach and focusing solely
on aligning foreground features is insufficient for effective DAOD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlightBench: Benchmarking Learning-based Methods for Ego-vision-based
  Quadrotors Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05687v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05687v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu-Ang Yu, Chao Yu, Feng Gao, Yi Wu, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ego-vision-based navigation in cluttered environments is crucial for mobile
systems, particularly agile quadrotors. While learning-based methods have shown
promise recently, head-to-head comparisons with cutting-edge optimization-based
approaches are scarce, leaving open the question of where and to what extent
they truly excel. In this paper, we introduce FlightBench, the first
comprehensive benchmark that implements various learning-based methods for
ego-vision-based navigation and evaluates them against mainstream
optimization-based baselines using a broad set of performance metrics. More
importantly, we develop a suite of criteria to assess scenario difficulty and
design test cases that span different levels of difficulty based on these
criteria. Our results show that while learning-based methods excel in
high-speed flight and faster inference, they struggle with challenging
scenarios like sharp corners or view occlusion. Analytical experiments validate
the correlation between our difficulty criteria and flight performance.
Moreover, we verify the trend in flight performance within real-world
environments through full-pipeline and hardware-in-the-loop experiments. We
hope this benchmark and these criteria will drive future advancements in
learning-based navigation for ego-vision quadrotors. Code and documentation are
available at https://github.com/thu-uav/FlightBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first three authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantization-Free Autoregressive Action <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14259v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14259v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyad Sheebaelhamd, Michael Tschannen, Michael Muehlebach, Claire Vernade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current transformer-based imitation learning approaches introduce discrete
action representations and train an autoregressive transformer decoder on the
resulting latent code. However, the initial quantization breaks the continuous
structure of the action space thereby limiting the capabilities of the
generative model. We propose a quantization-free method instead that leverages
Generative Infinite-Vocabulary Transformers (GIVT) as a direct, continuous
policy parametrization for autoregressive transformers. This simplifies the
imitation learning pipeline while achieving state-of-the-art performance on a
variety of popular simulated robotics tasks. We enhance our policy roll-outs by
carefully studying sampling algorithms, further improving the results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Infinigen-Sim: Procedural Generation of Articulated Simulation Assets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Joshi, Beining Han, Jack Nugent, Yiming Zuo, Jonathan Liu, Hongyu Wen, Stamatis Alexandropoulos, Tao Sun, Alexander Raistrick, Gaowen Liu, Yi Shao, Jia Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Infinigen-Sim, a toolkit which enables users to create diverse
and realistic articulated object procedural generators. These tools are
composed of high-level utilities for use creating articulated assets in
Blender, as well as an export pipeline to integrate the resulting assets into
common robotics simulators. We demonstrate our system by creating procedural
generators for 5 common articulated object categories. Experiments show that
assets sampled from these generators are useful for movable object
segmentation, training generalizable reinforcement learning policies, and
sim-to-real transfer of imitation learning policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FACET: Force-Adaptive Control via Impedance Reference Tracking for
  Legged Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Botian Xu, Haoyang Weng, Qingzhou Lu, Yang Gao, Huazhe Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has made significant strides in legged robot
control, enabling locomotion across diverse terrains and complex
loco-manipulation capabilities. However, the commonly used position or velocity
tracking-based objectives are agnostic to forces experienced by the robot,
leading to stiff and potentially dangerous behaviors and poor control during
forceful interactions. To address this limitation, we present
\emph{Force-Adaptive Control via Impedance Reference Tracking} (FACET).
Inspired by impedance control, we use RL to train a control policy to imitate a
virtual mass-spring-damper system, allowing fine-grained control under external
forces by manipulating the virtual spring. In simulation, we demonstrate that
our quadruped robot achieves improved robustness to large impulses (up to 200
Ns) and exhibits controllable compliance, achieving an 80% reduction in
collision impulse. The policy is deployed to a physical robot to showcase both
compliance and the ability to engage with large forces by kinesthetic control
and pulling payloads up to 2/3 of its weight. Further extension to a legged
loco-manipulator and a humanoid shows the applicability of our method to more
complex settings to enable whole-body compliance control. Project Website:
https://facet.pages.dev/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Multi-robot Active SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Farhan Ahmed, Matteo Maragliano, Vincent Frémont, Carmine Tommaso Recchiuto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous exploration in unknown environments remains a fundamental
challenge in robotics, particularly for applications such as search and rescue,
industrial inspection, and planetary exploration. Multi-robot active SLAM
presents a promising solution by enabling collaborative mapping and exploration
while actively reducing uncertainty. However, existing approaches often suffer
from high computational costs and inefficient frontier management, making them
computationally expensive for real-time applications. In this paper, we
introduce an efficient multi-robot active SLAM framework that incorporates a
frontier-sharing strategy to enhance robot distribution in unexplored
environments. Our approach integrates a utility function that considers both
pose graph uncertainty and path entropy, achieving an optimal balance between
exploration coverage and computational efficiency. By filtering and
prioritizing goal frontiers, our method significantly reduces computational
overhead while preserving high mapping accuracy. The proposed framework has
been implemented in ROS and validated through simulations and real-world
experiments. Results demonstrate superior exploration performance and mapping
quality compared to state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Offboard Occupancy Refinement with Hybrid Propagation for Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08504v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08504v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Shi, Song Wang, Jiaming Zhang, Xiaoting Yin, Guangming Wang, Jianke Zhu, Kailun Yang, Kaiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based occupancy prediction, also known as 3D Semantic Scene Completion
(SSC), presents a significant challenge in computer vision. Previous methods,
confined to onboard processing, struggle with simultaneous geometric and
semantic estimation, continuity across varying viewpoints, and single-view
occlusion. Our paper introduces OccFiner, a novel offboard framework designed
to enhance the accuracy of vision-based occupancy predictions. OccFiner
operates in two hybrid phases: 1) a multi-to-multi local propagation network
that implicitly aligns and processes multiple local frames for correcting
onboard model errors and consistently enhancing occupancy accuracy across all
distances. 2) the region-centric global propagation, focuses on refining labels
using explicit multi-view geometry and integrating sensor bias, particularly
for increasing the accuracy of distant occupied voxels. Extensive experiments
demonstrate that OccFiner improves both geometric and semantic accuracy across
various types of coarse occupancy, setting a new state-of-the-art performance
on the SemanticKITTI dataset. Notably, OccFiner significantly boosts the
performance of vision-based SSC models, achieving accuracy levels competitive
with established LiDAR-based onboard SSC methods. Furthermore, OccFiner is the
first to achieve automatic annotation of SSC in a purely vision-based approach.
Quantitative experiments prove that OccFiner successfully facilitates occupancy
data loop-closure in autonomous driving. Additionally, we quantitatively and
qualitatively validate the superiority of the offboard approach on city-level
SSC static maps. The source code will be made publicly available at
https://github.com/MasterHow/OccFiner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Intelligent Transportation Systems
  (T-ITS). The source code will be made publicly available at
  https://github.com/MasterHow/OccFiner</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DexGarmentLab: Dexterous Garment Manipulation Environment with
  Generalizable Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11032v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11032v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuran Wang, Ruihai Wu, Yue Chen, Jiarui Wang, Jiaqi Liang, Ziyu Zhu, Haoran Geng, Jitendra Malik, Pieter Abbeel, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Garment manipulation is a critical challenge due to the diversity in garment
categories, geometries, and deformations. Despite this, humans can effortlessly
handle garments, thanks to the dexterity of our hands. However, existing
research in the field has struggled to replicate this level of dexterity,
primarily hindered by the lack of realistic simulations of dexterous garment
manipulation. Therefore, we propose DexGarmentLab, the first environment
specifically designed for dexterous (especially bimanual) garment manipulation,
which features large-scale high-quality 3D assets for 15 task scenarios, and
refines simulation techniques tailored for garment modeling to reduce the
sim-to-real gap. Previous data collection typically relies on teleoperation or
training expert reinforcement learning (RL) policies, which are labor-intensive
and inefficient. In this paper, we leverage garment structural correspondence
to automatically generate a dataset with diverse trajectories using only a
single expert demonstration, significantly reducing manual intervention.
However, even extensive demonstrations cannot cover the infinite states of
garments, which necessitates the exploration of new algorithms. To improve
generalization across diverse garment shapes and deformations, we propose a
Hierarchical gArment-manipuLation pOlicy (HALO). It first identifies
transferable affordance points to accurately locate the manipulation area, then
generates generalizable trajectories to complete the task. Through extensive
experiments and detailed analysis of our method and baseline, we demonstrate
that HALO consistently outperforms existing methods, successfully generalizing
to previously unseen instances even with significant variations in shape and
deformation where others fail. Our project page is available at:
https://wayrise.github.io/DexGarmentLab/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Verification of Embodied Reasoning for Generative Skill
  Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Yue, Shuqi Guo, Kaiyu Hu, Chujiao Wang, Benyou Wang, Kui Jia, Guiliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative skill acquisition enables embodied agents to actively learn a
scalable and evolving repertoire of control skills, crucial for the advancement
of large decision models. While prior approaches often rely on supervision
signals from generalist agents (e.g., LLMs), their effectiveness in complex 3D
environments remains unclear; exhaustive evaluation incurs substantial
computational costs, significantly hindering the efficiency of skill learning.
Inspired by recent successes in verification models for mathematical reasoning,
we propose VERGSA (Verifying Embodied Reasoning in Generative Skill
Acquisition), a framework that systematically integrates real-time verification
principles into embodied skill learning. VERGSA establishes 1) a seamless
extension from verification of mathematical reasoning into embodied learning by
dynamically incorporating contextually relevant tasks into prompts and defining
success metrics for both subtasks and overall tasks, and 2) an automated,
scalable reward labeling scheme that synthesizes dense reward signals by
iteratively finalizing the contribution of scene configuration and subtask
learning to overall skill acquisition. To the best of our knowledge, this
approach constitutes the first comprehensive training dataset for
verification-driven generative skill acquisition, eliminating arduous manual
reward engineering. Experiments validate the efficacy of our approach: 1) the
exemplar task pool improves the average task success rates by 21%, 2) our
verification model boosts success rates by 24% for novel tasks and 36% for
encountered tasks, and 3) outperforms LLM-as-a-Judge baselines in verification
quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Loop closure grasping: Topological transformations enable strong,
  gentle, and versatile grasps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10552v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10552v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kentaro Barhydt, O. Godson Osele, Sreela Kodali, Cosima du Pasquier, Chase M. Hartquist, H. Harry Asada, Allison M. Okamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grasping mechanisms must both create and subsequently hold grasps that permit
safe and effective object manipulation. Existing mechanisms address the
different functional requirements of grasp creation and grasp holding using a
single morphology, but have yet to achieve the simultaneous strength,
gentleness, and versatility needed for many applications. We present "loop
closure grasping", a class of robotic grasping that addresses these different
functional requirements through topological transformations between open-loop
and closed-loop morphologies. We formalize these morphologies for grasping,
formulate the loop closure grasping method, and present principles and a design
architecture that we implement using soft growing inflated beams, winches, and
clamps. The mechanisms' initial open-loop topology enables versatile grasp
creation via unencumbered tip movement, and closing the loop enables strong and
gentle holding with effectively infinite bending compliance. Loop closure
grasping circumvents the tradeoffs of single-morphology designs, enabling
grasps involving historically challenging objects, environments, and
configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpatialVLA: Exploring Spatial Representations for Visual-Language-Action
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15830v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15830v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we claim that spatial understanding is the keypoint in robot
manipulation, and propose SpatialVLA to explore effective spatial
representations for the robot foundation model. Specifically, we introduce
Ego3D Position Encoding to inject 3D information into the input observations of
the visual-language-action model, and propose Adaptive Action Grids to
represent spatial robot movement actions with adaptive discretized action
grids, facilitating learning generalizable and transferrable spatial action
knowledge for cross-robot control. SpatialVLA is first pre-trained on top of a
vision-language model with 1.1 Million real-world robot episodes, to learn a
generalist manipulation policy across multiple robot environments and tasks.
After pre-training, SpatialVLA is directly applied to perform numerous tasks in
a zero-shot manner. The superior results in both simulation and real-world
robots demonstrate its advantage of inferring complex robot motion trajectories
and its strong in-domain multi-task generalization ability. We further show the
proposed Adaptive Action Grids offer a new and effective way to fine-tune the
pre-trained SpatialVLA model for new simulation and real-world setups, where
the pre-learned action grids are re-discretized to capture robot-specific
spatial action movements of new setups. The superior results from extensive
evaluations demonstrate the exceptional in-distribution generalization and
out-of-distribution adaptation capability, highlighting the crucial benefit of
the proposed spatial-aware representations for generalist robot policy
learning. All the details and codes will be open-sourced.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">23</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Based Compact Reranking with Document Features for Scientific
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runchu Tian, Xueqiang Xu, Bowen Jin, SeongKu Kang, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific retrieval is essential for advancing academic discovery. Within
this process, document reranking plays a critical role by refining first-stage
retrieval results. However, large language model (LLM) listwise reranking faces
unique challenges in the scientific domain. First-stage retrieval is often
suboptimal in the scientific domain, so relevant documents are ranked lower.
Moreover, conventional listwise reranking uses the full text of candidate
documents in the context window, limiting the number of candidates that can be
considered. As a result, many relevant documents are excluded before reranking,
which constrains overall retrieval performance. To address these challenges, we
explore compact document representations based on semantic features such as
categories, sections, and keywords, and propose a training-free, model-agnostic
reranking framework for scientific retrieval called CoRank. The framework
involves three stages: (i) offline extraction of document-level features, (ii)
coarse reranking using these compact representations, and (iii) fine-grained
reranking on full texts of the top candidates from stage (ii). This hybrid
design provides a high-level abstraction of document semantics, expands
candidate coverage, and retains critical details required for precise ranking.
Experiments on LitSearch and CSFCube show that CoRank significantly improves
reranking performance across different LLM backbones, increasing nDCG@10 from
32.0 to 39.7. Overall, these results highlight the value of information
extraction for reranking in scientific retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GMM-Based Comprehensive Feature Extraction and Relative Distance
  Preservation For Few-Shot Cross-Modal Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengsong Sun, Weiping Li, Xiang Li, Yuankun Liu, Lianlei Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot cross-modal retrieval focuses on learning cross-modal
representations with limited training samples, enabling the model to handle
unseen classes during inference. Unlike traditional cross-modal retrieval
tasks, which assume that both training and testing data share the same class
distribution, few-shot retrieval involves data with sparse representations
across modalities. Existing methods often fail to adequately model the
multi-peak distribution of few-shot cross-modal data, resulting in two main
biases in the latent semantic space: intra-modal bias, where sparse samples
fail to capture intra-class diversity, and inter-modal bias, where
misalignments between image and text distributions exacerbate the semantic gap.
These biases hinder retrieval accuracy. To address these issues, we propose a
novel method, GCRDP, for few-shot cross-modal retrieval. This approach
effectively captures the complex multi-peak distribution of data using a
Gaussian Mixture Model (GMM) and incorporates a multi-positive sample
contrastive learning mechanism for comprehensive feature modeling.
Additionally, we introduce a new strategy for cross-modal semantic alignment,
which constrains the relative distances between image and text feature
distributions, thereby improving the accuracy of cross-modal representations.
We validate our approach through extensive experiments on four benchmark
datasets, demonstrating superior performance over six state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAR: Setting Knowledge Tripwires for Retrieval Augmented Rejection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommaso Mario Buonocore, Enea Parimbelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content moderation for large language models (LLMs) remains a significant
challenge, requiring flexible and adaptable solutions that can quickly respond
to emerging threats. This paper introduces Retrieval Augmented Rejection (RAR),
a novel approach that leverages a retrieval-augmented generation (RAG)
architecture to dynamically reject unsafe user queries without model
retraining. By strategically inserting and marking malicious documents into the
vector database, the system can identify and reject harmful requests when these
documents are retrieved. Our preliminary results show that RAR achieves
comparable performance to embedded moderation in LLMs like Claude 3.5 Sonnet,
while offering superior flexibility and real-time customization capabilities, a
fundamental feature to timely address critical vulnerabilities. This approach
introduces no architectural changes to existing RAG systems, requiring only the
addition of specially crafted documents and a simple rejection mechanism based
on retrieval results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Retrieval Augmented Generation for Object Constraint Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Chenhao Li, Vahid Zolfaghari, Nenad Petrovic, Fengjunjie Pan, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Object Constraint Language (OCL) is essential for defining precise
constraints within Model-Based Systems Engineering (MBSE). However, manually
writing OCL rules is complex and time-consuming. This study explores the
optimization of Retrieval-Augmented Generation (RAG) for automating OCL rule
generation, focusing on the impact of different retrieval strategies. We
evaluate three retrieval approaches $\unicode{x2013}$ BM25 (lexical-based),
BERT-based (semantic retrieval), and SPLADE (sparse-vector retrieval)
$\unicode{x2013}$ analyzing their effectiveness in providing relevant context
for a large language model.
  To further assess our approach, we compare and benchmark our
retrieval-optimized generation results against PathOCL, a state-of-the-art
graph-based method. We directly compare BM25, BERT, and SPLADE retrieval
methods with PathOCL to understand how different retrieval methods perform for
a unified evaluation framework. Our experimental results, focusing on
retrieval-augmented generation, indicate that while retrieval can enhance
generation accuracy, its effectiveness depends on the retrieval method and the
number of retrieved chunks (k). BM25 underperforms the baseline, whereas
semantic approaches (BERT and SPLADE) achieve better results, with SPLADE
performing best at lower k values. However, excessive retrieval with high k
parameter can lead to retrieving irrelevant chunks which degrades model
performance. Our findings highlight the importance of optimizing retrieval
configurations to balance context relevance and output consistency. This
research provides insights into improving OCL rule generation using RAG and
underscores the need for tailoring retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Q${}^2$Forge: Minting Competency Questions and SPARQL Queries for
  Question-Answering Over Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yousouf Taghzouti, Franck Michel, Tao Jiang, Louis-Félix Nothias, Fabien Gandon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The SPARQL query language is the standard method to access knowledge graphs
(KGs). However, formulating SPARQL queries is a significant challenge for
non-expert users, and remains time-consuming for the experienced ones. Best
practices recommend to document KGs with competency questions and example
queries to contextualise the knowledge they contain and illustrate their
potential applications. In practice, however, this is either not the case or
the examples are provided in limited numbers. Large Language Models (LLMs) are
being used in conversational agents and are proving to be an attractive
solution with a wide range of applications, from simple question-answering
about common knowledge to generating code in a targeted programming language.
However, training and testing these models to produce high quality SPARQL
queries from natural language questions requires substantial datasets of
question-query pairs. In this paper, we present Q${}^2$Forge that addresses the
challenge of generating new competency questions for a KG and corresponding
SPARQL queries. It iteratively validates those queries with human feedback and
LLM as a judge. Q${}^2$Forge is open source, generic, extensible and modular,
meaning that the different modules of the application (CQ generation, query
generation and query refinement) can be used separately, as an integrated
pipeline, or replaced by alternative services. The result is a complete
pipeline from competency question formulation to query evaluation, supporting
the creation of reference query sets for any target KG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CPRet: A <span class="highlight-title">Dataset</span>, Benchmark, and Model for Retrieval in Competitive
  Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Deng, Yuan Meng, Shixiang Tang, Wanli Ouyang, Xinzhu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Competitive programming benchmarks are widely used in scenarios such as
programming contests and large language model assessments. However, the growing
presence of duplicate or highly similar problems raises concerns not only about
competition fairness, but also about the validity of competitive programming as
a benchmark for model evaluation. In this paper, we propose a new problem --
similar question retrieval -- to address this issue. Due to the lack of both
data and models, solving this problem is challenging. To this end, we introduce
CPRet, a retrieval-oriented benchmark suite for competitive programming,
covering four retrieval tasks: two code-centric (i.e., Text-to-Code and
Code-to-Code) and two newly proposed problem-centric tasks (i.e.,
Problem-to-Duplicate and Simplified-to-Full), built from a combination of
automatically crawled problem-solution data and manually curated annotations.
Our contribution includes both high-quality training data and temporally
separated test sets for reliable evaluation. In addition, we develop two
task-specialized retrievers based on this dataset: CPRetriever-Code, trained
with a novel Group-InfoNCE loss for problem-code alignment, and
CPRetriever-Prob, fine-tuned for identifying problem-level similarity. Both
models achieve strong results and are open-sourced for local use. Finally, we
analyze LiveCodeBench and find that high-similarity problems inflate model pass
rates and reduce differentiation, underscoring the need for similarity-aware
evaluation in future benchmarks.
  Code and data are available at: https://github.com/coldchair/CPRet
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMAQA: A Metadata-based QA <span class="highlight-title">Dataset</span> for RAG Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Bruni, Marco Avvenuti, Nicola Tonellotto, Maurizio Tesconi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) systems are widely used in
question-answering (QA) tasks, but current benchmarks lack metadata
integration, hindering evaluation in scenarios requiring both textual data and
external information. To address this, we present AMAQA, a new open-access QA
dataset designed to evaluate tasks combining text and metadata. The integration
of metadata is especially important in fields that require rapid analysis of
large volumes of data, such as cybersecurity and intelligence, where timely
access to relevant information is critical. AMAQA includes about 1.1 million
English messages collected from 26 public Telegram groups, enriched with
metadata such as timestamps, topics, emotional tones, and toxicity indicators,
which enable precise and contextualized queries by filtering documents based on
specific criteria. It also includes 450 high-quality QA pairs, making it a
valuable resource for advancing research on metadata-driven QA and RAG systems.
To the best of our knowledge, AMAQA is the first single-hop QA benchmark to
incorporate metadata and labels such as topics covered in the messages. We
conduct extensive tests on the benchmark, establishing a new standard for
future research. We show that leveraging metadata boosts accuracy from 0.12 to
0.61, highlighting the value of structured context. Building on this, we
explore several strategies to refine the LLM input by iterating over provided
context and enriching it with noisy documents, achieving a further 3-point gain
over the best baseline and a 14-point improvement over simple metadata
filtering. The dataset is available at
https://anonymous.4open.science/r/AMAQA-5D0D/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlearning for Federated Online Learning to Rank: A Reproducibility
  Study <span class="chip">SIGIR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiling Tao, Shuyi Wang, Jiaxi Yang, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reports on findings from a comparative study on the effectiveness
and efficiency of federated unlearning strategies within Federated Online
Learning to Rank (FOLTR), with specific attention to systematically analysing
the unlearning capabilities of methods in a verifiable manner.
  Federated approaches to ranking of search results have recently garnered
attention to address users privacy concerns. In FOLTR, privacy is safeguarded
by collaboratively training ranking models across decentralized data sources,
preserving individual user data while optimizing search results based on
implicit feedback, such as clicks.
  Recent legislation introduced across numerous countries is establishing the
so called "the right to be forgotten", according to which services based on
machine learning models like those in FOLTR should provide capabilities that
allow users to remove their own data from those used to train models. This has
sparked the development of unlearning methods, along with evaluation practices
to measure whether unlearning of a user data successfully occurred. Current
evaluation practices are however often controversial, necessitating the use of
multiple metrics for a more comprehensive assessment -- but previous proposals
of unlearning methods only used single evaluation metrics.
  This paper addresses this limitation: our study rigorously assesses the
effectiveness of unlearning strategies in managing both under-unlearning and
over-unlearning scenarios using adapted, and newly proposed evaluation metrics.
Thanks to our detailed analysis, we uncover the strengths and limitations of
five unlearning strategies, offering valuable insights into optimizing
federated unlearning to balance data privacy and system performance within
FOLTR. We publicly release our code and complete results at
https://github.com/Iris1026/Unlearning-for-FOLTR.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large
  Multimodal-Models Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Zhang, Xingyu Chen, Xiaofeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) have become a pivotal research focus in deep
learning, demonstrating remarkable capabilities in 3D scene understanding.
However, current 3D LMMs employing thousands of spatial tokens for multimodal
reasoning suffer from critical inefficiencies: excessive computational overhead
and redundant information flows. Unlike 2D VLMs processing single images, 3D
LMMs exhibit inherent architectural redundancy due to the heterogeneous
mechanisms between spatial tokens and visual tokens. To address this challenge,
we propose AdaToken-3D, an adaptive spatial token optimization framework that
dynamically prunes redundant tokens through spatial contribution analysis. Our
method automatically tailors pruning strategies to different 3D LMM
architectures by quantifying token-level information flows via attention
pattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM)
demonstrate that AdaToken-3D achieves 21\% faster inference speed and 63\%
FLOPs reduction while maintaining original task accuracy. Beyond efficiency
gains, this work systematically investigates redundancy patterns in multimodal
spatial information flows through quantitative token interaction analysis. Our
findings reveal that over 60\% of spatial tokens contribute minimally ($<$5\%)
to the final predictions, establishing theoretical foundations for efficient 3D
multimodal learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JIR-Arena: The First Benchmark <span class="highlight-title">Dataset</span> for Just-in-time Information
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Yang, Kevin Ros, Shankar Kumar Senthil Kumar, ChengXiang Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Just-in-time Information Recommendation (JIR) is a service designed to
deliver the most relevant information precisely when users need it, ,
addressing their knowledge gaps with minimal effort and boosting
decision-making and efficiency in daily life. Advances in device-efficient
deployment of foundation models and the growing use of intelligent wearable
devices have made always-on JIR assistants feasible. However, there has been no
systematic effort to formally define JIR tasks or establish evaluation
frameworks. To bridge this gap, we present the first mathematical definition of
JIR tasks and associated evaluation metrics. Additionally, we introduce
JIR-Arena, a multimodal benchmark dataset featuring diverse,
information-request-intensive scenarios to evaluate JIR systems across critical
dimensions: i) accurately inferring user information needs, ii) delivering
timely and relevant recommendations, and iii) avoiding irrelevant content that
may distract users.
  Developing a JIR benchmark dataset poses challenges due to subjectivity in
estimating user information needs and uncontrollable system variables affecting
reproducibility. To address these, JIR-Arena: i) combines input from multiple
humans and large AI models to approximate information need distributions; ii)
assesses JIR quality through information retrieval outcomes using static
knowledge base snapshots; and iii) employs a multi-turn, multi-entity
validation framework to improve objectivity and generality. Furthermore, we
implement a baseline JIR system capable of processing real-time information
streams aligned with user inputs. Our evaluation of this baseline system on
JIR-Arena indicates that while foundation model-based JIR systems simulate user
needs with reasonable precision, they face challenges in recall and effective
content retrieval. To support future research in this new area, we fully
release our code and data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards A Generalist Code Embedding Model Based On Massive Data
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaofan Li, Jianlyu Chen, Yingxia Shao, Defu Lian, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code embedding models attract increasing attention due to the widespread
popularity of retrieval-augmented generation (RAG) in software development.
These models are expected to capture the rich semantic relationships inherent
to code, which differ significantly from those found in text. However, existing
models remain severely limited due to the scarcity of high-quality training
data. In this work, we introduce \textbf{CodeR} (\underline{Code}
\underline{R}etrieval), a state-of-the-art embedding model for general-purpose
code retrieval. The superior performance of CodeR is built upon CodeR-Pile, a
large-scale synthetic dataset constructed under the DRU (Diversity,
Reliability, Usability) principle via a novel data synthesis pipeline. To
optimize training effectiveness, we propose Annealing, a curriculum learning
strategy that enables effective knowledge transfer across heterogeneous sources
of data. We evaluate CodeR based on 16 diverse code retrieval tasks, where it
significantly outperforms existing baselines and exhibits strong out-of-domain
generalization performance. We have publicly released our code and the
well-trained model to facilitate further research in this critical area.
https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_Coder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-based Query Expansion Fails for Unfamiliar and Ambiguous Queries <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenya Abe, Kunihiro Takeoka, Makoto P. Kato, Masafumi Oyamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query expansion (QE) enhances retrieval by incorporating relevant terms, with
large language models (LLMs) offering an effective alternative to traditional
rule-based and statistical methods. However, LLM-based QE suffers from a
fundamental limitation: it often fails to generate relevant knowledge,
degrading search performance. Prior studies have focused on hallucination, yet
its underlying cause--LLM knowledge deficiencies--remains underexplored. This
paper systematically examines two failure cases in LLM-based QE: (1) when the
LLM lacks query knowledge, leading to incorrect expansions, and (2) when the
query is ambiguous, causing biased refinements that narrow search coverage. We
conduct controlled experiments across multiple datasets, evaluating the effects
of knowledge and query ambiguity on retrieval performance using sparse and
dense retrieval models. Our results reveal that LLM-based QE can significantly
degrade the retrieval effectiveness when knowledge in the LLM is insufficient
or query ambiguity is high. We introduce a framework for evaluating QE under
these conditions, providing insights into the limitations of LLM-based
retrieval augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2025 short paper track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Know Or Not: a library for evaluating out-of-knowledge base robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica Foo, Pradyumna Shyama Prasad, Shaun Khoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the capabilities of large language models (LLMs) have progressed
significantly, their use in high-stakes applications have been limited due to
risks of hallucination. One key approach in reducing hallucination is
retrieval-augmented generation (RAG), but even in such setups, LLMs may still
hallucinate when presented with questions outside of the knowledge base. Such
behavior is unacceptable in high-stake applications where LLMs are expected to
abstain from answering queries it does not have sufficient context on. In this
work, we present a novel methodology for systematically evaluating
out-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not
know) in the RAG setting, without the need for manual annotation of gold
standard answers. We implement our methodology in knowornot, an open-source
library that enables users to develop their own customized evaluation data and
pipelines for OOKB robustness. knowornot comprises four main features. Firstly,
it provides a unified, high-level API that streamlines the process of setting
up and running robustness benchmarks. Secondly, its modular architecture
emphasizes extensibility and flexibility, allowing users to easily integrate
their own LLM clients and RAG settings. Thirdly, its rigorous data modeling
design ensures experiment reproducibility, reliability and traceability.
Lastly, it implements a comprehensive suite of tools for users to customize
their pipelines. We demonstrate the utility of knowornot by developing a
challenging benchmark, PolicyBench, which spans four Question-Answer (QA)
chatbots on government policies, and analyze its OOKB robustness. The source
code of knowornot is available
https://github.com/govtech-responsibleai/KnowOrNot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Think Before You Attribute: Improving the Performance of LLMs
  Attribution Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Eduardo Batista, Emil Vatai, Mohamed Wahib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly applied in various science
domains, yet their broader adoption remains constrained by a critical
challenge: the lack of trustworthy, verifiable outputs. Current LLMs often
generate answers without reliable source attribution, or worse, with incorrect
attributions, posing a barrier to their use in scientific and high-stakes
settings, where traceability and accountability are non-negotiable. To be
reliable, attribution systems need high accuracy and retrieve data with short
lengths, i.e., attribute to a sentence within a document rather than a whole
document. We propose a sentence-level pre-attribution step for
Retrieve-Augmented Generation (RAG) systems that classify sentences into three
categories: not attributable, attributable to a single quote, and attributable
to multiple quotes. By separating sentences before attribution, a proper
attribution method can be selected for the type of sentence, or the attribution
can be skipped altogether. Our results indicate that classifiers are
well-suited for this task. In this work, we propose a pre-attribution step to
reduce the computational complexity of attribution, provide a clean version of
the HAGRID dataset, and provide an end-to-end attribution system that works out
of the box.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages (9 pages of content, 4 pages of references, 9 pages of
  supplementary material), 7 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Pairwise Learning-To-Rank At Airbnb 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malay Haldar, Daochen Zha, Huiji Gao, Liwei He, Sanjeev Katariya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are three fundamental asks from a ranking algorithm: it should scale to
handle a large number of items, sort items accurately by their utility, and
impose a total order on the items for logical consistency. But here's the
catch-no algorithm can achieve all three at the same time. We call this
limitation the SAT theorem for ranking algorithms. Given the dilemma, how can
we design a practical system that meets user needs? Our current work at Airbnb
provides an answer, with a working solution deployed at scale. We start with
pairwise learning-to-rank (LTR) models-the bedrock of search ranking tech
stacks today. They scale linearly with the number of items ranked and perform
strongly on metrics like NDCG by learning from pairwise comparisons. They are
at a sweet spot of performance vs. cost, making them an ideal choice for
several industrial applications. However, they have a drawback-by ignoring
interactions between items, they compromise on accuracy. To improve accuracy,
we create a "true" pairwise LTR model-one that captures interactions between
items during pairwise comparisons. But accuracy comes at the expense of
scalability and total order, and we discuss strategies to counter these
challenges. For greater accuracy, we take each item in the search result, and
compare it against the rest of the items along two dimensions: (1) Superiority:
How strongly do searchers prefer the given item over the remaining ones? (2)
Similarity: How similar is the given item to all the other items? This forms
the basis of our "all-pairwise" LTR framework, which factors in interactions
across all items at once. Looking at items on the search result page all
together-superiority and similarity combined-gives us a deeper understanding of
what searchers truly want. We quantify the resulting improvements in searcher
experience through offline and online experiments at Airbnb.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supporting Evidence-Based Medicine by Finding Both Relevant and
  Significant Works 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18383v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18383v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sameh Frihat, Norbert Fuhr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a new approach to improving the relevance and
reliability of medical IR, which builds upon the concept of Level of Evidence
(LoE). LoE framework categorizes medical publications into 7 distinct levels
based on the underlying empirical evidence. Despite LoE framework's relevance
in medical research and evidence-based practice, only few medical publications
explicitly state their LoE. Therefore, we develop a classification model for
automatically assigning LoE to medical publications, which successfully
classifies over 26 million documents in MEDLINE database into LoE classes. The
subsequent retrieval experiments on TREC PM datasets show substantial
improvements in retrieval relevance, when LoE is used as a search filter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative Filtering Meets Spectrum Shift: Connecting User-Item
  Interaction with Graph-Structured Side Information <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhang He, Cong Xu, Jun Wang, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have demonstrated their superiority in
collaborative filtering, where the user-item (U-I) interaction bipartite graph
serves as the fundamental data format. However, when graph-structured side
information (e.g., multimodal similarity graphs or social networks) is
integrated into the U-I bipartite graph, existing graph collaborative filtering
methods fall short of achieving satisfactory performance. We quantitatively
analyze this problem from a spectral perspective. Recall that a bipartite graph
possesses a full spectrum within the range of [-1, 1], with the highest
frequency exactly achievable at -1 and the lowest frequency at 1; however, we
observe as more side information is incorporated, the highest frequency of the
augmented adjacency matrix progressively shifts rightward. This spectrum shift
phenomenon has caused previous approaches built for the full spectrum [-1, 1]
to assign mismatched importance to different frequencies. To this end, we
propose Spectrum Shift Correction (dubbed SSC), incorporating shifting and
scaling factors to enable spectral GNNs to adapt to the shifted spectrum.
Unlike previous paradigms of leveraging side information, which necessitate
tailored designs for diverse data types, SSC directly connects traditional
graph collaborative filtering with any graph-structured side information.
Experiments on social and multimodal recommendation demonstrate the
effectiveness of SSC, achieving relative improvements of up to 23% without
incurring any additional computational overhead. Our code is available at
https://github.com/yhhe2004/SSC-KDD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse
  Modalities and Granularities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.20734v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.20734v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Woongyeong Yeo, Kangsan Kim, Soyeong Jeong, Jinheon Baek, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has shown substantial promise in
improving factual accuracy by grounding model responses with external knowledge
relevant to queries. However, most existing RAG approaches are limited to a
text-only corpus, and while recent efforts have extended RAG to other
modalities such as images and videos, they typically operate over a single
modality-specific corpus. In contrast, real-world queries vary widely in the
type of knowledge they require, which a single type of knowledge source cannot
address. To address this, we introduce UniversalRAG, a novel RAG framework
designed to retrieve and integrate knowledge from heterogeneous sources with
diverse modalities and granularities. Specifically, motivated by the
observation that forcing all modalities into a unified representation space
derived from a single aggregated corpus causes a modality gap, where the
retrieval tends to favor items from the same modality as the query, we propose
a modality-aware routing mechanism that dynamically identifies the most
appropriate modality-specific corpus and performs targeted retrieval within it.
Also, beyond modality, we organize each modality into multiple granularity
levels, enabling fine-tuned retrieval tailored to the complexity and scope of
the query. We validate UniversalRAG on 8 benchmarks spanning multiple
modalities, showing its superiority over various modality-specific and unified
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page : https://universalrag.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FaMTEB: Massive Text Embedding Benchmark in Persian Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erfan Zinvandi, Morteza Alikhani, Mehran Sarmadi, Zahra Pourbahman, Sepehr Arvin, Reza Kazemi, Arash Amini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a comprehensive benchmark for Persian (Farsi)
text embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our
benchmark includes 63 datasets spanning seven different tasks: classification,
clustering, pair classification, reranking, retrieval, summary retrieval, and
semantic textual similarity. The datasets are formed as a combination of
existing, translated, and newly generated data, offering a diverse evaluation
framework for Persian language models. Given the increasing use of text
embedding models in chatbots, evaluation datasets are becoming inseparable
ingredients in chatbot challenges and Retrieval-Augmented Generation systems.
As a contribution, we include chatbot evaluation datasets in the MTEB benchmark
for the first time. In addition, in this paper, we introduce the new task of
summary retrieval which is not part of the tasks included in standard MTEB.
Another contribution of this paper is the introduction of a substantial number
of new Persian language NLP datasets suitable for training and evaluation, some
of which have no previous counterparts in Persian. We evaluate the performance
of several Persian and multilingual embedding models in a range of tasks. This
work introduces an open-source benchmark with datasets, code and a public
leaderboard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model as Universal Retriever in Industrial-Scale
  Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03041v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03041v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junguang Jiang, Yanwen Huang, Bin Liu, Xiaoyu Kong, Xinhang Li, Ziru Xu, Han Zhu, Jian Xu, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world recommender systems, different retrieval objectives are
typically addressed using task-specific datasets with carefully designed model
architectures. We demonstrate that Large Language Models (LLMs) can function as
universal retrievers, capable of handling multiple objectives within a
generative retrieval framework. To model complex user-item relationships within
generative retrieval, we propose multi-query representation. To address the
challenge of extremely large candidate sets in industrial recommender systems,
we introduce matrix decomposition to boost model learnability,
discriminability, and transferability, and we incorporate probabilistic
sampling to reduce computation costs. Finally, our Universal Retrieval Model
(URM) can adaptively generate a set from tens of millions of candidates based
on arbitrary given objective while keeping the latency within tens of
milliseconds. Applied to industrial-scale data, URM outperforms expert models
elaborately designed for different retrieval objectives on offline experiments
and significantly improves the core metric of online advertising platform by
$3\%$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Watermarking for Sequential Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sixiao Zhang, Cheng Long, Wei Yuan, Hongxu Chen, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of large foundation models, data has become a crucial component in
building high-performance AI systems. As the demand for high-quality and
large-scale data continues to rise, data copyright protection is attracting
increasing attention. In this work, we explore the problem of data watermarking
for sequential recommender systems, where a watermark is embedded into the
target dataset and can be detected in models trained on that dataset. We focus
on two settings: dataset watermarking, which protects the ownership of the
entire dataset, and user watermarking, which safeguards the data of individual
users. We present a method named Dataset Watermarking for Recommender Systems
(DWRS) to address them. We define the watermark as a sequence of consecutive
items inserted into normal users' interaction sequences. We define a Receptive
Field (RF) to guide the inserting process to facilitate the memorization of the
watermark. Extensive experiments on five representative sequential
recommendation models and three benchmark datasets demonstrate the
effectiveness of DWRS in protecting data copyright while preserving model
utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CXMArena: Unified <span class="highlight-title">Dataset</span> to benchmark performance in realistic CXM
  Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09436v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09436v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Garg, Kapil Sharma, Karan Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) hold immense potential for revolutionizing
Customer Experience Management (CXM), particularly in contact center
operations. However, evaluating their practical utility in complex operational
environments is hindered by data scarcity (due to privacy concerns) and the
limitations of current benchmarks. Existing benchmarks often lack realism,
failing to incorporate deep knowledge base (KB) integration, real-world noise,
or critical operational tasks beyond conversational fluency. To bridge this
gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset
specifically designed for evaluating AI in operational CXM contexts. Given the
diversity in possible contact center features, we have developed a scalable
LLM-powered pipeline that simulates the brand's CXM entities that form the
foundation of our datasets-such as knowledge articles including product
specifications, issue taxonomies, and contact center conversations. The
entities closely represent real-world distribution because of controlled noise
injection (informed by domain experts) and rigorous automated validation.
Building on this, we release CXMArena, which provides dedicated benchmarks
targeting five important operational tasks: Knowledge Base Refinement, Intent
Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with
Integrated Tools. Our baseline experiments underscore the benchmark's
difficulty: even state of the art embedding and generation models achieve only
68% accuracy on article search, while standard embedding methods yield a low F1
score of 0.3 for knowledge base refinement, highlighting significant challenges
for current models necessitating complex pipelines and solutions over
conventional techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Grained Patch Training for Efficient LLM-based Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Liao, Ruobing Xie, Sihang Li, Xiang Wang, Xingwu Sun, Zhanhui Kang, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have emerged as a new paradigm for
recommendation by converting interacted item history into language modeling.
However, constrained by the limited context length of LLMs, existing approaches
have to truncate item history in the prompt, focusing only on recent
interactions and sacrificing the ability to model long-term history. To enable
LLMs to model long histories, we pursue a concise embedding representation for
items and sessions. In the LLM embedding space, we construct an item's
embedding by aggregating its textual token embeddings; similarly, we construct
a session's embedding by aggregating its item embeddings. While efficient, this
way poses two challenges since it ignores the temporal significance of user
interactions and LLMs do not natively interpret our custom embeddings. To
overcome these, we propose PatchRec, a multi-grained patch training method
consisting of two stages: (1) Patch Pre-training, which familiarizes LLMs with
aggregated embeddings -- patches, and (2) Patch Fine-tuning, which enables LLMs
to capture time-aware significance in interaction history. Extensive
experiments show that PatchRec effectively models longer behavior histories
with improved efficiency. This work facilitates the practical use of LLMs for
modeling long behavior histories. Codes are available at
https://github.com/ljy0ustc/PatchRec.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-18T00:00:00Z">2025-05-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">39</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation
  with Bounding-Box Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florent Chiaroni, Ali Ayub, Ola Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In robotics applications, few-shot segmentation is crucial because it allows
robots to perform complex tasks with minimal training data, facilitating their
adaptation to diverse, real-world environments. However, pixel-level
annotations of even small amount of images is highly time-consuming and costly.
In this paper, we present a novel few-shot binary segmentation method based on
bounding-box annotations instead of pixel-level labels. We introduce, ProMi, an
efficient prototype-mixture-based method that treats the background class as a
mixture of distributions. Our approach is simple, training-free, and effective,
accommodating coarse annotations with ease. Compared to existing baselines,
ProMi achieves the best results across different datasets with significant
gains, demonstrating its effectiveness. Furthermore, we present qualitative
experiments tailored to real-world mobile robot tasks, demonstrating the
applicability of our approach in such scenarios. Our code:
https://github.com/ThalesGroup/promi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Reinforcement Learning-Based Locomotion for Resource-Constrained
  Quadrupeds with Exteroceptive Sensing <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Plozza, Patricia Apostol, Paul Joseph, Simon Schläpfer, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compact quadrupedal robots are proving increasingly suitable for deployment
in real-world scenarios. Their smaller size fosters easy integration into human
environments. Nevertheless, real-time locomotion on uneven terrains remains
challenging, particularly due to the high computational demands of terrain
perception. This paper presents a robust reinforcement learning-based
exteroceptive locomotion controller for resource-constrained small-scale
quadrupeds in challenging terrains, which exploits real-time elevation mapping,
supported by a careful depth sensor selection. We concurrently train both a
policy and a state estimator, which together provide an odometry source for
elevation mapping, optionally fused with visual-inertial odometry (VIO). We
demonstrate the importance of positioning an additional time-of-flight sensor
for maintaining robustness even without VIO, thus having the potential to free
up computational resources. We experimentally demonstrate that the proposed
controller can flawlessly traverse steps up to 17.5 cm in height and achieve an
80% success rate on 22.5 cm steps, both with and without VIO. The proposed
controller also achieves accurate forward and yaw velocity tracking of up to
1.0 m/s and 1.5 rad/s respectively. We open-source our training code at
github.com/ETH-PBL/elmap-rl-controller.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication at the IEEE
  International Conference on Robotics and Automation (ICRA), Atlanta 2025. The
  code is available at github.com/ETH-PBL/elmap-rl-controller</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Development of a non-wearable support robot capable of reproducing
  natural standing-up movements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsuya Kusui, Susumu Hirai, Asuka Takai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To reproduce natural standing-up motion, recent studies have emphasized the
importance of coordination between the assisting robot and the human. However,
many non-wearable assistive devices have struggled to replicate natural motion
trajectories. While wearable devices offer better coordination with the human
body, they present challenges in completely isolating mechanical and electrical
hazards. To address this, we developed a novel standing-assist robot that
integrates features of both wearable and non-wearable systems, aiming to
achieve high coordination while maintaining safety. The device employs a
four-link mechanism aligned with the human joint structure, designed to
reproduce the S-shaped trajectory of the hip and the arc trajectory of the knee
during natural standing-up motion. Subject-specific trajectory data were
obtained using a gyroscope, and the link lengths were determined to drive the
seat along the optimal path. A feedforward speed control using a stepping motor
was implemented, and the reproducibility of the trajectory was evaluated based
on the geometric constraints of the mechanism. A load-bearing experiment with
weights fixed to the seat was conducted to assess the trajectory accuracy under
different conditions. Results showed that the reproduction errors for the hip
and knee trajectories remained within approximately 4 percent of the seat's
total displacement, demonstrating high fidelity to the target paths. In
addition, durability testing, thermal safety evaluation, and risk assessment
confirmed the reliability and safety of the system for indoor use. These
findings suggest that the proposed design offers a promising approach for
developing assistive technologies that adapt to individual physical
characteristics, with potential applications in elderly care and
rehabilitation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event-Driven Simulation for Rapid Iterative Development of Distributed
  Space Flight Software 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toby Bell, Simone D'Amico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the design, development, and application of a novel space
simulation environment for rapidly prototyping and testing flight software for
distributed space systems. The environment combines the flexibility,
determinism, and observability of software-only simulation with the fidelity
and depth normally attained only by real-time hardware-in-the-loop testing.
Ultimately, this work enables an engineering process in which flight software
is continuously improved and delivered in its final, flight-ready form, and
which reduces the cost of design changes and software revisions with respect to
a traditional linear development process. Three key methods not found in
existing tools enable this environment's novel capabilities: first, a hybrid
event-driven simulation architecture that combines continuous-time and
discrete-event simulation paradigms; second, a lightweight application-layer
software virtualization design that allows executing compiled flight software
binaries while modeling process scheduling, input/output, and memory use; and
third, high-fidelity models for the multi-spacecraft space environment,
including for wireless communication, relative sensing such as differential GPS
and cameras, and flight computer health metrics like heap exhaustion and
fragmentation. The simulation environment's capabilities are applied to the
iterative development and testing of two flight-ready software packages: the
guidance, navigation, and control software for the VISORS mission, and the
Stanford Space Rendezvous Laboratory software kit for rendezvous and proximity
operations. Results from 33 months of flight software development demonstrate
the use of this simulation environment to rapidly and reliably identify and
resolve defects, characterize navigation and control performance, and
scrutinize implementation details like memory allocation and inter-spacecraft
network protocols.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Aerospace Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Robot Simulation Environment for Virtual Reality Enhanced Underwater
  Manipulation and Seabed Intervention Tasks <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumey El-Muftu, Berke Gur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the (MARUN)2 underwater robotic simulator. The simulator
architecture enables seamless integration with the ROS-based mission software
and web-based user interface of URSULA, a squid inspired biomimetic robot
designed for dexterous underwater manipulation and seabed intervention tasks.
(MARUN)2 utilizes the Unity game engine for physics-based rigid body dynamic
simulation and underwater environment modeling. Utilizing Unity as the
simulation environment enables the integration of virtual reality and haptic
feedback capabilities for a more immersive and realistic experience for
improved operator dexterity and experience. The utility of the simulator and
improved dexterity provided by the VR module is validated through user
experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages with 8 figures; presented in the AQ2UASIM: Advancing
  Quantitative and Qualitative SIMulators for Marine Applications workshop held
  as a part of the IEEE International Conference on Robotics and Automation
  (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BadNAVer: Exploring Jailbreak Attacks On Vision-and-Language Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Lyu, Zerui Li, Yanyuan Qiao, Qi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have recently gained attention for
their generalization and reasoning capabilities in Vision-and-Language
Navigation (VLN) tasks, leading to the rise of MLLM-driven navigators. However,
MLLMs are vulnerable to jailbreak attacks, where crafted prompts bypass safety
mechanisms and trigger undesired outputs. In embodied scenarios, such
vulnerabilities pose greater risks: unlike plain text models that generate
toxic content, embodied agents may interpret malicious instructions as
executable commands, potentially leading to real-world harm. In this paper, we
present the first systematic jailbreak attack paradigm targeting MLLM-driven
navigator. We propose a three-tiered attack framework and construct malicious
queries across four intent categories, concatenated with standard navigation
instructions. In the Matterport3D simulator, we evaluate navigation agents
powered by five MLLMs and report an average attack success rate over 90%. To
test real-world feasibility, we replicate the attack on a physical robot. Our
results show that even well-crafted prompts can induce harmful actions and
intents in MLLMs, posing risks beyond toxic output and potentially leading to
physical harm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Depth Transfer: Learning to See Like a Simulator for Real-World Drone
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Yu, Christophe De Wagter, Guido C. H. E de Croon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sim-to-real transfer is a fundamental challenge in robot reinforcement
learning. Discrepancies between simulation and reality can significantly impair
policy performance, especially if it receives high-dimensional inputs such as
dense depth estimates from vision. We propose a novel depth transfer method
based on domain adaptation to bridge the visual gap between simulated and
real-world depth data. A Variational Autoencoder (VAE) is first trained to
encode ground-truth depth images from simulation into a latent space, which
serves as input to a reinforcement learning (RL) policy. During deployment, the
encoder is refined to align stereo depth images with this latent space,
enabling direct policy transfer without fine-tuning. We apply our method to the
task of autonomous drone navigation through cluttered environments. Experiments
in IsaacGym show that our method nearly doubles the obstacle avoidance success
rate when switching from ground-truth to stereo depth input. Furthermore, we
demonstrate successful transfer to the photo-realistic simulator AvoidBench
using only IsaacGym-generated stereo data, achieving superior performance
compared to state-of-the-art baselines. Real-world evaluations in both indoor
and outdoor environments confirm the effectiveness of our approach, enabling
robust and generalizable depth-based navigation across diverse domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MTIL: Encoding Full History with Mamba for Temporal Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Zhou, Yuankai Lin, Fanzhe Peng, Jiahui Chen, Zhuang Zhou, Kaiji Huang, Hua Yang, Zhouping Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard imitation learning (IL) methods have achieved considerable success
in robotics, yet often rely on the Markov assumption, limiting their
applicability to tasks where historical context is crucial for disambiguating
current observations. This limitation hinders performance in long-horizon
sequential manipulation tasks where the correct action depends on past events
not fully captured by the current state. To address this fundamental challenge,
we introduce Mamba Temporal Imitation Learning (MTIL), a novel approach that
leverages the recurrent state dynamics inherent in State Space Models (SSMs),
specifically the Mamba architecture. MTIL encodes the entire trajectory history
into a compressed hidden state, conditioning action predictions on this
comprehensive temporal context alongside current multi-modal observations.
Through extensive experiments on simulated benchmarks (ACT dataset tasks,
Robomimic, LIBERO) and real-world sequential manipulation tasks specifically
designed to probe temporal dependencies, MTIL significantly outperforms
state-of-the-art methods like ACT and Diffusion Policy. Our findings affirm the
necessity of full temporal context for robust sequential decision-making and
validate MTIL as a powerful approach that transcends the inherent limitations
of Markovian imitation learning
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages,6 figures,Submitted to IEEE RAL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Semantic SLAM Ready for Embedded Systems ? A Comparative <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Calvin Galagain, Martyna Poreba, François Goulette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In embedded systems, robots must perceive and interpret their environment
efficiently to operate reliably in real-world conditions. Visual Semantic SLAM
(Simultaneous Localization and Mapping) enhances standard SLAM by incorporating
semantic information into the map, enabling more informed decision-making.
However, implementing such systems on resource-limited hardware involves
trade-offs between accuracy, computing efficiency, and power usage.
  This paper provides a comparative review of recent Semantic Visual SLAM
methods with a focus on their applicability to embedded platforms. We analyze
three main types of architectures - Geometric SLAM, Neural Radiance Fields
(NeRF), and 3D Gaussian Splatting - and evaluate their performance on
constrained hardware, specifically the NVIDIA Jetson AGX Orin. We compare their
accuracy, segmentation quality, memory usage, and energy consumption.
  Our results show that methods based on NeRF and Gaussian Splatting achieve
high semantic detail but demand substantial computing resources, limiting their
use on embedded devices. In contrast, Semantic Geometric SLAM offers a more
practical balance between computational cost and accuracy. The review
highlights a need for SLAM algorithms that are better adapted to embedded
environments, and it discusses key directions for improving their efficiency
through algorithm-hardware co-design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributional Soft Actor-Critic with Harmonic Gradient for Safe and
  Efficient Autonomous Driving in Multi-lane Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feihong Zhang, Guojian Zhan, Bin Shuai, Tianyi Zhang, Jingliang Duan, Shengbo Eben Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL), known for its self-evolution capability, offers
a promising approach to training high-level autonomous driving systems.
However, handling constraints remains a significant challenge for existing RL
algorithms, particularly in real-world applications. In this paper, we propose
a new safety-oriented training technique called harmonic policy iteration
(HPI). At each RL iteration, it first calculates two policy gradients
associated with efficient driving and safety constraints, respectively. Then, a
harmonic gradient is derived for policy updating, minimizing conflicts between
the two gradients and consequently enabling a more balanced and stable training
process. Furthermore, we adopt the state-of-the-art DSAC algorithm as the
backbone and integrate it with our HPI to develop a new safe RL algorithm,
DSAC-H. Extensive simulations in multi-lane scenarios demonstrate that DSAC-H
achieves efficient driving performance with near-zero safety constraint
violations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Intelligent Vehicles Symposium (IV 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Feng, Hidetoshi Shimodaira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Multimodal Large Language Models (MLLMs) excel at general
vision-language tasks, visuospatial cognition - reasoning about spatial
layouts, relations, and dynamics - remains a significant challenge. Existing
models often lack the necessary architectural components and specialized
training data for fine-grained spatial understanding. We introduce ViCA2
(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial
reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP
for semantics and Hiera for spatial structure, coupled with a token ratio
control mechanism for efficiency. We also developed ViCA-322K, a new
large-scale dataset with over 322,000 spatially grounded question-answer pairs
for targeted instruction tuning. On the challenging VSI-Bench benchmark, our
ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly
surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and
leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the
effectiveness of our approach in achieving strong visuospatial intelligence
with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset
to facilitate further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 19 figures, 4 tables. Code, models, and dataset are
  available at our project page: https://github.com/nkkbr/ViCA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive MPC-based quadrupedal robot control under periodic disturbances 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Pestova, Ilya Osokin, Danil Belov, Pavel Osinenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in adaptive control for reference trajectory tracking
enable quadrupedal robots to perform locomotion tasks under challenging
conditions. There are methods enabling the estimation of the external
disturbances in terms of forces and torques. However, a specific case of
disturbances that are periodic was not explicitly tackled in application to
quadrupeds. This work is devoted to the estimation of the periodic disturbances
with a lightweight regressor using simplified robot dynamics and extracting the
disturbance properties in terms of the magnitude and frequency. Experimental
evidence suggests performance improvement over the baseline static disturbance
compensation. All source files, including simulation setups, code, and
calculation scripts, are available on GitHub at
https://github.com/aidagroup/quad-periodic-mpc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A universal policy wrapper with guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Bolychev, Georgiy Malaniya, Grigory Yaremenko, Anastasia Krasnaya, Pavel Osinenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a universal policy wrapper for reinforcement learning agents
that ensures formal goal-reaching guarantees. In contrast to standard
reinforcement learning algorithms that excel in performance but lack rigorous
safety assurances, our wrapper selectively switches between a high-performing
base policy -- derived from any existing RL method -- and a fallback policy
with known convergence properties. Base policy's value function supervises this
switching process, determining when the fallback policy should override the
base policy to ensure the system remains on a stable path. The analysis proves
that our wrapper inherits the fallback policy's goal-reaching guarantees while
preserving or improving upon the performance of the base policy. Notably, it
operates without needing additional system knowledge or online constrained
optimization, making it readily deployable across diverse reinforcement
learning architectures and tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-CALF: A Policy Combination Approach with Statistical Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgiy Malaniya, Anton Bolychev, Grigory Yaremenko, Anastasia Krasnaya, Pavel Osinenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Multi-CALF, an algorithm that intelligently combines
reinforcement learning policies based on their relative value improvements. Our
approach integrates a standard RL policy with a theoretically-backed
alternative policy, inheriting formal stability guarantees while often
achieving better performance than either policy individually. We prove that our
combined policy converges to a specified goal set with known probability and
provide precise bounds on maximum deviation and convergence time. Empirical
validation on control tasks demonstrates enhanced performance while maintaining
stability guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structureless VIO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlin Song, Miguel Olivares-Mendez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual odometry (VO) is typically considered as a chicken-and-egg problem, as
the localization and mapping modules are tightly-coupled. The estimation of
visual map relies on accurate localization information. Meanwhile, localization
requires precise map points to provide motion constraints. This classical
design principle is naturally inherited by visual-inertial odometry (VIO).
Efficient localization solution that does not require a map has not been fully
investigated. To this end, we propose a novel structureless VIO, where the
visual map is removed from the odometry framework. Experimental results
demonstrated that, compared to the structure-based VIO baseline, our
structureless VIO not only substantially improves computational efficiency but
also has advantages in accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion
  Predictions <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert Zhao, Stefano Soatto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe a robust planning method for autonomous driving that mixes normal
and adversarial agent predictions output by a diffusion model trained for
motion prediction. We first train a diffusion model to learn an unbiased
distribution of normal agent behaviors. We then generate a distribution of
adversarial predictions by biasing the diffusion model at test time to generate
predictions that are likely to collide with a candidate plan. We score plans
using expected cost with respect to a mixture distribution of normal and
adversarial predictions, leading to a planner that is robust against
adversarial behaviors but not overly conservative when agents behave normally.
Unlike current approaches, we do not use risk measures that over-weight
adversarial behaviors while placing little to no weight on low-cost normal
behaviors or use hard safety constraints that may not be appropriate for all
driving scenarios. We show the effectiveness of our method on single-agent and
multi-agent jaywalking scenarios as well as a red light violation scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE International Conference on Robotics and Automation (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visuospatial Cognitive Assistant 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Feng, Hidetoshi Shimodaira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-based spatial cognition is vital for robotics and embodied AI but
challenges current Vision-Language Models (VLMs). This paper makes two key
contributions. First, we introduce ViCA (Visuospatial Cognitive
Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor
videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D
metadata-grounded queries and video-based complex reasoning. Second, we develop
ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all
eight VSI-Bench tasks, outperforming existing models, including larger ones
(e.g., +26.1 on Absolute Distance). For interpretability, we present
ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune
ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial
reasoning. Our work highlights the importance of targeted data and suggests
paths for improved temporal-spatial modeling. We release all resources to
foster research in robust visuospatial intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 10 figures, 6 tables. The implementation and fine-tuned
  model (ViCA-7B) are publicly available at https://huggingface.co/nkkbr/ViCA.
  The ViCA-322K dataset can be found at
  https://huggingface.co/datasets/nkkbr/ViCA-322K, and the ViCA-Thinking-2.68K
  dataset is at https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scene-Adaptive Motion Planning with Explicit Mixture of Experts and
  Interaction-Oriented Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbiao Zhu, Liulong Ma, Xian Wu, Xin Deng, Xiaoyao Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite over a decade of development, autonomous driving trajectory planning
in complex urban environments continues to encounter significant challenges.
These challenges include the difficulty in accommodating the multi-modal nature
of trajectories, the limitations of single expert in managing diverse
scenarios, and insufficient consideration of environmental interactions. To
address these issues, this paper introduces the EMoE-Planner, which
incorporates three innovative approaches. Firstly, the Explicit MoE (Mixture of
Experts) dynamically selects specialized experts based on scenario-specific
information through a shared scene router. Secondly, the planner utilizes
scene-specific queries to provide multi-modal priors, directing the model's
focus towards relevant target areas. Lastly, it enhances the prediction model
and loss calculation by considering the interactions between the ego vehicle
and other agents, thereby significantly boosting planning performance.
Comparative experiments were conducted using the Nuplan dataset against the
state-of-the-art methods. The simulation results demonstrate that our model
consistently outperforms SOTA models across nearly all test scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main text 9 pages, appendices 2 pages with 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DNOI-4DRO: Deep 4D Radar Odometry with Differentiable
  Neural-Optimization Iterations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shouyi Lu, Huanyu Zhou, Guirong Zhuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel learning-optimization-combined 4D radar odometry model, named
DNOI-4DRO, is proposed in this paper. The proposed model seamlessly integrates
traditional geometric optimization with end-to-end neural network training,
leveraging an innovative differentiable neural-optimization iteration operator.
In this framework, point-wise motion flow is first estimated using a neural
network, followed by the construction of a cost function based on the
relationship between point motion and pose in 3D space. The radar pose is then
refined using Gauss-Newton updates. Additionally, we design a dual-stream 4D
radar backbone that integrates multi-scale geometric features and
clustering-based class-aware features to enhance the representation of sparse
4D radar point clouds. Extensive experiments on the VoD and Snail-Radar
datasets demonstrate the superior performance of our model, which outperforms
recent classical and learning-based approaches. Notably, our method even
achieves results comparable to A-LOAM with mapping optimization using LiDAR
point clouds as input. Our models and code will be publicly released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages,10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PartDexTOG: Generating Dexterous Task-Oriented Grasping via
  Language-driven Part Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weishang Wu, Yifei Shi, Zhizhong Chen, Zhipong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented grasping is a crucial yet challenging task in robotic
manipulation. Despite the recent progress, few existing methods address
task-oriented grasping with dexterous hands. Dexterous hands provide better
precision and versatility, enabling robots to perform task-oriented grasping
more effectively. In this paper, we argue that part analysis can enhance
dexterous grasping by providing detailed information about the object's
functionality. We propose PartDexTOG, a method that generates dexterous
task-oriented grasps via language-driven part analysis. Taking a 3D object and
a manipulation task represented by language as input, the method first
generates the category-level and part-level grasp descriptions w.r.t the
manipulation task by LLMs. Then, a category-part conditional diffusion model is
developed to generate a dexterous grasp for each part, respectively, based on
the generated descriptions. To select the most plausible combination of grasp
and corresponding part from the generated ones, we propose a measure of
geometric consistency between grasp and part. We show that our method greatly
benefits from the open-world knowledge reasoning on object parts by LLMs, which
naturally facilitates the learning of grasp generation on objects with
different geometry and for different manipulation tasks. Our method ranks top
on the OakInk-shape dataset over all previous methods, improving the
Penetration Volume, the Grasp Displace, and the P-FID over the state-of-the-art
by $3.58\%$, $2.87\%$, and $41.43\%$, respectively. Notably, it demonstrates
good generality in handling novel categories and tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emergent Active Perception and Dexterity of Simulated Humanoids from
  Visual Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyi Luo, Chen Tessler, Toru Lin, Ye Yuan, Tairan He, Wenli Xiao, Yunrong Guo, Gal Chechik, Kris Kitani, Linxi Fan, Yuke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human behavior is fundamentally shaped by visual perception -- our ability to
interact with the world depends on actively gathering relevant information and
adapting our movements accordingly. Behaviors like searching for objects,
reaching, and hand-eye coordination naturally emerge from the structure of our
sensory system. Inspired by these principles, we introduce Perceptive Dexterous
Control (PDC), a framework for vision-driven dexterous whole-body control with
simulated humanoids. PDC operates solely on egocentric vision for task
specification, enabling object search, target placement, and skill selection
through visual cues, without relying on privileged state information (e.g., 3D
object positions and geometries). This perception-as-interface paradigm enables
learning a single policy to perform multiple household tasks, including
reaching, grasping, placing, and articulated object manipulation. We also show
that training from scratch with reinforcement learning can produce emergent
behaviors such as active search. These results demonstrate how vision-driven
control and complex tasks induce human-like behaviors and can serve as the key
ingredients in closing the perception-action loop for animation, robotics, and
embodied AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zhengyiluo.github.io/PDC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Spatial Reasoning by Mobile Robots for Reconstruction and
  Navigation in Dynamic LiDAR Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengdi Huang, Mingyang Wang, Huan Tian, Minglun Gong, Hao Zhang, Hui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our brain has an inner global positioning system which enables us to sense
and navigate 3D spaces in real time. Can mobile robots replicate such a
biological feat in a dynamic environment? We introduce the first spatial
reasoning framework for real-time surface reconstruction and navigation that is
designed for outdoor LiDAR scanning data captured by ground mobile robots and
capable of handling moving objects such as pedestrians. Our
reconstruction-based approach is well aligned with the critical cellular
functions performed by the border vector cells (BVCs) over all layers of the
medial entorhinal cortex (MEC) for surface sensing and tracking. To address the
challenges arising from blurred boundaries resulting from sparse single-frame
LiDAR points and outdated data due to object movements, we integrate real-time
single-frame mesh reconstruction, via visibility reasoning, with robot
navigation assistance through on-the-fly 3D free space determination. This
enables continuous and incremental updates of the scene and free space across
multiple frames. Key to our method is the utilization of line-of-sight (LoS)
vectors from LiDAR, which enable real-time surface normal estimation, as well
as robust and instantaneous per-voxel free space updates. We showcase two
practical applications: real-time 3D scene reconstruction and autonomous
outdoor robot navigation in real-world conditions. Comprehensive experiments on
both synthetic and real scenes highlight our method's superiority in speed and
quality over existing real-time LiDAR processing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design of a 3-DOF Hopping Robot with an Optimized Gearbox: An
  Intermediate Platform Toward Bipedal Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JongHun Choe, Gijeong Kim, Hajun Kim, Dongyun Kang, Min-Su Kim, Hae-Won Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a 3-DOF hopping robot with a human-like lower-limb joint
configuration and a flat foot, capable of performing dynamic and repetitive
jumping motions. To achieve both high torque output and a large hollow shaft
diameter for efficient cable routing, a compact 3K compound planetary gearbox
was designed using mixed-integer nonlinear programming for gear tooth
optimization. To meet performance requirements within the constrained joint
geometry, all major components-including the actuator, motor driver, and
communication interface-were custom-designed. The robot weighs 12.45 kg,
including a dummy mass, and measures 840 mm in length when the knee joint is
fully extended. A reinforcement learning-based controller was employed, and
robot's performance was validated through hardware experiments, demonstrating
stable and repetitive hopping motions in response to user inputs. These
experimental results indicate that the platform serves as a solid foundation
for future bipedal robot development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Behavior Synthesis via Contact-Aware Fisher Information Maximization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrishikesh Sathyanarayan, Ian Abraham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contact dynamics hold immense amounts of information that can improve a
robot's ability to characterize and learn about objects in their environment
through interactions. However, collecting information-rich contact data is
challenging due to its inherent sparsity and non-smooth nature, requiring an
active approach to maximize the utility of contacts for learning. In this work,
we investigate an optimal experimental design approach to synthesize robot
behaviors that produce contact-rich data for learning. Our approach derives a
contact-aware Fisher information measure that characterizes information-rich
contact behaviors that improve parameter learning. We observe emergent robot
behaviors that are able to excite contact interactions that efficiently learns
object parameters across a range of parameter learning examples. Last, we
demonstrate the utility of contact-awareness for learning parameters through
contact-seeking behaviors on several robotic experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Robotics Science and Systems 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatial-LLaVA: Enhancing Large Language Models with Spatial Referring
  Expressions for Visual Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuefei Sun, Doncey Albin, Cecilia Mauceri, Dusty Woods, Christoffer Heckman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have demonstrated remarkable
abilities in comprehending visual input alongside text input. Typically, these
models are trained on extensive data sourced from the internet, which are
sufficient for general tasks such as scene understanding and question
answering. However, they often underperform on specialized tasks where online
data is scarce, such as determining spatial relationships between objects or
localizing unique target objects within a group of objects sharing similar
features. In response to this challenge, we introduce the SUN-Spot v2.0
dataset1, now comprising a total of 90k image-caption pairs and additional
annotations on the landmark objects. Each image-caption pair utilizes
Set-of-Marks prompting as an additional indicator, mapping each landmark object
in the image to the corresponding object mentioned in the caption. Furthermore,
we present Spatial-LLaVA, an MLLM trained on conversational data generated by a
state-of-the-art language model using the SUNSpot v2.0 dataset. Our approach
ensures a robust alignment between the objects in the images and their
corresponding object mentions in the captions, enabling our model to learn
spatial referring expressions without bias from the semantic information of the
objects. Spatial-LLaVA outperforms previous methods by 3.15% on the zero-shot
Visual Spatial Reasoning benchmark dataset. Spatial-LLaVA is specifically
designed to precisely understand spatial referring expressions, making it
highly applicable for tasks in real-world scenarios such as autonomous
navigation and interactive robotics, where precise object recognition is
critical.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust Autonomous Landing Systems: Iterative Solutions and Key
  Lessons Learned 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Schroder, Yao Deng, Alice James, Avishkar Seth, Kye Morton, Subhas Mukhopadhyay, Richard Han, Xi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncrewed Aerial Vehicles (UAVs) have become a focal point of research, with
both established companies and startups investing heavily in their development.
This paper presents our iterative process in developing a robust autonomous
marker-based landing system, highlighting the key challenges encountered and
the solutions implemented. It reviews existing systems for autonomous landing
processes, and through this aims to contribute to the community by sharing
insights and challenges faced during development and testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Digital Twin Catalog: A Large-Scale Photorealistic 3D Object Digital
  Twin <span class="highlight-title">Dataset</span> <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Dong, Ka Chen, Zhaoyang Lv, Hong-Xing Yu, Yunzhi Zhang, Cheng Zhang, Yufeng Zhu, Stephen Tian, Zhengqin Li, Geordie Moffatt, Sean Christofferson, James Fort, Xiaqing Pan, Mingfei Yan, Jiajun Wu, Carl Yuheng Ren, Richard Newcombe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Digital Twin Catalog (DTC), a new large-scale photorealistic
3D object digital twin dataset. A digital twin of a 3D object is a highly
detailed, virtually indistinguishable representation of a physical object,
accurately capturing its shape, appearance, physical properties, and other
attributes. Recent advances in neural-based 3D reconstruction and inverse
rendering have significantly improved the quality of 3D object reconstruction.
Despite these advancements, there remains a lack of a large-scale, digital
twin-quality real-world dataset and benchmark that can quantitatively assess
and compare the performance of different reconstruction methods, as well as
improve reconstruction quality through training or fine-tuning. Moreover, to
democratize 3D digital twin creation, it is essential to integrate creation
techniques with next-generation egocentric computing platforms, such as AR
glasses. Currently, there is no dataset available to evaluate 3D object
reconstruction using egocentric captured images. To address these gaps, the DTC
dataset features 2,000 scanned digital twin-quality 3D objects, along with
image sequences captured under different lighting conditions using DSLR cameras
and egocentric AR glasses. This dataset establishes the first comprehensive
real-world evaluation benchmark for 3D digital twin creation tasks, offering a
robust foundation for comparing and improving existing reconstruction methods.
The DTC dataset is already released at
https://www.projectaria.com/datasets/dtc/ and we will also make the baseline
evaluations open-source.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to CVPR 2025 (Highlight). Dataset page:
  https://www.projectaria.com/datasets/dtc/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Reinforcement Learning by Guiding Generalist World Models with
  Non-Curated Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhao, Aidan Scannell, Wenshuai Zhao, Yuxin Hou, Tianyu Cui, Le Chen, Dieter Büchler, Arno Solin, Juho Kannala, Joni Pajarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging offline data is a promising way to improve the sample efficiency
of online reinforcement learning (RL). This paper expands the pool of usable
data for offline-to-online RL by leveraging abundant non-curated data that is
reward-free, of mixed quality, and collected across multiple embodiments.
Although learning a world model appears promising for utilizing such data, we
find that naive fine-tuning fails to accelerate RL training on many tasks.
Through careful investigation, we attribute this failure to the distributional
shift between offline and online data during fine-tuning. To address this issue
and effectively use the offline data, we propose two essential techniques:
\emph{i)} experience rehearsal and \emph{ii)} execution guidance. With these
modifications, the non-curated offline data substantially improves RL's sample
efficiency. Under limited sample budgets, our method achieves a 102.8\%
relative improvement in aggregate score over learning-from-scratch baselines
across 72 visuomotor tasks spanning 6 embodiments. On challenging tasks such as
locomotion and robotic manipulation, it outperforms prior methods that utilize
offline data by a decent margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continuously Optimizing Radar Placement with Model Predictive Path
  Integrals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18999v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18999v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Potter, Shuo Tang, Paul Ghanem, Milica Stojanovic, Pau Closas, Murat Akcakaya, Ben Wright, Marius Necsoiu, Deniz Erdogmus, Michael Everett, Tales Imbiriba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuously optimizing sensor placement is essential for precise target
localization in various military and civilian applications. While information
theory has shown promise in optimizing sensor placement, many studies
oversimplify sensor measurement models or neglect dynamic constraints of mobile
sensors. To address these challenges, we employ a range measurement model that
incorporates radar parameters and radar-target distance, coupled with Model
Predictive Path Integral (MPPI) control to manage complex environmental
obstacles and dynamic constraints. We compare the proposed approach against
stationary radars or simplified range measurement models based on the root mean
squared error (RMSE) of the Cubature Kalman Filter (CKF) estimator for the
targets' state. Additionally, we visualize the evolving geometry of radars and
targets over time, highlighting areas of highest measurement information gain,
demonstrating the strengths of the approach. The proposed strategy outperforms
stationary radars and simplified range measurement models in target
localization, achieving a 38-74% reduction in mean RMSE and a 33-79% reduction
in the upper tail of the 90% Highest Density Interval (HDI) over 500 Monte Carl
(MC) trials across all time steps.
  Code will be made publicly available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Aerospace and Electronic Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DYNUS: Uncertainty-aware Trajectory Planner in Dynamic Unknown
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.16734v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.16734v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kota Kondo, Mason Peterson, Nicholas Rober, Juan Rached Viso, Lucas Jia, Jialin Chen, Harvey Merton, Jonathan P. How
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces DYNUS, an uncertainty-aware trajectory planner designed
for dynamic unknown environments. Operating in such settings presents many
challenges -- most notably, because the agent cannot predict the ground-truth
future paths of obstacles, a previously planned trajectory can become unsafe at
any moment, requiring rapid replanning to avoid collisions.
  Recently developed planners have used soft-constraint approaches to achieve
the necessary fast computation times; however, these methods do not guarantee
collision-free paths even with static obstacles. In contrast, hard-constraint
methods ensure collision-free safety, but typically have longer computation
times.
  To address these issues, we propose three key contributions. First, the DYNUS
Global Planner (DGP) and Temporal Safe Corridor Generation operate in
spatio-temporal space and handle both static and dynamic obstacles in the 3D
environment. Second, the Safe Planning Framework leverages a combination of
exploratory, safe, and contingency trajectories to flexibly re-route when
potential future collisions with dynamic obstacles are detected. Finally, the
Fast Hard-Constraint Local Trajectory Formulation uses a variable elimination
approach to reduce the problem size and enable faster computation by
pre-computing dependencies between free and dependent variables while still
ensuring collision-free trajectories.
  We evaluated DYNUS in a variety of simulations, including dense forests,
confined office spaces, cave systems, and dynamic environments. Our experiments
show that DYNUS achieves a success rate of 100% and travel times that are
approximately 25.0% faster than state-of-the-art methods. We also evaluated
DYNUS on multiple platforms -- a quadrotor, a wheeled robot, and a quadruped --
in both simulation and hardware experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 30 figures, Under review at IEEE Transactions on Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IR2: Implicit Rendezvous for Robotic Exploration Teams under Sparse
  Intermittent Connectivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Derek Ming Siang Tan, Yixiao Ma, Jingsong Liang, Yi Cheng Chng, Yuhong Cao, Guillaume Sartoretti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information sharing is critical in time-sensitive and realistic multi-robot
exploration, especially for smaller robotic teams in large-scale environments
where connectivity may be sparse and intermittent. Existing methods often
overlook such communication constraints by assuming unrealistic global
connectivity. Other works account for communication constraints (by maintaining
close proximity or line of sight during information exchange), but are often
inefficient. For instance, preplanned rendezvous approaches typically involve
unnecessary detours resulting from poorly timed rendezvous, while pursuit-based
approaches often result in short-sighted decisions due to their greedy nature.
We present IR2, a deep reinforcement learning approach to information sharing
for multi-robot exploration. Leveraging attention-based neural networks trained
via reinforcement and curriculum learning, IR2 allows robots to effectively
reason about the longer-term trade-offs between disconnecting for solo
exploration and reconnecting for information sharing. In addition, we propose a
hierarchical graph formulation to maintain a sparse yet informative graph,
enabling our approach to scale to large-scale environments. We present
simulation results in three large-scale Gazebo environments, which show that
our approach yields 6.6-34.1% shorter exploration paths when compared to
state-of-the-art baselines, and lastly deploy our learned policy on hardware.
Our simulation training and testing code is available at
https://ir2-explore.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 20XX IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shape-Space Deformer: Unified Visuo-Tactile Representations for Robotic
  Manipulation of Deformable Objects <span class="chip">ICRA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12419v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12419v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean M. V. Collins, Brendan Tidd, Mahsa Baktashmotlagh, Peyman Moghadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate modelling of object deformations is crucial for a wide range of
robotic manipulation tasks, where interacting with soft or deformable objects
is essential. Current methods struggle to generalise to unseen forces or adapt
to new objects, limiting their utility in real-world applications. We propose
Shape-Space Deformer, a unified representation for encoding a diverse range of
object deformations using template augmentation to achieve robust, fine-grained
reconstructions that are resilient to outliers and unwanted artefacts. Our
method improves generalization to unseen forces and can rapidly adapt to novel
objects, significantly outperforming existing approaches. We perform extensive
experiments to test a range of force generalisation settings and evaluate our
method's ability to reconstruct unseen deformations, demonstrating significant
improvements in reconstruction accuracy and robustness. Our approach is
suitable for real-time performance, making it ready for downstream manipulation
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICRA2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied
  World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hu Yue, Siyuan Huang, Yue Liao, Shengcong Chen, Pengfei Zhou, Liliang Chen, Maoqing Yao, Guanghui Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in creative AI have enabled the synthesis of high-fidelity
images and videos conditioned on language instructions. Building on these
developments, text-to-video diffusion models have evolved into embodied world
models (EWMs) capable of generating physically plausible scenes from language
commands, effectively bridging vision and action in embodied AI applications.
This work addresses the critical challenge of evaluating EWMs beyond general
perceptual metrics to ensure the generation of physically grounded and
action-consistent behaviors. We propose the Embodied World Model Benchmark
(EWMBench), a dedicated framework designed to evaluate EWMs based on three key
aspects: visual scene consistency, motion correctness, and semantic alignment.
Our approach leverages a meticulously curated dataset encompassing diverse
scenes and motion patterns, alongside a comprehensive multi-dimensional
evaluation toolkit, to assess and compare candidate models. The proposed
benchmark not only identifies the limitations of existing video generation
models in meeting the unique requirements of embodied tasks but also provides
valuable insights to guide future advancements in the field. The dataset and
evaluation tools are publicly available at
https://github.com/AgibotTech/EWMBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://github.com/AgibotTech/EWMBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Omnigrasp: Grasping Diverse Objects with Simulated Humanoids 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyi Luo, Jinkun Cao, Sammy Christen, Alexander Winkler, Kris Kitani, Weipeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for controlling a simulated humanoid to grasp an object
and move it to follow an object's trajectory. Due to the challenges in
controlling a humanoid with dexterous hands, prior methods often use a
disembodied hand and only consider vertical lifts or short trajectories. This
limited scope hampers their applicability for object manipulation required for
animation and simulation. To close this gap, we learn a controller that can
pick up a large number (>1200) of objects and carry them to follow randomly
generated trajectories. Our key insight is to leverage a humanoid motion
representation that provides human-like motor skills and significantly speeds
up training. Using only simplistic reward, state, and object representations,
our method shows favorable scalability on diverse objects and trajectories. For
training, we do not need a dataset of paired full-body motion and object
trajectories. At test time, we only require the object mesh and desired
trajectories for grasping and transporting. To demonstrate the capabilities of
our method, we show state-of-the-art success rates in following object
trajectories and generalizing to unseen objects. Code and models will be
released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://www.zhengyiluo.com/Omnigrasp/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HWC-Loco: A Hierarchical Whole-Body Control Approach to Robust Humanoid
  Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00923v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00923v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sixu Lin, Guanren Qiao, Yunxin Tai, Ang Li, Kui Jia, Guiliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanoid robots, capable of assuming human roles in various workplaces, have
become essential to embodied intelligence. However, as robots with complex
physical structures, learning a control model that can operate robustly across
diverse environments remains inherently challenging, particularly under the
discrepancies between training and deployment environments. In this study, we
propose HWC-Loco, a robust whole-body control algorithm tailored for humanoid
locomotion tasks. By reformulating policy learning as a robust optimization
problem, HWC-Loco explicitly learns to recover from safety-critical scenarios.
While prioritizing safety guarantees, overly conservative behavior can
compromise the robot's ability to complete the given tasks. To tackle this
challenge, HWC-Loco leverages a hierarchical policy for robust control. This
policy can dynamically resolve the trade-off between goal-tracking and safety
recovery, guided by human behavior norms and dynamic constraints. To evaluate
the performance of HWC-Loco, we conduct extensive comparisons against
state-of-the-art humanoid control models, demonstrating HWC-Loco's superior
performance across diverse terrains, robot structures, and locomotion tasks
under both simulated and real-world environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ This&That: Language-Gesture Controlled Video Generation for Robot
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyang Wang, Nikhil Sridhar, Chao Feng, Mark Van der Merwe, Adam Fishman, Nima Fazeli, Jeong Joon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clear, interpretable instructions are invaluable when attempting any complex
task. Good instructions help to clarify the task and even anticipate the steps
needed to solve it. In this work, we propose a robot learning framework for
communicating, planning, and executing a wide range of tasks, dubbed This&That.
This&That solves general tasks by leveraging video generative models, which,
through training on internet-scale data, contain rich physical and semantic
context. In this work, we tackle three fundamental challenges in video-based
planning: 1) unambiguous task communication with simple human instructions, 2)
controllable video generation that respects user intent, and 3) translating
visual plans into robot actions. This&That uses language-gesture conditioning
to generate video predictions, as a succinct and unambiguous alternative to
existing language-only methods, especially in complex and uncertain
environments. These video predictions are then fed into a behavior cloning
architecture dubbed Diffusion Video to Action (DiVA), which outperforms prior
state-of-the-art behavior cloning and video-based planning methods by
substantial margins.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hand Dominance and Congruence for Wrist-worn Haptics using Custom
  Voice-Coil Actuation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayoade Adeyemi, Umit Sen, Samet Mert Ercan, Mine Sarac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During virtual interactions, rendering haptic feedback on a remote location
(like the wrist) instead of the fingertips freeing users' hands from mechanical
devices. This allows for real interactions while still providing information
regarding the mechanical properties of virtual objects. In this paper, we
present CoWrHap -- a novel wrist-worn haptic device with custom-made voice coil
actuation to render force feedback. Then, we investigate the impact of asking
participants to use their dominant or non-dominant hand for virtual
interactions and the best mapping between the active hand and the wrist
receiving the haptic feedback, which can be defined as hand-wrist congruence
through a user experiment based on a stiffness discrimination task. Our results
show that participants performed the tasks (i) better with non-congruent
mapping but reported better experiences with congruent mapping, and (ii) with
no statistical difference in terms of hand dominance but reported better user
experience and enjoyment using their dominant hands. This study indicates that
participants can perceive mechanical properties via haptic feedback provided
through CoWrHap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PseudoNeg-MAE: <span class="highlight-title">Self-Supervised</span> Point Cloud Learning using Conditional
  Pseudo-Negative Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sutharsan Mahendren, Saimunur Rahman, Piotr Koniusz, Tharindu Fernando, Sridha Sridharan, Clinton Fookes, Peyman Moghadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose PseudoNeg-MAE, a novel self-supervised learning framework that
enhances global feature representation of point cloud masked autoencoder by
making them both discriminative and sensitive to transformations. Traditional
contrastive learning methods focus on achieving invariance, discarding
transformation-specific information. Recent approaches incorporate
transformation sensitivity by explicitly modeling relationships between
original and transformed inputs. However, they report an invariant-collapse
phenomenon, where the predictor degenerates into identity mappings, resulting
in latent representations that have limited variation across transformations.
We propose a novel loss that explicitly penalizes invariant collapse, enabling
the network to capture richer transformation cues while preserving
discriminative representations. PseudoNeg-MAE uses a parametric network COPE,
which learns the localized displacements caused by transformations within the
latent space. However, jointly training COPE with the MAE leads to undesirable
trivial solutions where COPE outputs collapse to an identity. To address this,
we propose a loss that uses transformation-conditioned pseudo-negatives, to
penalize such trivial invariant solutions. We validate PseudoNeg-MAE on shape
classification and relative pose estimation tasks, where it achieves
competitive performance on the ModelNet40 and ScanObjectNN datasets under
challenging evaluation protocols and demonstrates superior accuracy in
estimating relative poses compared to supervised methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deploying Ten Thousand Robots: Scalable Imitation Learning for Lifelong
  Multi-Agent Path Finding <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21415v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21415v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Jiang, Yutong Wang, Rishi Veerapaneni, Tanishq Duhan, Guillaume Sartoretti, Jiaoyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lifelong Multi-Agent Path Finding (LMAPF) repeatedly finds collision-free
paths for multiple agents that are continually assigned new goals when they
reach current ones. Recently, this field has embraced learning-based methods,
which reactively generate single-step actions based on individual local
observations. However, it is still challenging for them to match the
performance of the best search-based algorithms, especially in large-scale
settings. This work proposes an imitation-learning-based LMAPF solver that
introduces a novel communication module as well as systematic single-step
collision resolution and global guidance techniques. Our proposed solver,
Scalable Imitation Learning for LMAPF (SILLM), inherits the fast reasoning
speed of learning-based methods and the high solution quality of search-based
methods with the help of modern GPUs. Across six large-scale maps with up to
10,000 agents and varying obstacle structures, SILLM surpasses the best
learning- and search-based baselines, achieving average throughput improvements
of 137.7% and 16.0%, respectively. Furthermore, SILLM also beats the winning
solution of the 2023 League of Robot Runners, an international LMAPF
competition. Finally, we validated SILLM with 10 real robots and 100 virtual
robots in a mock warehouse environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PoisonArena: Uncovering Competing Poisoning Attacks in
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liuji Chen, Xiaofang Yang, Yuanzhuo Lu, Jinghao Zhang, Xin Sun, Qiang Liu, Shu Wu, Jing Dong, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems, widely used to improve the
factual grounding of large language models (LLMs), are increasingly vulnerable
to poisoning attacks, where adversaries inject manipulated content into the
retriever's corpus. While prior research has predominantly focused on
single-attacker settings, real-world scenarios often involve multiple,
competing attackers with conflicting objectives. In this work, we introduce
PoisonArena, the first benchmark to systematically study and evaluate competing
poisoning attacks in RAG. We formalize the multi-attacker threat model, where
attackers vie to control the answer to the same query using mutually exclusive
misinformation. PoisonArena leverages the Bradley-Terry model to quantify each
method's competitive effectiveness in such adversarial environments. Through
extensive experiments on the Natural Questions and MS MARCO datasets, we
demonstrate that many attack strategies successful in isolation fail under
competitive pressure. Our findings highlight the limitations of conventional
evaluation metrics like Attack Success Rate (ASR) and F1 score and underscore
the need for competitive evaluation to assess real-world attack robustness.
PoisonArena provides a standardized framework to benchmark and develop future
attack and defense strategies under more realistic, multi-adversary conditions.
Project page: https://github.com/yxf203/PoisonArena.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Batched Self-Consistency Improves LLM Relevance Assessment and Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Korikov, Pan Du, Scott Sanner, Navid Rekabsaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given some information need, Large Language Models (LLMs) are increasingly
used for candidate text relevance assessment, typically using a one-by-one
pointwise (PW) strategy where each LLM call evaluates one candidate at a time.
Meanwhile, it has been shown that LLM performance can be improved through
self-consistency: prompting the LLM to do the same task multiple times
(possibly in perturbed ways) and then aggregating the responses. To take
advantage of self-consistency, we hypothesize that batched PW strategies, where
multiple passages are judged in one LLM call, are better suited than one-by-one
PW methods since a larger input context can induce more diverse LLM sampling
across self-consistency calls. We first propose several candidate batching
strategies to create prompt diversity across self-consistency calls through
subset reselection and permutation. We then test our batched PW methods on
relevance assessment and ranking tasks against one-by-one PW and listwise LLM
ranking baselines with and without self-consistency, using three passage
retrieval datasets and GPT-4o, Claude Sonnet 3, and Amazon Nova Pro. We find
that batched PW methods outperform all baselines, and show that batching can
greatly amplify the positive effects of self-consistency. For instance, on our
legal search dataset, GPT-4o one-by-one PW ranking NDCG@10 improves only from
44.9% to 46.8% without self-consistency vs. with 15 self consistency calls,
while batched PW ranking improves from 43.8% to 51.3%, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAGXplain: From Explainable Evaluation to Actionable Guidance of RAG
  Pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dvir Cohen, Lin Burg, Gilad Barkan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems show promise by coupling large
language models with external knowledge, yet traditional RAG evaluation methods
primarily report quantitative scores while offering limited actionable guidance
for refining these complex pipelines. In this paper, we introduce RAGXplain, an
evaluation framework that quantifies RAG performance and translates these
assessments into clear insights that clarify the workings of its complex,
multi-stage pipeline and offer actionable recommendations. Using LLM reasoning,
RAGXplain converts raw scores into coherent narratives identifying performance
gaps and suggesting targeted improvements. By providing transparent
explanations for AI decision-making, our framework fosters user trust-a key
challenge in AI adoption. Our LLM-based metric assessments show strong
alignment with human judgments, and experiments on public question-answering
datasets confirm that applying RAGXplain's actionable recommendations
measurably improves system performance. RAGXplain thus bridges quantitative
evaluation and practical optimization, empowering users to understand, trust,
and enhance their AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information Extraction from Visually Rich Documents using LLM-based
  Organization of Documents into Independent Textual Segments <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Bhattacharyya, Anurag Tripathi, Ujjal Das, Archan Karmakar, Amit Pathak, Maneesh Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information extraction (IE) from Visually Rich Documents (VRDs) containing
layout features along with text is a critical and well-studied task.
Specialized non-LLM NLP-based solutions typically involve training models using
both textual and geometric information to label sequences/tokens as named
entities or answers to specific questions. However, these approaches lack
reasoning, are not able to infer values not explicitly present in documents,
and do not generalize well to new formats. Generative LLM-based approaches
proposed recently are capable of reasoning, but struggle to comprehend clues
from document layout especially in previously unseen document formats, and do
not show competitive performance in heterogeneous VRD benchmark datasets. In
this paper, we propose BLOCKIE, a novel LLM-based approach that organizes VRDs
into localized, reusable semantic textual segments called $\textit{semantic
blocks}$, which are processed independently. Through focused and more
generalizable reasoning,our approach outperforms the state-of-the-art on public
VRD benchmarks by 1-3% in F1 scores, is resilient to document formats
previously not encountered and shows abilities to correctly extract information
not explicitly present in documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL Main 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introspective Growth: Automatically Advancing LLM Expertise in
  Technology Judgment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyang Wu, Honglin Bao, Nadav Kunievsky, James A. Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) increasingly demonstrate signs of conceptual
understanding, yet much of their internal knowledge remains latent, loosely
structured, and difficult to access or evaluate. We propose self-questioning as
a lightweight and scalable strategy to improve LLMs' understanding,
particularly in domains where success depends on fine-grained semantic
distinctions. To evaluate this approach, we introduce a challenging new
benchmark of 1.3 million post-2015 computer science patent pairs, characterized
by dense technical jargon and strategically complex writing. The benchmark
centers on a pairwise differentiation task: can a model distinguish between
closely related but substantively different inventions? We show that prompting
LLMs to generate and answer their own questions - targeting the background
knowledge required for the task - significantly improves performance. These
self-generated questions and answers activate otherwise underutilized internal
knowledge. Allowing LLMs to retrieve answers from external scientific texts
further enhances performance, suggesting that model knowledge is compressed and
lacks the full richness of the training data. We also find that
chain-of-thought prompting and self-questioning converge, though
self-questioning remains more effective for improving understanding of
technical concepts. Notably, we uncover an asymmetry in prompting: smaller
models often generate more fundamental, more open-ended, better-aligned
questions for mid-sized models than large models with better understanding do,
revealing a new strategy for cross-model collaboration. Altogether, our
findings establish self-questioning as both a practical mechanism for
automatically improving LLM comprehension, especially in domains with sparse
and underrepresented knowledge, and a diagnostic probe of how internal and
external knowledge are organized.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We commit to fully open-source our patent dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in
  Structured Biomedical Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Ofer, Michal Linial, Dafna Shahaf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding interesting phenomena is the core of scientific discovery, but it is
a manual, ill-defined concept. We present an integrative pipeline for
automating the discovery of interesting simple hypotheses (feature-target
relations with effect direction and a potential underlying mechanism) in
structured biomedical data. The pipeline combines machine learning, knowledge
graphs, literature search and Large Language Models. We formalize
"interestingness" as a combination of novelty, utility and plausibility. On 8
major diseases from the UK Biobank, our pipeline consistently recovers risk
factors years before their appearance in the literature. 40--53% of our top
candidates were validated as interesting, compared to 0--7% for a SHAP-based
baseline. Overall, 28% of 109 candidates were interesting to medical experts.
The pipeline addresses the challenge of operationalizing "interestingness"
scalably and for any target. We release data and code:
https://github.com/LinialLab/InterFeat
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group
  Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailong Luo, Bin Wu, Hongyong Jia, Qingqing Zhu, Lianlei Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have advanced recommender systems by modeling
interaction relationships. However, existing graph-based recommenders rely on
sparse ID features and do not fully exploit textual information, resulting in
low information density within representations. Furthermore, graph contrastive
learning faces challenges. Random negative sampling can introduce false
negative samples, while fixed temperature coefficients cannot adapt to the
heterogeneity of different nodes. In addition, current efforts to enhance
recommendations with large language models (LLMs) have not fully utilized their
Chain-of-Thought (CoT) reasoning capabilities to guide representation learning.
To address these limitations, we introduces LGHRec (LLM-CoT Enhanced Graph
Neural Recommendation with Harmonized Group Policy Optimization). This
framework leverages the CoT reasoning ability of LLMs to generate semantic IDs,
enriching reasoning processes and improving information density and semantic
quality of representations. Moreover, we design a reinforcement learning
algorithm, Harmonized Group Policy Optimization (HGPO), to optimize negative
sampling strategies and temperature coefficients in contrastive learning. This
approach enhances long-tail recommendation performance and ensures optimization
consistency across different groups. Experimental results on three datasets
demonstrate that LGHRec improves representation quality through semantic IDs
generated by LLM's CoT reasoning and effectively boosts contrastive learning
with HGPO. Our method outperforms several baseline models. The code is
available at: https://anonymous.4open.science/r/LLM-Rec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Missing Data Issue for Diffusion-based Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Mao, Zhengyi Yang, Jiancan Wu, Haozhe Liu, Yancheng Yuan, Xiang Wang, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have shown significant potential in generating oracle items
that best match user preference with guidance from user historical interaction
sequences. However, the quality of guidance is often compromised by
unpredictable missing data in observed sequence, leading to suboptimal item
generation. Since missing data is uncertain in both occurrence and content,
recovering it is impractical and may introduce additional errors. To tackle
this challenge, we propose a novel dual-side Thompson sampling-based Diffusion
Model (TDM), which simulates extra missing data in the guidance signals and
allows diffusion models to handle existing missing data through extrapolation.
To preserve user preference evolution in sequences despite extra missing data,
we introduce Dual-side Thompson Sampling to implement simulation with two
probability models, sampling by exploiting user preference from both item
continuity and sequence stability. TDM strategically removes items from
sequences based on dual-side Thompson sampling and treats these edited
sequences as guidance for diffusion models, enhancing models' robustness to
missing data through consistency regularization. Additionally, to enhance the
generation efficiency, TDM is implemented under the denoising diffusion
implicit models to accelerate the reverse process. Extensive experiments and
theoretical analysis validate the effectiveness of TDM in addressing missing
data in sequential recommendations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Side Information-driven Session-based Recommendation: From a
  Data-centric Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaokun Zhang, Bo Xu, Chenliang Li, Bowei He, Hongfei Lin, Chen Ma, Fenglong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Session-based recommendation is gaining increasing attention due to its
practical value in predicting the intents of anonymous users based on limited
behaviors. Emerging efforts incorporate various side information to alleviate
inherent data scarcity issues in this task, leading to impressive performance
improvements. The core of side information-driven session-based recommendation
is the discovery and utilization of diverse data. In this survey, we provide a
comprehensive review of this task from a data-centric perspective.
Specifically, this survey commences with a clear formulation of the task. This
is followed by a detailed exploration of various benchmarks rich in side
information that are pivotal for advancing research in this field. Afterwards,
we delve into how different types of side information enhance the task,
underscoring data characteristics and utility. Moreover, we discuss the usage
of various side information, including data encoding, data injection, and
involved techniques. A systematic review of research progress is then
presented, with the taxonomy by the types of side information. Finally, we
summarize the current limitations and present the future prospects of this
vibrant topic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by IEEE TKDE as a survey paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x
  Faster Query Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyuan Ma, Yongliang Ma, Xuanrui Gou, Zhenpeng Su, Ming Zhou, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode
queries and documents into low-dimensional dense or high-dimensional sparse
vectors. It retrieves documents relevant to search queries based on vector
similarities. Documents are pre-encoded offline, while queries arrive in
real-time, necessitating an efficient online query encoder. Although LLMs
significantly enhance retrieval capabilities, serving deeply parameterized LLMs
slows down query inference throughput and increases demands for online
deployment resources. In this paper, we propose LightRetriever, a novel
LLM-based hybrid retriever with extremely lightweight query encoders. Our
method retains a full-sized LLM for document encoding, but reduces the workload
of query encoding to no more than an embedding lookup. Compared to serving a
full-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for
query inference with GPU acceleration, and even a 20x speedup without GPU.
Experiments on large-scale retrieval benchmarks demonstrate that our method
generalizes well across diverse retrieval tasks, retaining an average of 95%
full-sized performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Based User Simulation for Low-Knowledge Shilling Attacks on
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengkang Gu, Jiahao Liu, Dongsheng Li, Guangping Zhang, Mingzhe Han, Hansu Gu, Peng Zhang, Ning Gu, Li Shang, Tun Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RS) are increasingly vulnerable to shilling attacks,
where adversaries inject fake user profiles to manipulate system outputs.
Traditional attack strategies often rely on simplistic heuristics, require
access to internal RS data, and overlook the manipulation potential of textual
reviews. In this work, we introduce Agent4SR, a novel framework that leverages
Large Language Model (LLM)-based agents to perform low-knowledge, high-impact
shilling attacks through both rating and review generation. Agent4SR simulates
realistic user behavior by orchestrating adversarial interactions, selecting
items, assigning ratings, and crafting reviews, while maintaining behavioral
plausibility. Our design includes targeted profile construction, hybrid memory
retrieval, and a review attack strategy that propagates target item features
across unrelated reviews to amplify manipulation. Extensive experiments on
multiple datasets and RS architectures demonstrate that Agent4SR outperforms
existing low-knowledge baselines in both effectiveness and stealth. Our
findings reveal a new class of emergent threats posed by LLM-driven agents,
underscoring the urgent need for enhanced defenses in modern recommender
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geography-Aware Large Language Models for Next POI Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Liu, Wei Liu, Huajie Zhu, Jianxing Yu, Jian Yin, Wang-Chien Lee, Shun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The next Point-of-Interest (POI) recommendation task aims to predict users'
next destinations based on their historical movement data and plays a key role
in location-based services and personalized applications. Accurate next POI
recommendation depends on effectively modeling geographic information and POI
transition relations, which are crucial for capturing spatial dependencies and
user movement patterns. While Large Language Models (LLMs) exhibit strong
capabilities in semantic understanding and contextual reasoning, applying them
to spatial tasks like next POI recommendation remains challenging. First, the
infrequent nature of specific GPS coordinates makes it difficult for LLMs to
model precise spatial contexts. Second, the lack of knowledge about POI
transitions limits their ability to capture potential POI-POI relationships. To
address these issues, we propose GA-LLM (Geography-Aware Large Language Model),
a novel framework that enhances LLMs with two specialized components. The
Geographic Coordinate Injection Module (GCIM) transforms GPS coordinates into
spatial representations using hierarchical and Fourier-based positional
encoding, enabling the model to understand geographic features from multiple
perspectives. The POI Alignment Module (PAM) incorporates POI transition
relations into the LLM's semantic space, allowing it to infer global POI
relationships and generalize to unseen POIs. Experiments on three real-world
datasets demonstrate the state-of-the-art performance of GA-LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annotative Indexing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06256v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06256v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles L. A. Clarke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces annotative indexing, a novel framework that unifies and
generalizes traditional inverted indexes, column stores, object stores, and
graph databases. As a result, annotative indexing can provide the underlying
indexing framework for databases that support retrieval augmented generation,
knowledge graphs, entity retrieval, semi-structured data, and ranked retrieval.
While we primarily focus on human language data in the form of text, annotative
indexing is sufficiently general to support a range of other datatypes, and we
provide examples of SQL-like queries over a JSON store that includes numbers
and dates. Taking advantage of the flexibility of annotative indexing, we also
demonstrate a fully dynamic annotative index incorporating support for ACID
properties of transactions with hundreds of multiple concurrent readers and
writers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at https://github.com/claclark/Cottontail</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why These Documents? Explainable Generative Retrieval with Hierarchical
  Category Paths 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangam Lee, Ryang Heo, SeongKu Kang, Susik Yoon, Jinyoung Yeo, Dongha Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative retrieval has recently emerged as a new alternative of traditional
information retrieval approaches. However, existing generative retrieval
methods directly decode docid when a query is given, making it impossible to
provide users with explanations as an answer for "Why this document is
retrieved?". To address this limitation, we propose Hierarchical Category
Path-Enhanced Generative Retrieval(HyPE), which enhances explainability by
generating hierarchical category paths step-by-step before decoding docid. HyPE
leverages hierarchical category paths as explanation, progressing from broad to
specific semantic categories. This approach enables diverse explanations for
the same document depending on the query by using shared category paths between
the query and the document, and provides reasonable explanation by reflecting
the document's semantic structure through a coarse-to-fine manner. HyPE
constructs category paths with external high-quality semantic hierarchy,
leverages LLM to select appropriate candidate paths for each document, and
optimizes the generative retrieval model with path-augmented dataset. During
inference, HyPE utilizes path-aware reranking strategy to aggregate diverse
topic information, allowing the most relevant documents to be prioritized in
the final ranked list of docids. Our extensive experiments demonstrate that
HyPE not only offers a high level of explainability but also improves the
retrieval performance in the document retrieval task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMelodyGen: Emotion-Conditioned Melody Generation in ABC Notation with
  the Musical Feature Template <span class="chip">ICME</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13259v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13259v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Monan Zhou, Xiaobing Li, Feng Yu, Wei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The EMelodyGen system focuses on emotional melody generation in ABC notation
controlled by the musical feature template. Owing to the scarcity of
well-structured and emotionally labeled sheet music, we designed a template for
controlling emotional melody generation by statistical correlations between
musical features and emotion labels derived from small-scale emotional symbolic
music datasets and music psychology conclusions. We then automatically
annotated a large, well-structured sheet music collection with rough emotional
labels by the template, converted them into ABC notation, and reduced label
imbalance by data augmentation, resulting in a dataset named Rough4Q. Our
system backbone pre-trained on Rough4Q can achieve up to 99% music21 parsing
rate and melodies generated by our template can lead to a 91% alignment on
emotional expressions in blind listening tests. Ablation studies further
validated the effectiveness of the feature controls in the template. Available
code and demos are at https://github.com/monetjoe/EMelodyGen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, accepted by ICMEW2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Fusion Strategies for Federated Multimodal Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08478v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08478v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Li, Guodong Long, Jing Jiang, Chengqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Delivering deeply personalized recommendations necessitates understanding
user interactions with diverse multimedia features, but achieving this within
the constraints of Federated Recommendation Systems (FedRec) is severely
hampered by communication bottlenecks, user heterogeneity, and the complexity
of privacy-preserving multimodal fusion. To this end, we propose FedMR, a novel
multimodal FedRec framework centered around the Mixing Feature Fusion Module
(MFFM). FedMR employs a two-stage process: (1) Server-side centralized
multimedia content processing provides rich, shared item context using
pre-trained models, mitigating limitations from client sparsity and resource
constraints efficiently. (2) Client-Side Personalized Refinement, where the
MFFM dynamically adapts these server-provided multimodal representations based
on client-specific interaction patterns, effectively tailoring recommendations
and resolving heterogeneity in user preferences towards different modalities.
Extensive experiments validate that FedMR seamlessly enhances existing ID-based
FedRecs, effectively transforming them into high-performing federated
multimodal systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures, 6 tables, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation
  Alignment <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01711v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01711v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weicong Qin, Yi Xu, Weijie Yu, Chenglei Shen, Ming He, Jianping Fan, Xiao Zhang, Jun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized product search aims to retrieve and rank items that match users'
preferences and search intent. Despite their effectiveness, existing approaches
typically assume that users' query fully captures their real motivation.
However, our analysis of a real-world e-commerce platform reveals that users
often engage in relevant consultations before searching, indicating they refine
intents through consultations based on motivation and need. The implied
motivation in consultations is a key enhancing factor for personalized search.
This unexplored area comes with new challenges including aligning contextual
motivations with concise queries, bridging the category-text gap, and filtering
noise within sequence history. To address these, we propose a Motivation-Aware
Personalized Search (MAPS) method. It embeds queries and consultations into a
unified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE)
to prioritize critical semantics, and introduces dual alignment: (1)
contrastive learning aligns consultations, reviews, and product features; (2)
bidirectional attention integrates motivation-aware embeddings with user
preferences. Extensive experiments on real and synthetic data show MAPS
outperforms existing methods in both retrieval and ranking tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-17T00:00:00Z">2025-05-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Deep Reinforcement Learning for Privacy-Preserving
  Robotic-Assisted Surgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sana Hafeez, Sundas Rafat Mulkana, Muhammad Ali Imran, Michele Sevegnani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Reinforcement Learning (RL) into robotic-assisted surgery
(RAS) holds significant promise for advancing surgical precision, adaptability,
and autonomous decision-making. However, the development of robust RL models in
clinical settings is hindered by key challenges, including stringent patient
data privacy regulations, limited access to diverse surgical datasets, and high
procedural variability. To address these limitations, this paper presents a
Federated Deep Reinforcement Learning (FDRL) framework that enables
decentralized training of RL models across multiple healthcare institutions
without exposing sensitive patient information. A central innovation of the
proposed framework is its dynamic policy adaptation mechanism, which allows
surgical robots to select and tailor patient-specific policies in real-time,
thereby ensuring personalized and Optimised interventions. To uphold rigorous
privacy standards while facilitating collaborative learning, the FDRL framework
incorporates secure aggregation, differential privacy, and homomorphic
encryption techniques. Experimental results demonstrate a 60\% reduction in
privacy leakage compared to conventional methods, with surgical precision
maintained within a 1.5\% margin of a centralized baseline. This work
establishes a foundational approach for adaptive, secure, and patient-centric
AI-driven surgical robotics, offering a pathway toward clinical translation and
scalable deployment across diverse healthcare environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Take Your Best Shot: Sampling-Based Next-Best-View Planning for
  Autonomous Photography & Inspection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Gao, Lauren Bramblett, Nicola Bezzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous mobile robots (AMRs) equipped with high-quality cameras have
revolutionized the field of inspections by providing efficient and
cost-effective means of conducting surveys. The use of autonomous inspection is
becoming more widespread in a variety of contexts, yet it is still challenging
to acquire the best inspection information autonomously. In situations where
objects may block a robot's view, it is necessary to use reasoning to determine
the optimal points for collecting data. Although researchers have explored
cloud-based applications to store inspection data, these applications may not
operate optimally under network constraints, and parsing these datasets can be
manually intensive. Instead, there is an emerging requirement for AMRs to
autonomously capture the most informative views efficiently. To address this
challenge, we present an autonomous Next-Best-View (NBV) framework that
maximizes the inspection information while reducing the number of pictures
needed during operations. The framework consists of a formalized evaluation
metric using ray-tracing and Gaussian process interpolation to estimate
information reward based on the current understanding of the partially-known
environment. A derivative-free optimization (DFO) method is used to sample
candidate views in the environment and identify the NBV point. The proposed
approach's effectiveness is shown by comparing it with existing methods and
further validated through simulations and experiments with various vehicles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For code and videos, see https://www.bezzorobotics.com/sg-lb-icra25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Surprising Effectiveness of Spectrum Clipping in Learning Stable
  Linear Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01168v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01168v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanyao Guo, Yunhai Han, Harish Ravichandar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When learning stable linear dynamical systems from data, three important
properties are desirable: i) predictive accuracy, ii) provable stability, and
iii) computational efficiency. Unconstrained minimization of reconstruction
errors leads to high accuracy and efficiency but cannot guarantee stability.
Existing methods to enforce stability often preserve accuracy, but do so only
at the cost of increased computation. In this work, we investigate if a
straightforward approach can simultaneously offer all three desiderata of
learning stable linear systems. Specifically, we consider a post-hoc approach
that manipulates the spectrum of the learned system matrix that was computed
using unconstrained least squares. We call this approach spectrum clipping (SC)
as it involves eigen decomposition and subsequent reconstruction of the system
matrix after clipping any eigenvalues that are larger than one to one (without
altering the eigenvectors). Through comprehensive experiments involving two
different applications and publicly available benchmark datasets, we show that
this simple technique can efficiently learn highly-accurate linear systems that
are provably-stable. Notably, we find that SC can match or outperform strong
baselines while being orders-of-magnitude faster. We also show that SC can be
readily combined with Koopman operators to learn stable nonlinear dynamics,
such as those underlying complex dexterous manipulation skills involving
multi-fingered robotic hands. Finally, we find that SC can learn stable robot
policies even when the training data includes unsuccessful or truncated
demonstrations. Our codes and dataset can be found at
https://github.com/GT-STAR-Lab/spec_clip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AORRTC: Almost-Surely Asymptotically Optimal Planning with RRT-Connect 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Wilson, Wil Thomason, Zachary Kingston, Jonathan Gammell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding high-quality solutions quickly is an important objective in motion
planning. This is especially true for high-degree-of-freedom robots.
Satisficing planners have traditionally found feasible solutions quickly but
provide no guarantees on their optimality, while almost-surely asymptotically
optimal (a.s.a.o.) planners have probabilistic guarantees on their convergence
towards an optimal solution but are more computationally expensive.
  This paper uses the AO-x meta-algorithm to extend the satisficing RRT-Connect
planner to optimal planning. The resulting Asymptotically Optimal RRT-Connect
(AORRTC) finds initial solutions in similar times as RRT-Connect and uses any
additional planning time to converge towards the optimal solution in an anytime
manner. It is proven to be probabilistically complete and a.s.a.o.
  AORRTC was tested with the Panda (7 DoF) and Fetch (8 DoF) robotic arms on
the MotionBenchMaker dataset. These experiments show that AORRTC finds initial
solutions as fast as RRT-Connect and faster than the tested state-of-the-art
a.s.a.o. algorithms while converging to better solutions faster. AORRTC finds
solutions to difficult high-DoF planning problems in milliseconds where the
other a.s.a.o. planners could not consistently find solutions in seconds. This
performance was demonstrated both with and without single instruction/multiple
data (SIMD) acceleration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Robotics and Automation Letters (RA-L). Manuscript
  #25-1915. 8 pages, 4 figures, 1 table. A video of AORRTC can be found at
  https://www.youtube.com/watch?v=j1itxP3KuiM . Information on the
  implementation of AORRTC is available at https://robotic-esp.com/code/aorrtc/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-Driven Co-Design of Mobile Manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Schneider, Daniel Honerkamp, Tim Welschehold, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent interest in mobile manipulation has resulted in a wide range of new
robot designs. A large family of these designs focuses on modular platforms
that combine existing mobile bases with static manipulator arms. They combine
these modules by mounting the arm in a tabletop configuration. However, the
operating workspaces and heights for common mobile manipulation tasks, such as
opening articulated objects, significantly differ from tabletop manipulation
tasks. As a result, these standard arm mounting configurations can result in
kinematics with restricted joint ranges and motions. To address these problems,
we present the first Concurrent Design approach for mobile manipulators to
optimize key arm-mounting parameters. Our approach directly targets task
performance across representative household tasks by training a powerful
multitask-capable reinforcement learning policy in an inner loop while
optimizing over a distribution of design configurations guided by Bayesian
Optimization and HyperBand (BOHB) in an outer loop. This results in novel
designs that significantly improve performance across both seen and unseen test
tasks, and outperform designs generated by heuristic-based performance indices
that are cheaper to evaluate but only weakly correlated with the motions of
interest. We evaluate the physical feasibility of the resulting designs and
show that they are practical and remain modular, affordable, and compatible
with existing commercial components. We open-source the approach and generated
designs to facilitate further improvements of these platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at RA-L. Project website:
  https://moma-codesign.cs.uni-freiburg.de/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Reward Design for Reinforcement Learning <span class="chip">UAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjae Kwon, Ingy ElSayed-Aly, Lu Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a surge of interest in using formal languages such as Linear
Temporal Logic (LTL) to precisely and succinctly specify complex tasks and
derive reward functions for Reinforcement Learning (RL). However, existing
methods often assign sparse rewards (e.g., giving a reward of 1 only if a task
is completed and 0 otherwise). By providing feedback solely upon task
completion, these methods fail to encourage successful subtask completion. This
is particularly problematic in environments with inherent uncertainty, where
task completion may be unreliable despite progress on intermediate goals. To
address this limitation, we propose a suite of reward functions that
incentivize an RL agent to complete a task specified by an LTL formula as much
as possible, and develop an adaptive reward shaping approach that dynamically
updates reward functions during the learning process. Experimental results on a
range of benchmark RL environments demonstrate that the proposed approach
generally outperforms baselines, achieving earlier convergence to a better
policy with higher expected return and task completion rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>UAI 2025 Camera Ready Version</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Time-Tagged Data Acquisition for Entanglement Distribution in
  Quantum Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abderrahim Amlou, Thomas Gerrits, Anouar Rahmouni, Amar Abane, Mheni Merzouki, Ya-Shian Li-Baboud, Ahmed Lbath, Abdella Battou, Oliver Slattery
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In distributed quantum applications such as entanglement distribution,
precise time synchronization and efficient time-tagged data handling are
essential. Traditional systems often suffer from overflow, synchronization
drift, and storage inefficiencies. We propose a modular Time Tagging (TT) agent
that uses a 1 pulse per second (PPS) signal from White Rabbit (WR) devices to
achieve network-wide synchronization, while applying real-time calibration,
overflow mitigation, and compression. A live two-lab entanglement distribution
experiment validated the system's performance, achieving synchronized
coincidence detection at 25,000 counts/sec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demystifying and Enhancing the Efficiency of Large Language Model Based
  Search Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiannuo Yang, Zebin Yao, Bowen Jin, Lixiao Cui, Yusen Li, Gang Wang, Xiaoguang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM)-based search agents have shown remarkable
capabilities in solving complex tasks by dynamically decomposing problems and
addressing them through interleaved reasoning and retrieval. However, this
interleaved paradigm introduces substantial efficiency bottlenecks. First, we
observe that both highly accurate and overly approximate retrieval methods
degrade system efficiency: exact search incurs significant retrieval overhead,
while coarse retrieval requires additional reasoning steps during generation.
Second, we identify inefficiencies in system design, including improper
scheduling and frequent retrieval stalls, which lead to cascading latency --
where even minor delays in retrieval amplify end-to-end inference time. To
address these challenges, we introduce SearchAgent-X, a high-efficiency
inference framework for LLM-based search agents. SearchAgent-X leverages
high-recall approximate retrieval and incorporates two key techniques:
priority-aware scheduling and non-stall retrieval. Extensive experiments
demonstrate that SearchAgent-X consistently outperforms state-of-the-art
systems such as vLLM and HNSW-based retrieval across diverse tasks, achieving
up to 3.4$\times$ higher throughput and 5$\times$ lower latency, without
compromising generation quality. SearchAgent-X is available at
https://github.com/tiannuo-yang/SearchAgent-X.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Retrieval: Joint Supervision and Multimodal Document Ranking for
  Textbook Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hessa Alawwad, Usman Naseem, Areej Alhothali, Ali Alkhathlan, Amani Jamal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Textbook question answering (TQA) is a complex task, requiring the
interpretation of complex multimodal context. Although recent advances have
improved overall performance, they often encounter difficulties in educational
settings where accurate semantic alignment and task-specific document retrieval
are essential. In this paper, we propose a novel approach to multimodal
textbook question answering by introducing a mechanism for enhancing semantic
representations through multi-objective joint training. Our model, Joint
Embedding Training With Ranking Supervision for Textbook Question Answering
(JETRTQA), is a multimodal learning framework built on a retriever--generator
architecture that uses a retrieval-augmented generation setup, in which a
multimodal large language model generates answers. JETRTQA is designed to
improve the relevance of retrieved documents in complex educational contexts.
Unlike traditional direct scoring approaches, JETRTQA learns to refine the
semantic representations of questions and documents through a supervised signal
that combines pairwise ranking and implicit supervision derived from answers.
We evaluate our method on the CK12-QA dataset and demonstrate that it
significantly improves the discrimination between informative and irrelevant
documents, even when they are long, complex, and multimodal. JETRTQA
outperforms the previous state of the art, achieving a 2.4\% gain in accuracy
on the validation set and 11.1\% on the test set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 16 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Let's have a chat with the EU AI Act 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Kovari, Yasin Ghafourian, Csaba Hegedus, Belal Abu Naim, Kitti Mezei, Pal Varga, Markus Tauber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As artificial intelligence (AI) regulations evolve and the regulatory
landscape develops and becomes more complex, ensuring compliance with ethical
guidelines and legal frameworks remains a challenge for AI developers. This
paper introduces an AI-driven self-assessment chatbot designed to assist users
in navigating the European Union AI Act and related standards. Leveraging a
Retrieval-Augmented Generation (RAG) framework, the chatbot enables real-time,
context-aware compliance verification by retrieving relevant regulatory texts
and providing tailored guidance. By integrating both public and proprietary
standards, it streamlines regulatory adherence, reduces complexity, and fosters
responsible AI development. The paper explores the chatbot's architecture,
comparing naive and graph-based RAG models, and discusses its potential impact
on AI governance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Basic model for ranking microfinance institutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitry Dudukalov, Evgeny Prokopenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper discusses the challenges encountered in building a ranking model
for aggregator site products, using the example of ranking microfinance
institutions (MFIs) based on post-click conversion. We suggest which features
of MFIs should be considered, and using an algorithm based on Markov chains, we
demonstrate the ``usefulness'' of these features on real data. The ideas
developed in this work can be applied to aggregator websites in microinsurance,
especially when personal data is unavailable. Since we did not find similar
datasets in the public domain, we are publishing our dataset with a detailed
description of its attributes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conversational Recommendation System using NLP and Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piyush Talegaonkar, Siddhant Hole, Shrinesh Kamble, Prashil Gulechha, Deepali Salapurkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's digitally-driven world, the demand for personalized and
context-aware recommendations has never been greater. Traditional recommender
systems have made significant strides in this direction, but they often lack
the ability to tap into the richness of conversational data. This paper
represents a novel approach to recommendation systems by integrating
conversational insights into the recommendation process. The Conversational
Recommender System integrates cutting-edge technologies such as deep learning,
leveraging machine learning algorithms like Apriori for Association Rule
Mining, Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN),
and Long Short-Term Memory (LTSM). Furthermore, sophisticated voice recognition
technologies, including Hidden Markov Models (HMMs) and Dynamic Time Warping
(DTW) algorithms, play a crucial role in accurate speech-to-text conversion,
ensuring robust performance in diverse environments. The methodology
incorporates a fusion of content-based and collaborative recommendation
approaches, enhancing them with NLP techniques. This innovative integration
ensures a more personalized and context-aware recommendation experience,
particularly in marketing applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented in ISETE conference (International Conference on Artificial
  Intelligence, Machine Learning and Big Data Engineering 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuro-Symbolic Query Compiler <span class="chip">ACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyao Zhang, Zhicheng Dou, Xiaoxi Li, Jiajie Jin, Yongkang Wu, Zhonghua Li, Qi Ye, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise recognition of search intent in Retrieval-Augmented Generation (RAG)
systems remains a challenging goal, especially under resource constraints and
for complex queries with nested structures and dependencies. This paper
presents QCompiler, a neuro-symbolic framework inspired by linguistic grammar
rules and compiler design, to bridge this gap. It theoretically designs a
minimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize
complex queries. Unlike previous methods, this grammar maintains completeness
while minimizing redundancy. Based on this, QCompiler includes a Query
Expression Translator, a Lexical Syntax Parser, and a Recursive Descent
Processor to compile queries into Abstract Syntax Trees (ASTs) for execution.
The atomicity of the sub-queries in the leaf nodes ensures more precise
document retrieval and response generation, significantly improving the RAG
system's ability to address complex queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL2025, codes are available at this url:
  https://github.com/YuyaoZhangQAQ/Query_Compiler</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recursive Question Understanding for Complex Question Answering over
  Heterogeneous Personal Data <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Christmann, Gerhard Weikum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering over mixed sources, like text and tables, has been
advanced by verbalizing all contents and encoding it with a language model. A
prominent case of such heterogeneous data is personal information: user devices
log vast amounts of data every day, such as calendar entries, workout
statistics, shopping records, streaming history, and more. Information needs
range from simple look-ups to queries of analytical nature. The challenge is to
provide humans with convenient access with small footprint, so that all
personal data stays on the user devices. We present ReQAP, a novel method that
creates an executable operator tree for a given question, via recursive
decomposition. Operators are designed to enable seamless integration of
structured and unstructured sources, and the execution of the operator tree
yields a traceable answer. We further release the PerQA benchmark, with
persona-based data and questions, covering a diverse spectrum of realistic user
needs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2025 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Telco-oRAG: Optimizing Retrieval-augmented Generation for Telecom
  Queries via Hybrid Retrieval and Neural Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei-Laurentiu Bornea, Fadhel Ayed, Antonio De Domenico, Nicola Piovesan, Tareq Si Salem, Ali Maatouk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence will be one of the key pillars of the next generation
of mobile networks (6G), as it is expected to provide novel added-value
services and improve network performance. In this context, large language
models have the potential to revolutionize the telecom landscape through intent
comprehension, intelligent knowledge retrieval, coding proficiency, and
cross-domain orchestration capabilities. This paper presents Telco-oRAG, an
open-source Retrieval-Augmented Generation (RAG) framework optimized for
answering technical questions in the telecommunications domain, with a
particular focus on 3GPP standards. Telco-oRAG introduces a hybrid retrieval
strategy that combines 3GPP domain-specific retrieval with web search,
supported by glossary-enhanced query refinement and a neural router for
memory-efficient retrieval. Our results show that Telco-oRAG improves the
accuracy in answering 3GPP-related questions by up to 17.6% and achieves a
10.6% improvement in lexicon queries compared to baselines. Furthermore,
Telco-oRAG reduces memory usage by 45% through targeted retrieval of relevant
3GPP series compared to baseline RAG, and enables open-source LLMs to reach
GPT-4-level accuracy on telecom benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Effects of Demographic Instructions on LLM Personas <span class="chip">SIGIR'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angel Felipe Magnossão de Paula, J. Shane Culpepper, Alistair Moffat, Sachin Pathiyan Cherumanal, Falk Scholer, Johanne Trippas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media platforms must filter sexist content in compliance with
governmental regulations. Current machine learning approaches can reliably
detect sexism based on standardized definitions, but often neglect the
subjective nature of sexist language and fail to consider individual users'
perspectives. To address this gap, we adopt a perspectivist approach, retaining
diverse annotations rather than enforcing gold-standard labels or their
aggregations, allowing models to account for personal or group-specific views
of sexism. Using demographic data from Twitter, we employ large language models
(LLMs) to personalize the identification of sexism.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR'25, Padua, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tree-based Focused Web Crawling with Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.07620v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.07620v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Kontogiannis, Dimitrios Kelesis, Vasilis Pollatos, George Giannakopoulos, Georgios Paliouras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A focused crawler aims at discovering as many web pages and web sites
relevant to a target topic as possible, while avoiding irrelevant ones.
Reinforcement Learning (RL) has been a promising direction for optimizing
focused crawling, because RL can naturally optimize the long-term profit of
discovering relevant web locations within the context of a reward. In this
paper, we propose TRES, a novel RL-empowered framework for focused crawling
that aims at maximizing both the number of relevant web pages (aka
\textit{harvest rate}) and the number of relevant web sites (\textit{domains}).
We model the focused crawling problem as a novel Markov Decision Process (MDP),
which the RL agent aims to solve by determining an optimal crawling strategy.
To overcome the computational infeasibility of exhaustively searching for the
best action at each time step, we propose Tree-Frontier, a provably efficient
tree-based sampling algorithm that adaptively discretizes the large state and
action spaces and evaluates only a few representative actions. Experimentally,
utilizing online real-world data, we show that TRES significantly outperforms
and Pareto-dominates state-of-the-art methods in terms of harvest rate and the
number of retrieved relevant domains, while it provably reduces by orders of
magnitude the number of URLs needed to be evaluated at each crawling step.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised Learning-enhanced Multi-Group Actor Critic for Live Stream
  Allocation in Feed 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10381v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10381v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingxin Liu, Xiang Gao, Yisha Li, Xin Li, Haiyang Lu, Ben Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of a short video & live stream mixed recommendation scenario,
the live stream recommendation system (RS) decides whether to allocate at most
one live stream into the video feed for each user request. To maximize
long-term user engagement, it is crucial to determine an optimal live stream
policy for accurate live stream allocation. The inappropriate live stream
allocation policy can significantly affect the duration of the usage app and
user retention, which ignores the long-term negative impact of live stream
allocation. Recently, reinforcement learning (RL) has been widely applied in
recommendation systems to capture long-term user engagement. However,
traditional RL algorithms often face divergence and instability problems, which
restricts the application and deployment in the large-scale industrial
recommendation systems, especially in the aforementioned challenging scenario.
To address these challenges, we propose a novel Supervised Learning-enhanced
Multi-Group Actor Critic algorithm (SL-MGAC). Specifically, we introduce a
supervised learning-enhanced actor-critic framework that incorporates variance
reduction techniques, where multi-task reward learning helps restrict
bootstrapping error accumulation during critic learning. Additionally, we
design a multi-group state decomposition module for both actor and critic
networks to reduce prediction variance and improve model stability. We also
propose a novel reward function to prevent overly greedy live stream
allocation. Empirically, we evaluate the SL-MGAC algorithm using offline policy
evaluation (OPE) and online A/B testing. Experimental results demonstrate that
the proposed method not only outperforms baseline methods under the
platform-level constraints but also exhibits enhanced stability in online
recommendation scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-16T00:00:00Z">2025-05-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Second SIGIR Workshop on Simulations for Information Access (Sim4IA
  2025) <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Schaer, Christin Katharina Kreutz, Krisztian Balog, Timo Breuer, Andreas Konstantin Kruff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulations in information access (IA) have recently gained interest, as
shown by various tutorials and workshops around that topic. Simulations can be
key contributors to central IA research and evaluation questions, especially
around interactive settings when real users are unavailable, or their
participation is impossible due to ethical reasons. In addition, simulations in
IA can help contribute to a better understanding of users, reduce complexity of
evaluation experiments, and improve reproducibility. Building on recent
developments in methods and toolkits, the second iteration of our Sim4IA
workshop aims to again bring together researchers and practitioners to form an
interactive and engaging forum for discussions on the future perspectives of
the field. An additional aim is to plan an upcoming TREC/CLEF campaign.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 48th International ACM SIGIR Conference on
  Research and Development in Information Retrieval (SIGIR '25), July 13--18,
  2025, Padua, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Terminators: Terms of Service Parsing and Auditing Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maruf Ahmed Mridul, Inwon Kang, Oshani Seneviratne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Terms of Service (ToS) documents are often lengthy and written in complex
legal language, making them difficult for users to read and understand. To
address this challenge, we propose Terminators, a modular agentic framework
that leverages large language models (LLMs) to parse and audit ToS documents.
Rather than treating ToS understanding as a black-box summarization problem,
Terminators breaks the task down to three interpretable steps: term extraction,
verification, and accountability planning. We demonstrate the effectiveness of
our method on the OpenAI ToS using GPT-4o, highlighting strategies to minimize
hallucinations and maximize auditability. Our results suggest that structured,
agent-based LLM workflows can enhance both the usability and enforceability of
complex legal documents. By translating opaque terms into actionable,
verifiable components, Terminators promotes ethical use of web content by
enabling greater transparency, empowering users to understand their digital
rights, and supporting automated policy audits for regulatory or civic
oversight.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIRACL-VISION: A Large, multilingual, visual document retrieval
  benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Radek Osmulsk, Gabriel de Souza P. Moreira, Ronay Ak, Mengyao Xu, Benedikt Schifferer, Even Oldridge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document retrieval is an important task for search and Retrieval-Augmented
Generation (RAG) applications. Large Language Models (LLMs) have contributed to
improving the accuracy of text-based document retrieval. However, documents
with complex layout and visual elements like tables, charts and infographics
are not perfectly represented in textual format. Recently, image-based document
retrieval pipelines have become popular, which use visual large language models
(VLMs) to retrieve relevant page images given a query. Current evaluation
benchmarks on visual document retrieval are limited, as they primarily focus
only English language, rely on synthetically generated questions and offer a
small corpus size. Therefore, we introduce MIRACL-VISION, a multilingual visual
document retrieval evaluation benchmark. MIRACL-VISION covers 18 languages, and
is an extension of the MIRACL dataset, a popular benchmark to evaluate
text-based multilingual retrieval pipelines. MIRACL was built using a
human-intensive annotation process to generate high-quality questions. In order
to reduce MIRACL-VISION corpus size to make evaluation more compute friendly
while keeping the datasets challenging, we have designed a method for
eliminating the "easy" negatives from the corpus. We conducted extensive
experiments comparing MIRACL-VISION with other benchmarks, using popular public
text and image models. We observe a gap in state-of-the-art VLM-based embedding
models on multilingual capabilities, with up to 59.7% lower retrieval accuracy
than a text-based retrieval models. Even for the English language, the visual
models retrieval accuracy is 12.1% lower compared to text-based models.
MIRACL-VISION is a challenging, representative, multilingual evaluation
benchmark for visual retrieval pipelines and will help the community build
robust models for document retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRISP: Clustering Multi-Vector Representations for Denoising and Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Veneroso, Rajesh Jayaram, Jinmeng Rao, Gustavo Hernández Ábrego, Majid Hadian, Daniel Cer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-vector models, such as ColBERT, are a significant advancement in neural
information retrieval (IR), delivering state-of-the-art performance by
representing queries and documents by multiple contextualized token-level
embeddings. However, this increased representation size introduces considerable
storage and computational overheads which have hindered widespread adoption in
practice. A common approach to mitigate this overhead is to cluster the model's
frozen vectors, but this strategy's effectiveness is fundamentally limited by
the intrinsic clusterability of these embeddings. In this work, we introduce
CRISP (Clustered Representations with Intrinsic Structure Pruning), a novel
multi-vector training method which learns inherently clusterable
representations directly within the end-to-end training process. By integrating
clustering into the training phase rather than imposing it post-hoc, CRISP
significantly outperforms post-hoc clustering at all representation sizes, as
well as other token pruning methods. On the BEIR retrieval benchmarks, CRISP
achieves a significant rate of ~3x reduction in the number of vectors while
outperforming the original unpruned model. This indicates that learned
clustering effectively denoises the model by filtering irrelevant information,
thereby generating more robust multi-vector representations. With more
aggressive clustering, CRISP achieves an 11x reduction in the number of vectors
with only a $3.6\%$ quality loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Lexical and Semantic Vector Search Methods When Classifying
  Medical Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lee Harris, Philippe De Wilde, James Bentham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification is a common AI problem, and vector search is a typical
solution. This transforms a given body of text into a numerical representation,
known as an embedding, and modern improvements to vector search focus on
optimising speed and predictive accuracy. This is often achieved through neural
methods that aim to learn language semantics. However, our results suggest that
these are not always the best solution. Our task was to classify
rigidly-structured medical documents according to their content, and we found
that using off-the-shelf semantic vector search produced slightly worse
predictive accuracy than creating a bespoke lexical vector search model, and
that it required significantly more time to execute. These findings suggest
that traditional methods deserve to be contenders in the information retrieval
toolkit, despite the prevalence and success of neural models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Future is Sparse: Embedding Compression for Scalable Retrieval in
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petr Kasalický, Martin Spišák, Vojtěch Vančura, Daniel Bohuněk, Rodrigo Alves, Pavel Kordík
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industry-scale recommender systems face a core challenge: representing
entities with high cardinality, such as users or items, using dense embeddings
that must be accessible during both training and inference. However, as
embedding sizes grow, memory constraints make storage and access increasingly
difficult. We describe a lightweight, learnable embedding compression technique
that projects dense embeddings into a high-dimensional, sparsely activated
space. Designed for retrieval tasks, our method reduces memory requirements
while preserving retrieval performance, enabling scalable deployment under
strict resource constraints. Our results demonstrate that leveraging sparsity
is a promising approach for improving the efficiency of large-scale
recommenders. We release our code at https://github.com/recombee/CompresSAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Role of Weight Decay in Collaborative Filtering: A Popularity
  Perspective <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donald Loveland, Mingxuan Ju, Tong Zhao, Neil Shah, Danai Koutra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering (CF) enables large-scale recommendation systems by
encoding information from historical user-item interactions into dense
ID-embedding tables. However, as embedding tables grow, closed-form solutions
become impractical, often necessitating the use of mini-batch gradient descent
for training. Despite extensive work on designing loss functions to train CF
models, we argue that one core component of these pipelines is heavily
overlooked: weight decay. Attaining high-performing models typically requires
careful tuning of weight decay, regardless of loss, yet its necessity is not
well understood. In this work, we question why weight decay is crucial in CF
pipelines and how it impacts training. Through theoretical and empirical
analysis, we surprisingly uncover that weight decay's primary function is to
encode popularity information into the magnitudes of the embedding vectors.
Moreover, we find that tuning weight decay acts as a coarse, non-linear knob to
influence preference towards popular or unpopular items. Based on these
findings, we propose PRISM (Popularity-awaRe Initialization Strategy for
embedding Magnitudes), a straightforward yet effective solution to simplify the
training of high-performing CF models. PRISM pre-encodes the popularity
information typically learned through weight decay, eliminating its necessity.
Our experiments show that PRISM improves performance by up to 4.77% and reduces
training times by 38.48%, compared to state-of-the-art training strategies.
Additionally, we parameterize PRISM to modulate the initialization strength,
offering a cost-effective and meaningful strategy to mitigate popularity bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Caching of Contextual Summaries for Efficient
  Question-Answering with Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camille Couturier, Spyros Mastorakis, Haiying Shen, Saravan Rajmohan, Victor Rühle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly deployed across edge and cloud
platforms for real-time question-answering and retrieval-augmented generation.
However, processing lengthy contexts in distributed systems incurs high
computational overhead, memory usage, and network bandwidth. This paper
introduces a novel semantic caching approach for storing and reusing
intermediate contextual summaries, enabling efficient information reuse across
similar queries in LLM-based QA workflows. Our method reduces redundant
computations by up to 50-60% while maintaining answer accuracy comparable to
full document processing, as demonstrated on NaturalQuestions, TriviaQA, and a
synthetic ArXiv dataset. This approach balances computational cost and response
quality, critical for real-time AI assistants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Paper accepted at ICCCN 2025, the final version will appear
  in the proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ User-centric Music Recommendations <span class="chip">UAI2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaime Ramirez Castillo, M. Julia Flores, Ann E. Nicholson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a user-centric recommendation framework, designed as a
pipeline with four distinct, connected, and customizable phases. These phases
are intended to improve explainability and boost user engagement.
  We have collected the historical Last.fm track playback records of a single
user over approximately 15 years. The collected dataset includes more than
90,000 playbacks and approximately 14,000 unique tracks.
  From track playback records, we have created a dataset of user temporal
contexts (each row is a specific moment when the user listened to certain music
descriptors). As music descriptors, we have used community-contributed Last.fm
tags and Spotify audio features. They represent the music that, throughout
years, the user has been listening to.
  Next, given the most relevant Last.fm tags of a moment (e.g. the hour of the
day), we predict the Spotify audio features that best fit the user preferences
in that particular moment. Finally, we use the predicted audio features to find
tracks similar to these features. The final aim is to recommend (and discover)
tracks that the user may feel like listening to at a particular moment.
  For our initial study case, we have chosen to predict only a single audio
feature target: danceability. The framework, however, allows to include more
target variables.
  The ability to learn the musical habits from a single user can be quite
powerful, and this framework could be extended to other users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 16th Bayesian Modelling Applications Workshop
  (@UAI2022) (BMAW 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mmRAG: A Modular Benchmark for Retrieval-Augmented Generation over Text,
  Tables, and Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan Xu, Qiaosheng Chen, Yutong Feng, Gong Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing the capabilities of large language models. However, existing RAG
evaluation predominantly focuses on text retrieval and relies on opaque,
end-to-end assessments of generated outputs. To address these limitations, we
introduce mmRAG, a modular benchmark designed for evaluating multi-modal RAG
systems. Our benchmark integrates queries from six diverse question-answering
datasets spanning text, tables, and knowledge graphs, which we uniformly
convert into retrievable documents. To enable direct, granular evaluation of
individual RAG components -- such as the accuracy of retrieval and query
routing -- beyond end-to-end generation quality, we follow standard information
retrieval procedures to annotate document relevance and derive dataset
relevance. We establish baseline performance by evaluating a wide range of RAG
implementations on mmRAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Intent Discovery to Recognition with Topic Modeling and Synthetic
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Rodrigues, Mahmood Hegazy, Azzam Naeem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and recognizing customer intents in AI systems is crucial,
particularly in domains characterized by short utterances and the cold start
problem, where recommender systems must include new products or services
without sufficient real user data. Customer utterances are characterized by
infrequent word co-occurences and high term variability, which poses
significant challenges for traditional methods in specifying distinct user
needs and preparing synthetic queries. To address this, we propose an agentic
LLM framework for topic modeling and synthetic query generation, which
accelerates the discovery and recognition of customer intents. We first apply
hierarchical topic modeling and intent discovery to expand a human-curated
taxonomy from 36 generic user intents to 278 granular intents, demonstrating
the potential of LLMs to significantly enhance topic specificity and diversity.
Next, to support newly discovered intents and address the cold start problem,
we generate synthetic user query data, which augments real utterances and
reduces dependency on human annotation, especially in low-resource settings.
Topic model experiments show substantial improvements in coherence and
relevance after topic expansion, while synthetic data experiments indicate that
in-class few-shot prompting significantly improves the quality and utility of
synthetic queries without compromising diversity. We also show that
LLM-generated intent descriptions and keywords can effectively substitute for
human-curated versions when used as context for synthetic query generation. Our
research underscores the scalability and utility of LLM agents in topic
modeling and highlights the strategic use of synthetic utterances to enhance
dataset variability and coverage for intent recognition. We present a
comprehensive and robust framework for online discovery and recognition of new
customer intents in dynamic domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An agentic system with reinforcement-learned subsystem improvements for
  parsing form-like documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayesha Amjad, Saurav Sthapit, Tahir Qasim Syed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting alphanumeric data from form-like documents such as invoices,
purchase orders, bills, and financial documents is often performed via vision
(OCR) and learning algorithms or monolithic pipelines with limited potential
for systemic improvements. We propose an agentic AI system that leverages Large
Language Model (LLM) agents and a reinforcement learning (RL) driver agent to
automate consistent, self-improving extraction under LLM inference uncertainty.
Our work highlights the limitations of monolithic LLM-based extraction and
introduces a modular, multi-agent framework with task-specific prompts and an
RL policy of rewards and penalties to guide a meta-prompting agent to learn
from past errors and improve prompt-based actor agents. This self-corrective
adaptive system handles diverse documents, file formats, layouts, and LLMs,
aiming to automate accurate information extraction without the need for human
intervention. Results as reported on two benchmark datasets of SOIRE, and CORD,
are promising for the agentic AI framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explain What You Mean: Intent Augmented Knowledge Graph Recommender
  Built With LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqing Zheng, Noah Fatsi, Daniel Barcklow, Dmitri Kalaev, Steven Yao, Owen Reinert, C. Bayan Bruss, Daniele Rosa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interaction sparsity is the primary obstacle for recommendation systems.
Sparsity manifests in environments with disproportional cardinality of
groupings of entities, such as users and products in an online marketplace. It
also is found for newly introduced entities, described as the cold-start
problem. Recent efforts to mitigate this sparsity issue shifts the performance
bottleneck to other areas in the computational pipeline. Those that focus on
enriching sparse representations with connectivity data from other external
sources propose methods that are resource demanding and require careful domain
expert aided addition of this newly introduced data. Others that turn to Large
Language Model (LLM) based recommenders will quickly encounter limitations
surrounding data quality and availability. In this work, we propose LLM-based
Intent Knowledge Graph Recommender (IKGR), a novel framework that leverages
retrieval-augmented generation and an encoding approach to construct and
densify a knowledge graph. IKGR learns latent user-item affinities from an
interaction knowledge graph and further densifies it through mutual intent
connectivity. This addresses sparsity issues and allows the model to make
intent-grounded recommendations with an interpretable embedding translation
layer. Through extensive experiments on real-world datasets, we demonstrate
that IKGR overcomes knowledge gaps and achieves substantial gains over
state-of-the-art baselines on both publicly available and our internal
recommendation datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improve Rule Retrieval and Reasoning with Self-Induction and Relevance
  ReEstimate <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Huang, Wangtao Sun, Jun Zhao, Kang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper systematically addresses the challenges of rule retrieval, a
crucial yet underexplored area. Vanilla retrieval methods using sparse or dense
retrievers to directly search for relevant rules to support downstream
reasoning, often suffer from low accuracy. This is primarily due to a
significant semantic gap between the instantiated facts in the queries and the
abstract representations of the rules. Such misalignment results in suboptimal
retrieval quality, which in turn negatively impacts reasoning performance. To
overcome these challenges, we propose Self-Induction Augmented Retrieval
(SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce
potential inferential rules that might offer benefits for reasoning by
abstracting the underlying knowledge and logical structure in queries. These
induced rules are then used for query augmentation to improve retrieval
effectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a
method that re-estimates the relevance of retrieved rules by assessing whether
the abstract knowledge they contain can be instantiated to align with the facts
in the queries and the helpfulness for reasoning. Extensive experiments across
various settings demonstrate the effectiveness and versatility of our proposed
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iAgent: LLM Agent as a Shield between User and Recommender Systems <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14662v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14662v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wujiang Xu, Yunxiao Shi, Zujie Liang, Xuying Ning, Kai Mei, Kun Wang, Xi Zhu, Min Xu, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional recommender systems usually take the user-platform paradigm,
where users are directly exposed under the control of the platform's
recommendation algorithms. However, the defect of recommendation algorithms may
put users in very vulnerable positions under this paradigm. First, many
sophisticated models are often designed with commercial objectives in mind,
focusing on the platform's benefits, which may hinder their ability to protect
and capture users' true interests. Second, these models are typically optimized
using data from all users, which may overlook individual user's preferences.
Due to these shortcomings, users may experience several disadvantages under the
traditional user-platform direct exposure paradigm, such as lack of control
over the recommender system, potential manipulation by the platform, echo
chamber effects, or lack of personalization for less active users due to the
dominance of active users during collaborative learning. Therefore, there is an
urgent need to develop a new paradigm to protect user interests and alleviate
these issues. Recently, some researchers have introduced LLM agents to simulate
user behaviors, these approaches primarily aim to optimize platform-side
performance, leaving core issues in recommender systems unresolved. To address
these limitations, we propose a new user-agent-platform paradigm, where agent
serves as the protective shield between user and recommender system that
enables indirect exposure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL 2025 and WWW2025@HCRS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flexible Generation of Preference Data for Recommendation Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Mungari, Erica Coppolillo, Ettore Ritacco, Giuseppe Manco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulating a recommendation system in a controlled environment, to identify
specific behaviors and user preferences, requires highly flexible synthetic
data generation models capable of mimicking the patterns and trends of real
datasets. In this context, we propose HYDRA, a novel preferences data
generation model driven by three main factors: user-item interaction level,
item popularity, and user engagement level. The key innovations of the proposed
process include the ability to generate user communities characterized by
similar item adoptions, reflecting real-world social influences and trends.
Additionally, HYDRA considers item popularity and user engagement as mixtures
of different probability distributions, allowing for a more realistic
simulation of diverse scenarios. This approach enhances the model's capacity to
simulate a wide range of real-world cases, capturing the complexity and
variability found in actual user behavior. We demonstrate the effectiveness of
HYDRA through extensive experiments on well-known benchmark datasets. The
results highlight its capability to replicate real-world data patterns,
offering valuable insights for developing and testing recommendation systems in
a controlled and realistic manner. The code used to perform the experiments is
publicly available at https://github.com/SimoneMungari/HYDRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Item-Language Model for Conversational Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Yang, Anushya Subbiah, Hardik Patel, Judith Yue Li, Yanwei Song, Reza Mirghaderi, Vikram Aggarwal, Qifan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-language Models (LLMs) have been extremely successful at tasks like
complex dialogue understanding, reasoning and coding due to their emergent
abilities. These emergent abilities have been extended with multi-modality to
include image, audio, and video capabilities. Recommender systems, on the other
hand, have been critical for information seeking and item discovery needs.
Recently, there have been attempts to apply LLMs for recommendations. One
difficulty of current attempts is that the underlying LLM is usually not
trained on the recommender system data, which largely contains user interaction
signals and is often not publicly available. Another difficulty is user
interaction signals often have a different pattern from natural language text,
and it is currently unclear if the LLM training setup can learn more
non-trivial knowledge from interaction signals compared with traditional
recommender system methods. Finally, it is difficult to train multiple LLMs for
different use-cases, and to retain the original language and reasoning
abilities when learning from recommender system data. To address these three
limitations, we propose an Item-Language Model (ILM), which is composed of an
item encoder to produce text-aligned item representations that encode user
interaction signals, and a frozen LLM that can understand those item
representations with preserved pretrained knowledge. We conduct extensive
experiments which demonstrate both the importance of the language-alignment and
of user interaction knowledge in the item encoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrievable Domain-Sensitive Feature Memory for Multi-Domain
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Zhao, Zhaocheng Du, Qinglin Jia, Linxuan Zhang, Zhenhua Dong, Ruiming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increase in the business scale and number of domains in online
advertising, multi-domain ad recommendation has become a mainstream solution in
the industry. The core of multi-domain recommendation is effectively modeling
the commonalities and distinctions among domains. Existing works are dedicated
to designing model architectures for implicit multi-domain modeling while
overlooking an in-depth investigation from a more fundamental perspective of
feature distributions. This paper focuses on features with significant
differences across various domains in both distributions and effects on model
predictions. We refer to these features as domain-sensitive features, which
serve as carriers of domain distinctions and are crucial for multi-domain
modeling. Experiments demonstrate that existing multi-domain modeling methods
may neglect domain-sensitive features, indicating insufficient learning of
domain distinctions. To avoid this neglect, we propose a domain-sensitive
feature attribution method to identify features that best reflect domain
distinctions from the feature set. Further, we design a memory architecture
that extracts domain-specific information from domain-sensitive features for
the model to retrieve and integrate, thereby enhancing the awareness of domain
distinctions. Extensive offline and online experiments demonstrate the
superiority of our method in capturing domain distinctions and improving
multi-domain recommendation performance.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-15T00:00:00Z">2025-05-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">44</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loop closure grasping: Topological transformations enable strong,
  gentle, and versatile grasps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kentaro Barhydt, O. Godson Osele, Sreela Kodali, Cosima du Pasquier, Chase M. Hartquist, H. Harry Asada, Allison M. Okamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grasping mechanisms must both create and subsequently hold grasps that permit
safe and effective object manipulation. Existing mechanisms address the
different functional requirements of grasp creation and grasp holding using a
single morphology, but have yet to achieve the simultaneous strength,
gentleness, and versatility needed for many applications. We present "loop
closure grasping", a class of robotic grasping that addresses these different
functional requirements through topological transformations between open-loop
and closed-loop morphologies. We formalize these morphologies for grasping,
formulate the loop closure grasping method, and present principles and a design
architecture that we implement using soft growing inflated beams, winches, and
clamps. The mechanisms' initial open-loop topology enables versatile grasp
creation via unencumbered tip movement, and closing the loop enables strong and
gentle holding with effectively infinite bending compliance. Loop closure
grasping circumvents the tradeoffs of single-morphology designs, enabling
grasps involving historically challenging objects, environments, and
configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Out-of-Distribution Failure Prevention via Multi-Modal
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milan Ganai, Rohan Sinha, Christopher Agia, Daniel Morton, Marco Pavone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models can provide robust high-level reasoning on appropriate
safety interventions in hazardous scenarios beyond a robot's training data,
i.e. out-of-distribution (OOD) failures. However, due to the high inference
latency of Large Vision and Language Models, current methods rely on manually
defined intervention policies to enact fallbacks, thereby lacking the ability
to plan generalizable, semantically safe motions. To overcome these challenges
we present FORTRESS, a framework that generates and reasons about semantically
safe fallback strategies in real time to prevent OOD failures. At a low
frequency in nominal operations, FORTRESS uses multi-modal reasoners to
identify goals and anticipate failure modes. When a runtime monitor triggers a
fallback response, FORTRESS rapidly synthesizes plans to fallback goals while
inferring and avoiding semantically unsafe regions in real time. By bridging
open-world, multi-modal reasoning with dynamics-aware planning, we eliminate
the need for hard-coded fallbacks and human safety interventions. FORTRESS
outperforms on-the-fly prompting of slow reasoning models in safety
classification accuracy on synthetic benchmarks and real-world ANYmal robot
data, and further improves system safety and planning success in simulation and
on quadrotor hardware for urban navigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://milanganai.github.io/fortress/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AORRTC: Almost-Surely Asymptotically Optimal Planning with RRT-Connect 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Wilson, Wil Thomason, Zachary Kingston, Jonathan Gammell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding high-quality solutions quickly is an important objective in motion
planning. This is especially true for high-degree-of-freedom robots.
Satisficing planners have traditionally found feasible solutions quickly but
provide no guarantees on their optimality, while almost-surely asymptotically
optimal (a.s.a.o.) planners have probabilistic guarantees on their convergence
towards an optimal solution but are more computationally expensive.
  This paper uses the AO-x meta-algorithm to extend the satisficing RRT-Connect
planner to optimal planning. The resulting Asymptotically Optimal RRT-Connect
(AORRTC) finds initial solutions in similar times as RRT-Connect and uses any
additional planning time to converge towards the optimal solution in an anytime
manner. It is proven to be probabilistically complete and a.s.a.o.
  AORRTC was tested with the Panda (7 DoF) and Fetch (8 DoF) robotic arms on
the MotionBenchMaker dataset. These experiments show that AORRTC finds initial
solutions as fast as RRT-Connect and faster than the tested state-of-the-art
a.s.a.o. algorithms while converging to better solutions faster. AORRTC finds
solutions to difficult high-DoF planning problems in milliseconds where the
other a.s.a.o. planners could not consistently find solutions in seconds. This
performance was demonstrated both with and without single instruction/multiple
data (SIMD) acceleration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 1 table. A video of AORRTC can be found at
  https://www.youtube.com/watch?v=j1itxP3KuiM . Information on the
  implementation of AORRTC is available at https://robotic-esp.com/code/aorrtc/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge capture, adaptation and composition (KCAC): A framework for
  cross-task curriculum learning in robotic manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinrui Wang, Yan Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has demonstrated remarkable potential in robotic
manipulation but faces challenges in sample inefficiency and lack of
interpretability, limiting its applicability in real world scenarios. Enabling
the agent to gain a deeper understanding and adapt more efficiently to diverse
working scenarios is crucial, and strategic knowledge utilization is a key
factor in this process. This paper proposes a Knowledge Capture, Adaptation,
and Composition (KCAC) framework to systematically integrate knowledge transfer
into RL through cross-task curriculum learning. KCAC is evaluated using a two
block stacking task in the CausalWorld benchmark, a complex robotic
manipulation environment. To our knowledge, existing RL approaches fail to
solve this task effectively, reflecting deficiencies in knowledge capture. In
this work, we redesign the benchmark reward function by removing rigid
constraints and strict ordering, allowing the agent to maximize total rewards
concurrently and enabling flexible task completion. Furthermore, we define two
self-designed sub-tasks and implement a structured cross-task curriculum to
facilitate efficient learning. As a result, our KCAC approach achieves a 40
percent reduction in training time while improving task success rates by 10
percent compared to traditional RL methods. Through extensive evaluation, we
identify key curriculum design parameters subtask selection, transition timing,
and learning rate that optimize learning efficiency and provide conceptual
guidance for curriculum based RL frameworks. This work offers valuable insights
into curriculum design in RL and robotic learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy
  Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dechen Gao, Hang Wang, Hanchu Zhou, Nejib Ammar, Shatadal Mishra, Ahmadreza Moradipari, Iman Soltani, Junshan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning (IL) and reinforcement learning (RL) each offer distinct
advantages for robotics policy learning: IL provides stable learning from
demonstrations, and RL promotes generalization through exploration. While
existing robot learning approaches using IL-based pre-training followed by
RL-based fine-tuning are promising, this two-step learning paradigm often
suffers from instability and poor sample efficiency during the RL fine-tuning
phase. In this work, we introduce IN-RIL, INterleaved Reinforcement learning
and Imitation Learning, for policy fine-tuning, which periodically injects IL
updates after multiple RL updates and hence can benefit from the stability of
IL and the guidance of expert data for more efficient exploration throughout
the entire fine-tuning process. Since IL and RL involve different optimization
objectives, we develop gradient separation mechanisms to prevent destructive
interference during \ABBR fine-tuning, by separating possibly conflicting
gradient updates in orthogonal subspaces. Furthermore, we conduct rigorous
analysis, and our findings shed light on why interleaving IL with RL stabilizes
learning and improves sample-efficiency. Extensive experiments on 14 robot
manipulation and locomotion tasks across 3 benchmarks, including
FurnitureBench, OpenAI Gym, and Robomimic, demonstrate that \ABBR can
significantly improve sample efficiency and mitigate performance collapse
during online finetuning in both long- and short-horizon tasks with either
sparse or dense rewards. IN-RIL, as a general plug-in compatible with various
state-of-the-art RL algorithms, can significantly improve RL fine-tuning, e.g.,
from 12\% to 88\% with 6.3x improvement in the success rate on Robomimic
Transport. Project page: https://github.com/ucd-dare/IN-RIL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Internal State Estimation in Groups via Active Information Gathering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuebo Ji, Zherong Pan, Xifeng Gao, Lei Yang, Xinxin Du, Kaiyun Li, Yongjin Liu, Wenping Wang, Changhe Tu, Jia Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating human internal states, such as personality traits or
behavioral patterns, is critical for enhancing the effectiveness of human-robot
interaction, particularly in group settings. These insights are key in
applications ranging from social navigation to autism diagnosis. However, prior
methods are limited by scalability and passive observation, making real-time
estimation in complex, multi-human settings difficult. In this work, we propose
a practical method for active human personality estimation in groups, with a
focus on applications related to Autism Spectrum Disorder (ASD). Our method
combines a personality-conditioned behavior model, based on the Eysenck
3-Factor theory, with an active robot information gathering policy that
triggers human behaviors through a receding-horizon planner. The robot's belief
about human personality is then updated via Bayesian inference. We demonstrate
the effectiveness of our approach through simulations, user studies with
typical adults, and preliminary experiments involving participants with ASD.
Our results show that our method can scale to tens of humans and reduce
personality prediction error by 29.2% and uncertainty by 79.9% in simulation.
User studies with typical adults confirm the method's ability to generalize
across complex personality distributions. Additionally, we explore its
application in autism-related scenarios, demonstrating that the method can
identify the difference between neurotypical and autistic behavior,
highlighting its potential for diagnosing ASD. The results suggest that our
framework could serve as a foundation for future ASD-specific interventions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoCam: Hierarchical Path Planning for an Autonomous Auxiliary Camera
  in Surgical Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Banks, Randy Moore, Sayem Nazmuz Zaman, Alaa Eldin Abdelaal, Septimiu E. Salcudean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating an autonomous auxiliary camera into robot-assisted minimally
invasive surgery (RAMIS) enhances spatial awareness and eliminates manual
viewpoint control. Existing path planning methods for auxiliary cameras track
two-dimensional surgical features but do not simultaneously account for camera
orientation, workspace constraints, and robot joint limits. This study presents
AutoCam: an automatic auxiliary camera placement method to improve
visualization in RAMIS. Implemented on the da Vinci Research Kit, the system
uses a priority-based, workspace-constrained control algorithm that combines
heuristic geometric placement with nonlinear optimization to ensure robust
camera tracking. A user study (N=6) demonstrated that the system maintained
99.84% visibility of a salient feature and achieved a pose error of 4.36 $\pm$
2.11 degrees and 1.95 $\pm$ 5.66 mm. The controller was computationally
efficient, with a loop time of 6.8 $\pm$ 12.8 ms. An additional pilot study
(N=6), where novices completed a Fundamentals of Laparoscopic Surgery training
task, suggests that users can teleoperate just as effectively from AutoCam's
viewpoint as from the endoscope's while still benefiting from AutoCam's
improved visual coverage of the scene. These results indicate that an auxiliary
camera can be autonomously controlled using the da Vinci patient-side
manipulators to track a salient feature, laying the groundwork for new
multi-camera visualization methods in RAMIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NVSPolicy: Adaptive Novel-View Synthesis for Generalizable
  Language-Conditioned Policy Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Shi, Yifei Shi, Xin Xu, Tenglong Liu, Junhua Xi, Chengyuan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep generative models demonstrate unprecedented zero-shot
generalization capabilities, offering great potential for robot manipulation in
unstructured environments. Given a partial observation of a scene, deep
generative models could generate the unseen regions and therefore provide more
context, which enhances the capability of robots to generalize across unseen
environments. However, due to the visual artifacts in generated images and
inefficient integration of multi-modal features in policy learning, this
direction remains an open challenge. We introduce NVSPolicy, a generalizable
language-conditioned policy learning method that couples an adaptive novel-view
synthesis module with a hierarchical policy network. Given an input image,
NVSPolicy dynamically selects an informative viewpoint and synthesizes an
adaptive novel-view image to enrich the visual context. To mitigate the impact
of the imperfect synthesized images, we adopt a cycle-consistent VAE mechanism
that disentangles the visual features into the semantic feature and the
remaining feature. The two features are then fed into the hierarchical policy
network respectively: the semantic feature informs the high-level meta-skill
selection, and the remaining feature guides low-level action estimation.
Moreover, we propose several practical mechanisms to make the proposed method
efficient. Extensive experiments on CALVIN demonstrate the state-of-the-art
performance of our method. Specifically, it achieves an average success rate of
90.4\% across all tasks, greatly outperforming the recent methods. Ablation
studies confirm the significance of our adaptive novel-view synthesis paradigm.
In addition, we evaluate NVSPolicy on a real-world robotic platform to
demonstrate its practical applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ pc-dbCBS: Kinodynamic Motion Planning of Physically-Coupled Robot Teams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khaled Wahba, Wolfgang Hönig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion planning problems for physically-coupled multi-robot systems in
cluttered environments are challenging due to their high dimensionality.
Existing methods combining sampling-based planners with trajectory optimization
produce suboptimal results and lack theoretical guarantees. We propose
Physically-coupled discontinuity-bounded Conflict-Based Search (pc-dbCBS), an
anytime kinodynamic motion planner, that extends discontinuity-bounded CBS to
rigidly-coupled systems. Our approach proposes a tri-level conflict detection
and resolution framework that includes the physical coupling between the
robots. Moreover, pc-dbCBS alternates iteratively between state space
representations, thereby preserving probabilistic completeness and asymptotic
optimality while relying only on single-robot motion primitives. Across 25
simulated and six real-world problems involving multirotors carrying a
cable-suspended payload and differential-drive robots linked by rigid rods,
pc-dbCBS solves up to 92% more instances than a state-of-the-art baseline and
plans trajectories that are 50-60% faster while reducing planning time by an
order of magnitude.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inferring Driving Maps by Deep Learning-based Trail Map Extraction <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hubbertz, Pascal Colling, Qi Han, Tobias Meisen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-definition (HD) maps offer extensive and accurate environmental
information about the driving scene, making them a crucial and essential
element for planning within autonomous driving systems. To avoid extensive
efforts from manual labeling, methods for automating the map creation have
emerged. Recent trends have moved from offline mapping to online mapping,
ensuring availability and actuality of the utilized maps. While the performance
has increased in recent years, online mapping still faces challenges regarding
temporal consistency, sensor occlusion, runtime, and generalization. We propose
a novel offline mapping approach that integrates trails - informal routes used
by drivers - into the map creation process. Our method aggregates trail data
from the ego vehicle and other traffic participants to construct a
comprehensive global map using transformer-based deep learning models. Unlike
traditional offline mapping, our approach enables continuous updates while
remaining sensor-agnostic, facilitating efficient data transfer. Our method
demonstrates superior performance compared to state-of-the-art online mapping
approaches, achieving improved generalization to previously unseen environments
and sensor configurations. We validate our approach on two benchmark datasets,
highlighting its robustness and applicability in autonomous driving systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted at the CVPR WAD 2025 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SRT-H: A Hierarchical Framework for Autonomous Surgery via Language
  Conditioned Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Woong Kim, Juo-Tung Chen, Pascal Hansen, Lucy X. Shi, Antony Goldenberg, Samuel Schmidgall, Paul Maria Scheikl, Anton Deguet, Brandon M. White, De Ru Tsai, Richard Cha, Jeffrey Jopling, Chelsea Finn, Axel Krieger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on autonomous robotic surgery has largely focused on simple task
automation in controlled environments. However, real-world surgical
applications require dexterous manipulation over extended time scales while
demanding generalization across diverse variations in human tissue. These
challenges remain difficult to address using existing logic-based or
conventional end-to-end learning strategies. To bridge this gap, we propose a
hierarchical framework for dexterous, long-horizon surgical tasks. Our method
employs a high-level policy for task planning and a low-level policy for
generating task-space controls for the surgical robot. The high-level planner
plans tasks using language, producing task-specific or corrective instructions
that guide the robot at a coarse level. Leveraging language as a planning
modality offers an intuitive and generalizable interface, mirroring how
experienced surgeons instruct traineers during procedures. We validate our
framework in ex-vivo experiments on a complex minimally invasive procedure,
cholecystectomy, and conduct ablative studies to assess key design choices. Our
approach achieves a 100% success rate across n=8 different ex-vivo
gallbladders, operating fully autonomously without human intervention. The
hierarchical approach greatly improves the policy's ability to recover from
suboptimal states that are inevitable in the highly dynamic environment of
realistic surgical applications. This work represents the first demonstration
of step-level autonomy, marking a critical milestone toward autonomous surgical
systems for clinical studies. By advancing generalizable autonomy in surgical
robotics, our approach brings the field closer to real-world deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-aware collaborative pushing of heavy objects using
  skeleton-based intention prediction <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokhan Solak, Gustavo J. G. Lahr, Idil Ozdamar, Arash Ajoudani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In physical human-robot interaction, force feedback has been the most common
sensing modality to convey the human intention to the robot. It is widely used
in admittance control to allow the human to direct the robot. However, it
cannot be used in scenarios where direct force feedback is not available since
manipulated objects are not always equipped with a force sensor. In this work,
we study one such scenario: the collaborative pushing and pulling of heavy
objects on frictional surfaces, a prevalent task in industrial settings. When
humans do it, they communicate through verbal and non-verbal cues, where body
poses, and movements often convey more than words. We propose a novel
context-aware approach using Directed Graph Neural Networks to analyze
spatio-temporal human posture data to predict human motion intention for
non-verbal collaborative physical manipulation. Our experiments demonstrate
that robot assistance significantly reduces human effort and improves task
efficiency. The results indicate that incorporating posture-based context
recognition, either together with or as an alternative to force sensing,
enhances robot decision-making and control efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to be presented at ICRA 2025 conference. Video:
  https://youtu.be/qy7l_wGOyzo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quad-LCD: Layered Control Decomposition Enables Actuator-Feasible
  Quadrotor Trajectory Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anusha Srikanthan, Hanli Zhang, Spencer Folk, Vijay Kumar, Nikolai Matni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we specialize contributions from prior work on data-driven
trajectory generation for a quadrotor system with motor saturation constraints.
When motors saturate in quadrotor systems, there is an ``uncontrolled drift" of
the vehicle that results in a crash. To tackle saturation, we apply a control
decomposition and learn a tracking penalty from simulation data consisting of
low, medium and high-cost reference trajectories. Our approach reduces crash
rates by around $49\%$ compared to baselines on aggressive maneuvers in
simulation. On the Crazyflie hardware platform, we demonstrate feasibility
through experiments that lead to successful flights. Motivated by the growing
interest in data-driven methods to quadrotor planning, we provide open-source
lightweight code with an easy-to-use abstraction of hardware platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Force-Driven Validation for Collaborative Robotics in Automated Avionics
  Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro Dardano, Paolo Rocco, David Frisini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ARTO is a project combining collaborative robots (cobots) and Artificial
Intelligence (AI) to automate functional test procedures for civilian and
military aircraft certification. This paper proposes a Deep Learning (DL) and
eXplainable AI (XAI) approach, equipping ARTO with interaction analysis
capabilities to verify and validate the operations on cockpit components.
During these interactions, forces, torques, and end effector poses are recorded
and preprocessed to filter disturbances caused by low performance force
controllers and embedded Force Torque Sensors (FTS). Convolutional Neural
Networks (CNNs) then classify the cobot actions as Success or Fail, while also
identifying and reporting the causes of failure. To improve interpretability,
Grad CAM, an XAI technique for visual explanations, is integrated to provide
insights into the models decision making process. This approach enhances the
reliability and trustworthiness of the automated testing system, facilitating
the diagnosis and rectification of errors that may arise during testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Safe Robot Foundation Models Using Inductive Biases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Tölle, Theo Gruner, Daniel Palenicek, Tim Schneider, Jonas Günster, Joe Watson, Davide Tateo, Puze Liu, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety is a critical requirement for the real-world deployment of robotic
systems. Unfortunately, while current robot foundation models show promising
generalization capabilities across a wide variety of tasks, they fail to
address safety, an important aspect for ensuring long-term operation. Current
robot foundation models assume that safe behavior should emerge by learning
from a sufficiently large dataset of demonstrations. However, this approach has
two clear major drawbacks. Firstly, there are no formal safety guarantees for a
behavior cloning policy trained using supervised learning. Secondly, without
explicit knowledge of any safety constraints, the policy may require an
unreasonable number of additional demonstrations to even approximate the
desired constrained behavior. To solve these key issues, we show how we can
instead combine robot foundation models with geometric inductive biases using
ATACOM, a safety layer placed after the foundation policy that ensures safe
state transitions by enforcing action constraints. With this approach, we can
ensure formal safety guarantees for generalist policies without providing
extensive demonstrations of safe behavior, and without requiring any specific
fine-tuning for safety. Our experiments show that our approach can be
beneficial both for classical manipulation tasks, where we avoid unwanted
collisions with irrelevant objects, and for dynamic tasks, such as the robot
air hockey environment, where we can generate fast trajectories respecting
complex tasks and joint space constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training People to Reward Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Endong Sun, Yuqing Zhu, Matthew Howard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from demonstration (LfD) is a technique that allows expert teachers
to teach task-oriented skills to robotic systems. However, the most effective
way of guiding novice teachers to approach expert-level demonstrations
quantitatively for specific teaching tasks remains an open question. To this
end, this paper investigates the use of machine teaching (MT) to guide novice
teachers to improve their teaching skills based on reinforcement learning from
demonstration (RLfD). The paper reports an experiment in which novices receive
MT-derived guidance to train their ability to teach a given motor skill with
only 8 demonstrations and generalise this to previously unseen ones. Results
indicate that the MT-guidance not only enhances robot learning performance by
89% on the training skill but also causes a 70% improvement in robot learning
performance on skills not seen by subjects during training. These findings
highlight the effectiveness of MT-guidance in upskilling human teaching
behaviours, ultimately improving demonstration quality in RLfD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zibin Dong, Fei Ni, Yifu Yuan, Yinchuan Li, Jianye Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present EmbodiedMAE, a unified 3D multi-modal representation for robot
manipulation. Current approaches suffer from significant domain gaps between
training datasets and robot manipulation tasks, while also lacking model
architectures that can effectively incorporate 3D information. To overcome
these limitations, we enhance the DROID dataset with high-quality depth maps
and point clouds, constructing DROID-3D as a valuable supplement for 3D
embodied vision research. Then we develop EmbodiedMAE, a multi-modal masked
autoencoder that simultaneously learns representations across RGB, depth, and
point cloud modalities through stochastic masking and cross-modal fusion.
Trained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-art
vision foundation models (VFMs) in both training efficiency and final
performance across 70 simulation tasks and 20 real-world robot manipulation
tasks on two robot platforms. The model exhibits strong scaling behavior with
size and promotes effective policy learning from 3D inputs. Experimental
results establish EmbodiedMAE as a reliable unified 3D multi-modal VFM for
embodied AI systems, particularly in precise tabletop manipulation settings
where spatial perception is critical.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlowDreamer: A RGB-D World Model with Flow-based Motion Representations
  for Robot Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Guo, Xiaojian Ma, Yikai Wang, Min Yang, Huaping Liu, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates training better visual world models for robot
manipulation, i.e., models that can predict future visual observations by
conditioning on past frames and robot actions. Specifically, we consider world
models that operate on RGB-D frames (RGB-D world models). As opposed to
canonical approaches that handle dynamics prediction mostly implicitly and
reconcile it with visual rendering in a single model, we introduce FlowDreamer,
which adopts 3D scene flow as explicit motion representations. FlowDreamer
first predicts 3D scene flow from past frame and action conditions with a
U-Net, and then a diffusion model will predict the future frame utilizing the
scene flow. FlowDreamer is trained end-to-end despite its modularized nature.
We conduct experiments on 4 different benchmarks, covering both video
prediction and visual planning tasks. The results demonstrate that FlowDreamer
achieves better performance compared to other baseline RGB-D world models by 7%
on semantic similarity, 11% on pixel quality, and 6% on success rate in various
robot manipulation domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: see https://sharinka0715.github.io/FlowDreamer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Robot Task Allocation for Homogeneous Tasks with Collision
  Avoidance via Spatial Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rathin Chandra Shit, Sharmila Subudhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a novel framework is presented that achieves a combined
solution based on Multi-Robot Task Allocation (MRTA) and collision avoidance
with respect to homogeneous measurement tasks taking place in industrial
environments. The spatial clustering we propose offers to simultaneously solve
the task allocation problem and deal with collision risks by cutting the
workspace into distinguishable operational zones for each robot. To divide task
sites and to schedule robot routes within corresponding clusters, we use
K-means clustering and the 2-Opt algorithm. The presented framework shows
satisfactory performance, where up to 93\% time reduction (1.24s against
17.62s) with a solution quality improvement of up to 7\% compared to the best
performing method is demonstrated. Our method also completely eliminates
collision points that persist in comparative methods in a most significant
sense. Theoretical analysis agrees with the claim that spatial partitioning
unifies the apparently disjoint tasks allocation and collision avoidance
problems under conditions of many identical tasks to be distributed over sparse
geographical areas. Ultimately, the findings in this work are of substantial
importance for real world applications where both computational efficiency and
operation free from collisions is of paramount importance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, Scheduled for presentation at an upcoming
  conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Robustness of Deep Reinforcement Learning for Autonomous
  Surface Vehicle Control in Field Tests <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis F. W. Batista, Stéphanie Aravecchia, Seth Hutchinson, Cédric Pradalier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in Deep Reinforcement Learning (DRL) for
Autonomous Surface Vehicles (ASVs), their robustness in real-world conditions,
particularly under external disturbances, remains insufficiently explored. In
this paper, we evaluate the resilience of a DRL-based agent designed to capture
floating waste under various perturbations. We train the agent using domain
randomization and evaluate its performance in real-world field tests, assessing
its ability to handle unexpected disturbances such as asymmetric drag and an
off-center payload. We assess the agent's performance under these perturbations
in both simulation and real-world experiments, quantifying performance
degradation and benchmarking it against an MPC baseline. Results indicate that
the DRL agent performs reliably despite significant disturbances. Along with
the open-source release of our implementation, we provide insights into
effective training strategies, real-world challenges, and practical
considerations for deploying DRLbased ASV controllers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on Field Robotics at ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Heuristic Scheduling and Trajectory Planning for Robotic Fruit
  Harvesters with Multiple Cartesian Arms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuankai Zhu, Stavros Vougioukas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a fast heuristic algorithm for the coupled scheduling and
trajectory planning of multiple Cartesian robotic arms harvesting fruits. Our
method partitions the workspace, assigns fruit-picking sequences to arms,
determines tight and feasible fruit-picking schedules and vehicle travel speed,
and generates smooth, collision-free arm trajectories. The fruit-picking
throughput achieved by the algorithm was assessed using synthetically generated
fruit coordinates and a harvester design featuring up to 12 arms. The
throughput increased monotonically as more arms were added. Adding more arms
when fruit densities were low resulted in diminishing gains because it took
longer to travel from one fruit to another. However, when there were enough
fruits, the proposed algorithm achieved a linear speedup as the number of arms
increased.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work will be submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ APEX: Action Priors Enable Efficient Exploration for Skill Imitation on
  Articulated Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Sood, Laukik B Nakhwa, Yuhong Cao, Sun Ge, Guillaume Sartoretti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning by imitation provides an effective way for robots to develop
well-regulated complex behaviors and directly benefit from natural
demonstrations. State-of-the-art imitation learning (IL) approaches typically
leverage Adversarial Motion Priors (AMP), which, despite their impressive
results, suffer from two key limitations. They are prone to mode collapse,
which often leads to overfitting to the simulation environment and thus
increased sim-to-real gap, and they struggle to learn diverse behaviors
effectively. To overcome these limitations, we introduce APEX (Action Priors
enable Efficient eXploration): a simple yet versatile imitation learning
framework that integrates demonstrations directly into reinforcement learning
(RL), maintaining high exploration while grounding behavior with
expert-informed priors. We achieve this through a combination of decaying
action priors, which initially bias exploration toward expert demonstrations
but gradually allow the policy to explore independently. This is complemented
by a multi-critic RL framework that effectively balances stylistic consistency
with task performance. Our approach achieves sample-efficient imitation
learning and enables the acquisition of diverse skills within a single policy.
APEX generalizes to varying velocities and preserves reference-like styles
across complex tasks such as navigating rough terrain and climbing stairs,
utilizing only flat-terrain kinematic motion data as a prior. We validate our
framework through extensive hardware experiments on the Unitree Go2 quadruped.
There, APEX yields diverse and agile locomotion gaits, inherent gait
transitions, and the highest reported speed for the platform to the best of our
knowledge (peak velocity of ~3.3 m/s on hardware). Our results establish APEX
as a compelling alternative to existing IL methods, offering better efficiency,
adaptability, and real-world performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Threshold Strategy for Leaking Corner-Free Hamilton-Jacobi Reachability
  with Decomposed Computations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong He, Mugilan Mariappan, Keval Vora, Mo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hamilton-Jacobi (HJ) Reachability is widely used to compute value functions
for states satisfying specific control objectives. However, it becomes
intractable for high-dimensional problems due to the curse of dimensionality.
Dimensionality reduction approaches are essential for mitigating this
challenge, whereas they could introduce the ``leaking corner issue", leading to
inaccuracies in the results. In this paper, we define the ``leaking corner
issue" in terms of value functions, propose and prove a necessary condition for
its occurrence. We then use these theoretical contributions to introduce a new
local updating method that efficiently corrects inaccurate value functions
while maintaining the computational efficiency of the dimensionality reduction
approaches. We demonstrate the effectiveness of our method through numerical
simulations. Although we validate our method with the self-contained subsystem
decomposition (SCSD), our approach is applicable to other dimensionality
reduction techniques that introduce the ``leaking corners".
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, Submitted to Conference on Decision and Control (CDC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEMON-Mapping: Loop-Enhanced Large-Scale Multi-Session Point Cloud
  Merging and Optimization for Globally Consistent Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Wang, Xiaoyi Zhong, Ziyi Xu, Kaixin Chai, Anke Zhao, Tianyu Zhao, Qianhao Wang, Fei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of robotics, multi-robot collaboration has become
critical and challenging. One key problem is integrating data from multiple
robots to build a globally consistent and accurate map for robust cooperation
and precise localization. While traditional multi-robot pose graph optimization
(PGO) maintains basic global consistency, it focuses primarily on pose
optimization and ignores the geometric structure of the map. Moreover, PGO only
uses loop closure as a constraint between two nodes, failing to fully exploit
its capability to maintaining local consistency of multi-robot maps. Therefore,
PGO-based multi-robot mapping methods often suffer from serious map divergence
and blur, especially in regions with overlapping submaps. To address this
issue, we propose Lemon-Mapping, a loop-enhanced framework for large-scale
multi-session point cloud map fusion and optimization, which reasonably
utilizes loop closure and improves the geometric quality of the map. We
re-examine the role of loops for multi-robot mapping and introduce three key
innovations. First, we develop a robust loop processing mechanism that
effectively rejects outliers and a novel loop recall strategy to recover
mistakenly removed loops. Second, we introduce a spatial bundle adjustment
method for multi-robot maps that significantly reduces the divergence in
overlapping regions and eliminates map blur. Third, we design a PGO strategy
that leverages the refined constraints of bundle adjustment to extend the local
accuracy to the global map. We validate our framework on several public
datasets and a self-collected dataset. Experimental results demonstrate that
our method outperforms traditional map merging approaches in terms of mapping
accuracy and reduction of map divergence. Scalability experiments also
demonstrate the strong capability of our framework to handle scenarios
involving numerous robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably safe and human-like car-following behaviors: Part 2. A
  parsimonious multi-phase model with projected braking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Long Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring safe and human-like trajectory planning for automated vehicles
amidst real-world uncertainties remains a critical challenge. While existing
car-following models often struggle to consistently provide rigorous safety
proofs alongside human-like acceleration and deceleration patterns, we
introduce a novel multi-phase projection-based car-following model. This model
is designed to balance safety and performance by incorporating bounded
acceleration and deceleration rates while emulating key human driving
principles. Building upon a foundation of fundamental driving principles and a
multi-phase dynamical systems analysis (detailed in Part 1 of this study
\citep{jin2025WA20-02_Part1}), we first highlight the limitations of extending
standard models like Newell's with simple bounded deceleration. Inspired by
human drivers' anticipatory behavior, we mathematically define and analyze
projected braking profiles for both leader and follower vehicles, establishing
safety criteria and new phase definitions based on the projected braking
lead-vehicle problem. The proposed parsimonious model combines an extended
Newell's model for nominal driving with a new control law for scenarios
requiring projected braking. Using speed-spacing phase plane analysis, we
provide rigorous mathematical proofs of the model's adherence to defined safe
and human-like driving principles, including collision-free operation, bounded
deceleration, and acceptable safe stopping distance, under reasonable initial
conditions. Numerical simulations validate the model's superior performance in
achieving both safety and human-like braking profiles for the stationary
lead-vehicle problem. Finally, we discuss the model's implications and future
research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably safe and human-like car-following behaviors: Part 1. Analysis
  of phases and dynamics in standard models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Long Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory planning is essential for ensuring safe driving in the face of
uncertainties related to communication, sensing, and dynamic factors such as
weather, road conditions, policies, and other road users. Existing
car-following models often lack rigorous safety proofs and the ability to
replicate human-like driving behaviors consistently. This article applies
multi-phase dynamical systems analysis to well-known car-following models to
highlight the characteristics and limitations of existing approaches. We begin
by formulating fundamental principles for safe and human-like car-following
behaviors, which include zeroth-order principles for comfort and minimum jam
spacings, first-order principles for speeds and time gaps, and second-order
principles for comfort acceleration/deceleration bounds as well as braking
profiles. From a set of these zeroth- and first-order principles, we derive
Newell's simplified car-following model. Subsequently, we analyze phases within
the speed-spacing plane for the stationary lead-vehicle problem in Newell's
model and its extensions, which incorporate both bounded acceleration and
deceleration. We then analyze the performance of the Intelligent Driver Model
and the Gipps model. Through this analysis, we highlight the limitations of
these models with respect to some of the aforementioned principles. Numerical
simulations and empirical observations validate the theoretical insights.
Finally, we discuss future research directions to further integrate safety,
human-like behaviors, and vehicular automation in car-following models, which
are addressed in Part 2 of this study \citep{jin2025WA20-02_Part2}, where we
develop a novel multi-phase projection-based car-following model that addresses
the limitations identified here.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Diverse Natural Behaviors for Enhancing the Agility of
  Quadrupedal Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiqiao Fu, Haoyu Dong, Wentao Xu, Zhehao Zhou, Guizhou Deng, Kaiqiang Tang, Daoyi Dong, Chunlin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving animal-like agility is a longstanding goal in quadrupedal robotics.
While recent studies have successfully demonstrated imitation of specific
behaviors, enabling robots to replicate a broader range of natural behaviors in
real-world environments remains an open challenge. Here we propose an
integrated controller comprising a Basic Behavior Controller (BBC) and a
Task-Specific Controller (TSC) which can effectively learn diverse natural
quadrupedal behaviors in an enhanced simulator and efficiently transfer them to
the real world. Specifically, the BBC is trained using a novel semi-supervised
generative adversarial imitation learning algorithm to extract diverse
behavioral styles from raw motion capture data of real dogs, enabling smooth
behavior transitions by adjusting discrete and continuous latent variable
inputs. The TSC, trained via privileged learning with depth images as input,
coordinates the BBC to efficiently perform various tasks. Additionally, we
employ evolutionary adversarial simulator identification to optimize the
simulator, aligning it closely with reality. After training, the robot exhibits
diverse natural behaviors, successfully completing the quadrupedal agility
challenge at an average speed of 1.1 m/s and achieving a peak speed of 3.2 m/s
during hurdling. This work represents a substantial step toward animal-like
agility in quadrupedal robots, opening avenues for their deployment in
increasingly complex real-world environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyper Yoshimura: How a slight tweak on a classical folding pattern
  unleashes meta-stability for deployable robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Zhou, Yogesh Phalak, Vishrut Deshpande, Ian Walker, Suyi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deployable structures inspired by origami offer lightweight, compact, and
reconfigurable solutions for robotic and architectural applications. We present
a geometric and mechanical framework for Yoshimura-Ori modules that supports a
diverse set of metastable states, including newly identified asymmetric
"pop-out" and "hyperfolded" configurations. These states are governed by three
parameters -- tilt angle, phase shift, and slant height -- and enable discrete,
programmable transformations. Using this model, we develop forward and inverse
kinematic strategies to stack modules into deployable booms that approximate
complex 3D shapes. We validate our approach through mechanical tests and
demonstrate a tendon- and pneumatically-actuated Yoshimura Space Crane capable
of object manipulation, solar tracking, and high load-bearing performance. A
meter-scale solar charging station further illustrates the design's
scalability. These results establish Yoshimura-Ori structures as a promising
platform for adaptable, multifunctional deployable systems in both terrestrial
and space environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large-Scale Gaussian Splatting SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Xin, Chenyang Wu, Penghui Huang, Yanyong Zhang, Yinian Mao, Guoquan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS) have shown encouraging and impressive results for visual SLAM.
However, most representative methods require RGBD sensors and are only
available for indoor environments. The robustness of reconstruction in
large-scale outdoor scenarios remains unexplored. This paper introduces a
large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The
proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses
under large view changes. In tracking, we introduce feature-alignment warping
constraints to alleviate the adverse effects of appearance similarity in
rendering losses. For the scalability of large-scale scenarios, we introduce
continuous Gaussian Splatting submaps to tackle unbounded scenes with limited
memory. Loops are detected between GS submaps by place recognition and the
relative pose between looped keyframes is optimized utilizing rendering and
feature warping losses. After the global optimization of camera poses and
Gaussian points, a structure refinement module enhances the reconstruction
quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM
achieves superior performance over existing Neural, 3DGS-based, and even
traditional approaches. Project page: https://lsg-slam.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-SAFE: Shared Autonomy Framework with Diffusion for Safe
  Human-to-Robot Driving Handover 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxin Fan, Monroe Kennedy III
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe handover in shared autonomy for vehicle control is well-established in
modern vehicles. However, avoiding accidents often requires action several
seconds in advance. This necessitates understanding human driver behavior and
an expert control strategy for seamless intervention when a collision or unsafe
state is predicted. We propose Diffusion-SAFE, a closed-loop shared autonomy
framework leveraging diffusion models to: (1) predict human driving behavior
for detection of potential risks, (2) generate safe expert trajectories, and
(3) enable smooth handovers by blending human and expert policies over a short
time horizon. Unlike prior works which use engineered score functions to rate
driving performance, our approach enables both performance evaluation and
optimal action sequence generation from demonstrations. By adjusting the
forward and reverse processes of the diffusion-based copilot, our method
ensures a gradual transition of control authority, by mimicking the drivers'
behavior before intervention, which mitigates abrupt takeovers, leading to
smooth transitions. We evaluated Diffusion-SAFE in both simulation
(CarRacing-v0) and real-world (ROS-based race car), measuring human-driving
similarity, safety, and computational efficiency. Results demonstrate a 98.5\%
successful handover rate, highlighting the framework's effectiveness in
progressively correcting human actions and continuously sampling optimal robot
actions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Radar Point Cloud Enhancement via Arbitrary LiDAR Guided
  Diffusion Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanlong Yang, Jianan Liu, Guanxiong Luo, Hao Li, Euijoon Ahn, Mostafa Rahimi Azghadi, Tao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In industrial automation, radar is a critical sensor in machine perception.
However, the angular resolution of radar is inherently limited by the Rayleigh
criterion, which depends on both the radar's operating wavelength and the
effective aperture of its antenna array.To overcome these hardware-imposed
limitations, recent neural network-based methods have leveraged high-resolution
LiDAR data, paired with radar measurements, during training to enhance radar
point cloud resolution. While effective, these approaches require extensive
paired datasets, which are costly to acquire and prone to calibration error.
These challenges motivate the need for methods that can improve radar
resolution without relying on paired high-resolution ground-truth data. Here,
we introduce an unsupervised radar points enhancement algorithm that employs an
arbitrary LiDAR-guided diffusion model as a prior without the need for paired
training data. Specifically, our approach formulates radar angle estimation
recovery as an inverse problem and incorporates prior knowledge through a
diffusion model with arbitrary LiDAR domain knowledge. Experimental results
demonstrate that our method attains high fidelity and low noise performance
compared to traditional regularization techniques. Additionally, compared to
paired training methods, it not only achieves comparable performance but also
offers improved generalization capability. To our knowledge, this is the first
approach that enhances radar points output by integrating prior knowledge via a
diffusion model rather than relying on paired training data. Our code is
available at https://github.com/yyxr75/RadarINV.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 15 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniSkill: Imitating Human Videos via Cross-Embodiment Skill
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanjung Kim, Jaehyun Kang, Hyolim Kang, Meedeum Cho, Seon Joo Kim, Youngwoon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mimicry is a fundamental learning mechanism in humans, enabling individuals
to learn new tasks by observing and imitating experts. However, applying this
ability to robots presents significant challenges due to the inherent
differences between human and robot embodiments in both their visual appearance
and physical capabilities. While previous methods bridge this gap using
cross-embodiment datasets with shared scenes and tasks, collecting such aligned
data between humans and robots at scale is not trivial. In this paper, we
propose UniSkill, a novel framework that learns embodiment-agnostic skill
representations from large-scale cross-embodiment video data without any
labels, enabling skills extracted from human video prompts to effectively
transfer to robot policies trained only on robot data. Our experiments in both
simulation and real-world environments show that our cross-embodiment skills
successfully guide robots in selecting appropriate actions, even with unseen
video prompts. The project website can be found at:
https://kimhanjung.github.io/UniSkill.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://kimhanjung.github.io/UniSkill/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Remote Manipulation of Multiple Objects with Airflow Field Using
  Model-Based Learning Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03254v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03254v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artur Kopitca, Shahriar Haeri, Quan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-contact manipulation is a promising methodology in robotics, offering a
wide range of scientific and industrial applications. Among the proposed
approaches, airflow stands out for its ability to project across considerable
distances and its flexibility in actuating objects of varying materials, sizes,
and shapes. However, predicting airflow fields at a distance-and the motion of
objects within them-remains notoriously challenging due to their nonlinear and
stochastic nature. Here, we propose a model-based learning approach using a
jet-induced airflow field for remote multi-object manipulation on a surface.
Our approach incorporates an analytical model of the field, learned object
dynamics, and a model-based controller. The model predicts an air velocity
field over an infinite surface for a specified jet orientation, while the
object dynamics are learned through a robust system identification algorithm.
Using the model-based controller, we can automatically and remotely, at
meter-scale distances, control the motion of single and multiple objects for
different tasks, such as path-following, aggregating, and sorting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embodied Intelligent Industrial Robotics: Concepts and Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoran Zhang, Chenhao Zhang, Zhaobo Xu, Qinghongbing Xie, Pingfa Feng, Long Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, embodied intelligent robotics (EIR) has made significant
progress in multi-modal perception, autonomous decision-making, and physical
interaction. Some robots have already been tested in general-purpose scenarios
such as homes and shopping malls. We aim to advance the research and
application of embodied intelligence in industrial scenes. However, current EIR
lacks a deep understanding of industrial environment semantics and the
normative constraints between industrial operating objects. To address this
gap, this paper first reviews the history of industrial robotics and the
mainstream EIR frameworks. We then introduce the concept of the embodied
intelligent industrial robotics (EIIR) and propose a knowledge-driven EIIR
technology framework for industrial environments. The framework includes four
main modules: world model, high-level task planner, low-level skill controller,
and simulator. We also review the current development of technologies related
to each module and highlight recent progress in adapting them to industrial
applications. Finally, we summarize the key challenges EIIR faces in industrial
scenarios and suggest future research directions. We believe that EIIR
technology will shape the next generation of industrial robotics. Industrial
systems based on embodied intelligent industrial robots offer strong potential
for enabling intelligent manufacturing. We will continue to track and summarize
new research in this area and hope this review will serve as a valuable
reference for scholars and engineers interested in industrial embodied
intelligence. Together, we can help drive the rapid advancement and application
of this technology. The associated project can be found at
https://github.com/jackyzengl/EIIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>60 pages, 11 figures. The associated project can be found at
  https://github.com/jackyzengl/EIIR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Action <span class="highlight-title">Pretrain</span>ing from Videos <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, Lars Liden, Kimin Lee, Jianfeng Gao, Luke Zettlemoyer, Dieter Fox, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Latent Action Pretraining for general Action models (LAPA), an
unsupervised method for pretraining Vision-Language-Action (VLA) models without
ground-truth robot action labels. Existing Vision-Language-Action models
require action labels typically collected by human teleoperators during
pretraining, which significantly limits possible data sources and scale. In
this work, we propose a method to learn from internet-scale videos that do not
have robot action labels. We first train an action quantization model
leveraging VQ-VAE-based objective to learn discrete latent actions between
image frames, then pretrain a latent VLA model to predict these latent actions
from observations and task descriptions, and finally finetune the VLA on
small-scale robot manipulation data to map from latent to robot actions.
Experimental results demonstrate that our method significantly outperforms
existing techniques that train robot manipulation policies from large-scale
videos. Furthermore, it outperforms the state-of-the-art VLA model trained with
robotic action labels on real-world manipulation tasks that require language
conditioning, generalization to unseen objects, and semantic generalization to
unseen instructions. Training only on human manipulation videos also shows
positive transfer, opening up the potential for leveraging web-scale data for
robotics foundation model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Website: https://latentactionpretraining.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM A*: Human in the Loop Large Language Models Enabled A* Search for
  Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01797v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01797v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengjia Xiao, Peng Wang, Mingzhe Yu, Mattia Robbiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research focuses on how Large Language Models (LLMs) can help with
(path) planning for mobile embodied agents such as robots, in a
human-in-the-loop and interactive manner. A novel framework named LLM A*, aims
to leverage the commonsense of LLMs, and the utility-optimal A* is proposed to
facilitate few-shot near-optimal path planning. Prompts are used for two main
purposes: 1) to provide LLMs with essential information like environments,
costs, heuristics, etc.; 2) to communicate human feedback on intermediate
planning results to LLMs. This approach takes human feedback on board and
renders the entire planning process transparent (akin to a `white box') to
humans. Moreover, it facilitates code-free path planning, thereby fostering the
accessibility and inclusiveness of artificial intelligence techniques to
communities less proficient in coding. Comparative analysis against A* and RL
demonstrates that LLM A* exhibits greater efficiency in terms of search space
and achieves paths comparable to A* while outperforming RL. The interactive
nature of LLM A* also makes it a promising tool for deployment in collaborative
human-robot tasks. Codes and Supplemental Materials can be found at GitHub:
https://github.com/speedhawk/LLM-A-.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 figures, 8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniVLA: Learning to Act Anywhere with Task-centric Latent Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06111v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06111v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, Hongyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A generalist robot should perform effectively across various environments.
However, most existing approaches heavily rely on scaling action-annotated data
to enhance their capabilities. Consequently, they are often limited to single
physical specification and struggle to learn transferable knowledge across
different embodiments and environments. To confront these limitations, we
propose UniVLA, a new framework for learning cross-embodiment
vision-language-action (VLA) policies. Our key innovation is to derive
task-centric action representations from videos with a latent action model.
This enables us to exploit extensive data across a wide spectrum of embodiments
and perspectives. To mitigate the effect of task-irrelevant dynamics, we
incorporate language instructions and establish a latent action model within
the DINO feature space. Learned from internet-scale videos, the generalist
policy can be deployed to various robots through efficient latent action
decoding. We obtain state-of-the-art results across multiple manipulation and
navigation benchmarks, as well as real-robot deployments. UniVLA achieves
superior performance over OpenVLA with less than 1/20 of pretraining compute
and 1/10 of downstream data. Continuous performance improvements are observed
as heterogeneous data, even including human videos, are incorporated into the
training pipeline. The results underscore UniVLA's potential to facilitate
scalable and efficient robot policy learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to RSS 2025. Code is available at
  https://github.com/OpenDriveLab/UniVLA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decentralized Nonlinear Model Predictive Control-Based Flock Navigation
  with Real-Time Obstacle Avoidance in Unknown Obstructed Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuthasith Gerdpratoom, Kaoru Yamamoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work extends our prior work on the distributed nonlinear model
predictive control (NMPC) for navigating a robot fleet following a certain
flocking behavior in unknown obstructed environments with a more realistic
local obstacle avoidance strategy. More specifically, we integrate the local
obstacle avoidance constraint using point clouds into the NMPC framework. Here,
each agent relies on data from its local sensor to perceive and respond to
nearby obstacles. A point cloud processing technique is presented for both
two-dimensional and three-dimensional point clouds to minimize the
computational burden during the optimization. The process consists of
directional filtering and down-sampling that significantly reduce the number of
data points. The algorithm's performance is validated through realistic 3D
simulations in Gazebo, and its practical feasibility is further explored via
hardware-in-the-loop (HIL) simulations on embedded platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 14 figures, to be published in Frontiers in Robotics and AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Triplane <span class="highlight-title">Transformer</span>s as Occupancy World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Xu, Peixi Peng, Guang Tan, Yiqian Chang, Yisen Zhao, Yonghong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models aim to learn or construct representations of the environment
that enable the prediction of future scenes, thereby supporting intelligent
motion planning. However, existing models often struggle to produce
fine-grained predictions and to operate in real time. In this work, we propose
T$^3$Former, a novel 4D occupancy world model for autonomous driving.
T$^3$Former begins by pre-training a compact {\em triplane} representation that
efficiently encodes 3D occupancy. It then extracts multi-scale temporal motion
features from historical triplanes and employs an autoregressive approach to
iteratively predict future triplane changes. Finally, these triplane changes
are combined with previous states to decode future occupancy and ego-motion
trajectories. Experimental results show that T$^3$Former achieves 1.44$\times$
speedup (26 FPS), improves mean IoU to 36.09, and reduces mean absolute
planning error to 1.0 meters. Demos are available in the supplementary
material.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Achref Doula, Max Mühlhäuser, Alejandro Sanchez Guinea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) show growing promise in autonomous driving by
reasoning over complex traffic scenarios to generate path plans. However, their
tendencies toward overconfidence, and hallucinations raise critical safety
concerns. We introduce SafePath, a modular framework that augments LLM-based
path planning with formal safety guarantees using conformal prediction.
SafePath operates in three stages. In the first stage, we use an LLM that
generates a set of diverse candidate paths, exploring possible trajectories
based on agent behaviors and environmental cues. In the second stage, SafePath
filters out high-risk trajectories while guaranteeing that at least one safe
option is included with a user-defined probability, through a multiple-choice
question-answering formulation that integrates conformal prediction. In the
final stage, our approach selects the path with the lowest expected collision
risk when uncertainty is low or delegates control to a human when uncertainty
is high. We theoretically prove that SafePath guarantees a safe trajectory with
a user-defined probability, and we show how its human delegation rate can be
tuned to balance autonomy and safety. Extensive experiments on nuScenes and
Highway-env show that SafePath reduces planning uncertainty by 77\% and
collision rates by up to 70\%, demonstrating effectiveness in making LLM-driven
path planning more safer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-layer Motion Planning with Kinodynamic and Spatio-Temporal
  Constraints <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07762v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07762v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeel Chatrola, Abhiroop Ajith, Kevin Leahy, Constantinos Chamzas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel, multi-layered planning approach for computing paths that
satisfy both kinodynamic and spatiotemporal constraints. Our three-part
framework first establishes potential sequences to meet spatial constraints,
using them to calculate a geometric lead path. This path then guides an
asymptotically optimal sampling-based kinodynamic planner, which minimizes an
STL-robustness cost to jointly satisfy spatiotemporal and kinodynamic
constraints. In our experiments, we test our method with a velocity-controlled
Ackerman-car model and demonstrate significant efficiency gains compared to
prior art. Additionally, our method is able to generate complex path maneuvers,
such as crossovers, something that previous methods had not demonstrated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Hybrid Systems: Computation and Control (HSCC) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VGC-RIO: A Tightly Integrated Radar-Inertial Odometry with Spatial
  Weighted Doppler Velocity and Local Geometric Constrained RCS Histograms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09103v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09103v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianguang Xiang, Xiaofeng He, Zizhuo Chen, Lilian Zhang, Xincan Luo, Jun Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in 4D radar-inertial odometry have demonstrated promising
potential for autonomous lo calization in adverse conditions. However,
effective handling of sparse and noisy radar measurements remains a critical
challenge. In this paper, we propose a radar-inertial odometry with a spatial
weighting method that adapts to unevenly distributed points and a novel
point-description histogram for challenging point registration. To make full
use of the Doppler velocity from different spatial sections, we propose a
weighting calculation model. To enhance the point cloud registration
performance under challenging scenarios, we con struct a novel point histogram
descriptor that combines local geometric features and radar cross-section (RCS)
features. We have also conducted extensive experiments on both public and
self-constructed datasets. The results demonstrate the precision and robustness
of the proposed VGC-RIO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in
  Open Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.08604v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.08604v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongping Li, Tielong Cai, Tianci Tang, Wenhao Chai, Katherine Rose Driggs-Campbell, Gaoang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing autonomous home robots controlled by natural language has long
been a pursuit of humanity. While advancements in large language models (LLMs)
and embodied intelligence make this goal closer, several challenges persist:
the lack of a unified benchmark for more complex robot tasks, limited
evaluation methods and metrics, data incompatibility between LLMs and mobile
manipulation trajectories. To address these issues, we propose Embodied Mobile
Manipulation in Open Environments (EMMOE), a benchmark that requires agents to
interpret user instructions and execute long-horizon everyday tasks in
continuous space. EMMOE seamlessly integrates high-level and low-level embodied
tasks into a unified framework, along with three new metrics for more diverse
assessment. Additionally, we collect~\dataset, which features in various task
attributes, detailed process annotations, re-plans after failures, and two
sub-datasets for LLM training. Furthermore, we design~\model, a sophisticated
agent system consists of LLM with Direct Preference Optimization (DPO), light
weighted navigation and manipulation models, and multiple error detection
mechanisms. Finally, we demonstrate~\model's performance and evaluations of
different models and policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prithwish Dan, Kushal Kedia, Angela Chao, Edward Weiyi Duan, Maximus Adrian Pace, Wei-Chiu Ma, Sanjiban Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human videos offer a scalable way to train robot manipulation policies, but
lack the action labels needed by standard imitation learning algorithms.
Existing cross-embodiment approaches try to map human motion to robot actions,
but often fail when the embodiments differ significantly. We propose X-Sim, a
real-to-sim-to-real framework that uses object motion as a dense and
transferable signal for learning robot policies. X-Sim starts by reconstructing
a photorealistic simulation from an RGBD human video and tracking object
trajectories to define object-centric rewards. These rewards are used to train
a reinforcement learning (RL) policy in simulation. The learned policy is then
distilled into an image-conditioned diffusion policy using synthetic rollouts
rendered with varied viewpoints and lighting. To transfer to the real world,
X-Sim introduces an online domain adaptation technique that aligns real and
simulated observations during deployment. Importantly, X-Sim does not require
any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2
environments and show that it: (1) improves task progress by 30% on average
over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with
10x less data collection time, and (3) generalizes to new camera viewpoints and
test-time changes. Code and videos are available at
https://portal-cornell.github.io/X-Sim/.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Thermodynamic Laws for Large Language Model Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Liu, Yizhou Liu, Jeff Gore, Max Tegmark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beyond neural scaling laws, little is known about the laws underlying large
language models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new
framework that offers fresh insights into LLM training dynamics. On the
theoretical side, we demonstrate that key thermodynamic quantities (e.g.,
temperature, entropy, heat capacity, thermal conduction) and classical
thermodynamic principles (e.g., the three laws of thermodynamics and the
equipartition theorem) naturally emerge under river-valley loss landscape
assumptions. On the practical side, this scientific perspective yields
intuitive guidelines for designing learning rate schedules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal
  Mathematical Reasoning <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language image-caption datasets, widely used for training Large
Multimodal Models, mainly focus on natural scenarios and overlook the intricate
details of mathematical figures that are critical for problem-solving,
hindering the advancement of current LMMs in multimodal mathematical reasoning.
To this end, we propose leveraging code as supervision for cross-modal
alignment, since code inherently encodes all information needed to generate
corresponding figures, establishing a precise connection between the two
modalities. Specifically, we co-develop our image-to-code model and dataset
with model-in-the-loop approach, resulting in an image-to-code model,
FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.
Furthermore, we utilize FigCodifier to synthesize novel mathematical figures
and then construct MM-MathInstruct-3M, a high-quality multimodal math
instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with
ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on
MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a
new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and
Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista,
achieving improvements of 8.9% and 9.2%. The dataset and models will be
released at https://github.com/mathllm/MathCoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Feasibility Matter? Understanding the Impact of Feasibility on
  Synthetic Training Data <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Liu, Jessica Bader, Jae Myung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of photorealistic diffusion models, models trained in
part or fully on synthetic data achieve progressively better results. However,
diffusion models still routinely generate images that would not exist in
reality, such as a dog floating above the ground or with unrealistic texture
artifacts. We define the concept of feasibility as whether attributes in a
synthetic image could realistically exist in the real-world domain; synthetic
images containing attributes that violate this criterion are considered
infeasible. Intuitively, infeasible images are typically considered
out-of-distribution; thus, training on such images is expected to hinder a
model's ability to generalize to real-world data, and they should therefore be
excluded from the training set whenever possible. However, does feasibility
really matter? In this paper, we investigate whether enforcing feasibility is
necessary when generating synthetic training data for CLIP-based classifiers,
focusing on three target attributes: background, color, and texture. We
introduce VariReal, a pipeline that minimally edits a given source image to
include feasible or infeasible attributes given by the textual prompt generated
by a large language model. Our experiments show that feasibility minimally
affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference
in top-1 accuracy across three fine-grained datasets. Also, the attribute
matters on whether the feasible/infeasible images adversarially influence the
classification performance. Finally, mixing feasible and infeasible images in
training datasets does not significantly impact performance compared to using
purely feasible or infeasible datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPRW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Out-of-Distribution Failure Prevention via Multi-Modal
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milan Ganai, Rohan Sinha, Christopher Agia, Daniel Morton, Marco Pavone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models can provide robust high-level reasoning on appropriate
safety interventions in hazardous scenarios beyond a robot's training data,
i.e. out-of-distribution (OOD) failures. However, due to the high inference
latency of Large Vision and Language Models, current methods rely on manually
defined intervention policies to enact fallbacks, thereby lacking the ability
to plan generalizable, semantically safe motions. To overcome these challenges
we present FORTRESS, a framework that generates and reasons about semantically
safe fallback strategies in real time to prevent OOD failures. At a low
frequency in nominal operations, FORTRESS uses multi-modal reasoners to
identify goals and anticipate failure modes. When a runtime monitor triggers a
fallback response, FORTRESS rapidly synthesizes plans to fallback goals while
inferring and avoiding semantically unsafe regions in real time. By bridging
open-world, multi-modal reasoning with dynamics-aware planning, we eliminate
the need for hard-coded fallbacks and human safety interventions. FORTRESS
outperforms on-the-fly prompting of slow reasoning models in safety
classification accuracy on synthetic benchmarks and real-world ANYmal robot
data, and further improves system safety and planning success in simulation and
on quadrotor hardware for urban navigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://milanganai.github.io/fortress/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Deeper Understanding of Reasoning Capabilities in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annie Wong, Thomas Bäck, Aske Plaat, Niki van Stein, Anna V. Kononova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models demonstrate impressive performance on static
benchmarks, the true potential of large language models as self-learning and
reasoning agents in dynamic environments remains unclear. This study
systematically evaluates the efficacy of self-reflection, heuristic mutation,
and planning as prompting techniques to test the adaptive capabilities of
agents. We conduct experiments with various open-source language models in
dynamic environments and find that larger models generally outperform smaller
ones, but that strategic prompting can close this performance gap. Second, a
too-long prompt can negatively impact smaller models on basic reactive tasks,
while larger models show more robust behaviour. Third, advanced prompting
techniques primarily benefit smaller models on complex games, but offer less
improvement for already high-performing large language models. Yet, we find
that advanced reasoning methods yield highly variable outcomes: while capable
of significantly improving performance when reasoning and decision-making
align, they also introduce instability and can lead to big performance drops.
Compared to human performance, our findings reveal little evidence of true
emergent reasoning. Instead, large language model performance exhibits
persistent limitations in crucial areas such as planning, reasoning, and
spatial coordination, suggesting that current-generation large language models
still suffer fundamental shortcomings that may not be fully overcome through
self-reflective prompting alone. Reasoning is a multi-faceted task, and while
reasoning methods like Chain of thought improves multi-step reasoning on math
word problems, our findings using dynamic benchmarks highlight important
shortcomings in general reasoning capabilities, indicating a need to move
beyond static benchmarks to capture the complexity of reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Olimpieri, Noemi Giustini, Andrea Lacava, Salvatore D'Oro, Tommaso Melodia, Francesca Cuomo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The O-RAN architecture is transforming cellular networks by adopting RAN
softwarization and disaggregation concepts to enable data-driven monitoring and
control of the network. Such management is enabled by RICs, which facilitate
near-real-time and non-real-time network control through xApps and rApps.
However, they face limitations, including latency overhead in data exchange
between the RAN and RIC, restricting real-time monitoring, and the inability to
access user plain data due to privacy and security constraints, hindering use
cases like beamforming and spectrum classification. In this paper, we leverage
the dApps concept to enable real-time RF spectrum classification with LibIQ, a
novel library for RF signals that facilitates efficient spectrum monitoring and
signal classification by providing functionalities to read I/Q samples as
time-series, create datasets and visualize time-series data through plots and
spectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to
detect external RF signals, which are subsequently classified using a CNN
inside the library. To achieve accurate spectrum analysis, we created an
extensive dataset of time-series-based I/Q samples, representing distinct
signal types captured using a custom dApp running on a 5G deployment over the
Colosseum network emulator and an OTA testbed. We evaluate our model by
deploying LibIQ in heterogeneous scenarios with varying center frequencies,
time windows, and external RF signals. In real-time analysis, the model
classifies the processed I/Q samples, achieving an average accuracy of
approximately 97.8\% in identifying signal types across all scenarios. We
pledge to release both LibIQ and the dataset created as a publicly available
framework upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge capture, adaptation and composition (KCAC): A framework for
  cross-task curriculum learning in robotic manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinrui Wang, Yan Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has demonstrated remarkable potential in robotic
manipulation but faces challenges in sample inefficiency and lack of
interpretability, limiting its applicability in real world scenarios. Enabling
the agent to gain a deeper understanding and adapt more efficiently to diverse
working scenarios is crucial, and strategic knowledge utilization is a key
factor in this process. This paper proposes a Knowledge Capture, Adaptation,
and Composition (KCAC) framework to systematically integrate knowledge transfer
into RL through cross-task curriculum learning. KCAC is evaluated using a two
block stacking task in the CausalWorld benchmark, a complex robotic
manipulation environment. To our knowledge, existing RL approaches fail to
solve this task effectively, reflecting deficiencies in knowledge capture. In
this work, we redesign the benchmark reward function by removing rigid
constraints and strict ordering, allowing the agent to maximize total rewards
concurrently and enabling flexible task completion. Furthermore, we define two
self-designed sub-tasks and implement a structured cross-task curriculum to
facilitate efficient learning. As a result, our KCAC approach achieves a 40
percent reduction in training time while improving task success rates by 10
percent compared to traditional RL methods. Through extensive evaluation, we
identify key curriculum design parameters subtask selection, transition timing,
and learning rate that optimize learning efficiency and provide conceptual
guidance for curriculum based RL frameworks. This work offers valuable insights
into curriculum design in RL and robotic learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Token Prediction Needs Registers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasios Gerontopoulos, Spyros Gidaris, Nikos Komodakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-token prediction has emerged as a promising objective for improving
language model pretraining, but its benefits have not consistently generalized
to other settings such as fine-tuning. In this paper, we propose MuToR, a
simple and effective approach to multi-token prediction that interleaves
learnable register tokens into the input sequence, each tasked with predicting
future targets. Compared to existing methods, MuToR offers several key
advantages: it introduces only a negligible number of additional parameters,
requires no architectural changes--ensuring compatibility with off-the-shelf
pretrained language models--and remains aligned with the next-token pretraining
objective, making it especially well-suited for supervised fine-tuning.
Moreover, it naturally supports scalable prediction horizons. We demonstrate
the effectiveness and versatility of MuToR across a range of use cases,
including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and
pretraining, on challenging generative tasks in both language and vision
domains. Our code will be available at: https://github.com/nasosger/MuToR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PnPXAI: A Universal XAI Framework Providing Automatic Explanations
  Across Diverse Modalities and Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongun Kim, Sol A Kim, Geonhyeong Kim, Enver Menadjiev, Chanwoo Lee, Seongwook Chung, Nari Kim, Jaesik Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, post hoc explanation methods have emerged to enhance model
transparency by attributing model outputs to input features. However, these
methods face challenges due to their specificity to certain neural network
architectures and data modalities. Existing explainable artificial intelligence
(XAI) frameworks have attempted to address these challenges but suffer from
several limitations. These include limited flexibility to diverse model
architectures and data modalities due to hard-coded implementations, a
restricted number of supported XAI methods because of the requirements for
layer-specific operations of attribution methods, and sub-optimal
recommendations of explanations due to the lack of evaluation and optimization
phases. Consequently, these limitations impede the adoption of XAI technology
in real-world applications, making it difficult for practitioners to select the
optimal explanation method for their domain. To address these limitations, we
introduce \textbf{PnPXAI}, a universal XAI framework that supports diverse data
modalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI
automatically detects model architectures, recommends applicable explanation
methods, and optimizes hyperparameters for optimal explanations. We validate
the framework's effectiveness through user surveys and showcase its versatility
across various domains, including medicine and finance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniEval: Unified Holistic Evaluation for Unified Multimodal
  Understanding and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Li, Haonan Wang, Qixiang Zhang, Boyu Xiao, Chenchang Hu, Hualiang Wang, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of unified multimodal understanding and generation models is
rapidly attracting attention because of their ability to enhance
instruction-following capabilities while minimizing model redundancy. However,
there is a lack of a unified evaluation framework for these models, which would
enable an elegant, simplified, and overall evaluation. Current models conduct
evaluations on multiple task-specific benchmarks, but there are significant
limitations, such as the lack of overall results, errors from extra evaluation
models, reliance on extensive labeled images, benchmarks that lack diversity,
and metrics with limited capacity for instruction-following evaluation. To
tackle these challenges, we introduce UniEval, the first evaluation framework
designed for unified multimodal models without extra models, images, or
annotations. This facilitates a simplified and unified evaluation process. The
UniEval framework contains a holistic benchmark, UniBench (supports both
unified and visual generation models), along with the corresponding UniScore
metric. UniBench includes 81 fine-grained tags contributing to high diversity.
Experimental results indicate that UniBench is more challenging than existing
benchmarks, and UniScore aligns closely with human evaluations, surpassing
current metrics. Moreover, we extensively evaluated SoTA unified and visual
generation models, uncovering new insights into Univeral's unique values.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>UniEval is the first evaluation framework designed for unified
  multimodal models, including a holistic benchmark UniBench and the UniScore
  metric</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-tuning Diffusion Policies with Backpropagation Through Diffusion
  Timesteps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningyuan Yang, Jiaxuan Gao, Feng Gao, Yi Wu, Chao Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion policies, widely adopted in decision-making scenarios such as
robotics, gaming and autonomous driving, are capable of learning diverse skills
from demonstration data due to their high representation power. However, the
sub-optimal and limited coverage of demonstration data could lead to diffusion
policies that generate sub-optimal trajectories and even catastrophic failures.
While reinforcement learning (RL)-based fine-tuning has emerged as a promising
solution to address these limitations, existing approaches struggle to
effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This
challenge stems from the computational intractability of action likelihood
estimation during the denoising process, which leads to complicated
optimization objectives. In our experiments starting from randomly initialized
policies, we find that online tuning of Diffusion Policies demonstrates much
lower sample efficiency compared to directly applying PPO on MLP policies
(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework
that reformulates Diffusion Policy as a noise-conditioned deterministic policy.
By treating each denoising step as a differentiable transformation conditioned
on pre-sampled noise, NCDPO enables tractable likelihood evaluation and
gradient backpropagation through all diffusion timesteps. Our experiments
demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when
training from scratch, outperforming existing methods in both sample efficiency
and final performance across diverse benchmarks, including continuous robot
control and multi-agent game scenarios. Furthermore, our experimental results
show that our method is robust to the number denoising timesteps in the
Diffusion Policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages for main text, 23 pages in total, submitted to Neurips, 13
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and
  Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ranjan Sapkota, Konstantinos I. Roumeliotis, Manoj Karkee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study critically distinguishes between AI Agents and Agentic AI,
offering a structured conceptual taxonomy, application mapping, and challenge
analysis to clarify their divergent design philosophies and capabilities. We
begin by outlining the search strategy and foundational definitions,
characterizing AI Agents as modular systems driven by Large Language Models
(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.
Generative AI is positioned as a precursor, with AI Agents advancing through
tool integration, prompt engineering, and reasoning enhancements. In contrast,
Agentic AI systems represent a paradigmatic shift marked by multi-agent
collaboration, dynamic task decomposition, persistent memory, and orchestrated
autonomy. Through a sequential evaluation of architectural evolution,
operational mechanisms, interaction styles, and autonomy levels, we present a
comparative analysis across both paradigms. Application domains such as
customer support, scheduling, and data summarization are contrasted with
Agentic AI deployments in research automation, robotic coordination, and
medical decision support. We further examine unique challenges in each paradigm
including hallucination, brittleness, emergent behavior, and coordination
failure and propose targeted solutions such as ReAct loops, RAG, orchestration
layers, and causal modeling. This work aims to provide a definitive roadmap for
developing robust, scalable, and explainable AI agent and Agentic AI-driven
systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision
Support System, Agentic-AI Applications
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 14 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superposition Yields Robust Neural Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou liu, Ziming Liu, Jeff Gore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of today's large language models (LLMs) depends on the
observation that larger models perform better. However, the origin of this
neural scaling law -- the finding that loss decreases as a power law with model
size -- remains unclear. Starting from two empirical principles -- that LLMs
represent more things than the model dimensions (widths) they have (i.e.,
representations are superposed), and that words or concepts in language occur
with varying frequencies -- we constructed a toy model to study the loss
scaling with model size. We found that when superposition is weak, meaning only
the most frequent features are represented without interference, the scaling of
loss with model size depends on the underlying feature frequency; if feature
frequencies follow a power law, so does the loss. In contrast, under strong
superposition, where all features are represented but overlap with each other,
the loss becomes inversely proportional to the model dimension across a wide
range of feature frequency distributions. This robust scaling behavior is
explained geometrically: when many more vectors are packed into a lower
dimensional space, the interference (squared overlaps) between vectors scales
inversely with that dimension. We then analyzed four families of open-sourced
LLMs and found that they exhibit strong superposition and quantitatively match
the predictions of our toy model. The Chinchilla scaling law turned out to also
agree with our results. We conclude that representation superposition is an
important mechanism underlying the observed neural scaling laws. We anticipate
that these insights will inspire new training strategies and model
architectures to achieve better performance with less computation and fewer
parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 23 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEAL: Searching Expandable Architectures for Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Gambella, Vicente Javier Castro Solar, Manuel Roveri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incremental learning is a machine learning paradigm where a model learns from
a sequential stream of tasks. This setting poses a key challenge: balancing
plasticity (learning new tasks) and stability (preserving past knowledge).
Neural Architecture Search (NAS), a branch of AutoML, automates the design of
the architecture of Deep Neural Networks and has shown success in static
settings. However, existing NAS-based approaches to incremental learning often
rely on expanding the model at every task, making them impractical in
resource-constrained environments. In this work, we introduce SEAL, a NAS-based
framework tailored for data-incremental learning, a scenario where disjoint
data samples arrive sequentially and are not stored for future access. SEAL
adapts the model structure dynamically by expanding it only when necessary,
based on a capacity estimation metric. Stability is preserved through
cross-distillation training after each expansion step. The NAS component
jointly searches for both the architecture and the optimal expansion policy.
Experiments across multiple benchmarks demonstrate that SEAL effectively
reduces forgetting and enhances accuracy while maintaining a lower model size
compared to prior methods. These results highlight the promise of combining NAS
and selective expansion for efficient, adaptive learning in incremental
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision language models have difficulty recognizing virtual objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Tran, Sangeet Khemlani, J. G. Trafton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision language models (VLMs) are AI systems paired with both language and
vision encoders to process multimodal input. They are capable of performing
complex semantic tasks such as automatic captioning, but it remains an open
question about how well they comprehend the visuospatial properties of scenes
depicted in the images they process. We argue that descriptions of virtual
objects -- objects that are not visually represented in an image -- can help
test scene comprehension in these AI systems. For example, an image that
depicts a person standing under a tree can be paired with the following prompt:
imagine that a kite is stuck in the tree. VLMs that comprehend the scene should
update their representations and reason sensibly about the spatial relations
between all three objects. We describe systematic evaluations of
state-of-the-art VLMs and show that their ability to process virtual objects is
inadequate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Large Language Models Robust in Understanding Code Against
  Semantics-Preserving Mutations? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Orvalho, Marta Kwiatkowska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the reasoning and robustness of Large Language Models (LLMs) is
critical for their reliable use in programming tasks. While recent studies have
assessed LLMs' ability to predict program outputs, most focus solely on the
accuracy of those predictions, without evaluating the reasoning behind them.
Moreover, it has been observed on mathematical reasoning tasks that LLMs can
arrive at correct answers through flawed logic, raising concerns about similar
issues in code understanding.
  In this work, we evaluate whether state-of-the-art LLMs with up to 8B
parameters can reason about Python programs or are simply guessing. We apply
five semantics-preserving code mutations: renaming variables, mirroring
comparison expressions, swapping if-else branches, converting for loops to
while, and loop unrolling. These mutations maintain program semantics while
altering its syntax. We evaluated six LLMs and performed a human expert
analysis using LiveCodeBench to assess whether the correct predictions are
based on sound reasoning. We also evaluated prediction stability across
different code mutations on LiveCodeBench and CruxEval. Our findings show that
some LLMs, such as Llama3.2, produce correct predictions based on flawed
reasoning in up to 61% of cases. Furthermore, LLMs often change predictions in
response to our code mutations, indicating limited robustness in their semantic
understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 tables, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy
  Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dechen Gao, Hang Wang, Hanchu Zhou, Nejib Ammar, Shatadal Mishra, Ahmadreza Moradipari, Iman Soltani, Junshan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning (IL) and reinforcement learning (RL) each offer distinct
advantages for robotics policy learning: IL provides stable learning from
demonstrations, and RL promotes generalization through exploration. While
existing robot learning approaches using IL-based pre-training followed by
RL-based fine-tuning are promising, this two-step learning paradigm often
suffers from instability and poor sample efficiency during the RL fine-tuning
phase. In this work, we introduce IN-RIL, INterleaved Reinforcement learning
and Imitation Learning, for policy fine-tuning, which periodically injects IL
updates after multiple RL updates and hence can benefit from the stability of
IL and the guidance of expert data for more efficient exploration throughout
the entire fine-tuning process. Since IL and RL involve different optimization
objectives, we develop gradient separation mechanisms to prevent destructive
interference during \ABBR fine-tuning, by separating possibly conflicting
gradient updates in orthogonal subspaces. Furthermore, we conduct rigorous
analysis, and our findings shed light on why interleaving IL with RL stabilizes
learning and improves sample-efficiency. Extensive experiments on 14 robot
manipulation and locomotion tasks across 3 benchmarks, including
FurnitureBench, OpenAI Gym, and Robomimic, demonstrate that \ABBR can
significantly improve sample efficiency and mitigate performance collapse
during online finetuning in both long- and short-horizon tasks with either
sparse or dense rewards. IN-RIL, as a general plug-in compatible with various
state-of-the-art RL algorithms, can significantly improve RL fine-tuning, e.g.,
from 12\% to 88\% with 6.3x improvement in the success rate on Robomimic
Transport. Project page: https://github.com/ucd-dare/IN-RIL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIF: Anomaly detection via preference embedding <span class="chip">ICPR
  2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Leveni, Luca Magri, Giacomo Boracchi, Cesare Alippi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of detecting anomalies with respect to structured
patterns. To this end, we conceive a novel anomaly detection method called PIF,
that combines the advantages of adaptive isolation methods with the flexibility
of preference embedding. Specifically, we propose to embed the data in a high
dimensional space where an efficient tree-based method, PI-Forest, is employed
to compute an anomaly score. Experiments on synthetic and real datasets
demonstrate that PIF favorably compares with state-of-the-art anomaly detection
techniques, and confirm that PI-Forest is better at measuring arbitrary
distances and isolate points in the preference space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Conference on Pattern Recognition (ICPR
  2020)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learned Lightweight Smartphone ISP with Unpaired Data <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Arhire, Radu Timofte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Image Signal Processor (ISP) is a fundamental component in modern
smartphone cameras responsible for conversion of RAW sensor image data to RGB
images with a strong focus on perceptual quality. Recent work highlights the
potential of deep learning approaches and their ability to capture details with
a quality increasingly close to that of professional cameras. A difficult and
costly step when developing a learned ISP is the acquisition of pixel-wise
aligned paired data that maps the raw captured by a smartphone camera sensor to
high-quality reference images. In this work, we address this challenge by
proposing a novel training method for a learnable ISP that eliminates the need
for direct correspondences between raw images and ground-truth data with
matching content. Our unpaired approach employs a multi-term loss function
guided by adversarial training with multiple discriminators processing feature
maps from pre-trained networks to maintain content structure while learning
color and texture characteristics from the target RGB dataset. Using
lightweight neural network architectures suitable for mobile devices as
backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm
UltraISP datasets. Compared to paired training methods, our unpaired learning
strategy shows strong potential and achieves high fidelity across multiple
evaluation metrics. The code and pre-trained models are available at
https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPRW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Fidelity Index for Generative Semantic Communications with
  Critical Information Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhao Huang, Qunsong Zeng, Kaibin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative semantic communication (Gen-SemCom) with large artificial
intelligence (AI) model promises a transformative paradigm for 6G networks,
which reduces communication costs by transmitting low-dimensional prompts
rather than raw data. However, purely prompt-driven generation loses
fine-grained visual details. Additionally, there is a lack of systematic
metrics to evaluate the performance of Gen-SemCom systems. To address these
issues, we develop a hybrid Gen-SemCom system with a critical information
embedding (CIE) framework, where both text prompts and semantically critical
features are extracted for transmissions. First, a novel approach of semantic
filtering is proposed to select and transmit the semantically critical features
of images relevant to semantic label. By integrating the text prompt and
critical features, the receiver reconstructs high-fidelity images using a
diffusion-based generative model. Next, we propose the generative visual
information fidelity (GVIF) metric to evaluate the visual quality of the
generated image. By characterizing the statistical models of image features,
the GVIF metric quantifies the mutual information between the distorted
features and their original counterparts. By maximizing the GVIF metric, we
design a channel-adaptive Gen-SemCom system that adaptively control the volume
of features and compression rate according to the channel state. Experimental
results validate the GVIF metric's sensitivity to visual fidelity, correlating
with both the PSNR and critical information volume. In addition, the optimized
system achieves superior performance over benchmarking schemes in terms of
higher PSNR and lower FID scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Repetition Problems of LLMs in Code Generation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Dong, Yuchen Liu, Xue Jiang, Zhi Jin, Ge Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of neural language models, the performance of code generation
has been significantly boosted. However, the problem of repetitions during the
generation process continues to linger. Previous work has primarily focused on
content repetition, which is merely a fraction of the broader repetition
problem in code generation. A more prevalent and challenging problem is
structural repetition. In structural repetition, the repeated code appears in
various patterns but possesses a fixed structure, which can be inherently
reflected in grammar. In this paper, we formally define structural repetition
and propose an efficient decoding approach called RPG, which stands for
Repetition Penalization based on Grammar, to alleviate the repetition problems
in code generation for LLMs. Specifically, RPG first leverages grammar rules to
identify repetition problems during code generation, and then strategically
decays the likelihood of critical tokens that contribute to repetitions,
thereby mitigating them in code generation. To facilitate this study, we
construct a new dataset CodeRepetEval to comprehensively evaluate approaches
for mitigating the repetition problems in code generation. Extensive
experimental results demonstrate that RPG substantially outperforms the
best-performing baselines on CodeRepetEval dataset as well as HumanEval and
MBPP benchmarks, effectively reducing repetitions and enhancing the quality of
generated code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 (main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Model Explanations without Ground Truth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaivalya Rawal, Zihao Fu, Eoin Delaney, Chris Russell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There can be many competing and contradictory explanations for a single model
prediction, making it difficult to select which one to use. Current explanation
evaluation frameworks measure quality by comparing against ideal "ground-truth"
explanations, or by verifying model sensitivity to important inputs. We outline
the limitations of these approaches, and propose three desirable principles to
ground the future development of explanation evaluation strategies for local
feature importance explanations. We propose a ground-truth Agnostic eXplanation
Evaluation framework (AXE) for evaluating and comparing model explanations that
satisfies these principles. Unlike prior approaches, AXE does not require
access to ideal ground-truth explanations for comparison, or rely on model
sensitivity - providing an independent measure of explanation quality. We
verify AXE by comparing with baselines, and show how it can be used to detect
explanation fairwashing. Our code is available at
https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inconsistency Handling in DatalogMTL <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meghyn Bienvenu, Camille Bourgaux, Atefe Khodadaditaghanaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the issue of inconsistency handling in DatalogMTL,
an extension of Datalog with metric temporal operators. Since facts are
associated with time intervals, there are different manners to restore
consistency when they contradict the rules, such as removing facts or modifying
their time intervals. Our first contribution is the definition of relevant
notions of conflicts (minimal explanations for inconsistency) and repairs
(possible ways of restoring consistency) for this setting and the study of the
properties of these notions and the associated inconsistency-tolerant
semantics. Our second contribution is a data complexity analysis of the tasks
of generating a single conflict / repair and query entailment under
repair-based semantics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an extended version of a paper appearing at the 34th
  International Joint Conference on Artificial Intelligence (IJCAI 2025). 24
  pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering Magnetic Phases with Synthetic Data and Physics-Informed
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agustin Medina, Marcelo Arlego, Carlos A. Lamas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the efficient learning of magnetic phases using artificial
neural networks trained on synthetic data, combining computational simplicity
with physics-informed strategies. Focusing on the diluted Ising model, which
lacks an exact analytical solution, we explore two complementary approaches: a
supervised classification using simple dense neural networks, and an
unsupervised detection of phase transitions using convolutional autoencoders
trained solely on idealized spin configurations.
  To enhance model performance, we incorporate two key forms of
physics-informed guidance. First, we exploit architectural biases which
preferentially amplify features related to symmetry breaking. Second, we
include training configurations that explicitly break $\mathbb{Z}_2$ symmetry,
reinforcing the network's ability to detect ordered phases. These mechanisms,
acting in tandem, increase the network's sensitivity to phase structure even in
the absence of explicit labels. We validate the machine learning predictions
through comparison with direct numerical estimates of critical temperatures and
percolation thresholds.
  Our results show that synthetic, structured, and computationally efficient
training schemes can reveal physically meaningful phase boundaries, even in
complex systems. This framework offers a low-cost and robust alternative to
conventional methods, with potential applications in broader condensed matter
and statistical physics contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Schreier-Coset Graph Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryan Mishra, Lizhen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) offer a principled framework for learning over
graph-structured data, yet their expressive capacity is often hindered by
over-squashing, wherein information from distant nodes is compressed into
fixed-size vectors. Existing solutions, including graph rewiring and
bottleneck-resistant architectures such as Cayley and expander graphs, avoid
this problem but introduce scalability bottlenecks. In particular, the Cayley
graphs constructed over $SL(2,\mathbb{Z}_n)$ exhibit strong theoretical
properties, yet suffer from cubic node growth $O(n^3)$, leading to high memory
usage. To address this, this work introduces Schrier-Coset Graph Propagation
(SCGP), a group-theoretic augmentation method that enriches node features
through Schreier-coset embeddings without altering the input graph topology.
SCGP embeds bottleneck-free connectivity patterns into a compact feature space,
improving long-range message passing while maintaining computational
efficiency. Empirical evaluations across standard node and graph classification
benchmarks demonstrate that SCGP achieves performance comparable to, or
exceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits
particular advantages in processing hierarchical and modular graph structures,
offering reduced inference latency, improved scalability, and a low memory
footprint, making it suitable for real-time and resource-constrained
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 1 figure , preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Path Finding For Large Agents Is Intractable 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Agafonov, Konstantin Yakovlev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multi-agent path finding (MAPF) problem asks to find a set of paths on a
graph such that when synchronously following these paths the agents never
encounter a conflict. In the most widespread MAPF formulation, the so-called
Classical MAPF, the agents sizes are neglected and two types of conflicts are
considered: occupying the same vertex or using the same edge at the same time
step. Meanwhile in numerous practical applications, e.g. in robotics, taking
into account the agents' sizes is vital to ensure that the MAPF solutions can
be safely executed. Introducing large agents yields an additional type of
conflict arising when one agent follows an edge and its body overlaps with the
body of another agent that is actually not using this same edge (e.g. staying
still at some distinct vertex of the graph). Until now it was not clear how
harder the problem gets when such conflicts are to be considered while
planning. Specifically, it was known that Classical MAPF problem on an
undirected graph can be solved in polynomial time, however no complete
polynomial-time algorithm was presented to solve MAPF with large agents. In
this paper we, for the first time, establish that the latter problem is NP-hard
and, thus, if P!=NP no polynomial algorithm for it can, unfortunately, be
presented. Our proof is based on the prevalent in the field technique of
reducing the seminal 3SAT problem (which is known to be an NP-complete problem)
to the problem at hand. In particular, for an arbitrary 3SAT formula we
procedurally construct a dedicated graph with specific start and goal vertices
and show that the given 3SAT formula is satisfiable iff the corresponding path
finding instance has a solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Sparse Autoencoders Useful for Java Function Bug Detection? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Melo, Claudia Mamede, Andre Catarino, Rui Abreu, Henrique Lopes Cardoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software vulnerabilities such as buffer overflows and SQL injections are a
major source of security breaches. Traditional methods for vulnerability
detection remain essential but are limited by high false positive rates,
scalability issues, and reliance on manual effort. These constraints have
driven interest in AI-based approaches to automated vulnerability detection and
secure code generation. While Large Language Models (LLMs) have opened new
avenues for classification tasks, their complexity and opacity pose challenges
for interpretability and deployment. Sparse Autoencoder offer a promising
solution to this problem. We explore whether SAEs can serve as a lightweight,
interpretable alternative for bug detection in Java functions. We evaluate the
effectiveness of SAEs when applied to representations from GPT-2 Small and
Gemma 2B, examining their capacity to highlight buggy behaviour without
fine-tuning the underlying LLMs. We found that SAE-derived features enable bug
detection with an F1 score of up to 89%, consistently outperforming fine-tuned
transformer encoder baselines. Our work provides the first empirical evidence
that SAEs can be used to detect software bugs directly from the internal
representations of pretrained LLMs, without any fine-tuning or task-specific
supervision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ILIF: Temporal Inhibitory Leaky Integrate-and-Fire Neuron for
  Overactivation in Spiking Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Sun, Peibo Duan, Levin Kuhlmann, Beilun Wang, Bin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Spiking Neural Network (SNN) has drawn increasing attention for its
energy-efficient, event-driven processing and biological plausibility. To train
SNNs via backpropagation, surrogate gradients are used to approximate the
non-differentiable spike function, but they only maintain nonzero derivatives
within a narrow range of membrane potentials near the firing threshold,
referred to as the surrogate gradient support width gamma. We identify a major
challenge, termed the dilemma of gamma: a relatively large gamma leads to
overactivation, characterized by excessive neuron firing, which in turn
increases energy consumption, whereas a small gamma causes vanishing gradients
and weakens temporal dependencies. To address this, we propose a temporal
Inhibitory Leaky Integrate-and-Fire (ILIF) neuron model, inspired by biological
inhibitory mechanisms. This model incorporates interconnected inhibitory units
for membrane potential and current, effectively mitigating overactivation while
preserving gradient propagation. Theoretical analysis demonstrates ILIF
effectiveness in overcoming the gamma dilemma, and extensive experiments on
multiple datasets show that ILIF improves energy efficiency by reducing firing
rates, stabilizes training, and enhances accuracy. The code is available at
github.com/kaisun1/ILIF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plasticity as the Mirror of Empowerment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Abel, Michael Bowling, André Barreto, Will Dabney, Shi Dong, Steven Hansen, Anna Harutyunyan, Khimya Khetarpal, Clare Lyle, Razvan Pascanu, Georgios Piliouras, Doina Precup, Jonathan Richens, Mark Rowland, Tom Schaul, Satinder Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents are minimally entities that are influenced by their past observations
and act to influence future observations. This latter capacity is captured by
empowerment, which has served as a vital framing concept across artificial
intelligence and cognitive science. This former capacity, however, is equally
foundational: In what ways, and to what extent, can an agent be influenced by
what it observes? In this paper, we ground this concept in a universal
agent-centric measure that we refer to as plasticity, and reveal a fundamental
connection to empowerment. Following a set of desiderata on a suitable
definition, we define plasticity using a new information-theoretic quantity we
call the generalized directed information. We show that this new quantity
strictly generalizes the directed information introduced by Massey (1990) while
preserving all of its desirable properties. Our first finding is that
plasticity is the mirror of empowerment: The agent's plasticity is identical to
the empowerment of the environment, and vice versa. Our second finding
establishes a tension between the plasticity and empowerment of an agent,
suggesting that agent design needs to be mindful of both characteristics. We
explore the implications of these findings, and suggest that plasticity,
empowerment, and their relationship are essential to understanding agency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FactsR: A Safer Method for Producing High Quality Healthcare
  Documentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Petrén Bach Hansen, Lasse Krogsbøll, Jonas Lyngsø, Mathias Baltzersen, Andreas Motzfeldt, Kevin Pelgrims, Lars Maaløe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are now a multitude of AI-scribing solutions for healthcare promising
the utilization of large language models for ambient documentation. However,
these AI scribes still rely on one-shot, or few-shot prompts for generating
notes after the consultation has ended, employing little to no reasoning. This
risks long notes with an increase in hallucinations, misrepresentation of the
intent of the clinician, and reliance on the proofreading of the clinician to
catch errors. A dangerous combination for patient safety if vigilance is
compromised by workload and fatigue. In this paper, we introduce a method for
extracting salient clinical information in real-time alongside the healthcare
consultation, denoted Facts, and use that information recursively to generate
the final note. The FactsR method results in more accurate and concise notes by
placing the clinician-in-the-loop of note generation, while opening up new use
cases within real-time decision support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpikeVideoFormer: An Efficient Spike-Driven Video <span class="highlight-title">Transformer</span> with
  Hamming Attention and $\mathcal{O}(T)$ Complexity <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Zou, Qingfeng Li, Wei Ji, Jingjing Li, Yongkui Yang, Guoqi Li, Chao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) have shown competitive performance to
Artificial Neural Networks (ANNs) in various vision tasks, while offering
superior energy efficiency. However, existing SNN-based Transformers primarily
focus on single-image tasks, emphasizing spatial features while not effectively
leveraging SNNs' efficiency in video-based vision tasks. In this paper, we
introduce SpikeVideoFormer, an efficient spike-driven video Transformer,
featuring linear temporal complexity $\mathcal{O}(T)$. Specifically, we design
a spike-driven Hamming attention (SDHA) which provides a theoretically guided
adaptation from traditional real-valued attention to spike-driven attention.
Building on SDHA, we further analyze various spike-driven space-time attention
designs and identify an optimal scheme that delivers appealing performance for
video tasks, while maintaining only linear temporal complexity. The
generalization ability and efficiency of our model are demonstrated across
diverse downstream video tasks, including classification, human pose tracking,
and semantic segmentation. Empirical results show our method achieves
state-of-the-art (SOTA) performance compared to existing SNN approaches, with
over 15\% improvement on the latter two tasks. Additionally, it matches the
performance of recent ANN-based methods while offering significant efficiency
gains, achieving $\times 16$, $\times 10$ and $\times 5$ improvements on the
three tasks. https://github.com/JimmyZou/SpikeVideoFormer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uniform Loss vs. Specialized Optimization: A Comparative Analysis in
  Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel S. Gama, Valdir Grassi Jr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task
Learning by addressing issues like conflicting gradients and differing gradient
norms, which hinder equal-weighted task training. However, recent critiques
suggest that equally weighted tasks can achieve competitive results compared to
SMTOs, arguing that previous SMTO results were influenced by poor
hyperparameter optimization and lack of regularization. In this work, we
evaluate these claims through an extensive empirical evaluation of SMTOs,
including some of the latest methods, on more complex multi-task problems to
clarify this behavior. Our findings indicate that SMTOs perform well compared
to uniform loss and that fixed weights can achieve competitive performance
compared to SMTOs. Furthermore, we demonstrate why uniform loss perform
similarly to SMTOs in some instances. The code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emergence of Structure in Ensembles of Random Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Muscarnera, Luigi Loreti, Giovanni Todeschini, Alessio Fumagalli, Francesco Regazzoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Randomness is ubiquitous in many applications across data science and machine
learning. Remarkably, systems composed of random components often display
emergent global behaviors that appear deterministic, manifesting a transition
from microscopic disorder to macroscopic organization. In this work, we
introduce a theoretical model for studying the emergence of collective
behaviors in ensembles of random classifiers. We argue that, if the ensemble is
weighted through the Gibbs measure defined by adopting the classification loss
as an energy, then there exists a finite temperature parameter for the
distribution such that the classification is optimal, with respect to the loss
(or the energy). Interestingly, for the case in which samples are generated by
a Gaussian distribution and labels are constructed by employing a teacher
perceptron, we analytically prove and numerically confirm that such optimal
temperature does not depend neither on the teacher classifier (which is, by
construction of the learning problem, unknown), nor on the number of random
classifiers, highlighting the universal nature of the observed behavior.
Experiments on the MNIST dataset underline the relevance of this phenomenon in
high-quality, noiseless, datasets. Finally, a physical analogy allows us to
shed light on the self-organizing nature of the studied phenomenon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Adaptation of Reinforcement Learning Agents to Sudden
  Environmental Change 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Clifford Balloch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world autonomous decision-making systems, from robots to recommendation
engines, must operate in environments that change over time. While deep
reinforcement learning (RL) has shown an impressive ability to learn optimal
policies in stationary environments, most methods are data intensive and assume
a world that does not change between training and test time. As a result,
conventional RL methods struggle to adapt when conditions change. This poses a
fundamental challenge: how can RL agents efficiently adapt their behavior when
encountering novel environmental changes during deployment without
catastrophically forgetting useful prior knowledge? This dissertation
demonstrates that efficient online adaptation requires two key capabilities:
(1) prioritized exploration and sampling strategies that help identify and
learn from relevant experiences, and (2) selective preservation of prior
knowledge through structured representations that can be updated without
disruption to reusable components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD Dissertation, 131 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of SMT and MILP for the Nurse Rostering Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alvin Combrink, Stephie Do, Kristofer Bengtsson, Sabino Francesco Roselli, Martin Fabian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effects of personnel scheduling on the quality of care and working
conditions for healthcare personnel have been thoroughly documented. However,
the ever-present demand and large variation of constraints make healthcare
scheduling particularly challenging. This problem has been studied for decades,
with limited research aimed at applying Satisfiability Modulo Theories (SMT).
SMT has gained momentum within the formal verification community in the last
decades, leading to the advancement of SMT solvers that have been shown to
outperform standard mathematical programming techniques.
  In this work, we propose generic constraint formulations that can model a
wide range of real-world scheduling constraints. Then, the generic constraints
are formulated as SMT and MILP problems and used to compare the respective
state-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired
rostering problems. Experimental results show how each solver excels for
certain types of problems; the MILP solver generally performs better when the
problem is highly constrained or infeasible, while the SMT solver performs
better otherwise. On real-world inspired problems containing a more varied set
of shifts and personnel, the SMT solver excels. Additionally, it was noted
during experimentation that the SMT solver was more sensitive to the way the
generic constraints were formulated, requiring careful consideration and
experimentation to achieve better performance. We conclude that SMT-based
methods present a promising avenue for future research within the domain of
personnel scheduling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoPentest: Enhancing Vulnerability Management With Autonomous LLM
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius Henke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent area of increasing research is the use of Large Language Models
(LLMs) in penetration testing, which promises to reduce costs and thus allow
for higher frequency. We conduct a review of related work, identifying best
practices and common evaluation issues. We then present AutoPentest, an
application for performing black-box penetration tests with a high degree of
autonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent
framework LangChain. It can perform complex multi-step tasks, augmented by
external tools and knowledge bases. We conduct a study on three
capture-the-flag style Hack The Box (HTB) machines, comparing our
implementation AutoPentest with the baseline approach of manually using the
ChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the
subtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT.
We measure a total cost of \$96.20 US when using AutoPentest across all
experiments, while a one-month subscription to ChatGPT Plus costs \$20. The
results show that further implementation efforts and the use of more powerful
LLMs released in the future are likely to make this a viable part of
vulnerability management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 1 figure, for implementation, see
  https://github.com/JuliusHenke/autopentest</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, Swarnadeep Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The progress of AI is bottlenecked by the quality of evaluation, and powerful
LLM-as-a-Judge models have proved to be a core solution. Improved judgment
ability is enabled by stronger chain-of-thought reasoning, motivating the need
to find the best recipes for training such models to think. In this work we
introduce J1, a reinforcement learning approach to training such models. Our
method converts both verifiable and non-verifiable prompts to judgment tasks
with verifiable rewards that incentivize thinking and mitigate judgment bias.
In particular, our approach outperforms all other existing 8B or 70B models
when trained at those sizes, including models distilled from DeepSeek-R1. J1
also outperforms o1-mini, and even R1 on some benchmarks, despite training a
smaller model. We provide analysis and ablations comparing Pairwise-J1 vs
Pointwise-J1 models, offline vs online training recipes, reward strategies,
seed prompts, and variations in thought length and content. We find that our
models make better judgments by learning to outline evaluation criteria,
comparing against self-generated reference answers, and re-evaluating the
correctness of model responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 tables, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Private <span class="highlight-title">Transformer</span> Inference in MLaaS: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Li, Xinyu Zhou, Yitong Wang, Liangxin Qian, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models have revolutionized AI, powering applications like content
generation and sentiment analysis. However, their deployment in Machine
Learning as a Service (MLaaS) raises significant privacy concerns, primarily
due to the centralized processing of sensitive user data. Private Transformer
Inference (PTI) offers a solution by utilizing cryptographic techniques such as
secure multi-party computation and homomorphic encryption, enabling inference
while preserving both user data and model privacy. This paper reviews recent
PTI advancements, highlighting state-of-the-art solutions and challenges. We
also introduce a structured taxonomy and evaluation framework for PTI, focusing
on balancing resource efficiency with privacy and bridging the gap between
high-performance inference and data privacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empirically evaluating commonsense intelligence in large language models
  with large-scale human judgments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan Dung Nguyen, Duncan J. Watts, Mark E. Whiting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commonsense intelligence in machines is often assessed by static benchmarks
that compare a model's output against human-prescribed correct labels. An
important, albeit implicit, assumption of these labels is that they accurately
capture what any human would think, effectively treating human common sense as
homogeneous. However, recent empirical work has shown that humans vary
enormously in what they consider commonsensical; thus what appears self-evident
to one benchmark designer may not be so to another. Here, we propose a novel
method for evaluating common sense in artificial intelligence (AI),
specifically in large language models (LLMs), that incorporates empirically
observed heterogeneity among humans by measuring the correspondence between a
model's judgment and that of a human population. We first find that, when
treated as independent survey respondents, most LLMs remain below the human
median in their individual commonsense competence. Second, when used as
simulators of a hypothetical population, LLMs correlate with real humans only
modestly in the extent to which they agree on the same set of statements. In
both cases, smaller, open-weight models are surprisingly more competitive than
larger, proprietary frontier models. Our evaluation framework, which ties
commonsense intelligence to its cultural basis, contributes to the growing call
for adapting AI models to human collectivities that possess different, often
incompatible, social stocks of knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI LEGO: Scaffolding Cross-Functional Collaboration in Industrial
  Responsible AI Practices during Early Design Stages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muzhe Wu, Yanzhi Zhao, Shuyi Han, Michael Xieyang Liu, Hong Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Responsible AI (RAI) efforts increasingly emphasize the importance of
addressing potential harms early in the AI development lifecycle through
social-technical lenses. However, in cross-functional industry teams, this work
is often stalled by a persistent knowledge handoff challenge: the difficulty of
transferring high-level, early-stage technical design rationales from technical
experts to non-technical or user-facing roles for ethical evaluation and harm
identification. Through literature review and a co-design study with 8
practitioners, we unpack how this challenge manifests -- technical design
choices are rarely handed off in ways that support meaningful engagement by
non-technical roles; collaborative workflows lack shared, visual structures to
support mutual understanding; and non-technical practitioners are left without
scaffolds for systematic harm evaluation. Existing tools like JIRA or Google
Docs, while useful for product tracking, are ill-suited for supporting joint
harm identification across roles, often requiring significant extra effort to
align understanding. To address this, we developed AI LEGO, a web-based
prototype that supports cross-functional AI practitioners in effectively
facilitating knowledge handoff and identifying harmful design choices in the
early design stages. Technical roles use interactive blocks to draft
development plans, while non-technical roles engage with those blocks through
stage-specific checklists and LLM-driven persona simulations to surface
potential harms. In a study with 18 cross-functional practitioners, AI LEGO
increased the volume and likelihood of harms identified compared to baseline
worksheets. Participants found that its modular structure and persona prompts
made harm identification more accessible, fostering clearer and more
collaborative RAI practices in early design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defending the Edge: Representative-Attention for Mitigating Backdoor
  Attacks in Federated Learning <span class="chip">ESORICS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chibueze Peace Obioma, Youcheng Sun, Mustafa A. Mustafa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enhances privacy and reduces communication cost for
resource-constrained edge clients by supporting distributed model training at
the edge. However, the heterogeneous nature of such devices produces diverse,
non-independent, and identically distributed (non-IID) data, making the
detection of backdoor attacks more challenging. In this paper, we propose a
novel federated representative-attention-based defense mechanism, named FeRA,
that leverages cross-client attention over internal feature representations to
distinguish benign from malicious clients. FeRA computes an anomaly score based
on representation reconstruction errors, effectively identifying clients whose
internal activations significantly deviate from the group consensus. Our
evaluation demonstrates FeRA's robustness across various FL scenarios,
including challenging non-IID data distributions typical of edge devices.
Experimental results show that it effectively reduces backdoor attack success
rates while maintaining high accuracy on the main task. The method is
model-agnostic, attack-agnostic, and does not require labeled reference data,
making it well suited to heterogeneous and resource-limited edge deployments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ESORICS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MASS: Multi-Agent Simulation Scaling for Portfolio Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taian Guo, Haiyang Shen, Jinsheng Huang, Zhengyang Mao, Junyu Luo, Zhuoru Chen, Xuhui Liu, Bingyu Xia, Luchen Liu, Yun Ma, Ming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based multi-agent has gained significant attention for their potential in
simulation and enhancing performance. However, existing works are limited to
pure simulations or are constrained by predefined workflows, restricting their
applicability and effectiveness. In this paper, we introduce the Multi-Agent
Scaling Simulation (MASS) for portfolio construction. MASS achieves stable and
continuous excess returns by progressively increasing the number of agents for
large-scale simulations to gain a superior understanding of the market and
optimizing agent distribution end-to-end through a reverse optimization
process, rather than relying on a fixed workflow. We demonstrate its
superiority through performance experiments, ablation studies, backtesting
experiments, experiments on updated data and stock pools, scaling experiments,
parameter sensitivity experiments, and visualization experiments, conducted in
comparison with 6 state-of-the-art baselines on 3 challenging A-share stock
pools. We expect the paradigm established by MASS to expand to other tasks with
similar characteristics. The implementation of MASS has been open-sourced at
https://github.com/gta0804/MASS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AttentionGuard: <span class="highlight-title">Transformer</span>-based Misbehavior Detection for Secure
  Vehicular Platoons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hexu Li, Konstantinos Kalogiannis, Ahmed Mohamed Hussain, Panos Papadimitratos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle platooning, with vehicles traveling in close formation coordinated
through Vehicle-to-Everything (V2X) communications, offers significant benefits
in fuel efficiency and road utilization. However, it is vulnerable to
sophisticated falsification attacks by authenticated insiders that can
destabilize the formation and potentially cause catastrophic collisions. This
paper addresses this challenge: misbehavior detection in vehicle platooning
systems. We present AttentionGuard, a transformer-based framework for
misbehavior detection that leverages the self-attention mechanism to identify
anomalous patterns in mobility data. Our proposal employs a multi-head
transformer-encoder to process sequential kinematic information, enabling
effective differentiation between normal mobility patterns and falsification
attacks across diverse platooning scenarios, including steady-state
(no-maneuver) operation, join, and exit maneuvers. Our evaluation uses an
extensive simulation dataset featuring various attack vectors (constant,
gradual, and combined falsifications) and operational parameters (controller
types, vehicle speeds, and attacker positions). Experimental results
demonstrate that AttentionGuard achieves up to 0.95 F1-score in attack
detection, with robust performance maintained during complex maneuvers.
Notably, our system performs effectively with minimal latency (100ms decision
intervals), making it suitable for real-time transportation safety
applications. Comparative analysis reveals superior detection capabilities and
establishes the transformer-encoder as a promising approach for securing
Cooperative Intelligent Transport Systems (C-ITS) against sophisticated insider
threats.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Author's version; Accepted for presentation at the ACM Workshop on
  Wireless Security and Machine Learning (WiseML 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack
  in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Diana, André Nusser, Chuan Xu, Giovanni Neglia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) enables collaborative training of machine learning
models across distributed clients without sharing raw data, ostensibly
preserving data privacy. Nevertheless, recent studies have revealed critical
vulnerabilities in FL, showing that a malicious central server can manipulate
model updates to reconstruct clients' private training data. Existing data
reconstruction attacks have important limitations: they often rely on
assumptions about the clients' data distribution or their efficiency
significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes
these limitations. Our method leverages a new geometric perspective on fully
connected layers to craft malicious model parameters, enabling the perfect
recovery of arbitrarily large data batches in classification tasks without any
prior knowledge of clients' data. Through extensive experiments on both image
and tabular datasets, we demonstrate that our attack outperforms existing
methods and achieves perfect reconstruction of data batches two orders of
magnitude larger than the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Evolving Landscape of Generative Large Language Models and
  Traditional Natural Language Processing in Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yang, Huitao Li, Matthew Yu Heng Wong, Yuhe Ke, Xin Li, Kunyu Yu, Jingchi Liao, Jonathan Chong Kai Liew, Sabarinath Vinod Nair, Jasmine Chiat Ling Ong, Irene Li, Douglas Teodoro, Chuan Hong, Daniel Shu Wei Ting, Nan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing (NLP) has been traditionally applied to medicine,
and generative large language models (LLMs) have become prominent recently.
However, the differences between them across different medical tasks remain
underexplored. We analyzed 19,123 studies, finding that generative LLMs
demonstrate advantages in open-ended tasks, while traditional NLP dominates in
information extraction and analysis tasks. As these technologies advance,
ethical use of them is essential to ensure their potential in medical
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing LLM Text Annotation Skills: A Study on Human Rights Violations
  in Social Media Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Poli Apollinaire Nemkova, Solomon Ubani, Mark V. Albert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of increasingly sophisticated natural language processing (NLP)
systems, large language models (LLMs) have demonstrated remarkable potential
for diverse applications, including tasks requiring nuanced textual
understanding and contextual reasoning. This study investigates the
capabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,
Mistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex
textual dataset comprising social media posts in Russian and Ukrainian.
Specifically, the focus is on the binary classification task of identifying
references to human rights violations within the dataset.
  To evaluate the effectiveness of these models, their annotations are compared
against a gold standard set of human double-annotated labels across 1000
samples. The analysis includes assessing annotation performance under different
prompting conditions, with prompts provided in both English and Russian.
Additionally, the study explores the unique patterns of errors and
disagreements exhibited by each model, offering insights into their strengths,
limitations, and cross-linguistic adaptability.
  By juxtaposing LLM outputs with human annotations, this research contributes
to understanding the reliability and applicability of LLMs for sensitive,
domain-specific tasks in multilingual contexts. It also sheds light on how
language models handle inherently subjective and context-dependent judgments, a
critical consideration for their deployment in real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Interplay of Human-AI Alignment,Fairness, and Performance
  Trade-offs in Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Luo, Ziyu Zhou, Zixin Shu, Aurélie Pahud de Mortanges, Robert Berke, Mauricio Reyes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks excel in medical imaging but remain prone to biases,
leading to fairness gaps across demographic groups. We provide the first
systematic exploration of Human-AI alignment and fairness in this domain. Our
results show that incorporating human insights consistently reduces fairness
gaps and enhances out-of-domain generalization, though excessive alignment can
introduce performance trade-offs, emphasizing the need for calibrated
strategies. These findings highlight Human-AI alignment as a promising approach
for developing fair, robust, and generalizable medical AI systems, striking a
balance between expert guidance and automated efficiency. Our code is available
at https://github.com/Roypic/Aligner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs Memorize Recommendation <span class="highlight-title">Dataset</span>s? A Preliminary Study on
  MovieLens-1M 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dario Di Palma, Felice Antonio Merra, Maurizio Sfilio, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become increasingly central to
recommendation scenarios due to their remarkable natural language understanding
and generation capabilities. Although significant research has explored the use
of LLMs for various recommendation tasks, little effort has been dedicated to
verifying whether they have memorized public recommendation dataset as part of
their training data. This is undesirable because memorization reduces the
generalizability of research findings, as benchmarking on memorized datasets
does not guarantee generalization to unseen datasets. Furthermore, memorization
can amplify biases, for example, some popular items may be recommended more
frequently than others.
  In this work, we investigate whether LLMs have memorized public
recommendation datasets. Specifically, we examine two model families (GPT and
Llama) across multiple sizes, focusing on one of the most widely used dataset
in recommender systems: MovieLens-1M. First, we define dataset memorization as
the extent to which item attributes, user profiles, and user-item interactions
can be retrieved by prompting the LLMs. Second, we analyze the impact of
memorization on recommendation performance. Lastly, we examine whether
memorization varies across model families and model sizes. Our results reveal
that all models exhibit some degree of memorization of MovieLens-1M, and that
recommendation performance is related to the extent of memorization. We have
made all the code publicly available at:
https://github.com/sisinflab/LLM-MemoryInspector
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Fine-Grained Complexity View on Propositional Abduction -- Algorithms
  and Lower Bounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Lagerkvist, Mohamed Maizia, Johannes Schmidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Boolean satisfiability problem (SAT) is a well-known example of monotonic
reasoning, of intense practical interest due to fast solvers, complemented by
rigorous fine-grained complexity results. However, for non-monotonic reasoning,
e.g., abductive reasoning, comparably little is known outside classic
complexity theory. In this paper we take a first step of bridging the gap
between monotonic and non-monotonic reasoning by analyzing the complexity of
intractable abduction problems under the seemingly overlooked but natural
parameter n: the number of variables in the knowledge base. We obtain several
positive results for $\Sigma^P_2$- as well as NP- and coNP-complete fragments,
which implies the first example of beating exhaustive search for a
$\Sigma^P_2$-complete problem (to the best of our knowledge). We complement
this with lower bounds and for many fragments rule out improvements under the
(strong) exponential-time hypothesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Community Detection with Graph Convolutional Neural Networks:
  Bridging Topological and Attributive Cohesion <span class="chip">IJCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjali de Silva, Gang Chen, Hui Ma, Seyed Mohammad Nekooei, Xingquan Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Community detection, a vital technology for real-world applications, uncovers
cohesive node groups (communities) by leveraging both topological and attribute
similarities in social networks. However, existing Graph Convolutional Networks
(GCNs) trained to maximize modularity often converge to suboptimal solutions.
Additionally, directly using human-labeled communities for training can
undermine topological cohesiveness by grouping disconnected nodes based solely
on node attributes. We address these issues by proposing a novel Topological
and Attributive Similarity-based Community detection (TAS-Com) method. TAS-Com
introduces a novel loss function that exploits the highly effective and
scalable Leiden algorithm to detect community structures with global optimal
modularity. Leiden is further utilized to refine human-labeled communities to
ensure connectivity within each community, enabling TAS-Com to detect community
structures with desirable trade-offs between modularity and compliance with
human labels. Experimental results on multiple benchmark networks confirm that
TAS-Com can significantly outperform several state-of-the-art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IJCAI (International Joint Conference
  on Artificial Intelligence) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Zheng, Qi Shao, Guijun Han, Wei Li, Hong Li, Xuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mesoscale eddies dominate the spatiotemporal multiscale variability of the
ocean, and their impact on the energy cascade of the global ocean cannot be
ignored. Eddy-resolving ocean forecasting is providing more reliable protection
for fisheries and navigational safety, but also presents significant scientific
challenges and high computational costs for traditional numerical models.
Artificial intelligence (AI)-based weather and ocean forecasting systems are
becoming powerful tools that balance forecast performance with computational
efficiency. However, the complex multiscale features in the ocean dynamical
system make AI models still face many challenges in mesoscale eddy forecasting
(especially regional modelling). Here, we develop LanTu, a regional
eddy-resolving ocean forecasting system based on dynamics-enhanced deep
learning. We incorporate cross-scale interactions into LanTu and construct
multiscale physical constraint for optimising LanTu guided by knowledge of eddy
dynamics in order to improve the forecasting skill of LanTu for mesoscale
evolution. The results show that LanTu outperforms the existing advanced
operational numerical ocean forecasting system (NOFS) and AI-based ocean
forecasting system (AI-OFS) in temperature, salinity, sea level anomaly and
current prediction, with a lead time of more than 10 days. Our study highlights
that dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for
eddy-resolving ocean forecasting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A User Study Evaluating Argumentative Explanations in Diagnostic
  Decision Support <span class="chip">ECAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Liedeker, Olivia Sanchez-Graillet, Moana Seidler, Christian Brandt, Jörg Wellmer, Philipp Cimiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the field of healthcare increasingly adopts artificial intelligence, it
becomes important to understand which types of explanations increase
transparency and empower users to develop confidence and trust in the
predictions made by machine learning (ML) systems. In shared decision-making
scenarios where doctors cooperate with ML systems to reach an appropriate
decision, establishing mutual trust is crucial. In this paper, we explore
different approaches to generating explanations in eXplainable AI (XAI) and
make their underlying arguments explicit so that they can be evaluated by
medical experts. In particular, we present the findings of a user study
conducted with physicians to investigate their perceptions of various types of
AI-generated explanations in the context of diagnostic decision support. The
study aims to identify the most effective and useful explanations that enhance
the diagnostic process. In the study, medical doctors filled out a survey to
assess different types of explanations. Further, an interview was carried out
post-survey to gain qualitative insights on the requirements of explanations
incorporated in diagnostic decision support. Overall, the insights gained from
this study contribute to understanding the types of explanations that are most
effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at 'The First Workshop on Natural Language Argument-Based
  Explanations', co-located with ECAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a
  Reasoning Model will Think 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongyun Lee, Seungone Kim, Minju Seo, Yongrae Jo, Dongyoung Go, Hyeonbin Hwang, Jinho Park, Xiang Yue, Sean Welleck, Graham Neubig, Moontae Lee, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long chain-of-thought (CoT) is an essential ingredient in effective usage of
modern large language models, but our understanding of the reasoning strategies
underlying these capabilities remains limited. While some prior works have
attempted to categorize CoTs using predefined strategy types, such approaches
are constrained by human intuition and fail to capture the full diversity of
model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up
framework for analyzing and steering model reasoning. Our method automatically
extracts diverse reasoning criteria from model-generated CoTs, embeds them into
a semantic space, clusters them into representative categories, and derives
contrastive rubrics to interpret reasoning behavior. Human evaluations show
that this framework produces more interpretable and comprehensive analyses than
existing methods. Moreover, we demonstrate that this understanding enables
performance gains: we can predict which strategy a model is likely to use and
guide it toward more effective alternatives. Finally, we provide practical
insights, such as that training data format (e.g., free-form vs.
multiple-choice) has a far greater impact on reasoning behavior than data
domain, underscoring the importance of format-aware model design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KAITIAN: A Unified Communication Framework for Enabling Efficient
  Collaboration Across Heterogeneous Accelerators in Embodied AI Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieke Lin, Wanyu Wang, Longxiang Yin, Yinhe Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied Artificial Intelligence (AI) systems, such as autonomous robots and
intelligent vehicles, are increasingly reliant on diverse heterogeneous
accelerators (e.g., GPGPUs, NPUs, FPGAs) to meet stringent real-time processing
and energy-efficiency demands. However, the proliferation of vendor-specific
proprietary communication libraries creates significant interoperability
barriers, hindering seamless collaboration between different accelerator types
and leading to suboptimal resource utilization and performance bottlenecks in
distributed AI workloads. This paper introduces KAITIAN, a novel distributed
communication framework designed to bridge this gap. KAITIAN provides a unified
abstraction layer that intelligently integrates vendor-optimized communication
libraries for intra-group efficiency with general-purpose communication
protocols for inter-group interoperability. Crucially, it incorporates a
load-adaptive scheduling mechanism that dynamically balances computational
tasks across heterogeneous devices based on their real-time performance
characteristics. Implemented as an extension to PyTorch and rigorously
evaluated on a testbed featuring NVIDIA GPUs and Cambricon MLUs, KAITIAN
demonstrates significant improvements in resource utilization and scalability
for distributed training tasks. Experimental results show that KAITIAN can
accelerate training time by up to 42% compared to baseline homogeneous systems,
while incurring minimal communication overhead (2.8--4.3%) and maintaining
model accuracy. KAITIAN paves the way for more flexible and powerful
heterogeneous computing in complex embodied AI applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures. Jieke Lin and Wanyu Wang contributed equally to
  this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Scaling Law Apply in Time Series Forecasting? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyan Li, Libing Chen, Yin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid expansion of model size has emerged as a key challenge in time series
forecasting. From early Transformer with tens of megabytes to recent
architectures like TimesNet with thousands of megabytes, performance gains have
often come at the cost of exponentially increasing parameter counts. But is
this scaling truly necessary? To question the applicability of the scaling law
in time series forecasting, we propose Alinear, an ultra-lightweight
forecasting model that achieves competitive performance using only k-level
parameters. We introduce a horizon-aware adaptive decomposition mechanism that
dynamically rebalances component emphasis across different forecast lengths,
alongside a progressive frequency attenuation strategy that achieves stable
prediction in various forecasting horizons without incurring the computational
overhead of attention mechanisms. Extensive experiments on seven benchmark
datasets demonstrate that Alinear consistently outperforms large-scale models
while using less than 1% of their parameters, maintaining strong accuracy
across both short and ultra-long forecasting horizons. Moreover, to more fairly
evaluate model efficiency, we propose a new parameter-aware evaluation metric
that highlights the superiority of ALinear under constrained model budgets. Our
analysis reveals that the relative importance of trend and seasonal components
varies depending on data characteristics rather than following a fixed pattern,
validating the necessity of our adaptive design. This work challenges the
prevailing belief that larger models are inherently better and suggests a
paradigm shift toward more efficient time series modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Saliency <span class="highlight-title">Dataset</span> Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Kümmerer, Harneet Khanuja, Matthias Bethge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in image-based saliency prediction are approaching gold
standard performance levels on existing benchmarks. Despite this success, we
show that predicting fixations across multiple saliency datasets remains
challenging due to dataset bias. We find a significant performance drop (around
40%) when models trained on one dataset are applied to another. Surprisingly,
increasing dataset diversity does not resolve this inter-dataset gap, with
close to 60% attributed to dataset-specific biases. To address this remaining
generalization gap, we propose a novel architecture extending a mostly
dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific
parameters that govern interpretable mechanisms such as multi-scale structure,
center bias, and fixation spread. Adapting only these parameters to new data
accounts for more than 75% of the generalization gap, with a large fraction of
the improvement achieved with as few as 50 samples. Our model sets a new
state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark
(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from
unrelated datasets, but with a substantial boost when adapting to the
respective training datasets. The model also provides valuable insights into
spatial saliency properties, revealing complex multi-scale effects that combine
both absolute and relative sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QuXAI: Explainers for Hybrid Quantum Machine Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saikat Barua, Mostafizur Rahman, Shehenaz Khaled, Md Jafor Sadek, Rafiul Islam, Shahnewaz Siddique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of hybrid quantum-classical machine learning (HQML) models
opens new horizons of computational intelligence but their fundamental
complexity frequently leads to black box behavior that undermines transparency
and reliability in their application. Although XAI for quantum systems still in
its infancy, a major research gap is evident in robust global and local
explainability approaches that are designed for HQML architectures that employ
quantized feature encoding followed by classical learning. The gap is the focus
of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an
explainer for explaining feature importance in these hybrid systems. Our model
entails the creation of HQML models incorporating quantum feature maps, the use
of Q-MEDLEY, which combines feature based inferences, preserving the quantum
transformation stage and visualizing the resulting attributions. Our result
shows that Q-MEDLEY delineates influential classical aspects in HQML models, as
well as separates their noise, and competes well against established XAI
techniques in classical validation settings. Ablation studies more
significantly expose the virtues of the composite structure used in Q-MEDLEY.
The implications of this work are critically important, as it provides a route
to improve the interpretability and reliability of HQML models, thus promoting
greater confidence and being able to engage in safer and more responsible use
of quantum-enhanced AI technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures, 7 equations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Wireless Localization Model (LWLM): A Foundation Model for
  Positioning in 6G Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangjin Pan, Kaixuan Huang, Hui Chen, Shunqing Zhang, Christian Häger, Henk Wymeersch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and robust localization is a critical enabler for emerging 5G and 6G
applications, including autonomous driving, extended reality (XR), and smart
manufacturing. While data-driven approaches have shown promise, most existing
models require large amounts of labeled data and struggle to generalize across
deployment scenarios and wireless configurations. To address these limitations,
we propose a foundation-model-based solution tailored for wireless
localization. We first analyze how different self-supervised learning (SSL)
tasks acquire general-purpose and task-specific semantic features based on
information bottleneck (IB) theory. Building on this foundation, we design a
pretraining methodology for the proposed Large Wireless Localization Model
(LWLM). Specifically, we propose an SSL framework that jointly optimizes three
complementary objectives: (i) spatial-frequency masked channel modeling
(SF-MCM), (ii) domain-transformation invariance (DTI), and (iii)
position-invariant contrastive learning (PICL). These objectives jointly
capture the underlying semantics of wireless channel from multiple
perspectives. We further design lightweight decoders for key downstream tasks,
including time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation,
single base station (BS) localization, and multiple BS localization.
Comprehensive experimental results confirm that LWLM consistently surpasses
both model-based and supervised learning baselines across all localization
tasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformer
models without pretraining, and exhibits strong generalization under
label-limited fine-tuning and unseen BS configurations, confirming its
potential as a foundation model for wireless localization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,16 figures.This work has been submitted to the IEEE for
  possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Federated Learning on Edge Devices with Domain Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy Q. Le, Latif U. Khan, Choong Seon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) allows collaborative training while ensuring data
privacy across distributed edge devices, making it a popular solution for
privacy-sensitive applications. However, FL faces significant challenges due to
statistical heterogeneity, particularly domain heterogeneity, which impedes the
global mode's convergence. In this study, we introduce a new framework to
address this challenge by improving the generalization ability of the FL global
model under domain heterogeneity, using prototype augmentation. Specifically,
we introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a
prototype-based FL framework designed to enhance feature diversity and model
robustness. FedAPC leverages prototypes derived from the mean features of
augmented data to capture richer representations. By aligning local features
with global prototypes, we enable the model to learn meaningful semantic
features while reducing overfitting to any specific domain. Experimental
results on the Office-10 and Digits datasets illustrate that our framework
outperforms SOTA baselines, demonstrating superior performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IWCMC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All You Need Is Synthetic Task Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillaume Godin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Injecting rule-based models like Random Forests into differentiable neural
network frameworks remains an open challenge in machine learning. Recent
advancements have demonstrated that pretrained models can generate efficient
molecular embeddings. However, these approaches often require extensive
pretraining and additional techniques, such as incorporating posterior
probabilities, to boost performance. In our study, we propose a novel strategy
that jointly trains a single Graph Transformer neural network on both sparse
multitask molecular property experimental targets and synthetic targets derived
from XGBoost models trained on Osmordred molecular descriptors. These synthetic
tasks serve as independent auxiliary tasks. Our results show consistent and
significant performance improvement across all 19 molecular property prediction
tasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms
the XGBoost single-task learner. This demonstrates that synthetic task
augmentation is an effective method for enhancing neural model performance in
multitask molecular property prediction without the need for feature injection
or pretraining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 Figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zibin Dong, Fei Ni, Yifu Yuan, Yinchuan Li, Jianye Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present EmbodiedMAE, a unified 3D multi-modal representation for robot
manipulation. Current approaches suffer from significant domain gaps between
training datasets and robot manipulation tasks, while also lacking model
architectures that can effectively incorporate 3D information. To overcome
these limitations, we enhance the DROID dataset with high-quality depth maps
and point clouds, constructing DROID-3D as a valuable supplement for 3D
embodied vision research. Then we develop EmbodiedMAE, a multi-modal masked
autoencoder that simultaneously learns representations across RGB, depth, and
point cloud modalities through stochastic masking and cross-modal fusion.
Trained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-art
vision foundation models (VFMs) in both training efficiency and final
performance across 70 simulation tasks and 20 real-world robot manipulation
tasks on two robot platforms. The model exhibits strong scaling behavior with
size and promotes effective policy learning from 3D inputs. Experimental
results establish EmbodiedMAE as a reliable unified 3D multi-modal VFM for
embodied AI systems, particularly in precise tabletop manipulation settings
where spatial perception is critical.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and
  StyleGAN2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongmin Jung, Dasaem Jeong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces LAV (Latent Audio-Visual), a system that integrates
EnCodec's neural audio compression with StyleGAN2's generative capabilities to
produce visually dynamic outputs driven by pre-recorded audio. Unlike previous
works that rely on explicit feature mappings, LAV uses EnCodec embeddings as
latent representations, directly transformed into StyleGAN2's style latent
space via randomly initialized linear mapping. This approach preserves semantic
richness in the transformation, enabling nuanced and semantically coherent
audio-visual translations. The framework demonstrates the potential of using
pretrained audio compression models for artistic and computational
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at ISEA 2025, The 30th International Symposium on
  Electronic/Emerging Art, Seoul, Republic of Korea, 23 - 29 May 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Text to Network: Constructing a Knowledge Graph of Taiwan-Based
  China Studies Using Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsuan-Lei Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Taiwanese China Studies (CS) has developed into a rich, interdisciplinary
research field shaped by the unique geopolitical position and long standing
academic engagement with Mainland China. This study responds to the growing
need to systematically revisit and reorganize decades of Taiwan based CS
scholarship by proposing an AI assisted approach that transforms unstructured
academic texts into structured, interactive knowledge representations. We apply
generative AI (GAI) techniques and large language models (LLMs) to extract and
standardize entity relation triples from 1,367 peer reviewed CS articles
published between 1996 and 2019. These triples are then visualized through a
lightweight D3.js based system, forming the foundation of a domain specific
knowledge graph and vector database for the field. This infrastructure allows
users to explore conceptual nodes and semantic relationships across the corpus,
revealing previously uncharted intellectual trajectories, thematic clusters,
and research gaps. By decomposing textual content into graph structured
knowledge units, our system enables a paradigm shift from linear text
consumption to network based knowledge navigation. In doing so, it enhances
scholarly access to CS literature while offering a scalable, data driven
alternative to traditional ontology construction. This work not only
demonstrates how generative AI can augment area studies and digital humanities
but also highlights its potential to support a reimagined scholarly
infrastructure for regional knowledge systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Graph Retrieval-Augmented Generation to Support Learners'
  Understanding of Knowledge Concepts in MOOCs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Abdelmagied, Mohamed Amine Chatti, Shoeb Joarder, Qurat Ul Ain, Rawaa Alatrash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Massive Open Online Courses (MOOCs) lack direct interaction between learners
and instructors, making it challenging for learners to understand new knowledge
concepts. Recently, learners have increasingly used Large Language Models
(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to
hallucinations which limits their reliability. Retrieval-Augmented Generation
(RAG) addresses this issue by retrieving relevant documents before generating a
response. However, the application of RAG across different MOOCs is limited by
unstructured learning material. Furthermore, current RAG systems do not
actively guide learners toward their learning needs. To address these
challenges, we propose a Graph RAG pipeline that leverages Educational
Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide
learners to understand knowledge concepts in the MOOC platform CourseMapper.
Specifically, we implement (1) a PKG-based Question Generation method to
recommend personalized questions for learners in context, and (2) an
EduKG-based Question Answering method that leverages the relationships between
knowledge concepts in the EduKG to answer learner selected questions. To
evaluate both methods, we conducted a study with 3 expert instructors on 3
different MOOCs in the MOOC platform CourseMapper. The results of the
evaluation show the potential of Graph RAG to empower learners to understand
new knowledge concepts in a personalized learning experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMOOCs 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Robot Task Allocation for Homogeneous Tasks with Collision
  Avoidance via Spatial Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rathin Chandra Shit, Sharmila Subudhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a novel framework is presented that achieves a combined
solution based on Multi-Robot Task Allocation (MRTA) and collision avoidance
with respect to homogeneous measurement tasks taking place in industrial
environments. The spatial clustering we propose offers to simultaneously solve
the task allocation problem and deal with collision risks by cutting the
workspace into distinguishable operational zones for each robot. To divide task
sites and to schedule robot routes within corresponding clusters, we use
K-means clustering and the 2-Opt algorithm. The presented framework shows
satisfactory performance, where up to 93\% time reduction (1.24s against
17.62s) with a solution quality improvement of up to 7\% compared to the best
performing method is demonstrated. Our method also completely eliminates
collision points that persist in comparative methods in a most significant
sense. Theoretical analysis agrees with the claim that spatial partitioning
unifies the apparently disjoint tasks allocation and collision avoidance
problems under conditions of many identical tasks to be distributed over sparse
geographical areas. Ultimately, the findings in this work are of substantial
importance for real world applications where both computational efficiency and
operation free from collisions is of paramount importance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, Scheduled for presentation at an upcoming
  conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dark LLMs: The Growing Threat of Unaligned AI Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Fire, Yitzhak Elbazis, Adi Wasenstein, Lior Rokach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) rapidly reshape modern life, advancing fields
from healthcare to education and beyond. However, alongside their remarkable
capabilities lies a significant threat: the susceptibility of these models to
jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems
from the very data they learn from. As long as this training data includes
unfiltered, problematic, or 'dark' content, the models can inherently learn
undesirable patterns or weaknesses that allow users to circumvent their
intended safety controls. Our research identifies the growing threat posed by
dark LLMs models deliberately designed without ethical guardrails or modified
through jailbreak techniques. In our research, we uncovered a universal
jailbreak attack that effectively compromises multiple state-of-the-art models,
enabling them to answer almost any question and produce harmful outputs upon
request. The main idea of our attack was published online over seven months
ago. However, many of the tested LLMs were still vulnerable to this attack.
Despite our responsible disclosure efforts, responses from major LLM providers
were often inadequate, highlighting a concerning gap in industry practices
regarding AI safety. As model training becomes more accessible and cheaper, and
as open-source LLMs proliferate, the risk of widespread misuse escalates.
Without decisive intervention, LLMs may continue democratizing access to
dangerous knowledge, posing greater risks than anticipated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PsOCR: Benchmarking Large Multimodal Models for Optical Character
  Recognition in Low-resource Pashto Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ijazul Haq, Yingjie Zhang, Irfan Ali Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper evaluates the performance of Large Multimodal Models (LMMs) on
Optical Character Recognition (OCR) in the low-resource Pashto language.
Natural Language Processing (NLP) in Pashto faces several challenges due to the
cursive nature of its script and a scarcity of structured datasets. To address
this, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one
million images annotated with bounding boxes at word, line, and document
levels, suitable for training and evaluating models based on different
architectures, including Convolutional Neural Networks (CNNs) and Transformers.
PsOCR covers variations across 1,000 unique font families, colors, image sizes,
and layouts. A benchmark subset of 10K images was selected to evaluate the
performance of several LMMs, including seven open-source models: DeepSeek's
Janus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four
closed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results
demonstrate that Gemini achieves the best performance among all models, whereas
among open-source models, Qwen-7B stands out. This work provides an insightful
assessment of the capabilities and limitations of current LMMs for OCR tasks in
Pashto and establishes a foundation for further research not only in Pashto OCR
but also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is
available at https://github.com/zirak-ai/PashtoOCR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Financial Fraud Detection Using Explainable AI and Stacking Ensemble
  Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fahad Almalki, Mehedi Masud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional machine learning models often prioritize predictive accuracy,
often at the expense of model transparency and interpretability. The lack of
transparency makes it difficult for organizations to comply with regulatory
requirements and gain stakeholders trust. In this research, we propose a fraud
detection framework that combines a stacking ensemble of well-known gradient
boosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable
artificial intelligence (XAI) techniques are used to enhance the transparency
and interpretability of the model's decisions. We used SHAP (SHapley Additive
Explanations) for feature selection to identify the most important features.
Further efforts were made to explain the model's predictions using Local
Interpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots
(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection
dataset, which includes more than 590,000 real transaction records, was used to
evaluate the proposed model. The model achieved a high performance with an
accuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent
related approaches. These results indicate that combining high prediction
accuracy with transparent interpretability is possible and could lead to a more
ethical and trustworthy solution in financial fraud detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Text-to-Chart Retrieval through Training with Synthesized
  Semantic Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wu, Lutao Yan, Yizhang Zhu, Yinan Mei, Jiannan Wang, Nan Tang, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Charts are crucial for data analysis and decision-making.Text-to-chart
retrieval systems have become increasingly important for Business Intelligence
(BI), where users need to find relevant charts that match their analytical
needs. These needs can be categorized into precise queries that are
well-specified and fuzzy queries that are more exploratory -- both require
understanding the semantics and context of the charts. However, existing
text-to-chart retrieval solutions often fail to capture the semantic content
and contextual information of charts, primarily due to the lack of
comprehensive metadata (or semantic insights). To address this limitation, we
propose a training data development pipeline that automatically synthesizes
hierarchical semantic insights for charts, covering visual patterns
(visual-oriented), statistical properties (statistics-oriented), and practical
applications (task-oriented), which produces 207,498 semantic insights for
69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to
learn better representations of charts for text-to-chart retrieval. Our method
leverages rich semantic insights during the training phase to develop a model
that understands both visual and semantic aspects of charts.To evaluate
text-to-chart retrieval performance, we curate the first benchmark, CRBench,
for this task with 21,862 charts and 326 text queries from real-world BI
applications, with ground-truth labels verified by the crowd
workers.Experiments show that ChartFinder significantly outperforms existing
methods in text-to-chart retrieval tasks across various settings. For precise
queries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than
state-of-the-art models. In fuzzy query tasks, our method also demonstrates
consistent improvements, with an average increase of 5% across nearly all
metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal normalization in quantum-classical hybrid models for anti-cancer
  drug response prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takafumi Ito, Lysenko Artem, Tatsuhiko Tsunoda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum-classical Hybrid Machine Learning (QHML) models are recognized for
their robust performance and high generalization ability even for relatively
small datasets. These qualities offer unique advantages for anti-cancer drug
response prediction, where the number of available samples is typically small.
However, such hybrid models appear to be very sensitive to the data encoding
used at the interface of a neural network and a quantum circuit, with
suboptimal choices leading to stability issues. To address this problem, we
propose a novel strategy that uses a normalization function based on a
moderated gradient version of the $\tanh$. This method transforms the outputs
of the neural networks without concentrating them at the extreme value ranges.
Our idea was evaluated on a dataset of gene expression and drug response
measurements for various cancer cell lines, where we compared the prediction
performance of a classical deep learning model and several QHML models. These
results confirmed that QHML performed better than the classical models when
data was optimally normalized. This study opens up new possibilities for
biomedical data analysis using quantum computers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The First MPDD Challenge: Multimodal Personality-aware Depression
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changzeng Fu, Zelin Fu, Xinhe Kuang, Jiacheng Dong, Qi Zhang, Kaifeng Su, Yikai Su, Wenbo Shi, Junfeng Yao, Yuliang Zhao, Shiqi Zhao, Jiadong Wang, Siyang Song, Chaoran Liu, Yuichiro Yoshikawa, Björn Schuller, Hiroshi Ishiguro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depression is a widespread mental health issue affecting diverse age groups,
with notable prevalence among college students and the elderly. However,
existing datasets and detection methods primarily focus on young adults,
neglecting the broader age spectrum and individual differences that influence
depression manifestation. Current approaches often establish a direct mapping
between multimodal data and depression indicators, failing to capture the
complexity and diversity of depression across individuals. This challenge
includes two tracks based on age-specific subsets: Track 1 uses the
MPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses
the MPDD-Young dataset for detecting depression in younger participants. The
Multimodal Personality-aware Depression Detection (MPDD) Challenge aims to
address this gap by incorporating multimodal data alongside individual
difference factors. We provide a baseline model that fuses audio and video
modalities with individual difference information to detect depression
manifestations in diverse populations. This challenge aims to promote the
development of more personalized and accurate de pression detection methods,
advancing mental health research and fostering inclusive detection systems.
More details are available on the official challenge website:
https://hacilab.github.io/MPDDChallenge.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted as part of the MPDD Challenge in the
  ACMMM 2025 Grand Challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model
  Super-Resolution Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of remote sensing technology, super-resolution
image reconstruction is of great research and practical significance. Existing
deep learning methods have made progress but still face limitations in handling
complex scenes and preserving image details. This paper proposes a
reinforcement learning-based latent diffusion model (LDM) fine-tuning method
for remote sensing image super-resolution. The method constructs a
reinforcement learning environment with states, actions, and rewards,
optimizing decision objectives through proximal policy optimization (PPO)
during the reverse denoising process of the LDM model. Experiments on the
RESISC45 dataset show significant improvements over the baseline model in PSNR,
SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,
and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural
scenes. The results demonstrate the method's effectiveness in enhancing
super-resolution quality and adaptability across scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 4th International Conference on Computing Innovation
  and Applied Physics (CONF-CIAP 2025), and will be published in EAI Community
  Research Series-CORE or Theoretical and Natural Science (TNS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application of YOLOv8 in monocular downward multiple Car Target
  detection <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving technology is progressively transforming traditional car
driving methods, marking a significant milestone in modern transportation.
Object detection serves as a cornerstone of autonomous systems, playing a vital
role in enhancing driving safety, enabling autonomous functionality, improving
traffic efficiency, and facilitating effective emergency responses. However,
current technologies such as radar for environmental perception, cameras for
road perception, and vehicle sensor networks face notable challenges, including
high costs, vulnerability to weather and lighting conditions, and limited
resolution.To address these limitations, this paper presents an improved
autonomous target detection network based on YOLOv8. By integrating structural
reparameterization technology, a bidirectional pyramid structure network model,
and a novel detection pipeline into the YOLOv8 framework, the proposed approach
achieves highly efficient and precise detection of multi-scale, small, and
remote objects. Experimental results demonstrate that the enhanced model can
effectively detect both large and small objects with a detection accuracy of
65%, showcasing significant advancements over traditional methods.This improved
model holds substantial potential for real-world applications and is
well-suited for autonomous driving competitions, such as the Formula Student
Autonomous China (FSAC), particularly excelling in scenarios involving
single-target and small-object detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 5th International Conference on Signal Processing and
  Machine Learning (CONF-SPML 2025), to appear in Applied and Computational
  Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Computing and AI: Perspectives on Advanced Automation in Science
  and Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tadashi Kadowaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in artificial intelligence (AI) and quantum computing are
accelerating automation in scientific and engineering processes, fundamentally
reshaping research methodologies. This perspective highlights parallels between
scientific automation and established Computer-Aided Engineering (CAE)
practices, introducing Quantum CAE as a framework that leverages quantum
algorithms for simulation, optimization, and machine learning within
engineering design. Practical implementations of Quantum CAE are illustrated
through case studies for combinatorial optimization problems. Further
discussions include advancements toward higher automation levels, highlighting
the critical role of specialized AI agents proficient in quantum algorithm
design. The integration of quantum computing with AI raises significant
questions about the collaborative dynamics among human scientists and
engineers, AI systems, and quantum computational resources, underscoring a
transformative future for automated discovery and innovation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Greenferencing: Routing AI Inferencing to Green Modular Data Centers
  with Heron 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tella Rajashekhar Reddy,  Palak, Rohan Gandhi, Anjaly Parayil, Chaojie Zhang, Mike Shepperd, Liangcheng Yu, Jayashree Mohan, Srinivasan Iyengar, Shivkumar Kalyanaraman, Debopam Bhattacherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI power demand is growing unprecedentedly thanks to the high power density
of AI compute and the emerging inferencing workload. On the supply side,
abundant wind power is waiting for grid access in interconnection queues. In
this light, this paper argues bringing AI workload to modular compute clusters
co-located in wind farms. Our deployment right-sizing strategy makes it
economically viable to deploy more than 6 million high-end GPUs today that
could consume cheap, green power at its source. We built Heron, a cross-site
software router, that could efficiently leverage the complementarity of power
generation across wind farms by routing AI inferencing workload around power
drops. Using 1-week ofcoding and conversation production traces from Azure and
(real) variable wind power traces, we show how Heron improves aggregate goodput
of AI compute by up to 80% compared to the state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber
  Security Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adel ElZemity, Budi Arief, Shujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of large language models (LLMs) into cyber security
applications presents significant opportunities, such as enhancing threat
analysis and malware detection, but can also introduce critical risks and
safety concerns, including personal data leakage and automated generation of
new malware. We present a systematic evaluation of safety risks in fine-tuned
LLMs for cyber security applications. Using the OWASP Top 10 for LLM
Applications framework, we assessed seven open-source LLMs: Phi 3 Mini 3.8B,
Mistral 7B, Qwen 2.5 7B, Llama 3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B.
Our evaluation shows that fine-tuning reduces safety resilience across all
tested LLMs (e.g., the safety score of Llama 3.1 8B against prompt injection
drops from 0.95 to 0.15). We propose and evaluate a safety alignment approach
that carefully rewords instruction-response pairs to include explicit safety
precautions and ethical considerations. This approach demonstrates that it is
possible to maintain or even improve model safety while preserving technical
utility, offering a practical path forward for developing safer fine-tuning
methodologies. This work offers a systematic evaluation for safety risks in
LLMs, enabling safer adoption of generative AI in sensitive domains, and
contributing towards the development of secure, trustworthy, and ethically
aligned LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mrinal Rawat, Ambuje Gupta, Rushil Goomer, Alessandro Di Bari, Neha Gupta, Roberto Pieraccini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ReAct (Reasoning + Action) capability in large language models (LLMs) has
become the foundation of modern agentic systems. Recent LLMs, such as
DeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through
the generation of ample intermediate tokens, which help build a strong premise
before producing the final output tokens. In this paper, we introduce Pre-Act,
a novel approach that enhances the agent's performance by creating a multi-step
execution plan along with the detailed reasoning for the given user input. This
plan incrementally incorporates previous steps and tool outputs, refining
itself after each step execution until the final response is obtained. Our
approach is applicable to both conversational and non-conversational agents. To
measure the performance of task-oriented agents comprehensively, we propose a
two-level evaluation framework: (1) turn level and (2) end-to-end. Our
turn-level evaluation, averaged across five models, shows that our approach,
Pre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While
this approach is effective for larger models, smaller models crucial for
practical applications, where latency and cost are key constraints, often
struggle with complex reasoning tasks required for agentic systems. To address
this limitation, we fine-tune relatively small models such as Llama 3.1 (8B &
70B) using the proposed Pre-Act approach. Our experiments show that the
fine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action
accuracy (turn-level) and a 28% improvement in goal completion rate
(end-to-end) on the Almita (out-of-domain) dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Machine Learning Framework for Heart Disease Prediction:
  Performance Evaluation and Future Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Azimi Lamir, Shiva Razzagzadeh, Zeynab Rezaei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a machine learning-based framework for heart disease
prediction using the heart-disease dataset, comprising 303 samples with 14
features. The methodology involves data preprocessing, model training, and
evaluation using three classifiers: Logistic Regression, K-Nearest Neighbors
(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and
RandomizedSearchCV was employed to enhance model performance. The Random Forest
classifier outperformed other models, achieving an accuracy of 91% and an
F1-score of 0.89. Evaluation metrics, including precision, recall, and
confusion matrix, revealed balanced performance across classes. The proposed
model demonstrates strong potential for aiding clinical decision-making by
effectively predicting heart disease. Limitations such as dataset size and
generalizability underscore the need for future studies using larger and more
diverse datasets. This work highlights the utility of machine learning in
healthcare, offering insights for further advancements in predictive
diagnostics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series
  Unsupervised Domain Adaptation <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeho Kim, Seulki Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised domain adaptation (UDA) for time series data remains a critical
challenge in deep learning, with traditional pseudo-labeling strategies failing
to capture temporal patterns and channel-wise shifts between domains, producing
sub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that
addresses these limitations by modeling the joint distribution $P(\mathbf{X},
y)$ of the source domain through code transition matrices, where the codes are
derived from vector quantization (VQ) of time series patches. Our method
constructs class- and channel-wise code transition matrices from the source
domain and employs Bayes' rule for target domain adaptation, generating
pseudo-labels based on channel-wise weighted class-conditional likelihoods.
TransPL offers three key advantages: explicit modeling of temporal transitions
and channel-wise shifts between different domains, versatility towards
different UDA scenarios (e.g., weakly-supervised UDA), and explainable
pseudo-label generation. We validate TransPL's effectiveness through extensive
analysis on four time series UDA benchmarks and confirm that it consistently
outperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1%
accuracy improvement, 4.9% F1 improvement), while providing interpretable
insights into the domain adaptation process through its learned code transition
matrices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025 Accept</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-Core Memory Management and Consolidation for Long-term Continual
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Huai, Jie Zhou, Yuxuan Cai, Qin Chen, Wen Wu, Xingjiao Wu, Xipeng Qiu, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we focus on a long-term continual learning (CL) task, where a
model learns sequentially from a stream of vast tasks over time, acquiring new
knowledge while retaining previously learned information in a manner akin to
human learning. Unlike traditional CL settings, long-term CL involves handling
a significantly larger number of tasks, which exacerbates the issue of
catastrophic forgetting. Our work seeks to address two critical questions: 1)
How do existing CL methods perform in the context of long-term CL? and 2) How
can we mitigate the catastrophic forgetting that arises from prolonged
sequential updates? To tackle these challenges, we propose a novel framework
inspired by human memory mechanisms for long-term continual learning (Long-CL).
Specifically, we introduce a task-core memory management strategy to
efficiently index crucial memories and adaptively update them as learning
progresses. Additionally, we develop a long-term memory consolidation mechanism
that selectively retains hard and discriminative samples, ensuring robust
knowledge retention. To facilitate research in this area, we construct and
release two multi-modal and textual benchmarks, MMLongCL-Bench and
TextLongCL-Bench, providing a valuable resource for evaluating long-term CL
approaches. Experimental results show that Long-CL outperforms the previous
state-of-the-art by 7.4\% and 6.5\% AP on the two benchmarks, respectively,
demonstrating the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Neurips2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalizing Large Language Models using Retrieval Augmented Generation
  and Knowledge Graph <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deeksha Prahlad, Chanhee Lee, Dongha Kim, Hokeun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) has allowed numerous applications,
including the generation of queried responses, to be leveraged in chatbots and
other conversational assistants. Being trained on a plethora of data, LLMs
often undergo high levels of over-fitting, resulting in the generation of extra
and incorrect data, thus causing hallucinations in output generation. One of
the root causes of such problems is the lack of timely, factual, and
personalized information fed to the LLM. In this paper, we propose an approach
to address these problems by introducing retrieval augmented generation (RAG)
using knowledge graphs (KGs) to assist the LLM in personalized response
generation tailored to the users. KGs have the advantage of storing
continuously updated factual information in a structured way. While our KGs can
be used for a variety of frequently updated personal data, such as calendar,
contact, and location data, we focus on calendar data in this paper. Our
experimental results show that our approach works significantly better in
understanding personal information and generating accurate responses compared
to the baseline LLMs using personal data as text inputs, with a moderate
reduction in response time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Companion Proceedings of the ACM Web Conference 2025
  (WWW Companion '25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VRU-CIPI: Crossing Intention Prediction at Intersections for Improving
  Vulnerable Road Users Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed S. Abdelrahman, Mohamed Abdel-Aty, Quoc Dai Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and predicting human behavior in-thewild, particularly at urban
intersections, remains crucial for enhancing interaction safety between road
users. Among the most critical behaviors are crossing intentions of Vulnerable
Road Users (VRUs), where misinterpretation may result in dangerous conflicts
with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a
sequential attention-based model designed to predict VRU crossing intentions at
intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal
dynamics in VRU movements, combined with a multi-head Transformer
self-attention mechanism to encode contextual and spatial dependencies critical
for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed
achieves state-of-the-art performance with an accuracy of 96.45% and achieving
real-time inference speed reaching 33 frames per second. Furthermore, by
integrating with Infrastructure-to-Vehicles (I2V) communication, our approach
can proactively enhance intersection safety through timely activation of
crossing signals and providing early warnings to connected vehicles, ensuring
smoother and safer interactions for all road users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demystifying AI Agents: The Final Generation of Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin J McNamara, Rhea Pritham Marpu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The trajectory of artificial intelligence (AI) has been one of relentless
acceleration, evolving from rudimentary rule-based systems to sophisticated,
autonomous agents capable of complex reasoning and interaction. This whitepaper
chronicles this remarkable journey, charting the key technological
milestones--advancements in prompting, training methodologies, hardware
capabilities, and architectural innovations--that have converged to create the
AI agents of today. We argue that these agents, exemplified by systems like
OpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in
AI development, potentially constituting the "final generation" of intelligence
as we currently conceive it. We explore the capabilities and underlying
technologies of these agents, grounded in practical examples, while also
examining the profound societal implications and the unprecedented pace of
progress that suggests intelligence is now doubling approximately every six
months. The paper concludes by underscoring the critical need for wisdom and
foresight in navigating the opportunities and challenges presented by this
powerful new era of intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin-Bin Gao, Yue Zhu, Jiangtao Yan, Yuezhi Cai, Weixi Zhang, Meng Wang, Jun Liu, Yong Liu, Lei Wang, Chengjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universal visual anomaly detection aims to identify anomalies from novel or
unseen vision domains without additional fine-tuning, which is critical in open
scenarios. Recent studies have demonstrated that pre-trained vision-language
models like CLIP exhibit strong generalization with just zero or a few normal
images. However, existing methods struggle with designing prompt templates,
complex token interactions, or requiring additional fine-tuning, resulting in
limited flexibility. In this work, we present a simple yet effective method
called AdaptCLIP based on two key insights. First, adaptive visual and textual
representations should be learned alternately rather than jointly. Second,
comparative learning between query and normal image prompt should incorporate
both contextual and aligned residual features, rather than relying solely on
residual features. AdaptCLIP treats CLIP models as a foundational service,
adding only three simple adapters, visual adapter, textual adapter, and
prompt-query adapter, at its input or output ends. AdaptCLIP supports
zero-/few-shot generalization across domains and possesses a training-free
manner on target domains once trained on a base dataset. AdaptCLIP achieves
state-of-the-art performance on 12 anomaly detection benchmarks from industrial
and medical domains, significantly outperforming existing competitive methods.
We will make the code and model of AdaptCLIP available at
https://github.com/gaobb/AdaptCLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 15 figures, 22 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforced Interactive Continual Learning via Real-time Noisy Human
  Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Yang, Jie Zhou, Junsong Li, Qianjun Pan, Bihao Zhan, Qin Chen, Xipeng Qiu, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an interactive continual learning paradigm where AI
models dynamically learn new skills from real-time human feedback while
retaining prior knowledge. This paradigm distinctively addresses two major
limitations of traditional continual learning: (1) dynamic model updates using
streaming, real-time human-annotated data, rather than static datasets with
fixed labels, and (2) the assumption of clean labels, by explicitly handling
the noisy feedback common in real-world interactions. To tackle these problems,
we propose RiCL, a Reinforced interactive Continual Learning framework
leveraging Large Language Models (LLMs) to learn new skills effectively from
dynamic feedback. RiCL incorporates three key components: a temporal
consistency-aware purifier to automatically discern clean from noisy samples in
data streams; an interaction-aware direct preference optimization strategy to
align model behavior with human intent by reconciling AI-generated and
human-provided feedback; and a noise-resistant contrastive learning module that
captures robust representations by exploiting inherent data relationships, thus
avoiding reliance on potentially unreliable labels. Extensive experiments on
two benchmark datasets (FewRel and TACRED), contaminated with realistic noise
patterns, demonstrate that our RiCL approach substantially outperforms existing
combinations of state-of-the-art online continual learning and noisy-label
learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "There Is No Such Thing as a Dumb Question," But There Are Good Ones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjung Shin, Donghyun Kim, Jeh-Kwang Ryu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Questioning has become increasingly crucial for both humans and artificial
intelligence, yet there remains limited research comprehensively assessing
question quality. In response, this study defines good questions and presents a
systematic evaluation framework. We propose two key evaluation dimensions:
appropriateness (sociolinguistic competence in context) and effectiveness
(strategic competence in goal achievement). Based on these foundational
dimensions, a rubric-based scoring system was developed. By incorporating
dynamic contextual variables, our evaluation framework achieves structure and
flexibility through semi-adaptive criteria. The methodology was validated using
the CAUS and SQUARE datasets, demonstrating the ability of the framework to
access both well-formed and problematic questions while adapting to varied
contexts. As we establish a flexible and comprehensive framework for question
evaluation, this study takes a significant step toward integrating questioning
behavior with structured analytical methods grounded in the intrinsic nature of
questioning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures and 4 tables. This work has been accepted for
  presentation as a poster with full paper publication at CogSci 2025. This is
  the final submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline Reinforcement Learning for Microgrid Voltage Regulation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Yang, Yongli Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a study on using different offline reinforcement learning
algorithms for microgrid voltage regulation with solar power penetration. When
environment interaction is unviable due to technical or safety reasons, the
proposed approach can still obtain an applicable model through offline-style
training on a previously collected dataset, lowering the negative impact of
lacking online environment interactions. Experiment results on the IEEE 33-bus
system demonstrate the feasibility and effectiveness of the proposed approach
on different offline datasets, including the one with merely low-quality
experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted and presented at ICLR 2025 in Singapore,
  Apr. 28, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Avocado Price Prediction Using a Hybrid Deep Learning Model:
  TCN-MLP-Attention Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linwei Zhang,  LuFeng, Ruijia Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing demand for healthy foods, agricultural product price
forecasting has become increasingly important. Hass avocados, as a high-value
crop, exhibit complex price fluctuations influenced by factors such as
seasonality, region, and weather. Traditional prediction models often struggle
with highly nonlinear and dynamic data. To address this, we propose a hybrid
deep learning model, TCN-MLP-Attention Architecture, combining Temporal
Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer
Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for
dynamic feature weighting. The dataset used covers over 50,000 records of Hass
avocado sales across the U.S. from 2015 to 2018, including variables such as
sales volume, average price, time, region, weather, and variety type, collected
from point-of-sale systems and the Hass Avocado Board. After systematic
preprocessing, including missing value imputation and feature normalization,
the proposed model was trained and evaluated. Experimental results demonstrate
that the TCN-MLP-Attention model achieves excellent predictive performance,
with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.
This research provides a scalable and effective approach for time series
forecasting in agricultural markets and offers valuable insights for
intelligent supply chain management and price strategy optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Exploration-Exploitation Strategies of LLMs and Humans:
  Insights from Standard Multi-armed Bandit Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyuan Zhang, Darcy Wang, Ningyuan Chen, Rodrigo Mansur, Vahid Sarhangian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly used to simulate or automate
human behavior in complex sequential decision-making tasks. A natural question
is then whether LLMs exhibit similar decision-making behavior to humans, and
can achieve comparable (or superior) performance. In this work, we focus on the
exploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic
decision-making under uncertainty. We employ canonical multi-armed bandit (MAB)
tasks introduced in the cognitive science and psychiatry literature to conduct
a comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.
We use interpretable choice models to capture the E&E strategies of the agents
and investigate how explicit reasoning, through both prompting strategies and
reasoning-enhanced models, shapes LLM decision-making. We find that reasoning
shifts LLMs toward more human-like behavior, characterized by a mix of random
and directed exploration. In simple stationary tasks, reasoning-enabled LLMs
exhibit similar levels of random and directed exploration compared to humans.
However, in more complex, non-stationary environments, LLMs struggle to match
human adaptability, particularly in effective directed exploration, despite
achieving similar regret in certain scenarios. Our findings highlight both the
promise and limits of LLMs as simulators of human behavior and tools for
automated decision-making and point to potential areas of improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling Human Behavior in a Strategic Network Game with Complex Group
  Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Skaggs, Jacob W. Crandall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human networks greatly impact important societal outcomes, including wealth
and health inequality, poverty, and bullying. As such, understanding human
networks is critical to learning how to promote favorable societal outcomes. As
a step toward better understanding human networks, we compare and contrast
several methods for learning, from a small data set, models of human behavior
in a strategic network game called the Junior High Game (JHG). These modeling
methods differ with respect to the assumptions they use to parameterize human
behavior (behavior vs. community-aware behavior) and the moments they model
(mean vs. distribution). Results show that the highest-performing method,
called hCAB, models the distribution of human behavior rather than the mean and
assumes humans use community-aware behavior rather than behavior matching. When
applied to small societies (6-11 individuals), the hCAB model closely mirrors
the population dynamics of human groups (with notable differences).
Additionally, in a user study, human participants were unable to distinguish
individual hCAB agents from other humans, thus illustrating that the hCAB model
also produces plausible (individual) human behavior in this strategic network
game.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARR: Question Answering with Large Language Models via Analyzing,
  Retrieving, and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04689v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04689v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Yin, Giuseppe Carenini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive capabilities on
complex evaluation benchmarks, many of which are formulated as
question-answering (QA) tasks. Enhancing the performance of LLMs in QA contexts
is becoming increasingly vital for advancing their development and
applicability. This paper introduces ARR, an intuitive, effective, and general
QA solving method that explicitly incorporates three key steps: analyzing the
intent of the question, retrieving relevant information, and reasoning step by
step. Notably, this paper is the first to introduce intent analysis in QA,
which plays a vital role in ARR. Comprehensive evaluations across 10 diverse QA
tasks demonstrate that ARR consistently outperforms the baseline methods.
Ablation and case studies further validate the positive contributions of each
ARR component. Furthermore, experiments involving variations in prompt design
indicate that ARR maintains its effectiveness regardless of the specific prompt
formulation. Additionally, extensive evaluations across various model sizes,
LLM series, and generation settings solidify the effectiveness, robustness, and
generalizability of ARR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages. Code: https://github.com/YuweiYin/ARR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lightspeed Geometric <span class="highlight-title">Dataset</span> Distance via Sliced Optimal Transport <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khai Nguyen, Hai Nguyen, Tuan Pham, Nhat Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce sliced optimal transport dataset distance (s-OTDD), a
model-agnostic, embedding-agnostic approach for dataset comparison that
requires no training, is robust to variations in the number of classes, and can
handle disjoint label sets. The core innovation is Moment Transform Projection
(MTP), which maps a label, represented as a distribution over features, to a
real number. Using MTP, we derive a data point projection that transforms
datasets into one-dimensional distributions. The s-OTDD is defined as the
expected Wasserstein distance between the projected distributions, with respect
to random projection parameters. Leveraging the closed form solution of
one-dimensional optimal transport, s-OTDD achieves (near-)linear computational
complexity in the number of data points and feature dimensions and is
independent of the number of classes. With its geometrically meaningful
projection, s-OTDD strongly correlates with the optimal transport dataset
distance while being more efficient than existing dataset discrepancy measures.
Moreover, it correlates well with the performance gap in transfer learning and
classification accuracy in data augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2025, 16 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information
  Funneling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02069v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02069v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Yucheng Li, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Junjie Hu, Wen Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we investigate whether attention-based information flow inside
large language models (LLMs) is aggregated through noticeable patterns for long
context processing. Our observations reveal that LLMs aggregate information
through Pyramidal Information Funneling where attention is scattering widely in
lower layers, progressively consolidating within specific contexts, and
ultimately focusing on critical tokens (a.k.a massive activation or attention
sink) in higher layers. Motivated by these insights, we developed PyramidKV, a
novel and effective KV cache compression method. This approach dynamically
adjusts the KV cache size across different layers, allocating more cache in
lower layers and less in higher ones, diverging from traditional methods that
maintain a uniform KV cache size. Our experimental evaluations, utilizing the
LongBench benchmark, show that PyramidKV matches the performance of models with
a full KV cache while retaining only 12% of the KV cache, thus significantly
reducing memory usage. In scenarios emphasizing memory efficiency, where only
0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache
compression techniques, achieving up to a 20.5 absolute accuracy improvement on
TREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms
competing methods in maintaining long-context comprehension in LLMs; notably,
retaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve
100.0 Acc. performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Generative AI for Scoring Medical Student Interviews in
  Objective Structured Clinical Examinations (OSCEs) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jadon Geathers, Yann Hicke, Colleen Chan, Niroop Rajashekar, Justin Sewell, Susannah Cornes, Rene F. Kizilcec, Dennis Shung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective Structured Clinical Examinations (OSCEs) are widely used to assess
medical students' communication skills, but scoring interview-based assessments
is time-consuming and potentially subject to human bias. This study explored
the potential of large language models (LLMs) to automate OSCE evaluations
using the Master Interview Rating Scale (MIRS). We compared the performance of
four state-of-the-art LLMs (GPT-4o, Claude 3.5, Llama 3.1, and Gemini 1.5 Pro)
in evaluating OSCE transcripts across all 28 items of the MIRS under the
conditions of zero-shot, chain-of-thought (CoT), few-shot, and multi-step
prompting. The models were benchmarked against a dataset of 10 OSCE cases with
174 expert consensus scores available. Model performance was measured using
three accuracy metrics (exact, off-by-one, thresholded). Averaging across all
MIRS items and OSCE cases, LLMs performed with low exact accuracy (0.27 to
0.44), and moderate to high off-by-one accuracy (0.67 to 0.87) and thresholded
accuracy (0.75 to 0.88). A zero temperature parameter ensured high intra-rater
reliability ({\alpha} = 0.98 for GPT-4o). CoT, few-shot, and multi-step
techniques proved valuable when tailored to specific assessment items. The
performance was consistent across MIRS items, independent of encounter phases
and communication domains. We demonstrated the feasibility of AI-assisted OSCE
evaluation and provided benchmarking of multiple LLMs across multiple prompt
techniques. Our work provides a baseline performance assessment for LLMs that
lays a foundation for future research into automated assessment of clinical
communication skills.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages + 3 pages of references, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Driven Calibration of Prediction Sets in Large Vision-Language
  Models Based on Inductive Conformal Prediction <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17671v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17671v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchang Ye, Weiyan Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the critical challenge of hallucination mitigation in
Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks
through a Split Conformal Prediction (SCP) framework. While LVLMs excel in
multi-modal reasoning, their outputs often exhibit hallucinated content with
high confidence, posing risks in safety-critical applications. We propose a
model-agnostic uncertainty quantification method that integrates dynamic
threshold calibration and cross-modal consistency verification. By partitioning
data into calibration and test sets, the framework computes nonconformity
scores to construct prediction sets with statistical guarantees under
user-defined risk levels ($\alpha$). Key innovations include: (1) rigorous
control of \textbf{marginal coverage} to ensure empirical error rates remain
strictly below $\alpha$; (2) dynamic adjustment of prediction set sizes
inversely with $\alpha$, filtering low-confidence outputs; (3) elimination of
prior distribution assumptions and retraining requirements. Evaluations on
benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces
theoretical guarantees across all $\alpha$ values. The framework achieves
stable performance across varying calibration-to-test split ratios,
underscoring its robustness for real-world deployment in healthcare, autonomous
systems, and other safety-sensitive domains. This work bridges the gap between
theoretical reliability and practical applicability in multi-modal AI systems,
offering a scalable solution for hallucination detection and uncertainty-aware
decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICIPCA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MapExplorer: New Content Generation from Low-Dimensional Visualizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Zhang, Ziyang Xiong, Shixuan Liu, Yutong Xie, Tolga Ergen, Dongsub Shim, Hua Xu, Honglak Lee, Qiaozhu Me
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-dimensional visualizations, or "projection maps," are widely used in
scientific and creative domains to interpret large-scale and complex datasets.
These visualizations not only aid in understanding existing knowledge spaces
but also implicitly guide exploration into unknown areas. Although techniques
such as t-SNE and UMAP can generate these maps, there exists no systematic
method for leveraging them to generate new content. To address this, we
introduce MapExplorer, a novel knowledge discovery task that translates
coordinates within any projection map into coherent, contextually aligned
textual content. This allows users to interactively explore and uncover
insights embedded in the maps. To evaluate the performance of MapExplorer
methods, we propose Atometric, a fine-grained metric inspired by ROUGE that
quantifies logical coherence and alignment between generated and reference
text. Experiments on diverse datasets demonstrate the versatility of
MapExplorer in generating scientific hypotheses, crafting synthetic personas,
and devising strategies for attacking large language models-even with simple
baseline methods. By bridging visualization and generation, our work highlights
the potential of MapExplorer to enable intuitive human-AI collaboration in
large-scale data exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tokenization Matters! Degrading Large Language Models through
  Challenging Their Tokenization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17067v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17067v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dixuan Wang, Yanda Li, Junyuan Jiang, Zepeng Ding, Ziqin Luo, Guochao Jiang, Jiaqing Liang, Deqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities in language
understanding and generation. Nonetheless, it was also witnessed that LLMs tend
to produce inaccurate responses to specific queries. This deficiency can be
traced to the tokenization step LLMs must undergo, which is an inevitable
limitation inherent to all LLMs. In fact, incorrect tokenization is the
critical point that hinders LLMs in understanding the input precisely, thus
leading to unsatisfactory output. This defect is more obvious in Chinese
scenarios. To demonstrate this flaw of LLMs, we construct an adversarial
dataset, named as $\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which
draws upon the vocabularies of various open-source LLMs to challenge LLMs'
tokenization. ADT consists of two subsets: the manually constructed ADT-Human
and the automatically generated ADT-Auto. Our empirical results reveal that our
ADT is highly effective on challenging the tokenization of leading LLMs,
including GPT-4o, Llama-3, Deepseek-R1 and so on, thus degrading these LLMs'
capabilities. Moreover, our method of automatic data generation has been proven
efficient and robust, which can be applied to any open-source LLMs. In this
paper, we substantially investigate LLMs' vulnerability in terms of challenging
their token segmentation, which will shed light on the subsequent research of
improving LLMs' capabilities through optimizing their tokenization process and
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Examining the Source of Defects from a Mechanical Perspective for 3D
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhe Liang, Aoran Wang, Jie Zhou, Xin Jin, Can Gao, Jinbao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore a novel approach to 3D anomaly detection (AD) that
goes beyond merely identifying anomalies based on structural characteristics.
Our primary perspective is that most anomalies arise from unpredictable
defective forces originating from both internal and external sources. To
address these anomalies, we seek out opposing forces that can help correct
them. Therefore, we introduce the Mechanics Complementary Model-based Framework
for the 3D-AD task (MC4AD), which generates internal and external corrective
forces for each point. We first propose a Diverse Anomaly-Generation (DA-Gen)
module designed to simulate various types of anomalies. Next, we present the
Corrective Force Prediction Network (CFP-Net), which uses complementary
representations for point-level analysis to simulate the different
contributions from internal and external corrective forces. To ensure the
corrective forces are constrained effectively, we have developed a combined
loss function that includes a new symmetric loss and an overall loss. Notably,
we implement a Hierarchical Quality Control (HQC) strategy based on a three-way
decision process and contribute a dataset titled Anomaly-IntraVariance, which
incorporates intraclass variance to evaluate our model. As a result, the
proposed MC4AD has been proven effective through theory and experimentation.
The experimental results demonstrate that our approach yields nine
state-of-the-art performances, achieving optimal results with minimal
parameters and the fastest inference speed across five existing datasets, in
addition to the proposed Anomaly-IntraVariance dataset. The source is available
at https://github.com/hzzzzzhappy/MC4AD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Double Successive Over-Relaxation Q-Learning with an Extension to Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyas S R
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Q-learning is a widely used algorithm in reinforcement learning (RL), but its
convergence can be slow, especially when the discount factor is close to one.
Successive Over-Relaxation (SOR) Q-learning, which introduces a relaxation
factor to speed up convergence, addresses this issue but has two major
limitations: In the tabular setting, the relaxation parameter depends on
transition probability, making it not entirely model-free, and it suffers from
overestimation bias. To overcome these limitations, we propose a sample-based,
model-free double SOR Q-learning algorithm. Theoretically and empirically, this
algorithm is shown to be less biased than SOR Q-learning. Further, in the
tabular setting, the convergence analysis under boundedness assumptions on
iterates is discussed. The proposed algorithm is extended to large-scale
problems using deep RL. Finally, the tabular version of the proposed algorithm
is compared using roulette and grid world environments, while the deep RL
version is tested on a maximization bias example and OpenAI Gym environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at
  Intermediate Resolution with Structure-Aware Multimodal U-Nets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenwei Zhang, Khanh Dao Duc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at
intermediate resolution (4-8 {\AA}) is crucial in protein structure
determination. Recent advances in deep learning have led to the development of
automated approaches for enhancing experimental cryo-EM density maps. Yet,
these methods are not optimized for intermediate-resolution maps and rely on
map density features alone. To address this, we propose CryoSAMU, a novel
method designed to enhance 3D cryo-EM density maps of protein structures using
structure-aware multimodal U-Nets and trained on curated
intermediate-resolution density maps. We comprehensively evaluate CryoSAMU
across various metrics and demonstrate its competitive performance compared to
state-of-the-art methods. Notably, CryoSAMU achieves significantly faster
processing speed, showing promise for future practical applications. Our code
is available at https://github.com/chenwei-zhang/CryoSAMU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 main figures, 2 supplementary figures, 3 main tables, 4
  supplementary tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Scalable IoT Deployment for Visual Anomaly Detection via
  Efficient Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arianna Stropeni, Francesco Borsatti, Manuel Barusco, Davide Dalle Pezze, Marco Fabris, Gian Antonio Susto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Anomaly Detection (VAD) is a key task in industrial settings, where
minimizing operational costs is essential. Deploying deep learning models
within Internet of Things (IoT) environments introduces specific challenges due
to limited computational power and bandwidth of edge devices. This study
investigates how to perform VAD effectively under such constraints by
leveraging compact, efficient processing strategies. We evaluate several data
compression techniques, examining the tradeoff between system latency and
detection accuracy. Experiments on the MVTec AD benchmark demonstrate that
significant compression can be achieved with minimal loss in anomaly detection
performance compared to uncompressed data. Current results show up to 80%
reduction in end-to-end inference time, including edge processing,
transmission, and server computation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WildFireCan-MMD: A Multimodal <span class="highlight-title">Dataset</span> for Classification of
  User-Generated Content During Wildfires in Canada 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Braeden Sherritt, Isar Nejadgholi, Marzieh Amini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid information access is vital during wildfires, yet traditional data
sources are slow and costly. Social media offers real-time updates, but
extracting relevant insights remains a challenge. We present WildFireCan-MMD, a
new multimodal dataset of X posts from recent Canadian wildfires, annotated
across twelve key themes. Evaluating both vision-language models and
custom-trained classifiers, we show that while zero-shot prompting offers quick
deployment, even simple trained models outperform them when labelled data is
available. Our best-performing transformer-based fine-tuned model reaches 83%
f-score, outperforming gpt4 by 23%. As a use case, we demonstrate how this
model can be used to uncover trends during wildfires. Our findings highlight
the enduring importance of tailored datasets and task-specific training.
Importantly, such datasets should be localized, as disaster response
requirements vary across regions and contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient
  Fine-Tuning of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyegang Son, Yonglak Son, Changhoon Kim, Young Geun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based large-scale pre-trained models achieve great success.
Fine-tuning is the standard practice for leveraging these models in downstream
tasks. Among the fine-tuning methods, adapter-tuning provides a
parameter-efficient fine-tuning by introducing lightweight trainable modules
while keeping most pre-trained parameters frozen. However, existing
adapter-tuning methods still impose substantial resource usage. Through our
investigation, we show that each adapter unequally contributes to both task
performance and resource usage. Motivated by this insight, we propose Selective
Adapter FrEezing (SAFE), which gradually freezes less important adapters early
to reduce unnecessary resource usage while maintaining performance. In our
experiments, SAFE reduces memory usage, computation amount, and training time
by 42.85\%, 34.59\%, and 11.82\%, respectively, while achieving comparable or
better task performance compared to the baseline. We also demonstrate that SAFE
induces regularization effect, thereby smoothing the loss landscape, which
enables the model to generalize better by avoiding sharp minima.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>URL: https://aclanthology.org/2025.naacl-long.480/ Volume:
  Proceedings of the 2025 Conference of the Nations of the Americas Chapter of
  the Association for Computational Linguistics: Human Language Technologies
  (Volume 1: Long Papers) Year: 2025 Address: Albuquerque, New Mexico</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Graph Foundation Models: Training on Knowledge Graphs Enables
  Transferability to General Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12609v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12609v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Wang, Siqiang Luo, Caihua Shan, Yifei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the success of large language models, there is a trend toward
developing graph foundation models to conduct diverse downstream tasks in
various domains. However, current models often require extra fine-tuning to
apply their learned structural and semantic representations to new graphs,
which limits their versatility. Recent breakthroughs in zero-shot inductive
reasoning on knowledge graphs (KGs), offer us a new perspective on extending KG
reasoning to general graph applications. In this paper, we introduce SCR, a
unified graph reasoning framework designed to train on knowledge graphs and
effectively generalize across a wide range of graph tasks and domains. We begin
by designing the task-specific KG structures to establish a unified topology
for different task formats. Then we propose semantic-conditioned message
passing, a novel mechanism addressing the inherent semantic isolation in
traditional KG reasoning, by jointly modeling structural and semantic
invariance patterns in graph representations. To demonstrate the effectiveness,
we evaluate the inductive reasoning capability of SCR using 38 diverse graph
datasets, covering node-level, link-level, and graph-level tasks across
multiple domains. Our results show substantial performance gains over existing
foundation models and supervised baselines, highlighting the efficacy and
adaptability of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 Pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GPU Performance Portability needs Autotuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Burkhard Ringlein, Thomas Parnell, Radu Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLMs grow in complexity, achieving state-of-the-art performance requires
tight co-design across algorithms, software, and hardware. Today's reliance on
a single dominant platform limits portability, creates vendor lock-in, and
raises barriers for new AI hardware. In this work, we make the case for
combining just-in-time (JIT) compilation with kernel parameter autotuning to
enable portable LLM inference with state-of-the-art performance without code
changes. Focusing on flash attention -- a widespread performance critical LLM
kernel -- we demonstrate that this approach explores up to 15x more kernel
parameter configurations, produces significantly more diverse code across
multiple dimensions, and even outperforms vendor-optimized implementations by
up to 230%, all while reducing kernel code size by 70x and eliminating manual
code optimizations. Our results highlight autotuning as a promising path to
unlocking model portability across GPU vendors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>typos, fix grammatical mistakes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Lazy Student's Dream: Chat<span class="highlight-title">GPT</span> Passing an Engineering Course on Its
  Own 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.05760v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.05760v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokul Puthumanaillam, Melkior Ornik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive investigation into the capability of
Large Language Models (LLMs) to successfully complete a semester-long
undergraduate control systems course. Through evaluation of 115 course
deliverables, we assess LLM performance using ChatGPT under a "minimal effort"
protocol that simulates realistic student usage patterns. The investigation
employs a rigorous testing methodology across multiple assessment formats, from
auto-graded multiple choice questions to complex Python programming tasks and
long-form analytical writing. Our analysis provides quantitative insights into
AI's strengths and limitations in handling mathematical formulations, coding
challenges, and theoretical concepts in control systems engineering. The LLM
achieved a B-grade performance (82.24\%), approaching but not exceeding the
class average (84.99\%), with strongest results in structured assignments and
greatest limitations in open-ended projects. The findings inform discussions
about course design adaptation in response to AI advancement, moving beyond
simple prohibition towards thoughtful integration of these tools in engineering
education. Additional materials including syllabus, examination papers, design
projects, and example responses can be found at the project website:
https://gradegpt.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Power Grid Topologies with Reinforcement Learning: A <span class="highlight-title">Survey</span>
  of Methods and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erica van der Sar, Alessandro Zocca, Sandjai Bhulai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Power grid operation is becoming increasingly complex due to the rising
integration of renewable energy sources and the need for more adaptive control
strategies. Reinforcement Learning (RL) has emerged as a promising approach to
power network control (PNC), offering the potential to enhance decision-making
in dynamic and uncertain environments. The Learning To Run a Power Network
(L2RPN) competitions have played a key role in accelerating research by
providing standardized benchmarks and problem formulations, leading to rapid
advancements in RL-based methods. This survey provides a comprehensive and
structured overview of RL applications for power grid topology optimization,
categorizing existing techniques, highlighting key design choices, and
identifying gaps in current research. Additionally, we present a comparative
numerical study evaluating the impact of commonly applied RL-based methods,
offering insights into their practical effectiveness. By consolidating existing
research and outlining open challenges, this survey aims to provide a
foundation for future advancements in RL-driven power grid optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>60 pages, 26 figures, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time Awareness in Large Language Models: Benchmarking Fact Recall Across
  Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13338v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13338v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Herel, Vojtech Bartek, Jiri Jirak, Tomas Mikolov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Who is the US President? The answer changes depending on when the question is
asked. While large language models (LLMs) are evaluated on various reasoning
tasks, they often miss a crucial dimension: time. In real-world scenarios, the
correctness of answers is frequently tied to temporal context. To address this
gap, we present a novel framework and dataset spanning over 8,000 events from
2018 to 2024, annotated with day-level granularity and sourced globally across
domains such as politics, science, and business. Our TimeShift evaluation
method systematically probes LLMs for temporal reasoning, revealing that base
models often outperform instruction-tuned and synthetic-trained counterparts on
time-sensitive recall. Additionally, we find that even large-scale models
exhibit brittleness in handling paraphrased facts, highlighting unresolved
challenges in temporal consistency. By identifying these limitations, our work
provides a significant step toward advancing time-aware language models capable
of adapting to the dynamic nature of real-world knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intelligently Augmented Contrastive Tensor Factorization: Empowering
  Multi-dimensional Time Series Classification in Low-Data Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anushiya Arunan, Yan Qin, Xiaoli Li, Yuen Chau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification of multi-dimensional time series from real-world systems
require fine-grained learning of complex features such as cross-dimensional
dependencies and intra-class variations-all under the practical challenge of
low training data availability. However, standard deep learning (DL) struggles
to learn generalizable features in low-data environments due to model
overfitting. We propose a versatile yet data-efficient framework, Intelligently
Augmented Contrastive Tensor Factorization (ITA-CTF), to learn effective
representations from multi-dimensional time series. The CTF module learns core
explanatory components of the time series (e.g., sensor factors, temporal
factors), and importantly, their joint dependencies. Notably, unlike standard
tensor factorization (TF), the CTF module incorporates a new contrastive loss
optimization to induce similarity learning and class-awareness into the learnt
representations for better classification performance. To strengthen this
contrastive learning, the preceding ITA module generates targeted but
informative augmentations that highlight realistic intra-class patterns in the
original data, while preserving class-wise properties. This is achieved by
dynamically sampling a "soft" class prototype to guide the warping of each
query data sample, which results in an augmentation that is intelligently
pattern-mixed between the "soft" class prototype and the query sample. These
augmentations enable the CTF module to recognize complex intra-class variations
despite the limited original training data, and seek out invariant class-wise
properties for accurate classification performance. The proposed method is
comprehensively evaluated on five different classification tasks. Compared to
standard TF and several DL benchmarks, notable performance improvements up to
18.7% were achieved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Expert Systems with Applications
  (DOI:https://doi.org/10.1016/j.eswa.2025.127889)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Provable Scaling Laws for the Test-Time Compute of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19477v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19477v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose two simple, principled and practical algorithms that enjoy
provable scaling laws for the test-time compute of large language models
(LLMs). The first one is a two-stage knockout-style algorithm: given an input
problem, it first generates multiple candidate solutions, and then aggregate
them via a knockout tournament for the final output. Assuming that the LLM can
generate a correct solution with non-zero probability and do better than a
random guess in comparing a pair of correct and incorrect solutions, we prove
theoretically that the failure probability of this algorithm decays to zero
exponentially or by a power law (depending on the specific way of scaling) as
its test-time compute grows. The second one is a two-stage league-style
algorithm, where each candidate is evaluated by its average win rate against
multiple opponents, rather than eliminated upon loss to a single opponent.
Under analogous but more robust assumptions, we prove that its failure
probability also decays to zero exponentially with more test-time compute. Both
algorithms require a black-box LLM and nothing else (e.g., no verifier or
reward model) for a minimalistic implementation, which makes them appealing for
practical applications and easy to adapt for different tasks. Through extensive
experiments with diverse models and datasets, we validate the proposed theories
and demonstrate the outstanding scaling properties of both algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demonstrating specification gaming in reasoning models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13295v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13295v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Bondarenko, Denis Volk, Dmitrii Volkov, Jeffrey Ladish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate LLM agent specification gaming by instructing models to win
against a chess engine. We find reasoning models like OpenAI o3 and DeepSeek R1
will often hack the benchmark by default, while language models like GPT-4o and
Claude 3.5 Sonnet need to be told that normal play won't work to hack.
  We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024;
Weij et al., 2024) by using realistic task prompts and avoiding excess nudging.
Our results suggest reasoning models may resort to hacking to solve difficult
problems, as observed in OpenAI (2024)'s o1 Docker escape during cyber
capabilities testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated with o3 results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph neural networks and MSO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07816v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07816v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Veeti Ahvonen, Damian Heiman, Antti Kuusisto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We give an alternative proof for the existing result that recurrent graph
neural networks working with reals have the same expressive power in
restriction to monadic second-order logic MSO as the graded modal substitution
calculus. The proof is based on constructing distributed automata that capture
all MSO-definable node properties over trees. We also consider some variants of
the acceptance conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiari Pizzini Cavagna, Daniele Cesarini, Andrea Bartolini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing demand for generative AI as Large Language Models (LLMs)
services has driven the need for specialized hardware architectures that
optimize computational efficiency and energy consumption. This paper evaluates
the performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic
linear algebra kernels at reduced numerical precision, a fundamental operation
in LLM computations. We present a detailed characterization of Grayskull's
execution model, gridsize, matrix dimensions, data formats, and numerical
precision impact computational efficiency. Furthermore, we compare Grayskull's
performance against state-of-the-art architectures with tensor acceleration,
including Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).
Whilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a
competitive trade-off between power consumption and computational throughput,
reaching a peak of 1.55 TFLOPs/Watt with BF16.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Computational Aspects of Deep Learning Workshop at
  ISC High Performance 2025. To appear in the ISC High Performance 2025
  Workshop Proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KBAlign: Efficient Self Adaptation on Specific Knowledge Bases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14790v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14790v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheni Zeng, Yuxuan Chen, Shi Yu, Ruobing Wang, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although retrieval-augmented generation (RAG) remains essential for
knowledge-based question answering (KBQA), current paradigms face critical
challenges under specific domains. Existing methods struggle with targeted
adaptation on small-scale KBs: vanilla unsupervised training exhibits poor
effectiveness, while fine-tuning incurs prohibitive costs of external signals.
We present KBAlign, a self-supervised framework that enhances RAG systems
through efficient model adaptation. Our key insight is to leverage the model's
intrinsic capabilities for knowledge alignment through two innovative
mechanisms: multi-grained self-annotation that captures global knowledge for
data construction, and iterative tuning that accelerates convergence through
self verification. This framework enables cost-effective model adaptation to
specific textual KBs, without human supervision or external model assistance.
Experiments demonstrate that KBAlign can achieve 90\% of the performance gain
obtained through GPT-4-supervised adaptation, while relying entirely on
self-annotation of much smaller models. KBAlign significantly improves
downstream QA accuracy across multiple domains with tiny costs, particularly
benefiting scenarios requiring deep knowledge integration from specialized
corpora. We release our experimental data, models, and process analyses to the
community for further exploration (https://github.com/thunlp/KBAlign).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ImprovNet -- Generating Controllable Musical Improvisations with
  Iterative Corruption Refinement <span class="chip">IJCNN 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04522v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04522v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keshav Bhandari, Sungkyun Chang, Tongyu Lu, Fareza R. Enus, Louis B. Bradshaw, Dorien Herremans, Simon Colton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite deep learning's remarkable advances in style transfer across various
domains, generating controllable performance-level musical style transfer for
complete symbolically represented musical works remains a challenging area of
research. Much of this is owed to limited datasets, especially for genres such
as jazz, and the lack of unified models that can handle multiple music
generation tasks. This paper presents ImprovNet, a transformer-based
architecture that generates expressive and controllable musical improvisations
through a self-supervised corruption-refinement training strategy. The
improvisational style transfer is aimed at making meaningful modifications to
one or more musical elements - melody, harmony or rhythm of the original
composition with respect to the target genre. ImprovNet unifies multiple
capabilities within a single model: it can perform cross-genre and intra-genre
improvisations, harmonize melodies with genre-specific styles, and execute
short prompt continuation and infilling tasks. The model's iterative generation
framework allows users to control the degree of style transfer and structural
similarity to the original composition. Objective and subjective evaluations
demonstrate ImprovNet's effectiveness in generating musically coherent
improvisations while maintaining structural relationships with the original
pieces. The model outperforms Anticipatory Music Transformer in short
continuation and infilling tasks and successfully achieves recognizable genre
conversion, with 79\% of participants correctly identifying jazz-style
improvisations of classical pieces. Our code and demo page can be found at
https://github.com/keshavbhandari/improvnet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, IJCNN 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Convergence: Mutual Distillation is Secretly a Form of
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02481v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02481v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengpeng Xie, Jiahang Cao, Qiang Zhang, Jianxiong Zhang, Changwei Wang, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we argue that mutual distillation between reinforcement
learning policies serves as an implicit regularization, preventing them from
overfitting to irrelevant features. We highlight two key contributions: (a)
Theoretically, for the first time, we prove that enhancing the policy
robustness to irrelevant features leads to improved generalization performance.
(b) Empirically, we demonstrate that mutual distillation between policies
contributes to such robustness, enabling the spontaneous emergence of invariant
representations over pixel inputs. Overall, our findings challenge the
conventional view of distillation as merely a means of knowledge transfer,
offering a novel perspective on the generalization in deep reinforcement
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aggregating Concepts of Accuracy and Fairness in Prediction Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Kinney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An algorithm that outputs predictions about the state of the world will
almost always be designed with the implicit or explicit goal of outputting
accurate predictions (i.e., predictions that are likely to be true). In
addition, the rise of increasingly powerful predictive algorithms brought about
by the recent revolution in artificial intelligence has led to an emphasis on
building predictive algorithms that are fair, in the sense that their
predictions do not systematically evince bias or bring about harm to certain
individuals or groups. This state of affairs presents two conceptual
challenges. First, the goals of accuracy and fairness can sometimes be in
tension, and there are no obvious normative guidelines for managing the
trade-offs between these two desiderata when they arise. Second, there are many
distinct ways of measuring both the accuracy and fairness of a predictive
algorithm; here too, there are no obvious guidelines on how to aggregate our
preferences for predictive algorithms that satisfy disparate measures of
fairness and accuracy to various extents. The goal of this paper is to address
these challenges by arguing that there are good reasons for using a linear
combination of accuracy and fairness metrics to measure the
all-things-considered value of a predictive algorithm for agents who care about
both accuracy and fairness. My argument depends crucially on a classic result
in the preference aggregation literature due to Harsanyi. After making this
formal argument, I apply my result to an analysis of accuracy-fairness
trade-offs using the COMPAS dataset compiled by Angwin et al.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Progress Driven Multi-Agent Curriculum <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10016v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10016v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenshuai Zhao, Zhiyuan Li, Joni Pajarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The number of agents can be an effective curriculum variable for controlling
the difficulty of multi-agent reinforcement learning (MARL) tasks. Existing
work typically uses manually defined curricula such as linear schemes. We
identify two potential flaws while applying existing reward-based automatic
curriculum learning methods in MARL: (1) The expected episode return used to
measure task difficulty has high variance; (2) Credit assignment difficulty can
be exacerbated in tasks where increasing the number of agents yields higher
returns which is common in many MARL tasks. To address these issues, we propose
to control the curriculum by using a TD-error based *learning progress* measure
and by letting the curriculum proceed from an initial context distribution to
the final task specific one. Since our approach maintains a distribution over
the number of agents and measures learning progress rather than absolute
performance, which often increases with the number of agents, we alleviate
problem (2). Moreover, the learning progress measure naturally alleviates
problem (1) by aggregating returns. In three challenging sparse-reward MARL
benchmarks, our approach outperforms state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> Learning for Acoustic Few-Shot Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyong Liang, Bernd Meyer, Isaac Ning Lee, Thanh-Toan Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Labelled data are limited and self-supervised learning is one of the most
important approaches for reducing labelling requirements. While it has been
extensively explored in the image domain, it has so far not received the same
amount of attention in the acoustic domain. Yet, reducing labelling is a key
requirement for many acoustic applications. Specifically in bioacoustic, there
are rarely sufficient labels for fully supervised learning available. This has
led to the widespread use of acoustic recognisers that have been pre-trained on
unrelated data for bioacoustic tasks. We posit that training on the actual task
data and combining self-supervised pre-training with few-shot classification is
a superior approach that has the ability to deliver high accuracy even when
only a few labels are available. To this end, we introduce and evaluate a new
architecture that combines CNN-based preprocessing with feature extraction
based on state space models (SSMs). This combination is motivated by the fact
that CNN-based networks alone struggle to capture temporal information
effectively, which is crucial for classifying acoustic signals. SSMs,
specifically S4 and Mamba, on the other hand, have been shown to have an
excellent ability to capture long-range dependencies in sequence data. We
pre-train this architecture using contrastive learning on the actual task data
and subsequent fine-tuning with an extremely small amount of labelled data. We
evaluate the performance of this proposed architecture for ($n$-shot,
$n$-class) classification on standard benchmarks as well as real-world data.
Our evaluation shows that it outperforms state-of-the-art architectures on the
few-shot classification problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM A*: Human in the Loop Large Language Models Enabled A* Search for
  Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01797v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01797v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengjia Xiao, Peng Wang, Mingzhe Yu, Mattia Robbiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research focuses on how Large Language Models (LLMs) can help with
(path) planning for mobile embodied agents such as robots, in a
human-in-the-loop and interactive manner. A novel framework named LLM A*, aims
to leverage the commonsense of LLMs, and the utility-optimal A* is proposed to
facilitate few-shot near-optimal path planning. Prompts are used for two main
purposes: 1) to provide LLMs with essential information like environments,
costs, heuristics, etc.; 2) to communicate human feedback on intermediate
planning results to LLMs. This approach takes human feedback on board and
renders the entire planning process transparent (akin to a `white box') to
humans. Moreover, it facilitates code-free path planning, thereby fostering the
accessibility and inclusiveness of artificial intelligence techniques to
communities less proficient in coding. Comparative analysis against A* and RL
demonstrates that LLM A* exhibits greater efficiency in terms of search space
and achieves paths comparable to A* while outperforming RL. The interactive
nature of LLM A* also makes it a promising tool for deployment in collaborative
human-robot tasks. Codes and Supplemental Materials can be found at GitHub:
https://github.com/speedhawk/LLM-A-.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 figures, 8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Addressing and Visualizing Misalignments in Human Task-Solving
  Trajectories <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14191v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14191v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sejin Kim, Hosung Lee, Sundong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding misalignments in human task-solving trajectories is critical
for improving AI models trained to mimic human reasoning. This study
categorizes such misalignments into three types: \textbf{(1) Lack of functions
to express intent}, \textbf{(2) Inefficient action sequences}, and \textbf{(3)
Incorrect intentions that cannot solve the task}. To address these issues, we
first formalize and define these three types of misalignments. We then propose
a heuristic algorithm to detect these misalignments in O2ARC trajectories and
conduct a hierarchical and quantitative analysis of their impact. Furthermore,
we introduce an intention estimation algorithm that predicts missing alignment
information between user actions and inferred intentions, leveraging our
formalized framework. Through trajectory alignment, we experimentally
demonstrate that AI models trained on human task-solving trajectories improve
performance in mimicking human reasoning. Based on hierarchical analysis and
experiments, we highlight the importance of trajectory-intention alignment and
demonstrate the potential of intention learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD 2025 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring
  with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peichao Lai, Kexuan Zhang, Yi Lin, Linyihan Zhang, Feiyang Ye, Jinhao Yan, Yanwei Xu, Conghui He, Yilei Wang, Wentao Zhang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subjective Answer Grading (SAG) plays a crucial role in education,
standardized testing, and automated assessment systems, particularly for
evaluating short-form responses in Short Answer Scoring (SAS). However,
existing approaches often produce coarse-grained scores and lack detailed
reasoning. Although large language models (LLMs) have demonstrated potential as
zero-shot evaluators, they remain susceptible to bias, inconsistencies with
human judgment, and limited transparency in scoring decisions. To overcome
these limitations, we introduce SAS-Bench, a benchmark specifically designed
for LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,
expert-annotated error categories, and a diverse range of question types
derived from real-world subject-specific exams. This benchmark facilitates
detailed evaluation of model reasoning processes and explainability. We also
release an open-source dataset containing 1,030 questions and 4,109 student
responses, each annotated by domain experts. Furthermore, we conduct
comprehensive experiments with various LLMs, identifying major challenges in
scoring science-related questions and highlighting the effectiveness of
few-shot prompting in improving scoring accuracy. Our work offers valuable
insights into the development of more robust, fair, and educationally
meaningful LLM-based evaluation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AriGraph: Learning Knowledge Graph World Models with Episodic Memory for
  LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04363v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04363v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Andrey Kravchenko, Mikhail Burtsev, Evgeny Burnaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in the capabilities of Large Language Models (LLMs) have created
a promising foundation for developing autonomous agents. With the right tools,
these agents could learn to solve tasks in new environments by accumulating and
updating their knowledge. Current LLM-based agents process past experiences
using a full history of observations, summarization, retrieval augmentation.
However, these unstructured memory representations do not facilitate the
reasoning and planning essential for complex decision-making. In our study, we
introduce AriGraph, a novel method wherein the agent constructs and updates a
memory graph that integrates semantic and episodic memories while exploring the
environment. We demonstrate that our Ariadne LLM agent, consisting of the
proposed memory architecture augmented with planning and decision-making,
effectively handles complex tasks within interactive text game environments
difficult even for human players. Results show that our approach markedly
outperforms other established memory methods and strong RL baselines in a range
of problems of varying complexity. Additionally, AriGraph demonstrates
competitive performance compared to dedicated knowledge graph-based methods in
static multi-hop question-answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code for this work is avaliable at
  https://github.com/AIRI-Institute/AriGraph</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Risk-sensitive Reinforcement Learning Based on Convex Scoring Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.04553v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.04553v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanyu Han, Yang Liu, Xiang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a reinforcement learning (RL) framework under a broad class of
risk objectives, characterized by convex scoring functions. This class covers
many common risk measures, such as variance, Expected Shortfall, entropic
Value-at-Risk, and mean-risk utility. To resolve the time-inconsistency issue,
we consider an augmented state space and an auxiliary variable and recast the
problem as a two-state optimization problem. We propose a customized
Actor-Critic algorithm and establish some theoretical approximation guarantees.
A key theoretical contribution is that our results do not require the Markov
decision process to be continuous. Additionally, we propose an auxiliary
variable sampling method inspired by the alternating minimization algorithm,
which is convergent under certain conditions. We validate our approach in
simulation experiments with a financial application in statistical arbitrage
trading, demonstrating the effectiveness of the algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniVLA: Learning to Act Anywhere with Task-centric Latent Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06111v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06111v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, Hongyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A generalist robot should perform effectively across various environments.
However, most existing approaches heavily rely on scaling action-annotated data
to enhance their capabilities. Consequently, they are often limited to single
physical specification and struggle to learn transferable knowledge across
different embodiments and environments. To confront these limitations, we
propose UniVLA, a new framework for learning cross-embodiment
vision-language-action (VLA) policies. Our key innovation is to derive
task-centric action representations from videos with a latent action model.
This enables us to exploit extensive data across a wide spectrum of embodiments
and perspectives. To mitigate the effect of task-irrelevant dynamics, we
incorporate language instructions and establish a latent action model within
the DINO feature space. Learned from internet-scale videos, the generalist
policy can be deployed to various robots through efficient latent action
decoding. We obtain state-of-the-art results across multiple manipulation and
navigation benchmarks, as well as real-robot deployments. UniVLA achieves
superior performance over OpenVLA with less than 1/20 of pretraining compute
and 1/10 of downstream data. Continuous performance improvements are observed
as heterogeneous data, even including human videos, are incorporated into the
training pipeline. The results underscore UniVLA's potential to facilitate
scalable and efficient robot policy learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to RSS 2025. Code is available at
  https://github.com/OpenDriveLab/UniVLA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative <span class="highlight-title">Pre-train</span>ed Autoregressive Diffusion <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhang, Jiacheng Jiang, Guoqing Ma, Zhiying Lu, Haoyang Huang, Jianlong Yuan, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present GPDiT, a Generative Pre-trained Autoregressive
Diffusion Transformer that unifies the strengths of diffusion and
autoregressive modeling for long-range video synthesis, within a continuous
latent space. Instead of predicting discrete tokens, GPDiT autoregressively
predicts future latent frames using a diffusion loss, enabling natural modeling
of motion dynamics and semantic consistency across frames. This continuous
autoregressive framework not only enhances generation quality but also endows
the model with representation capabilities. Additionally, we introduce a
lightweight causal attention variant and a parameter-free rotation-based
time-conditioning mechanism, improving both the training and inference
efficiency. Extensive experiments demonstrate that GPDiT achieves strong
performance in video generation quality, video representation ability, and
few-shot learning tasks, highlighting its potential as an effective framework
for video modeling in continuous space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CreativeSynth: Cross-Art-Attention for Artistic Image Synthesis with
  Multimodal Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14066v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14066v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nisha Huang, Weiming Dong, Yuxin Zhang, Fan Tang, Ronghui Li, Chongyang Ma, Xiu Li, Tong-Yee Lee, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although remarkable progress has been made in image style transfer, style is
just one of the components of artistic paintings. Directly transferring
extracted style features to natural images often results in outputs with
obvious synthetic traces. This is because key painting attributes including
layout, perspective, shape, and semantics often cannot be conveyed and
expressed through style transfer. Large-scale pretrained text-to-image
generation models have demonstrated their capability to synthesize a vast
amount of high-quality images. However, even with extensive textual
descriptions, it is challenging to fully express the unique visual properties
and details of paintings. Moreover, generic models often disrupt the overall
artistic effect when modifying specific areas, making it more complicated to
achieve a unified aesthetic in artworks. Our main novel idea is to integrate
multimodal semantic information as a synthesis guide into artworks, rather than
transferring style to the real world. We also aim to reduce the disruption to
the harmony of artworks while simplifying the guidance conditions.
Specifically, we propose an innovative multi-task unified framework called
CreativeSynth, based on the diffusion model with the ability to coordinate
multimodal inputs. CreativeSynth combines multimodal features with customized
attention mechanisms to seamlessly integrate real-world semantic content into
the art domain through Cross-Art-Attention for aesthetic maintenance and
semantic fusion. We demonstrate the results of our method across a wide range
of different art categories, proving that CreativeSynth bridges the gap between
generative models and artistic expression. Code and results are available at
https://github.com/haha-lisa/CreativeSynth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flexible Graph Similarity Computation With A Proactive Optimization
  Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06533v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06533v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouyang Liu, Ning Liu, Yixin Chen, Jiezhong He, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Edit Distance (GED) offers a principled and flexible measure of graph
similarity, as it quantifies the minimum cost needed to transform one graph
into another with customizable edit operation costs. Despite recent
learning-based efforts to approximate GED via vector space representations,
existing methods struggle with adapting to varying operation costs.
Furthermore, they suffer from inefficient, reactive mapping refinements due to
reliance on isolated node-level distance as guidance. To address these issues,
we propose GEN, a novel learning-based approach for flexible GED approximation.
GEN addresses the varying costs adaptation by integrating operation costs prior
to match establishment, enabling mappings to dynamically adapt to cost
variations. Furthermore, GEN introduces a proactive guidance optimization
strategy that captures graph-level dependencies between matches, allowing
informed matching decisions in a single step without costly iterative
refinements. Extensive evaluations on real-world and synthetic datasets
demonstrate that GEN achieves up to 37.8% reduction in GED approximation error
and 72.7% reduction in inference time compared with state-of-the-art methods,
while consistently maintaining robustness under diverse cost settings and graph
sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Illegal Waste Detection in Remote Sensing Images: A Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06607v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06607v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Gibellini, Piero Fraternali, Giacomo Boracchi, Luca Morandini, Thomas Martinoli, Andrea Diecidue, Simona Malegori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environmental crime is the third largest criminal activity worldwide, with
significant revenues coming from illegal management of solid waste. Thanks to
the increasing availability and the decreasing cost of Very High Resolution
Remote Sensing (VHR RS) images, the fight against environmental crime can
nowadays rely on modern image-analysis tools to support photo-interpretation
for scanning vast territories in search of illegal waste disposal sites. This
paper illustrates a semi-automatic waste detection pipeline, developed in
collaboration with a regional environmental protection agency, for detecting
candidate illegal dumping sites in VHR RS images. To optimize the effectiveness
of the waste detector, extensive experiments evaluate such design choices as
the network architecture, the ground resolution and geographic span of the
input images, as well as the pretraining procedures. The best model attains
remarkable performance, achieving 92.02% F1-Score and 94.56% Accuracy. A
generalization study assesses the performance variation when the detector
processes images from a territory substantially different from the one used
during training, incurring only a moderate performance loss, i.e., 6.5%
decrease in the F1-Score. Finally, an exercise in which photo interpreters
compare the territory scanning effort with and without the support of the waste
detector assesses the concrete benefit of using a computer-aided image analysis
tool in a professional environment protection agency. Results show that a
reduction up to 30% of the time spent for waste site detection can be attained.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PersLLM: A Personified Training Approach for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12393v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12393v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheni Zeng, Jiayi Chen, Huimin Chen, Yukun Yan, Yuxuan Chen, Zhenghao Liu, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit human-like intelligence, enabling them
to simulate human behavior and support various applications that require both
humanized communication and extensive knowledge reserves. Efforts are made to
personify LLMs with special training data or hand-crafted prompts, while
correspondingly faced with challenges such as insufficient data usage or rigid
behavior patterns. Consequently, personified LLMs fail to capture personified
knowledge or express persistent opinion. To fully unlock the potential of LLM
personification, we propose PersLLM, a framework for better data construction
and model tuning. For insufficient data usage, we incorporate strategies such
as Chain-of-Thought prompting and anti-induction, improving the quality of data
construction and capturing the personality experiences, knowledge, and thoughts
more comprehensively. For rigid behavior patterns, we design the tuning process
and introduce automated DPO to enhance the specificity and dynamism of the
models' personalities, which leads to a more natural opinion communication.
Both automated metrics and expert human evaluations demonstrate the
effectiveness of our approach. Case studies in human-machine interactions and
multi-agent systems further suggest potential application scenarios and future
directions for LLM personification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages for main text, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Triplane <span class="highlight-title">Transformer</span>s as Occupancy World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.07338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.07338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Xu, Peixi Peng, Guang Tan, Yiqian Chang, Yisen Zhao, Yonghong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models aim to learn or construct representations of the environment
that enable the prediction of future scenes, thereby supporting intelligent
motion planning. However, existing models often struggle to produce
fine-grained predictions and to operate in real time. In this work, we propose
T$^3$Former, a novel 4D occupancy world model for autonomous driving.
T$^3$Former begins by pre-training a compact {\em triplane} representation that
efficiently encodes 3D occupancy. It then extracts multi-scale temporal motion
features from historical triplanes and employs an autoregressive approach to
iteratively predict future triplane changes. Finally, these triplane changes
are combined with previous states to decode future occupancy and ego-motion
trajectories. Experimental results show that T$^3$Former achieves 1.44$\times$
speedup (26 FPS), improves mean IoU to 36.09, and reduces mean absolute
planning error to 1.0 meters. Demos are available in the supplementary
material.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models for Cyber Security: A Systematic Literature <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04760v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04760v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxiang Xu, Shenao Wang, Ningke Li, Kailong Wang, Yanjie Zhao, Kai Chen, Ting Yu, Yang Liu, Haoyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has opened up new
opportunities for leveraging artificial intelligence in various domains,
including cybersecurity. As the volume and sophistication of cyber threats
continue to grow, there is an increasing need for intelligent systems that can
automatically detect vulnerabilities, analyze malware, and respond to attacks.
In this survey, we conduct a comprehensive review of the literature on the
application of LLMs in cybersecurity (LLM4Security). By comprehensively
collecting over 30K relevant papers and systematically analyzing 127 papers
from top security and software engineering venues, we aim to provide a holistic
view of how LLMs are being used to solve diverse problems across the
cybersecurity domain. Through our analysis, we identify several key findings.
First, we observe that LLMs are being applied to a wide range of cybersecurity
tasks, including vulnerability detection, malware analysis, network intrusion
detection, and phishing detection. Second, we find that the datasets used for
training and evaluating LLMs in these tasks are often limited in size and
diversity, highlighting the need for more comprehensive and representative
datasets. Third, we identify several promising techniques for adapting LLMs to
specific cybersecurity domains, such as fine-tuning, transfer learning, and
domain-specific pre-training. Finally, we discuss the main challenges and
opportunities for future research in LLM4Security, including the need for more
interpretable and explainable models, the importance of addressing data privacy
and security concerns, and the potential for leveraging LLMs for proactive
defense and threat hunting. Overall, our survey provides a comprehensive
overview of the current state-of-the-art in LLM4Security and identifies several
promising directions for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>56 pages,6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding In-context Learning of Addition via Activation Subspaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyan Hu, Kayo Yin, Michael I. Jordan, Jacob Steinhardt, Lijie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To perform in-context learning, language models must extract signals from
individual few-shot examples, aggregate these into a learned prediction rule,
and then apply this rule to new examples. How is this implemented in the
forward pass of modern transformer models? To study this, we consider a
structured family of few-shot learning tasks for which the true prediction rule
is to add an integer $k$ to the input. We find that Llama-3-8B attains high
accuracy on this task for a range of $k$, and localize its few-shot ability to
just three attention heads via a novel optimization approach. We further show
the extracted signals lie in a six-dimensional subspace, where four of the
dimensions track the unit digit and the other two dimensions track overall
magnitude. We finally examine how these heads extract information from
individual few-shot examples, identifying a self-correction mechanism in which
mistakes from earlier examples are suppressed by later examples. Our results
demonstrate how tracking low-dimensional subspaces across a forward pass can
provide insight into fine-grained computational structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SensorChat: Answering Qualitative and Quantitative Questions during
  Long-Term Multimodal Sensor Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofan Yu, Lanxiang Hu, Benjamin Reichman, Dylan Chu, Rushil Chandrupatla, Xiyuan Zhang, Larry Heck, Tajana Rosing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language interaction with sensing systems is crucial for addressing
users' personal concerns and providing health-related insights into their daily
lives. When a user asks a question, the system automatically analyzes the full
history of sensor data, extracts relevant information, and generates an
appropriate response. However, existing systems are limited to short-duration
(e.g., one minute) or low-frequency (e.g., daily step count) sensor data. In
addition, they struggle with quantitative questions that require precise
numerical answers. In this work, we introduce SensorChat, the first end-to-end
QA system designed for daily life monitoring using long-duration,
high-frequency time series data. Given raw sensor signals spanning multiple
days and a user-defined natural language question, SensorChat generates
semantically meaningful responses that directly address user concerns.
SensorChat effectively handles both quantitative questions that require
numerical precision and qualitative questions that require high-level reasoning
to infer subjective insights. To achieve this, SensorChat uses an innovative
three-stage pipeline including question decomposition, sensor data query, and
answer assembly. The first and third stages leverage Large Language Models
(LLMs) to interpret human queries and generate responses. The intermediate
querying stage extracts relevant information from the complete sensor data
history. Real-world implementation demonstrate SensorChat's capability for
real-time interactions on a cloud server while also being able to run entirely
on edge platforms after quantization. Comprehensive QA evaluations show that
SensorChat achieves up to 93% higher answer accuracy than state-of-the-art
systems on quantitative questions. Additionally, a user study with eight
volunteers highlights SensorChat's effectiveness in answering qualitative and
open-ended questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TSINR: Capturing Temporal Continuity via Implicit Neural Representations
  for Time Series Anomaly Detection <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11641v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11641v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengxuan Li, Ke Liu, Hongyang Chen, Jiajun Bu, Hongwei Wang, Haishuai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series anomaly detection aims to identify unusual patterns in data or
deviations from systems' expected behavior. The reconstruction-based methods
are the mainstream in this task, which learn point-wise representation via
unsupervised learning. However, the unlabeled anomaly points in training data
may cause these reconstruction-based methods to learn and reconstruct anomalous
data, resulting in the challenge of capturing normal patterns. In this paper,
we propose a time series anomaly detection method based on implicit neural
representation (INR) reconstruction, named TSINR, to address this challenge.
Due to the property of spectral bias, TSINR enables prioritizing low-frequency
signals and exhibiting poorer performance on high-frequency abnormal data.
Specifically, we adopt INR to parameterize time series data as a continuous
function and employ a transformer-based architecture to predict the INR of
given data. As a result, the proposed TSINR method achieves the advantage of
capturing the temporal continuity and thus is more sensitive to discontinuous
anomaly data. In addition, we further design a novel form of INR continuous
function to learn inter- and intra-channel information, and leverage a
pre-trained large language model to amplify the intense fluctuations in
anomalies. Extensive experiments demonstrate that TSINR achieves superior
overall performance on both univariate and multivariate time series anomaly
detection benchmarks compared to other state-of-the-art reconstruction-based
methods. Our codes are available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Translating Electrocardiograms to Cardiac Magnetic Resonance Imaging
  Useful for Cardiac Assessment and Disease Screening: A Multi-Center Study AI
  for ECG to CMR Translation Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13602v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13602v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyao Ding, Ziyu Li, Yujian Hu, Youyao Xu, Chengchen Zhao, Yiheng Mao, Haitao Li, Zhikang Li, Qian Li, Jing Wang, Yue Chen, Mengjia Chen, Longbo Wang, Xuesen Chu, Weichao Pan, Ziyi Liu, Fei Wu, Hongkun Zhang, Ting Chen, Zhengxing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiovascular diseases (CVDs) are the leading cause of global mortality,
necessitating accessible and accurate diagnostic tools. While cardiac magnetic
resonance imaging (CMR) provides gold-standard insights into cardiac structure
and function, its clinical utility is limited by high cost and complexity. In
contrast, electrocardiography (ECG) is inexpensive and widely available but
lacks the granularity of CMR. We propose CardioNets, a deep learning framework
that translates 12-lead ECG signals into CMR-level functional parameters and
synthetic images, enabling scalable cardiac assessment. CardioNets integrates
cross-modal contrastive learning and generative pretraining, aligning ECG with
CMR-derived cardiac phenotypes and synthesizing high-resolution CMR images via
a masked autoregressive model. Trained on 159,819 samples from five cohorts,
including the UK Biobank (n=42,483) and MIMIC-IV-ECG (n=164,550), and
externally validated on independent clinical datasets (n=3,767), CardioNets
achieved strong performance across disease screening and phenotype estimation
tasks. In the UK Biobank, it improved cardiac phenotype regression R2 by 24.8%
and cardiomyopathy AUC by up to 39.3% over baseline models. In MIMIC, it
increased AUC for pulmonary hypertension detection by 5.6%. Generated CMR
images showed 36.6% higher SSIM and 8.7% higher PSNR than prior approaches. In
a reader study, ECG-only CardioNets achieved 13.9% higher accuracy than human
physicians using both ECG and real CMR. These results suggest that CardioNets
offers a promising, low-cost alternative to CMR for large-scale CVD screening,
particularly in resource-limited settings. Future efforts will focus on
clinical deployment and regulatory validation of ECG-based synthetic imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards user-centered interactive medical image segmentation in VR with
  an assistive AI agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07214v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07214v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Spiegler, Arash Harirpoush, Yiming Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crucial in disease analysis and surgical planning, manual segmentation of
volumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and
challenging to master, while fully automatic algorithms can benefit from user
feedback. Therefore, with the complementary power of the latest radiological AI
foundation models and virtual reality (VR)'s intuitive data interaction, we
propose SAMIRA, a novel conversational AI agent that assists users with
localizing, segmenting, and visualizing 3D medical concepts in VR. Through
speech-based interaction, the agent helps users understand radiological
features, locate clinical targets, and generate segmentation masks that can be
refined with just a few point prompts. The system also supports true-to-scale
3D visualization of segmented pathology to enhance patient-specific anatomical
understanding. Furthermore, to determine the optimal interaction paradigm under
near-far attention-switching for refining segmentation masks in an immersive,
human-in-the-loop workflow, we compare VR controller pointing, head pointing,
and eye tracking as input modes. With a user study, evaluations demonstrated a
high usability score (SUS=90.0 $\pm$ 9.0), low overall task load, as well as
strong support for the proposed VR system's guidance, training potential, and
integration of AI in radiological segmentation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compensate Quantization Errors+: Quantized Models Are Inquisitive
  Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15508v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15508v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Gao, Jie Ou, Lei Wang, Jun Cheng, Mengchu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quantization of large language models (LLMs) has been a prominent
research area aimed at enabling their lightweight deployment in practice.
Existing research about LLM's quantization has mainly explored the interplay
between weights and activations, or employing auxiliary components while
neglecting the necessity of adjusting weights during quantization.
Consequently, original weight distributions frequently fail to yield desired
results after round-to-nearest (RTN) quantization. Even though incorporating
techniques such as mixed precision and low-rank error approximation in LLM's
quantization can yield improved results, they inevitably introduce additional
computational overhead. On the other hand, traditional techniques for weight
quantization, such as Generative Post-Training Quantization, rely on manually
tweaking weight distributions to minimize local errors, but they fall short of
achieving globally optimal outcomes. Although the recently proposed Learnable
Singular-value Increment improves global weight quantization by modifying
weight distributions, it disrupts the original distribution considerably. This
introduces pronounced bias toward the training data and can degrade downstream
task performance. In this paper, we introduce Singular-value Diagonal
Expansion, a more nuanced approach to refining weight distributions to achieve
better quantization alignment. Furthermore, we introduce Cross-layer Learning
that improves overall quantization outcomes by distributing errors more evenly
across layers. Our plug-and-play weight-quantization methods demonstrate
substantial performance improvements over state-of-the-art approaches,
including OmniQuant, DuQuant, and PrefixQuant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Effecient Quantization Methods for LLMs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Next Token Prediction: Patch-Level Training for Large Language
  Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12665v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12665v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenze Shao, Fandong Meng, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prohibitive training costs of Large Language Models (LLMs) have emerged
as a significant bottleneck in the development of next-generation LLMs. In this
paper, we show that it is possible to significantly reduce the training costs
of LLMs without sacrificing their performance. Specifically, we introduce
patch-level training for LLMs, in which multiple tokens are aggregated into a
unit of higher information density, referred to as a `patch', to serve as the
fundamental text unit for training LLMs. During patch-level training, we feed
the language model shorter sequences of patches and train it to predict the
next patch, thereby processing the majority of the training data at a
significantly reduced cost. Following this, the model continues token-level
training on the remaining training data to align with the inference mode.
Experiments on a diverse range of models (370M-2.7B parameters) demonstrate
that patch-level training can reduce the overall training costs to 0.5$\times$,
without compromising the model performance compared to token-level training.
Source code: https://github.com/shaochenze/PatchTrain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Commute Graph Neural Networks <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01635v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01635v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhuo, Han Yu, Guang Tan, Xiaoxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have shown remarkable success in learning from
graph-structured data. However, their application to directed graphs (digraphs)
presents unique challenges, primarily due to the inherent asymmetry in node
relationships. Traditional GNNs are adept at capturing unidirectional relations
but fall short in encoding the mutual path dependencies between nodes, such as
asymmetrical shortest paths typically found in digraphs. Recognizing this gap,
we introduce Commute Graph Neural Networks (CGNN), an approach that seamlessly
integrates node-wise commute time into the message passing scheme. The
cornerstone of CGNN is an efficient method for computing commute time using a
newly formulated digraph Laplacian. Commute time is then integrated into the
neighborhood aggregation process, with neighbor contributions weighted
according to their respective commute time to the central node in each layer.
It enables CGNN to directly capture the mutual, asymmetric relationships in
digraphs. Extensive experiments on 8 benchmarking datasets confirm the
superiority of CGNN against 13 state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in International Conference on Machine Learning (ICML),
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RM-R1: Reward Modeling as Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.02387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.02387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, Hanghang Tong, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward modeling is essential for aligning large language models (LLMs) with
human preferences through reinforcement learning (RL). To provide accurate
reward signals, a reward model (RM) should stimulate deep thinking and conduct
interpretable reasoning before assigning a score or a judgment. Inspired by
recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we
hypothesize and validate that integrating reasoning capabilities into reward
modeling significantly enhances RM's interpretability and performance. To this
end, we introduce a new class of generative reward models -- Reasoning Reward
Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We
propose a reasoning-oriented training pipeline and train a family of ReasRMs,
RM-R1. RM-R1 features a chain-of-rubrics (CoR) mechanism -- self-generating
sample-level chat rubrics or math/code solutions, and evaluating candidate
responses against them. The training of M-R1 consists of two key stages: (1)
distillation of high-quality reasoning chains and (2) reinforcement learning
with verifiable rewards. Empirically, our models achieve state-of-the-art
performance across three reward model benchmarks on average, outperforming much
larger open-weight models (e.g., INF-ORM-Llama3.1-70B) and proprietary ones
(e.g., GPT-4o) by up to 4.9%. Beyond final performance, we perform thorough
empirical analysis to understand the key ingredients of successful ReasRM
training. To facilitate future research, we release six ReasRM models along
with code and data at https://github.com/RM-R1-UIUC/RM-R1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural Language Reinforcement Learning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xidong Feng, Bo Liu, Ziyu Wan, Haotian Fu, Girish A. Koushik, Zhiyuan Hu, Mengyue Yang, Ying Wen, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) mathematically formulates decision-making with
Markov Decision Process (MDP). With MDPs, researchers have achieved remarkable
breakthroughs across various domains, including games, robotics, and language
models. This paper seeks a new possibility, Natural Language Reinforcement
Learning (NLRL), by extending traditional MDP to natural language-based
representation space. Specifically, NLRL innovatively redefines RL principles,
including task objectives, policy, value function, Bellman equation, and policy
iteration, into their language counterparts. With recent advancements in large
language models (LLMs), NLRL can be practically implemented to achieve RL-like
policy and value improvement by either pure prompting or gradient-based
training. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games
demonstrate the effectiveness, efficiency, and interpretability of the NLRL
framework among diverse use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025 Workshop SSI-FM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>Mobile: Efficient <span class="highlight-title">Prompt</span>us for Low Bandwidth Mobile Video
  Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16112v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16112v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liming Liu, Jiangkai Wu, Haoyang Wang, Peiheng Wang, Zongming Guo, Xinggong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional video compression algorithms exhibit significant quality
degradation at extremely low bitrates. Promptus emerges as a new paradigm for
video streaming, substantially cutting down the bandwidth essential for video
streaming. However, Promptus is computationally intensive and can not run in
real-time on mobile devices. This paper presents PromptMobile, an efficient
acceleration framework tailored for on-device Promptus. Specifically, we
propose (1) a two-stage efficient generation framework to reduce computational
cost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant
computations by 16.6%, (3) system-level optimizations to further enhance
efficiency. The evaluations demonstrate that compared with the original
Promptus, PromptMobile achieves a 13.6x increase in image generation speed.
Compared with other streaming methods, PromptMobile achives an average LPIPS
improvement of 0.016 (compared with H.265), reducing 60% of severely distorted
frames (compared to VQGAN).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages (excluding references), 10 figures, to appear in APNET 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAKR: Enhancing Retrieval-Augmented Generation via Streaming Algorithm
  and K-Means Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21300v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21300v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Kang, Yuzhou Zhu, Yukun Zhong, Ke Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has achieved significant success in
information retrieval to assist large language models LLMs because it builds an
external knowledge database. However, it also has many problems, it consumes a
lot of memory because of the enormous database, and it cannot update the
established index database in time when confronted with massive streaming data.
To reduce the memory required for building the database and maintain accuracy
simultaneously, we proposed a new approach integrating a streaming algorithm
with k-means clustering into RAG. Our approach applied a streaming algorithm to
update the index dynamically and reduce memory consumption. Additionally, the
k-means algorithm clusters highly similar documents, and the query time would
be shortened. We conducted comparative experiments on four methods, and the
results indicated that RAG with streaming algorithm and k-means clusters
outperforms traditional RAG in accuracy and memory, particularly when dealing
with large-scale streaming data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-cross Feature based Spiking Neural Networks for Efficient Few-shot
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07921v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07921v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Xu, Junyang Zhu, Dongdong Zhou, Hao Chen, Yang Liu, Jiangrong Shen, Qiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) excel in computer vision tasks, especially,
few-shot learning (FSL), which is increasingly important for generalizing from
limited examples. However, DNNs are computationally expensive with scalability
issues in real world. Spiking Neural Networks (SNNs), with their event-driven
nature and low energy consumption, are particularly efficient in processing
sparse and dynamic data, though they still encounter difficulties in capturing
complex spatiotemporal features and performing accurate cross-class
comparisons. To further enhance the performance and efficiency of SNNs in
few-shot learning, we propose a few-shot learning framework based on SNNs,
which combines a self-feature extractor module and a cross-feature contrastive
module to refine feature representation and reduce power consumption. We apply
the combination of temporal efficient training loss and InfoNCE loss to
optimize the temporal dynamics of spike trains and enhance the discriminative
power. Experimental results show that the proposed FSL-SNN significantly
improves the classification performance on the neuromorphic dataset N-Omniglot,
and also achieves competitive performance to ANNs on static datasets such as
CUB and miniImageNet with low power consumption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying AI Platform Design for Distributed Inference of
  Next-Generation LLM models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01698v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01698v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhimanyu Bambhaniya, Ritik Raj, Geonhwa Jeong, Souvik Kundu, Sudarshan Srinivasan, Suvinay Subramanian, Midhilesh Elavazhagan, Madhu Kumar, Tushar Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performance across a wide
range of applications, often outperforming human experts. However, deploying
these gigantic models efficiently for diverse inference use cases requires
carefully designed hardware platforms with ample computing, memory, and network
resources. With constant innovation in LLM serving optimizations and model
architecture evolving at breakneck speed, the hardware requirements to meet
Service Level Objectives (SLOs) remain an open research question.
  To answer the question, we present an analytical tool, GenZ, to efficiently
navigate the relationship between diverse LLM model architectures(Dense, GQA,
MoE, Mamba), LLM serving optimizations(Chunking, Speculative decoding,
quanitization), and AI platform design parameters. Our tool estimates LLM
inference performance metrics for the given scenario. We have validated against
real hardware platforms running various different LLM models, achieving a max
geomean error of 5.82.We use GenZ to identify compute, memory capacity, memory
bandwidth, network latency, and network bandwidth requirements across diverse
LLM inference use cases. We also study diverse architectural choices in use
today (inspired by LLM serving platforms from several vendors) to help inform
computer architects designing next-generation AI hardware accelerators and
platforms. The trends and insights derived from GenZ can guide AI engineers
deploying LLMs as well as computer architects designing next-generation
hardware accelerators and platforms. Ultimately, this work sheds light on the
platform design considerations for unlocking the full potential of large
language models across a spectrum of applications. The source code is available
at https://github.com/abhibambhaniya/GenZ-LLM-Analyzer . Users can also be
tried it on at https://genz-llm-analyzer.streamlit.app/ without any setup on
your web browser.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 Pages, https://github.com/abhibambhaniya/GenZ-LLM-Analyzer,
  https://genz-llm-analyzer.streamlit.app/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04526v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04526v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqiao Xue, Xiaojing Li, Fan Zhou, Qingyang Dai, Zhixuan Chu, Hongyuan Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce FAMMA, an open-source benchmark for
\underline{f}in\underline{a}ncial \underline{m}ultilingual
\underline{m}ultimodal question \underline{a}nswering (QA). Our benchmark aims
to evaluate the abilities of large language models (LLMs) in answering complex
reasoning questions that require advanced financial knowledge. The benchmark
has two versions: FAMMA-Basic consists of 1,945 questions extracted from
university textbooks and exams, along with human-annotated answers and
rationales; FAMMA-LivePro consists of 103 novel questions created by human
domain experts, with answers and rationales held out from the public for a
contamination-free evaluation. These questions cover advanced knowledge of 8
major subfields in finance (e.g., corporate finance, derivatives, and portfolio
management). Some are in Chinese or French, while a majority of them are in
English. Each question has some non-text data such as charts, diagrams, or
tables. Our experiments reveal that FAMMA poses a significant challenge on
LLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally,
we curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data,
and fine-tuned a series of open-source Qwen models using this reasoning data.
We found that training a model on these reasoning trajectories can
significantly improve its performance on FAMMA-LivePro. We released our
leaderboard, data, code, and trained models at
https://famma-bench.github.io/famma/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Construction and Application of Materials Knowledge Graph in
  Multidisciplinary Materials Science via Large Language Model <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03080v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03080v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanpeng Ye, Jie Ren, Shaozhou Wang, Yuwei Wan, Imran Razzak, Bram Hoex, Haofen Wang, Tong Xie, Wenjie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge in materials science is widely dispersed across extensive
scientific literature, posing significant challenges to the efficient discovery
and integration of new materials. Traditional methods, often reliant on costly
and time-consuming experimental approaches, further complicate rapid
innovation. Addressing these challenges, the integration of artificial
intelligence with materials science has opened avenues for accelerating the
discovery process, though it also demands precise annotation, data extraction,
and traceability of information. To tackle these issues, this article
introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural
language processing techniques integrated with large language models to extract
and systematically organize a decade's worth of high-quality research into
structured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes
information into comprehensive labels such as Name, Formula, and Application,
structured around a meticulously designed ontology, thus enhancing data
usability and integration. By implementing network-based algorithms, MKG not
only facilitates efficient link prediction but also significantly reduces
reliance on traditional experimental methods. This structured approach not only
streamlines materials research but also lays the groundwork for more
sophisticated science knowledge graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 38th Conference on Neural Information Processing Systems
  (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast and Robust: Task Sampling with Posterior and Diversity Synergies
  for Adaptive Decision-Makers in Randomized Environments <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19139v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19139v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Qu, Qi Cheems Wang, Yixiu Mao, Yiqin Lv, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task robust adaptation is a long-standing pursuit in sequential
decision-making. Some risk-averse strategies, e.g., the conditional
value-at-risk principle, are incorporated in domain randomization or meta
reinforcement learning to prioritize difficult tasks in optimization, which
demand costly intensive evaluations. The efficiency issue prompts the
development of robust active task sampling to train adaptive policies, where
risk-predictive models are used to surrogate policy evaluation. This work
characterizes the optimization pipeline of robust active task sampling as a
Markov decision process, posits theoretical and practical insights, and
constitutes robustness concepts in risk-averse scenarios. Importantly, we
propose an easy-to-implement method, referred to as Posterior and Diversity
Synergized Task Sampling (PDTS), to accommodate fast and robust sequential
decision-making. Extensive experiments show that PDTS unlocks the potential of
robust active task sampling, significantly improves the zero-shot and few-shot
adaptation robustness in challenging tasks, and even accelerates the learning
process under certain scenarios. Our project website is at
https://thu-rllab.github.io/PDTS_project_page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ro<span class="highlight-title">BERT</span>a-BiLSTM: A Context-Aware Hybrid Model for Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00367v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00367v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Mostafizer Rahman, Ariful Islam Shiplu, Yutaka Watanobe, Md. Ashad Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively analyzing the comments to uncover latent intentions holds immense
value in making strategic decisions across various domains. However, several
challenges hinder the process of sentiment analysis including the lexical
diversity exhibited in comments, the presence of long dependencies within the
text, encountering unknown symbols and words, and dealing with imbalanced
datasets. Moreover, existing sentiment analysis tasks mostly leveraged
sequential models to encode the long dependent texts and it requires longer
execution time as it processes the text sequentially. In contrast, the
Transformer requires less execution time due to its parallel processing nature.
In this work, we introduce a novel hybrid deep learning model, RoBERTa-BiLSTM,
which combines the Robustly Optimized BERT Pretraining Approach (RoBERTa) with
Bidirectional Long Short-Term Memory (BiLSTM) networks. RoBERTa is utilized to
generate meaningful word embedding vectors, while BiLSTM effectively captures
the contextual semantics of long-dependent texts. The RoBERTa-BiLSTM hybrid
model leverages the strengths of both sequential and Transformer models to
enhance performance in sentiment analysis. We conducted experiments using
datasets from IMDb, Twitter US Airline, and Sentiment140 to evaluate the
proposed model against existing state-of-the-art methods. Our experimental
findings demonstrate that the RoBERTa-BiLSTM model surpasses baseline models
(e.g., BERT, RoBERTa-base, RoBERTa-GRU, and RoBERTa-LSTM), achieving accuracies
of 80.74%, 92.36%, and 82.25% on the Twitter US Airline, IMDb, and Sentiment140
datasets, respectively. Additionally, the model achieves F1-scores of 80.73%,
92.35%, and 82.25% on the same datasets, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">114</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-Fixup: Advancing Photo Editing with 3D Priors <span class="chip">SIGGRAPH 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Chi Cheng, Krishna Kumar Singh, Jae Shin Yoon, Alex Schwing, Liangyan Gui, Matheus Gadelha, Paul Guerrero, Nanxuan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advances in modeling image priors via diffusion models,
3D-aware image editing remains challenging, in part because the object is only
specified via a single image. To tackle this challenge, we propose 3D-Fixup, a
new framework for editing 2D images guided by learned 3D priors. The framework
supports difficult editing situations such as object translation and 3D
rotation. To achieve this, we leverage a training-based approach that harnesses
the generative power of diffusion models. As video data naturally encodes
real-world physical dynamics, we turn to video data for generating training
data pairs, i.e., a source and a target frame. Rather than relying solely on a
single trained model to infer transformations between source and target frames,
we incorporate 3D guidance from an Image-to-3D model, which bridges this
challenging task by explicitly projecting 2D information into 3D space. We
design a data generation pipeline to ensure high-quality 3D guidance throughout
training. Results show that by integrating these 3D priors, 3D-Fixup
effectively supports complex, identity coherent 3D-aware edits, achieving
high-quality results and advancing the application of diffusion models in
realistic image manipulation. The code is provided at
https://3dfixup.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH 2025. Project page: https://3dfixup.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Depth Anything with Any Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehan Wang, Siyu Chen, Lihe Yang, Jialei Wang, Ziang Zhang, Hengshuang Zhao, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents Prior Depth Anything, a framework that combines incomplete
but precise metric information in depth measurement with relative but complete
geometric structures in depth prediction, generating accurate, dense, and
detailed metric depth maps for any scene. To this end, we design a
coarse-to-fine pipeline to progressively integrate the two complementary depth
sources. First, we introduce pixel-level metric alignment and distance-aware
weighting to pre-fill diverse metric priors by explicitly using depth
prediction. It effectively narrows the domain gap between prior patterns,
enhancing generalization across varying scenarios. Second, we develop a
conditioned monocular depth estimation (MDE) model to refine the inherent noise
of depth priors. By conditioning on the normalized pre-filled prior and
prediction, the model further implicitly merges the two complementary depth
sources. Our model showcases impressive zero-shot generalization across depth
completion, super-resolution, and inpainting over 7 real-world datasets,
matching or even surpassing previous task-specific methods. More importantly,
it performs well on challenging, unseen mixed priors and enables test-time
improvements by switching prediction models, providing a flexible
accuracy-efficiency trade-off while evolving with advancements in MDE models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Home page: https://prior-depth-anything.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Vision Tokenizer Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Fan Zhang, Yufeng Cui, Haiwen Diao, Zhuoyan Luo, Huchuan Lu, Jing Liu, Xinlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing vision tokenization isolates the optimization of vision tokenizers
from downstream training, implicitly assuming the visual tokens can generalize
well across various tasks, e.g., image generation and visual question
answering. The vision tokenizer optimized for low-level reconstruction is
agnostic to downstream tasks requiring varied representations and semantics.
This decoupled paradigm introduces a critical misalignment: The loss of the
vision tokenization can be the representation bottleneck for target tasks. For
example, errors in tokenizing text in a given image lead to poor results when
recognizing or generating them. To address this, we propose ETT, an end-to-end
vision tokenizer tuning approach that enables joint optimization between vision
tokenization and target autoregressive tasks. Unlike prior autoregressive
models that use only discrete indices from a frozen vision tokenizer, ETT
leverages the visual embeddings of the tokenizer codebook, and optimizes the
vision tokenizers end-to-end with both reconstruction and caption objectives.
ETT can be seamlessly integrated into existing training pipelines with minimal
architecture modifications. Our ETT is simple to implement and integrate,
without the need to adjust the original codebooks or architectures of the
employed large language models. Extensive experiments demonstrate that our
proposed end-to-end vision tokenizer tuning unlocks significant performance
gains, i.e., 2-6% for multimodal understanding and visual generation tasks
compared to frozen tokenizer baselines, while preserving the original
reconstruction capability. We hope this very simple and strong method can
empower multimodal foundation models besides image generation and
understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal
  Mathematical Reasoning <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language image-caption datasets, widely used for training Large
Multimodal Models, mainly focus on natural scenarios and overlook the intricate
details of mathematical figures that are critical for problem-solving,
hindering the advancement of current LMMs in multimodal mathematical reasoning.
To this end, we propose leveraging code as supervision for cross-modal
alignment, since code inherently encodes all information needed to generate
corresponding figures, establishing a precise connection between the two
modalities. Specifically, we co-develop our image-to-code model and dataset
with model-in-the-loop approach, resulting in an image-to-code model,
FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.
Furthermore, we utilize FigCodifier to synthesize novel mathematical figures
and then construct MM-MathInstruct-3M, a high-quality multimodal math
instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with
ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on
MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a
new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and
Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista,
achieving improvements of 8.9% and 9.2%. The dataset and models will be
released at https://github.com/mathllm/MathCoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Style Customization of Text-to-Vector Generation with Image Diffusion
  Priors <span class="chip">SIGGRAPH 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiying Zhang, Nanxuan Zhao, Jing Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scalable Vector Graphics (SVGs) are highly favored by designers due to their
resolution independence and well-organized layer structure. Although existing
text-to-vector (T2V) generation methods can create SVGs from text prompts, they
often overlook an important need in practical applications: style
customization, which is vital for producing a collection of vector graphics
with consistent visual appearance and coherent aesthetics. Extending existing
T2V methods for style customization poses certain challenges.
Optimization-based T2V models can utilize the priors of text-to-image (T2I)
models for customization, but struggle with maintaining structural regularity.
On the other hand, feed-forward T2V models can ensure structural regularity,
yet they encounter difficulties in disentangling content and style due to
limited SVG training data.
  To address these challenges, we propose a novel two-stage style customization
pipeline for SVG generation, making use of the advantages of both feed-forward
T2V models and T2I image priors. In the first stage, we train a T2V diffusion
model with a path-level representation to ensure the structural regularity of
SVGs while preserving diverse expressive capabilities. In the second stage, we
customize the T2V diffusion model to different styles by distilling customized
T2I models. By integrating these techniques, our pipeline can generate
high-quality and diverse SVGs in custom styles based on text prompts in an
efficient feed-forward manner. The effectiveness of our method has been
validated through extensive experiments. The project page is
https://customsvg.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGGRAPH 2025 (Conference Paper). Project page:
  https://customsvg.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Feasibility Matter? Understanding the Impact of Feasibility on
  Synthetic Training Data <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Liu, Jessica Bader, Jae Myung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of photorealistic diffusion models, models trained in
part or fully on synthetic data achieve progressively better results. However,
diffusion models still routinely generate images that would not exist in
reality, such as a dog floating above the ground or with unrealistic texture
artifacts. We define the concept of feasibility as whether attributes in a
synthetic image could realistically exist in the real-world domain; synthetic
images containing attributes that violate this criterion are considered
infeasible. Intuitively, infeasible images are typically considered
out-of-distribution; thus, training on such images is expected to hinder a
model's ability to generalize to real-world data, and they should therefore be
excluded from the training set whenever possible. However, does feasibility
really matter? In this paper, we investigate whether enforcing feasibility is
necessary when generating synthetic training data for CLIP-based classifiers,
focusing on three target attributes: background, color, and texture. We
introduce VariReal, a pipeline that minimally edits a given source image to
include feasible or infeasible attributes given by the textual prompt generated
by a large language model. Our experiments show that feasibility minimally
affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference
in top-1 accuracy across three fine-grained datasets. Also, the attribute
matters on whether the feasible/infeasible images adversarially influence the
classification performance. Finally, mixing feasible and infeasible images in
training datasets does not significantly impact performance compared to using
purely feasible or infeasible datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPRW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Implicit Visual Misunderstandings in Multimodal Large Language
  Models through Attention Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Wang, Guohai Xu, Weinong Wang, Junjie Yang, Jie Lou, Yunhua Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements have enhanced the capability of Multimodal Large Language
Models (MLLMs) to comprehend multi-image information. However, existing
benchmarks primarily evaluate answer correctness, overlooking whether models
genuinely comprehend the visual input. To address this, we define implicit
visual misunderstanding (IVM), where MLLMs provide correct answers without
fully comprehending the visual input. Through our analysis, we decouple the
visual and textual modalities within the causal attention module, revealing
that attention distribution increasingly converges on the image associated with
the correct answer as the network layers deepen. This insight leads to the
introduction of a scale-agnostic metric, \textit{attention accuracy}, and a
novel benchmark for quantifying IVMs. Attention accuracy directly evaluates the
model's visual understanding via internal mechanisms, remaining robust to
positional biases for more reliable assessments. Furthermore, we extend our
approach to finer granularities and demonstrate its effectiveness in unimodal
scenarios, underscoring its versatility and generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Multi-Image Question Answering via Submodular Subset Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaryan Sharma, Shivansh Gupta, Samar Agarwal, Vishak Prasad C., Ganesh Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models (LMMs) have achieved high performance in
vision-language tasks involving single image but they struggle when presented
with a collection of multiple images (Multiple Image Question Answering
scenario). These tasks, which involve reasoning over large number of images,
present issues in scalability (with increasing number of images) and retrieval
performance. In this work, we propose an enhancement for retriever framework
introduced in MIRAGE model using submodular subset selection techniques. Our
method leverages query-aware submodular functions, such as GraphCut, to
pre-select a subset of semantically relevant images before main retrieval
component. We demonstrate that using anchor-based queries and augmenting the
data improves submodular-retriever pipeline effectiveness, particularly in
large haystack sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative
  Decoding of Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mugilan Ganesan, Shane Segal, Ankur Aggarwal, Nish Sinnadurai, Sean Lie, Vithursan Thangarasa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding significantly accelerates language model inference by
enabling a lightweight draft model to propose multiple tokens that a larger
target model verifies simultaneously. However, applying this technique to
vision-language models (VLMs) presents two fundamental challenges: small
language models that could serve as efficient drafters lack the architectural
components to process visual inputs, and their token predictions fail to match
those of VLM target models that consider visual context. We introduce
Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of
Vision-Language Models (MASSV), which transforms existing small language models
into effective multimodal drafters through a two-phase approach. MASSV first
connects the target VLM's vision encoder to the draft model via a lightweight
trainable projector, then applies self-distilled visual instruction tuning
using responses generated by the target VLM to align token predictions.
Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families
demonstrate that MASSV increases accepted length by up to 30% and delivers
end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV
provides a scalable, architecture-compatible method for accelerating both
current and future VLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 11 pp., 4 figs., 3 tabs.; Supplementary: 2 pp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Token Prediction Needs Registers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasios Gerontopoulos, Spyros Gidaris, Nikos Komodakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-token prediction has emerged as a promising objective for improving
language model pretraining, but its benefits have not consistently generalized
to other settings such as fine-tuning. In this paper, we propose MuToR, a
simple and effective approach to multi-token prediction that interleaves
learnable register tokens into the input sequence, each tasked with predicting
future targets. Compared to existing methods, MuToR offers several key
advantages: it introduces only a negligible number of additional parameters,
requires no architectural changes--ensuring compatibility with off-the-shelf
pretrained language models--and remains aligned with the next-token pretraining
objective, making it especially well-suited for supervised fine-tuning.
Moreover, it naturally supports scalable prediction horizons. We demonstrate
the effectiveness and versatility of MuToR across a range of use cases,
including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and
pretraining, on challenging generative tasks in both language and vision
domains. Our code will be available at: https://github.com/nasosger/MuToR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face
  Morphing Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iurii Medvedev, Nuno Goncalves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition has evolved significantly with the advancement of deep
learning techniques, enabling its widespread adoption in various applications
requiring secure authentication. However, this progress has also increased its
exposure to presentation attacks, including face morphing, which poses a
serious security threat by allowing one identity to impersonate another.
Therefore, modern face recognition systems must be robust against such attacks.
  In this work, we propose a novel approach for training deep networks for face
recognition with enhanced robustness to face morphing attacks. Our method
modifies the classification task by introducing a dual-branch classification
strategy that effectively handles the ambiguity in the labeling of face morphs.
This adaptation allows the model to incorporate morph images into the training
process, improving its ability to distinguish them from bona fide samples.
  Our strategy has been validated on public benchmarks, demonstrating its
effectiveness in enhancing robustness against face morphing attacks.
Furthermore, our approach is universally applicable and can be integrated into
existing face recognition training pipelines to improve classification-based
recognition methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of
  Synthetic Chest Radiographs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raman Dutt, Pedro Sanchez, Yongchen Yao, Steven McDonagh, Sotirios A. Tsaftaris, Timothy Hospedales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CheXGenBench, a rigorous and multifaceted evaluation framework
for synthetic chest radiograph generation that simultaneously assesses
fidelity, privacy risks, and clinical utility across state-of-the-art
text-to-image generative models. Despite rapid advancements in generative AI
for real-world imagery, medical domain evaluations have been hindered by
methodological inconsistencies, outdated architectural comparisons, and
disconnected assessment criteria that rarely address the practical clinical
value of synthetic samples. CheXGenBench overcomes these limitations through
standardised data partitioning and a unified evaluation protocol comprising
over 20 quantitative metrics that systematically analyse generation quality,
potential privacy vulnerabilities, and downstream clinical applicability across
11 leading text-to-image architectures. Our results reveal critical
inefficiencies in the existing evaluation protocols, particularly in assessing
generative fidelity, leading to inconsistent and uninformative comparisons. Our
framework establishes a standardised benchmark for the medical AI community,
enabling objective and reproducible comparisons while facilitating seamless
integration of both existing and future generative models. Additionally, we
release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K
radiographs generated by the top-performing model (Sana 0.6B) in our benchmark
to support further research in this critical domain. Through CheXGenBench, we
establish a new state-of-the-art and release our framework, models, and
SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-contrast laser endoscopy for in vivo gastrointestinal imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taylor L. Bobrow, Mayank Golhar, Suchapa Arayakarnkul, Anthony A. Song, Saowanee Ngamruengphong, Nicholas J. Durr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  White light endoscopy is the clinical gold standard for detecting diseases in
the gastrointestinal tract. Most applications involve identifying visual
abnormalities in tissue color, texture, and shape. Unfortunately, the contrast
of these features is often subtle, causing many clinically relevant cases to go
undetected. To overcome this challenge, we introduce Multi-contrast Laser
Endoscopy (MLE): a platform for widefield clinical imaging with rapidly tunable
spectral, coherent, and directional illumination. We demonstrate three
capabilities of MLE: enhancing tissue chromophore contrast with multispectral
diffuse reflectance, quantifying blood flow using laser speckle contrast
imaging, and characterizing mucosal topography using photometric stereo. We
validate MLE with benchtop models, then demonstrate MLE in vivo during clinical
colonoscopies. MLE images from 31 polyps demonstrate an approximate three-fold
improvement in contrast and a five-fold improvement in color difference
compared to white light and narrow band imaging. With the ability to reveal
multiple complementary types of tissue contrast while seamlessly integrating
into the clinical environment, MLE shows promise as an investigative tool to
improve gastrointestinal imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniEval: Unified Holistic Evaluation for Unified Multimodal
  Understanding and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Li, Haonan Wang, Qixiang Zhang, Boyu Xiao, Chenchang Hu, Hualiang Wang, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of unified multimodal understanding and generation models is
rapidly attracting attention because of their ability to enhance
instruction-following capabilities while minimizing model redundancy. However,
there is a lack of a unified evaluation framework for these models, which would
enable an elegant, simplified, and overall evaluation. Current models conduct
evaluations on multiple task-specific benchmarks, but there are significant
limitations, such as the lack of overall results, errors from extra evaluation
models, reliance on extensive labeled images, benchmarks that lack diversity,
and metrics with limited capacity for instruction-following evaluation. To
tackle these challenges, we introduce UniEval, the first evaluation framework
designed for unified multimodal models without extra models, images, or
annotations. This facilitates a simplified and unified evaluation process. The
UniEval framework contains a holistic benchmark, UniBench (supports both
unified and visual generation models), along with the corresponding UniScore
metric. UniBench includes 81 fine-grained tags contributing to high diversity.
Experimental results indicate that UniBench is more challenging than existing
benchmarks, and UniScore aligns closely with human evaluations, surpassing
current metrics. Moreover, we extensively evaluated SoTA unified and visual
generation models, uncovering new insights into Univeral's unique values.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>UniEval is the first evaluation framework designed for unified
  multimodal models, including a holistic benchmark UniBench and the UniScore
  metric</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logos as a Well-Tempered <span class="highlight-title">Pre-train</span> for Sign Language Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilya Ovodov, Petr Surovtsev, Karina Kvanchiani, Alexander Kapitanov, Alexander Nagaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper examines two aspects of the isolated sign language recognition
(ISLR) task. First, despite the availability of a number of datasets, the
amount of data for most individual sign languages is limited. It poses the
challenge of cross-language ISLR model training, including transfer learning.
Second, similar signs can have different semantic meanings. It leads to
ambiguity in dataset labeling and raises the question of the best policy for
annotating such signs. To address these issues, this study presents Logos, a
novel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by
the number of signers and one of the largest available datasets while also the
largest RSL dataset in size and vocabulary. It is shown that a model,
pre-trained on the Logos dataset can be used as a universal encoder for other
language SLR tasks, including few-shot learning. We explore cross-language
transfer learning approaches and find that joint training using multiple
classification heads benefits accuracy for the target lowresource datasets the
most. The key feature of the Logos dataset is explicitly annotated visually
similar sign groups. We show that explicitly labeling visually similar signs
improves trained model quality as a visual encoder for downstream tasks. Based
on the proposed contributions, we outperform current state-of-the-art results
for the WLASL dataset and get competitive results for the AUTSL dataset, with a
single stream model processing solely RGB video. The source code, dataset, and
pre-trained models are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistent Quantity-Quality Control across Scenes for Deployment-Aware
  Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengdi Zhang, Hongkun Cao, Ruqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks
to minimize the number of Gaussians used while preserving high rendering
quality, introducing an inherent trade-off between Gaussian quantity and
rendering quality. Existing methods strive for better quantity-quality
performance, but lack the ability for users to intuitively adjust this
trade-off to suit practical needs such as model deployment under diverse
hardware and communication constraints. Here, we present ControlGS, a 3DGS
optimization method that achieves semantically meaningful and cross-scene
consistent quantity-quality control while maintaining strong quantity-quality
performance. Through a single training run using a fixed setup and a
user-specified hyperparameter reflecting quantity-quality preference, ControlGS
can automatically find desirable quantity-quality trade-off points across
diverse scenes, from compact objects to large outdoor scenes. It also
outperforms baselines by achieving higher rendering quality with fewer
Gaussians, and supports a broad adjustment range with stepless control over the
trade-off.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric
  Lesion Segmentation <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Liang, Lihuan Dai, Xiaoqi Sheng, Xiangguang Chen, Chun Yao, Guihua Tao, Qibin Leng, Honming Cai, Xi Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal medical image segmentation faces significant challenges in the
context of gastric cancer lesion analysis. This clinical context is defined by
the scarcity of independent multimodal datasets and the imperative to
amalgamate inherently misaligned modalities. As a result, algorithms are
constrained to train on approximate data and depend on application migration,
leading to substantial resource expenditure and a potential decline in analysis
accuracy. To address those challenges, we have made two major contributions:
First, we publicly disseminate the GCM 2025 dataset, which serves as the first
large-scale, open-source collection of gastric cancer multimodal MRI scans,
featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500
patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework
that employs an original HWA block with learnable window aggregation layers to
establish dynamic feature correspondences between different modalities'
anatomical structures, and leverages the innovative tri-orientated fusion mamba
mechanism for context modeling and capturing long-range spatial dependencies.
Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021
dataset validate the performance of our framework, demonstrating that the new
approach surpasses existing methods by up to 1.68\% in the Dice score while
maintaining solid robustness. The dataset and code are public via
https://github.com/JeMing-creater/HWA-UNETR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been provisionally accepted for MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEAL: Searching Expandable Architectures for Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Gambella, Vicente Javier Castro Solar, Manuel Roveri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incremental learning is a machine learning paradigm where a model learns from
a sequential stream of tasks. This setting poses a key challenge: balancing
plasticity (learning new tasks) and stability (preserving past knowledge).
Neural Architecture Search (NAS), a branch of AutoML, automates the design of
the architecture of Deep Neural Networks and has shown success in static
settings. However, existing NAS-based approaches to incremental learning often
rely on expanding the model at every task, making them impractical in
resource-constrained environments. In this work, we introduce SEAL, a NAS-based
framework tailored for data-incremental learning, a scenario where disjoint
data samples arrive sequentially and are not stored for future access. SEAL
adapts the model structure dynamically by expanding it only when necessary,
based on a capacity estimation metric. Stability is preserved through
cross-distillation training after each expansion step. The NAS component
jointly searches for both the architecture and the optimal expansion policy.
Experiments across multiple benchmarks demonstrate that SEAL effectively
reduces forgetting and enhances accuracy while maintaining a lower model size
compared to prior methods. These results highlight the promise of combining NAS
and selective expansion for efficient, adaptive learning in incremental
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision language models have difficulty recognizing virtual objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Tran, Sangeet Khemlani, J. G. Trafton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision language models (VLMs) are AI systems paired with both language and
vision encoders to process multimodal input. They are capable of performing
complex semantic tasks such as automatic captioning, but it remains an open
question about how well they comprehend the visuospatial properties of scenes
depicted in the images they process. We argue that descriptions of virtual
objects -- objects that are not visually represented in an image -- can help
test scene comprehension in these AI systems. For example, an image that
depicts a person standing under a tree can be paired with the following prompt:
imagine that a kite is stuck in the tree. VLMs that comprehend the scene should
update their representations and reason sensibly about the spatial relations
between all three objects. We describe systematic evaluations of
state-of-the-art VLMs and show that their ability to process virtual objects is
inadequate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIF: Anomaly detection via preference embedding <span class="chip">ICPR
  2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Leveni, Luca Magri, Giacomo Boracchi, Cesare Alippi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of detecting anomalies with respect to structured
patterns. To this end, we conceive a novel anomaly detection method called PIF,
that combines the advantages of adaptive isolation methods with the flexibility
of preference embedding. Specifically, we propose to embed the data in a high
dimensional space where an efficient tree-based method, PI-Forest, is employed
to compute an anomaly score. Experiments on synthetic and real datasets
demonstrate that PIF favorably compares with state-of-the-art anomaly detection
techniques, and confirm that PI-Forest is better at measuring arbitrary
distances and isolate points in the preference space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Conference on Pattern Recognition (ICPR
  2020)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learned Lightweight Smartphone ISP with Unpaired Data <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Arhire, Radu Timofte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Image Signal Processor (ISP) is a fundamental component in modern
smartphone cameras responsible for conversion of RAW sensor image data to RGB
images with a strong focus on perceptual quality. Recent work highlights the
potential of deep learning approaches and their ability to capture details with
a quality increasingly close to that of professional cameras. A difficult and
costly step when developing a learned ISP is the acquisition of pixel-wise
aligned paired data that maps the raw captured by a smartphone camera sensor to
high-quality reference images. In this work, we address this challenge by
proposing a novel training method for a learnable ISP that eliminates the need
for direct correspondences between raw images and ground-truth data with
matching content. Our unpaired approach employs a multi-term loss function
guided by adversarial training with multiple discriminators processing feature
maps from pre-trained networks to maintain content structure while learning
color and texture characteristics from the target RGB dataset. Using
lightweight neural network architectures suitable for mobile devices as
backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm
UltraISP datasets. Compared to paired training methods, our unpaired learning
strategy shows strong potential and achieves high fidelity across multiple
evaluation metrics. The code and pre-trained models are available at
https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPRW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Fidelity Index for Generative Semantic Communications with
  Critical Information Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhao Huang, Qunsong Zeng, Kaibin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative semantic communication (Gen-SemCom) with large artificial
intelligence (AI) model promises a transformative paradigm for 6G networks,
which reduces communication costs by transmitting low-dimensional prompts
rather than raw data. However, purely prompt-driven generation loses
fine-grained visual details. Additionally, there is a lack of systematic
metrics to evaluate the performance of Gen-SemCom systems. To address these
issues, we develop a hybrid Gen-SemCom system with a critical information
embedding (CIE) framework, where both text prompts and semantically critical
features are extracted for transmissions. First, a novel approach of semantic
filtering is proposed to select and transmit the semantically critical features
of images relevant to semantic label. By integrating the text prompt and
critical features, the receiver reconstructs high-fidelity images using a
diffusion-based generative model. Next, we propose the generative visual
information fidelity (GVIF) metric to evaluate the visual quality of the
generated image. By characterizing the statistical models of image features,
the GVIF metric quantifies the mutual information between the distorted
features and their original counterparts. By maximizing the GVIF metric, we
design a channel-adaptive Gen-SemCom system that adaptively control the volume
of features and compression rate according to the channel state. Experimental
results validate the GVIF metric's sensitivity to visual fidelity, correlating
with both the PSNR and critical information volume. In addition, the optimized
system achieves superior performance over benchmarking schemes in terms of
higher PSNR and lower FID scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpikeVideoFormer: An Efficient Spike-Driven Video <span class="highlight-title">Transformer</span> with
  Hamming Attention and $\mathcal{O}(T)$ Complexity <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Zou, Qingfeng Li, Wei Ji, Jingjing Li, Yongkui Yang, Guoqi Li, Chao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) have shown competitive performance to
Artificial Neural Networks (ANNs) in various vision tasks, while offering
superior energy efficiency. However, existing SNN-based Transformers primarily
focus on single-image tasks, emphasizing spatial features while not effectively
leveraging SNNs' efficiency in video-based vision tasks. In this paper, we
introduce SpikeVideoFormer, an efficient spike-driven video Transformer,
featuring linear temporal complexity $\mathcal{O}(T)$. Specifically, we design
a spike-driven Hamming attention (SDHA) which provides a theoretically guided
adaptation from traditional real-valued attention to spike-driven attention.
Building on SDHA, we further analyze various spike-driven space-time attention
designs and identify an optimal scheme that delivers appealing performance for
video tasks, while maintaining only linear temporal complexity. The
generalization ability and efficiency of our model are demonstrated across
diverse downstream video tasks, including classification, human pose tracking,
and semantic segmentation. Empirical results show our method achieves
state-of-the-art (SOTA) performance compared to existing SNN approaches, with
over 15\% improvement on the latter two tasks. Additionally, it matches the
performance of recent ANN-based methods while offering significant efficiency
gains, achieving $\times 16$, $\times 10$ and $\times 5$ improvements on the
three tasks. https://github.com/JimmyZou/SpikeVideoFormer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified and Scalable Membership Inference Method for Visual
  <span class="highlight-title">Self-supervised</span> Encoder via Part-aware Capability <span class="chip">CCS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zhu, Jirong Zha, Ding Li, Leye Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning shows promise in harnessing extensive unlabeled
data, but it also confronts significant privacy concerns, especially in vision.
In this paper, we perform membership inference on visual self-supervised models
in a more realistic setting: self-supervised training method and details are
unknown for an adversary when attacking as he usually faces a black-box system
in practice. In this setting, considering that self-supervised model could be
trained by completely different self-supervised paradigms, e.g., masked image
modeling and contrastive learning, with complex training details, we propose a
unified membership inference method called PartCrop. It is motivated by the
shared part-aware capability among models and stronger part response on the
training data. Specifically, PartCrop crops parts of objects in an image to
query responses within the image in representation space. We conduct extensive
attacks on self-supervised models with different training protocols and
structures using three widely used image datasets. The results verify the
effectiveness and generalization of PartCrop. Moreover, to defend against
PartCrop, we evaluate two common approaches, i.e., early stop and differential
privacy, and propose a tailored method called shrinking crop scale range. The
defense experiments indicate that all of them are effective. Finally, besides
prototype testing on toy visual encoders and small-scale image datasets, we
quantitatively study the impacts of scaling from both data and model aspects in
a realistic scenario and propose a scalable PartCrop-v2 by introducing two
structural improvements to PartCrop. Our code is at
https://github.com/JiePKU/PartCrop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An extension of our ACM CCS2024 conference paper (arXiv:2404.02462).
  We show the impacts of scaling from both data and model aspects on membership
  inference for self-supervised visual encoders</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SOS: A Shuffle Order Strategy for Data Augmentation in Industrial Human
  Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Tuan Ha, Hoang Khang Phan, Thai Minh Tien Ngo, Anh Phan Truong, Nhat Tan Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of Human Activity Recognition (HAR), obtaining high quality and
variance data is still a persistent challenge due to high costs and the
inherent variability of real-world activities. This study introduces a
generation dataset by deep learning approaches (Attention Autoencoder and
conditional Generative Adversarial Networks). Another problem that data
heterogeneity is a critical challenge, one of the solutions is to shuffle the
data to homogenize the distribution. Experimental results demonstrate that the
random sequence strategy significantly improves classification performance,
achieving an accuracy of up to 0.70 $\pm$ 0.03 and a macro F1 score of 0.64
$\pm$ 0.01. For that, disrupting temporal dependencies through random sequence
reordering compels the model to focus on instantaneous recognition, thereby
improving robustness against activity transitions. This approach not only
broadens the effective training dataset but also offers promising avenues for
enhancing HAR systems in complex, real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images
  using ViT Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillaume Balezo, Roger Trullo, Albert Pla Planas, Etienne Decenciere, Thomas Walter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Histopathological analysis is a cornerstone of cancer diagnosis, with
Hematoxylin and Eosin (H&E) staining routinely acquired for every patient to
visualize cell morphology and tissue architecture. On the other hand, multiplex
immunofluorescence (mIF) enables more precise cell type identification via
proteomic markers, but has yet to achieve widespread clinical adoption due to
cost and logistical constraints. To bridge this gap, we introduce MIPHEI
(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired
architecture that integrates state-of-the-art ViT foundation models as encoders
to predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of
markers spanning nuclear content, immune lineages (T cells, B cells, myeloid),
epithelium, stroma, vasculature, and proliferation. We train our model using
the publicly available ORION dataset of restained H&E and mIF images from
colorectal cancer tissue, and validate it on two independent datasets. MIPHEI
achieves accurate cell-type classification from H&E alone, with F1 scores of
0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,
substantially outperforming both a state-of-the-art baseline and a random
classifier for most markers. Our results indicate that our model effectively
captures the complex relationships between nuclear morphologies in their tissue
context, as visible in H&E images and molecular markers defining specific cell
types. MIPHEI offers a promising step toward enabling cell-type-aware analysis
of large-scale H&E datasets, in view of uncovering relationships between
spatial cellular organization and patient outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StoryReasoning <span class="highlight-title">Dataset</span>: Using Chain-of-Thought for Scene Understanding
  and Grounded Story Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel A. P. Oliveira, David Martins de Matos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual storytelling systems struggle to maintain character identity across
frames and link actions to appropriate subjects, frequently leading to
referential hallucinations. These issues can be addressed through grounding of
characters, objects, and other entities on the visual elements. We propose
StoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie
images, with both structured scene analyses and grounded stories. Each story
maintains character and object consistency across frames while explicitly
modeling multi-frame relationships through structured tabular representations.
Our approach features cross-frame object re-identification using visual
similarity and face recognition, chain-of-thought reasoning for explicit
narrative modeling, and a grounding scheme that links textual elements to
visual entities across multiple frames. We establish baseline performance by
fine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end
object detection, re-identification, and landmark detection while maintaining
consistent object references throughout the story. Evaluation demonstrates a
reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when
compared to a non-fine-tuned model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Wang, Shuai Xu, Xuelin Zhu, Yicong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object
combinations by leveraging known combinations. Existing studies basically rely
on the cross-modal alignment capabilities of CLIP but tend to overlook its
limitations in capturing fine-grained local features, which arise from its
architectural and training paradigm. To address this issue, we propose a
Multi-Stage Cross-modal Interaction (MSCI) model that effectively explores and
utilizes intermediate-layer information from CLIP's visual encoder.
Specifically, we design two self-adaptive aggregators to extract local
information from low-level visual features and integrate global information
from high-level visual features, respectively. These key information are
progressively incorporated into textual representations through a
stage-by-stage interaction mechanism, significantly enhancing the model's
perception capability for fine-grained local visual information. Additionally,
MSCI dynamically adjusts the attention weights between global and local visual
information based on different combinations, as well as different elements
within the same combination, allowing it to flexibly adapt to diverse
scenarios. Experiments on three widely used datasets fully validate the
effectiveness and superiority of the proposed model. Data and code are
available at https://github.com/ltpwy/MSCI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global
  Marine Fog Detection and Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengqiu Xu, Kaixin Chen, Heng Guo, Yixiang Huang, Ming Wu, Zhenwei Shi, Chuang Zhang, Jun Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning approaches for marine fog detection and forecasting have
outperformed traditional methods, demonstrating significant scientific and
practical importance. However, the limited availability of open-source datasets
remains a major challenge. Existing datasets, often focused on a single region
or satellite, restrict the ability to evaluate model performance across diverse
conditions and hinder the exploration of intrinsic marine fog characteristics.
To address these limitations, we introduce \textbf{MFogHub}, the first
multi-regional and multi-satellite dataset to integrate annotated marine fog
observations from 15 coastal fog-prone regions and six geostationary
satellites, comprising over 68,000 high-resolution samples. By encompassing
diverse regions and satellite perspectives, MFogHub facilitates rigorous
evaluation of both detection and forecasting methods under varying conditions.
Extensive experiments with 16 baseline models demonstrate that MFogHub can
reveal generalization fluctuations due to regional and satellite discrepancy,
while also serving as a valuable resource for the development of targeted and
scalable fog prediction techniques. Through MFogHub, we aim to advance both the
practical monitoring and scientific understanding of marine fog dynamics on a
global scale. The dataset and code are at
\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall
  Probabilities Over 8 Hours 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Pablos Sarabia, Joachim Nyborg, Morten Birk, Jeppe Liborius Sjørup, Anders Lillevang Vesterholt, Ira Assent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a deep learning model for high-resolution probabilistic
precipitation forecasting over an 8-hour horizon in Europe, overcoming the
limitations of radar-only deep learning models with short forecast lead times.
Our model efficiently integrates multiple data sources - including radar,
satellite, and physics-based numerical weather prediction (NWP) - while
capturing long-range interactions, resulting in accurate forecasts with robust
uncertainty quantification through consistent probabilistic maps. Featuring a
compact architecture, it enables more efficient training and faster inference
than existing models. Extensive experiments demonstrate that our model
surpasses current operational NWP systems, extrapolation-based methods, and
deep-learning nowcasting models, setting a new standard for high-resolution
precipitation forecasting in Europe, ensuring a balance between accuracy,
interpretability, and computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HandReader: Advanced Techniques for Efficient Fingerspelling Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavel Korotaev, Petr Surovtsev, Alexander Kapitanov, Karina Kvanchiani, Aleksandr Nagaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fingerspelling is a significant component of Sign Language (SL), allowing the
interpretation of proper names, characterized by fast hand movements during
signing. Although previous works on fingerspelling recognition have focused on
processing the temporal dimension of videos, there remains room for improving
the accuracy of these approaches. This paper introduces HandReader, a group of
three architectures designed to address the fingerspelling recognition task.
HandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to
process RGB features from videos of varying lengths while preserving important
sequential information. HandReader$_{KP}$ is built on the proposed Temporal
Pose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition
in a batch allows the encoder to pass them through 2D and 3D convolution
layers, utilizing temporal and spatial information and accumulating keypoints
coordinates. We also introduce HandReader_RGB+KP - architecture with a joint
encoder to benefit from RGB and keypoint modalities. Each HandReader model
possesses distinct advantages and achieves state-of-the-art results on the
ChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate
high performance on the first open dataset for Russian fingerspelling, Znaki,
presented in this paper. The Znaki dataset and HandReader pre-trained models
are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/ai-forever/handreader</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inferring Driving Maps by Deep Learning-based Trail Map Extraction <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hubbertz, Pascal Colling, Qi Han, Tobias Meisen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-definition (HD) maps offer extensive and accurate environmental
information about the driving scene, making them a crucial and essential
element for planning within autonomous driving systems. To avoid extensive
efforts from manual labeling, methods for automating the map creation have
emerged. Recent trends have moved from offline mapping to online mapping,
ensuring availability and actuality of the utilized maps. While the performance
has increased in recent years, online mapping still faces challenges regarding
temporal consistency, sensor occlusion, runtime, and generalization. We propose
a novel offline mapping approach that integrates trails - informal routes used
by drivers - into the map creation process. Our method aggregates trail data
from the ego vehicle and other traffic participants to construct a
comprehensive global map using transformer-based deep learning models. Unlike
traditional offline mapping, our approach enables continuous updates while
remaining sensor-agnostic, facilitating efficient data transfer. Our method
demonstrates superior performance compared to state-of-the-art online mapping
approaches, achieving improved generalization to previously unseen environments
and sensor configurations. We validate our approach on two benchmark datasets,
highlighting its robustness and applicability in autonomous driving systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted at the CVPR WAD 2025 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Lu, Jiaqi Tang, Jiyao Wang, Yunfan LU, Xu Cao, Qingyong Hu, Yin Wang, Yuting Zhang, Tianxin Xie, Yunpeng Zhang, Yong Chen, Jiayu. Gao, Bin Huang, Dengbo He, Shuiguang Deng, Hao Chen, Ying-Cong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intelligent driving cockpit, an important part of intelligent driving,
needs to match different users' comfort, interaction, and safety needs. This
paper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.
Sage Deer achieves three highlights: (1) Super alignment: It achieves different
reactions according to different people's preferences and biases. (2)
Generalist: It can understand the multi-view and multi-mode inputs to reason
the user's physiological indicators, facial emotions, hand movements, body
movements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It
can elicit implicit thought chains in the language space to further increase
generalist and super-aligned abilities. Besides, we collected multiple data
sets and built a large-scale benchmark. This benchmark measures the deer's
perceptual decision-making ability and the super alignment's accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct
  Preference Optimization <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Shen, Wanqi Yin, Xiaofeng Yang, Cheng Chen, Chaoyue Song, Zhongang Cai, Lei Yang, Hao Wang, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human mesh recovery (HMR) from a single image is inherently ill-posed due to
depth ambiguity and occlusions. Probabilistic methods have tried to solve this
by generating numerous plausible 3D human mesh predictions, but they often
exhibit misalignment with 2D image observations and weak robustness to
in-the-wild images. To address these issues, we propose ADHMR, a framework that
Aligns a Diffusion-based HMR model in a preference optimization manner. First,
we train a human mesh prediction assessment model, HMR-Scorer, capable of
evaluating predictions even for in-the-wild images without 3D annotations. We
then use HMR-Scorer to create a preference dataset, where each input image has
a pair of winner and loser mesh predictions. This dataset is used to finetune
the base model using direct preference optimization. Moreover, HMR-Scorer also
helps improve existing HMR models by data cleaning, even with fewer training
samples. Extensive experiments show that ADHMR outperforms current
state-of-the-art methods. Code is available at:
https://github.com/shenwenhao01/ADHMR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2025. Code: https://github.com/shenwenhao01/ADHMR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbo Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human image animation has gained increasing attention and developed rapidly
due to its broad applications in digital humans. However, existing methods rely
largely on 2D-rendered pose images for motion guidance, which limits
generalization and discards essential 3D information for open-world animation.
To tackle this problem, we propose MTVCrafter (Motion Tokenization Video
Crafter), the first framework that directly models raw 3D motion sequences
(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT
(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.
Compared to 2D-rendered pose images, 4D motion tokens offer more robust
spatio-temporal cues and avoid strict pixel-level alignment between pose image
and character, enabling more flexible and disentangled control. Then, we
introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention
with 4D positional encodings, MV-DiT can effectively leverage motion tokens as
4D compact yet expressive context for human image animation in the complex 3D
world. Hence, it marks a significant step forward in this field and opens a new
direction for pose-guided human video generation. Experiments show that our
MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,
surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter
also generalizes well to diverse open-world characters (single/multiple,
full/half-body) across various styles and scenarios. Our video demos and code
are provided in the supplementary material and at this anonymous GitHub link:
https://anonymous.4open.science/r/MTVCrafter-1B13.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Interplay of Human-AI Alignment,Fairness, and Performance
  Trade-offs in Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Luo, Ziyu Zhou, Zixin Shu, Aurélie Pahud de Mortanges, Robert Berke, Mauricio Reyes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks excel in medical imaging but remain prone to biases,
leading to fairness gaps across demographic groups. We provide the first
systematic exploration of Human-AI alignment and fairness in this domain. Our
results show that incorporating human insights consistently reduces fairness
gaps and enhances out-of-domain generalization, though excessive alignment can
introduce performance trade-offs, emphasizing the need for calibrated
strategies. These findings highlight Human-AI alignment as a promising approach
for developing fair, robust, and generalizable medical AI systems, striking a
balance between expert guidance and automated efficiency. Our code is available
at https://github.com/Roypic/Aligner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution
  Generalisation in MRI Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puru Vaish, Felix Meister, Tobias Heimann, Christoph Brune, Jelmer M. Wolterink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation models are often trained on curated datasets,
leading to performance degradation when deployed in real-world clinical
settings due to mismatches between training and test distributions. While data
augmentation techniques are widely used to address these challenges,
traditional visually consistent augmentation strategies lack the robustness
needed for diverse real-world scenarios. In this work, we systematically
evaluate alternative augmentation strategies, focusing on MixUp and Auxiliary
Fourier Augmentation. These methods mitigate the effects of multiple variations
without explicitly targeting specific sources of distribution shifts. We
demonstrate how these techniques significantly improve out-of-distribution
generalization and robustness to imaging variations across a wide range of
transformations in cardiac cine MRI and prostate MRI segmentation. We
quantitatively find that these augmentation methods enhance learned feature
representations by promoting separability and compactness. Additionally, we
highlight how their integration into nnU-Net training pipelines provides an
easy-to-implement, effective solution for enhancing the reliability of medical
segmentation models in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MIDL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umair Haroon, Ahmad AlMughrabi, Thanasis Zoumpekas, Ricardo Marques, Petia Radeva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate food volume estimation is crucial for medical nutrition management
and health monitoring applications, but current food volume estimation methods
are often limited by mononuclear data, leveraging single-purpose hardware such
as 3D scanners, gathering sensor-oriented information such as depth
information, or relying on camera calibration using a reference object. In this
paper, we present VolE, a novel framework that leverages mobile device-driven
3D reconstruction to estimate food volume. VolE captures images and camera
locations in free motion to generate precise 3D models, thanks to AR-capable
mobile devices. To achieve real-world measurement, VolE is a reference- and
depth-free framework that leverages food video segmentation for food mask
generation. We also introduce a new food dataset encompassing the challenging
scenarios absent in the previous benchmarks. Our experiments demonstrate that
VolE outperforms the existing volume estimation techniques across multiple
datasets by achieving 2.22 % MAPE, highlighting its superior performance in
food volume estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Saliency <span class="highlight-title">Dataset</span> Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Kümmerer, Harneet Khanuja, Matthias Bethge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in image-based saliency prediction are approaching gold
standard performance levels on existing benchmarks. Despite this success, we
show that predicting fixations across multiple saliency datasets remains
challenging due to dataset bias. We find a significant performance drop (around
40%) when models trained on one dataset are applied to another. Surprisingly,
increasing dataset diversity does not resolve this inter-dataset gap, with
close to 60% attributed to dataset-specific biases. To address this remaining
generalization gap, we propose a novel architecture extending a mostly
dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific
parameters that govern interpretable mechanisms such as multi-scale structure,
center bias, and fixation spread. Adapting only these parameters to new data
accounts for more than 75% of the generalization gap, with a large fraction of
the improvement achieved with as few as 50 samples. Our model sets a new
state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark
(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from
unrelated datasets, but with a substantial boost when adapting to the
respective training datasets. The model also provides valuable insights into
spatial saliency properties, revealing complex multi-scale effects that combine
both absolute and relative sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Source Collaborative Style Augmentation and Domain-Invariant
  Learning for Federated Domain Generalization <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikang Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated domain generalization aims to learn a generalizable model from
multiple decentralized source domains for deploying on the unseen target
domain. The style augmentation methods have achieved great progress on domain
generalization. However, the existing style augmentation methods either explore
the data styles within isolated source domain or interpolate the style
information across existing source domains under the data decentralization
scenario, which leads to limited style space. To address this issue, we propose
a Multi-source Collaborative Style Augmentation and Domain-invariant learning
method (MCSAD) for federated domain generalization. Specifically, we propose a
multi-source collaborative style augmentation module to generate data in the
broader style space. Furthermore, we conduct domain-invariant learning between
the original data and augmented data by cross-domain feature alignment within
the same class and classes relation ensemble distillation between different
classes to learn a domain-invariant model. By alternatively conducting
collaborative style augmentation and domain-invariant learning, the model can
generalize well on unseen target domain. Extensive experiments on multiple
domain generalization datasets indicate that our method significantly
outperforms the state-of-the-art federated domain generalization methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuechang Tu, Lukas Radl, Michael Steiner, Markus Steinberger, Bernhard Kerbl, Fernando de la Torre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has rapidly become a leading technique for
novel-view synthesis, providing exceptional performance through efficient
software-based GPU rasterization. Its versatility enables real-time
applications, including on mobile and lower-powered devices. However, 3DGS
faces key challenges in virtual reality (VR): (1) temporal artifacts, such as
popping during head movements, (2) projection-based distortions that result in
disturbing and view-inconsistent floaters, and (3) reduced framerates when
rendering large numbers of Gaussians, falling below the critical threshold for
VR. Compared to desktop environments, these issues are drastically amplified by
large field-of-view, constant head movements, and high resolution of
head-mounted displays (HMDs). In this work, we introduce VRSplat: we combine
and extend several recent advancements in 3DGS to address challenges of VR
holistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal
Projection can complement each other, by modifying the individual techniques
and core 3DGS rasterizer. Additionally, we propose an efficient foveated
rasterizer that handles focus and peripheral areas in a single GPU launch,
avoiding redundant computations and improving GPU utilization. Our method also
incorporates a fine-tuning step that optimizes Gaussian parameters based on
StopThePop depth evaluations and Optimal Projection. We validate our method
through a controlled user study with 25 participants, showing a strong
preference for VRSplat over other configurations of Mini-Splatting. VRSplat is
the first, systematically evaluated 3DGS approach capable of supporting modern
VR applications, achieving 72+ FPS while eliminating popping and
stereo-disrupting floaters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I3D'25 (PACMCGIT); Project Page: https://cekavis.site/VRSplat/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IMITATE: Image Registration with Context for unknown time frame recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziad Kheil, Lucas Robinet, Laurent Risser, Soleakhena Ken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we formulate a novel image registration formalism dedicated to
the estimation of unknown condition-related images, based on two or more known
images and their associated conditions. We show how to practically model this
formalism by using a new conditional U-Net architecture, which fully takes into
account the conditional information and does not need any fixed image. Our
formalism is then applied to image moving tumors for radiotherapy treatment at
different breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal
regions. This driving application is particularly complex as it requires to
stitch a collection of sequential 2D slices into several 3D volumes at
different organ positions. Movement interpolation with standard methods then
generates well known reconstruction artefacts in the assembled volumes due to
irregular patient breathing, hysteresis and poor correlation of breathing
signal to internal motion. Results obtained on 4D-CT clinical data showcase
artefact-free volumes achieved through real-time latencies. The code is
publicly available at https://github.com/Kheil-Z/IMITATE .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE ISBI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via
  Multi-Objective Balanced Covering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangfu Li, Hongjian Zhan, Tianyi Chen, Qi Liu, Yue Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing visual token pruning methods target prompt alignment and visual
preservation with static strategies, overlooking the varying relative
importance of these objectives across tasks, which leads to inconsistent
performance. To address this, we derive the first closed-form error bound for
visual token pruning based on the Hausdorff distance, uniformly characterizing
the contributions of both objectives. Moreover, leveraging $\epsilon$-covering
theory, we reveal an intrinsic trade-off between these objectives and quantify
their optimal attainment levels under a fixed budget. To practically handle
this trade-off, we propose Multi-Objective Balanced Covering (MoB), which
reformulates visual token pruning as a bi-objective covering problem. In this
framework, the attainment trade-off reduces to budget allocation via greedy
radius trading. MoB offers a provable performance bound and linear scalability
with respect to the number of input visual tokens, enabling adaptation to
challenging pruning scenarios. Extensive experiments show that MoB preserves
96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual
tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\times$ with negligible
performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm
that MoB integrates seamlessly into advanced MLLMs and diverse vision-language
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages,9 figures,conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMRL++: Parameter-Efficient and Interaction-Aware Representation
  Learning for Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuncheng Guo, Xiaodong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale pre-trained Vision-Language Models (VLMs) have significantly
advanced transfer learning across diverse tasks. However, adapting these models
with limited few-shot data often leads to overfitting, undermining their
ability to generalize to new tasks. To address this, we propose Multi-Modal
Representation Learning (MMRL), which introduces a shared, learnable,
modality-agnostic representation space. MMRL generates space tokens projected
into both text and image encoders as representation tokens, enabling more
effective cross-modal interactions. Unlike prior methods that mainly optimize
class token features, MMRL inserts representation tokens into higher encoder
layers--where task-specific features are more prominent--while preserving
general knowledge in the lower layers. During training, both class and
representation features are jointly optimized: a trainable projection layer is
applied to representation tokens for task adaptation, while the projection
layer for class token remains frozen to retain pre-trained knowledge. To
further promote generalization, we introduce a regularization term aligning
class and text features with the frozen VLM's zero-shot features. At inference,
a decoupling strategy uses both class and representation features for base
tasks, but only class features for novel tasks due to their stronger
generalization. Building upon this, we propose MMRL++, a parameter-efficient
and interaction-aware extension that significantly reduces trainable parameters
and enhances intra-modal interactions--particularly across the layers of
representation tokens--allowing gradient sharing and instance-specific
information to propagate more effectively through the network. Extensive
experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently
outperform state-of-the-art methods, achieving a strong balance between
task-specific adaptation and generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Due to the limitation "The abstract field cannot be longer than 1,920
  characters", the abstract appearing here is slightly shorter than that in the
  PDF file</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlowDreamer: A RGB-D World Model with Flow-based Motion Representations
  for Robot Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Guo, Xiaojian Ma, Yikai Wang, Min Yang, Huaping Liu, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates training better visual world models for robot
manipulation, i.e., models that can predict future visual observations by
conditioning on past frames and robot actions. Specifically, we consider world
models that operate on RGB-D frames (RGB-D world models). As opposed to
canonical approaches that handle dynamics prediction mostly implicitly and
reconcile it with visual rendering in a single model, we introduce FlowDreamer,
which adopts 3D scene flow as explicit motion representations. FlowDreamer
first predicts 3D scene flow from past frame and action conditions with a
U-Net, and then a diffusion model will predict the future frame utilizing the
scene flow. FlowDreamer is trained end-to-end despite its modularized nature.
We conduct experiments on 4 different benchmarks, covering both video
prediction and visual planning tasks. The results demonstrate that FlowDreamer
achieves better performance compared to other baseline RGB-D world models by 7%
on semantic similarity, 11% on pixel quality, and 6% on success rate in various
robot manipulation domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: see https://sharinka0715.github.io/FlowDreamer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head
  Avatars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui-Yang Ju, Sheng-Yen Huang, Yi-Ping Hung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of 3D Gaussian blendshapes has enabled the real-time
reconstruction of animatable head avatars from monocular video. Toonify, a
StyleGAN-based framework, has become widely used for facial image stylization.
To extend Toonify for synthesizing diverse stylized 3D head avatars using
Gaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.
In Stage 1 (stylized video generation), we employ an improved StyleGAN to
generate the stylized video from the input video frames, which addresses the
limitation of cropping aligned faces at a fixed resolution as preprocessing for
normal StyleGAN. This process provides a more stable video, which enables
Gaussian blendshapes to better capture the high-frequency details of the video
frames, and efficiently generate high-quality animation in the next stage. In
Stage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head
model and a set of expression blendshapes from the generated video. By
combining the neutral head model with expression blendshapes, ToonifyGB can
efficiently render stylized avatars with arbitrary expressions. We validate the
effectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane
and Pixar.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PsOCR: Benchmarking Large Multimodal Models for Optical Character
  Recognition in Low-resource Pashto Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ijazul Haq, Yingjie Zhang, Irfan Ali Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper evaluates the performance of Large Multimodal Models (LMMs) on
Optical Character Recognition (OCR) in the low-resource Pashto language.
Natural Language Processing (NLP) in Pashto faces several challenges due to the
cursive nature of its script and a scarcity of structured datasets. To address
this, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one
million images annotated with bounding boxes at word, line, and document
levels, suitable for training and evaluating models based on different
architectures, including Convolutional Neural Networks (CNNs) and Transformers.
PsOCR covers variations across 1,000 unique font families, colors, image sizes,
and layouts. A benchmark subset of 10K images was selected to evaluate the
performance of several LMMs, including seven open-source models: DeepSeek's
Janus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four
closed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results
demonstrate that Gemini achieves the best performance among all models, whereas
among open-source models, Qwen-7B stands out. This work provides an insightful
assessment of the capabilities and limitations of current LMMs for OCR tasks in
Pashto and establishes a foundation for further research not only in Pashto OCR
but also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is
available at https://github.com/zirak-ai/PashtoOCR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advances in Radiance Field for Dynamic Scene: From Neural Field to
  Gaussian Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlong Fan, Xuepu Zeng, Jing Zhang, Mingming Gong, Yuxiang Yang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic scene representation and reconstruction have undergone transformative
advances in recent years, catalyzed by breakthroughs in neural radiance fields
and 3D Gaussian splatting techniques. While initially developed for static
environments, these methodologies have rapidly evolved to address the
complexities inherent in 4D dynamic scenes through an expansive body of
research. Coupled with innovations in differentiable volumetric rendering,
these approaches have significantly enhanced the quality of motion
representation and dynamic scene reconstruction, thereby garnering substantial
attention from the computer vision and graphics communities. This survey
presents a systematic analysis of over 200 papers focused on dynamic scene
representation using radiance field, spanning the spectrum from implicit neural
representations to explicit Gaussian primitives. We categorize and evaluate
these works through multiple critical lenses: motion representation paradigms,
reconstruction techniques for varied scene dynamics, auxiliary information
integration strategies, and regularization approaches that ensure temporal
consistency and physical plausibility. We organize diverse methodological
approaches under a unified representational framework, concluding with a
critical examination of persistent challenges and promising research
directions. By providing this comprehensive overview, we aim to establish a
definitive reference for researchers entering this rapidly evolving field while
offering experienced practitioners a systematic understanding of both
conceptual principles and practical frontiers in dynamic scene reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Deep Fusion of Large Language Models and Diffusion
  <span class="highlight-title">Transformer</span>s for Text-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingda Tang, Boyang Zheng, Xichen Pan, Sayak Paul, Saining Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper does not describe a new method; instead, it provides a thorough
exploration of an important yet understudied design space related to recent
advances in text-to-image synthesis -- specifically, the deep fusion of large
language models (LLMs) and diffusion transformers (DiTs) for multi-modal
generation. Previous studies mainly focused on overall system performance
rather than detailed comparisons with alternative methods, and key design
details and training recipes were often left undisclosed. These gaps create
uncertainty about the real potential of this approach. To fill these gaps, we
conduct an empirical study on text-to-image generation, performing controlled
comparisons with established baselines, analyzing important design choices, and
providing a clear, reproducible recipe for training at scale. We hope this work
offers meaningful data points and practical guidelines for future research in
multi-modal generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection
  of Diseases in Cocos nucifera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miit Daga, Dhriti Parikh, Swarna Priya Ramu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coconut tree diseases are a serious risk to agricultural yield, particularly
in developing countries where conventional farming practices restrict early
diagnosis and intervention. Current disease identification methods are manual,
labor-intensive, and non-scalable. In response to these limitations, we come up
with DeepSeqCoco, a deep learning based model for accurate and automatic
disease identification from coconut tree images. The model was tested under
various optimizer settings, such as SGD, Adam, and hybrid configurations, to
identify the optimal balance between accuracy, minimization of loss, and
computational cost. Results from experiments indicate that DeepSeqCoco can
achieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than
existing models) with the hybrid SGD-Adam showing the lowest validation loss of
2.81%. It also shows a drop of up to 18% in training time and up to 85% in
prediction time for input images. The results point out the promise of the
model to improve precision agriculture through an AI-based, scalable, and
efficient disease monitoring system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted for publication in IEEE Access journal and is
  currently pending revisions before publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model
  Super-Resolution Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of remote sensing technology, super-resolution
image reconstruction is of great research and practical significance. Existing
deep learning methods have made progress but still face limitations in handling
complex scenes and preserving image details. This paper proposes a
reinforcement learning-based latent diffusion model (LDM) fine-tuning method
for remote sensing image super-resolution. The method constructs a
reinforcement learning environment with states, actions, and rewards,
optimizing decision objectives through proximal policy optimization (PPO)
during the reverse denoising process of the LDM model. Experiments on the
RESISC45 dataset show significant improvements over the baseline model in PSNR,
SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,
and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural
scenes. The results demonstrate the method's effectiveness in enhancing
super-resolution quality and adaptability across scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 4th International Conference on Computing Innovation
  and Applied Physics (CONF-CIAP 2025), and will be published in EAI Community
  Research Series-CORE or Theoretical and Natural Science (TNS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application of YOLOv8 in monocular downward multiple Car Target
  detection <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving technology is progressively transforming traditional car
driving methods, marking a significant milestone in modern transportation.
Object detection serves as a cornerstone of autonomous systems, playing a vital
role in enhancing driving safety, enabling autonomous functionality, improving
traffic efficiency, and facilitating effective emergency responses. However,
current technologies such as radar for environmental perception, cameras for
road perception, and vehicle sensor networks face notable challenges, including
high costs, vulnerability to weather and lighting conditions, and limited
resolution.To address these limitations, this paper presents an improved
autonomous target detection network based on YOLOv8. By integrating structural
reparameterization technology, a bidirectional pyramid structure network model,
and a novel detection pipeline into the YOLOv8 framework, the proposed approach
achieves highly efficient and precise detection of multi-scale, small, and
remote objects. Experimental results demonstrate that the enhanced model can
effectively detect both large and small objects with a detection accuracy of
65%, showcasing significant advancements over traditional methods.This improved
model holds substantial potential for real-world applications and is
well-suited for autonomous driving competitions, such as the Formula Student
Autonomous China (FSAC), particularly excelling in scenarios involving
single-target and small-object detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 5th International Conference on Signal Processing and
  Machine Learning (CONF-SPML 2025), to appear in Applied and Computational
  Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive
  3D Sketching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Zang, Yuanqi Hu, Xinyu Chen, Yuxia Xu, Suhui Wang, Chunan Yu, Lanyun Zhu, Deyi Ji, Xin Xu, Tianrun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of immersive consumer electronics, such as AR/VR headsets and
smart devices, people increasingly seek ways to express their identity through
virtual fashion. However, existing 3D garment design tools remain inaccessible
to everyday users due to steep technical barriers and limited data. In this
work, we introduce a 3D sketch-driven 3D garment generation framework that
empowers ordinary users - even those without design experience - to create
high-quality digital clothing through simple 3D sketches in AR/VR environments.
By combining a conditional diffusion model, a sketch encoder trained in a
shared latent space, and an adaptive curriculum learning strategy, our system
interprets imprecise, free-hand input and produces realistic, personalized
garments. To address the scarcity of training data, we also introduce
KO3DClothes, a new dataset of paired 3D garments and user-created sketches.
Extensive experiments and user studies confirm that our method significantly
outperforms existing baselines in both fidelity and usability, demonstrating
its promise for democratized fashion design on next-generation consumer
platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Descriptive Image-Text Matching with Graded Contextual Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhyun Jang, Jiyeong Lee, Kwanghoon Sohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-text matching aims to build correspondences between visual and textual
data by learning their pairwise similarities. Most existing approaches have
adopted sparse binary supervision, indicating whether a pair of images and
sentences matches or not. However, such sparse supervision covers a limited
subset of image-text relationships, neglecting their inherent many-to-many
correspondences; an image can be described in numerous texts at different
descriptive levels. Moreover, existing approaches overlook the implicit
connections from general to specific descriptions, which form the underlying
rationale for the many-to-many relationships between vision and language. In
this work, we propose descriptive image-text matching, called DITM, to learn
the graded contextual similarity between image and text by exploring the
descriptive flexibility of language. We formulate the descriptiveness score of
each sentence with cumulative term frequency-inverse document frequency
(TF-IDF) to balance the pairwise similarity according to the keywords in the
sentence. Our method leverages sentence descriptiveness to learn robust
image-text matching in two key ways: (1) to refine the false negative labeling,
dynamically relaxing the connectivity between positive and negative pairs, and
(2) to build more precise matching, aligning a set of relevant sentences in a
generic-to-specific order. By moving beyond rigid binary supervision, DITM
enhances the discovery of both optimal matches and potential positive pairs.
Extensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the
effectiveness of our method in representing complex image-text relationships
compared to state-of-the-art approaches. In addition, DITM enhances the
hierarchical reasoning ability of the model, supported by the extensive
analysis on HierarCaps benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PointArena: Probing Multimodal Grounding Through Language-Guided
  Pointing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Cheng, Jiafei Duan, Yi Ru Wang, Haoquan Fang, Boyang Li, Yushan Huang, Elvis Wang, Ainaz Eftekhar, Jason Lee, Wentao Yuan, Rose Hendrix, Noah A. Smith, Fei Xia, Dieter Fox, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pointing serves as a fundamental and intuitive mechanism for grounding
language within visual contexts, with applications spanning robotics, assistive
technologies, and interactive AI systems. While recent multimodal models have
started to support pointing capabilities, existing benchmarks typically focus
only on referential object localization tasks. We introduce PointArena, a
comprehensive platform for evaluating multimodal pointing across diverse
reasoning scenarios. PointArena comprises three components: (1) Point-Bench, a
curated dataset containing approximately 1,000 pointing tasks across five
reasoning categories; (2) Point-Battle, an interactive, web-based arena
facilitating blind, pairwise model comparisons, which has already gathered over
4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation
system allowing users to directly evaluate multimodal model pointing
capabilities in practical settings. We conducted extensive evaluations of both
state-of-the-art open-source and proprietary multimodal models. Results
indicate that Molmo-72B consistently outperforms other models, though
proprietary models increasingly demonstrate comparable performance.
Additionally, we find that supervised training specifically targeting pointing
tasks significantly enhances model performance. Across our multi-stage
evaluation pipeline, we also observe strong correlations, underscoring the
critical role of precise pointing capabilities in enabling multimodal models to
effectively bridge abstract reasoning with concrete, real-world actions.
Project page: https://pointarena.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Pages, Dataset and code:https://pointarena.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High Quality Underwater Image Compression with Adaptive Correction and
  Codebook-based Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimin Zhou, Yichong Xia, Sicheng Pan, Bin Chen, Baoyi An, Haoqian Wang, Zhi Wang, Yaowei Wang, Zikun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing exploration and exploitation of the underwater world,
underwater images have become a critical medium for human interaction with
marine environments, driving extensive research into their efficient
transmission and storage. However, contemporary underwater image compression
algorithms fail to fully leverage the unique characteristics distinguishing
underwater scenes from terrestrial images, resulting in suboptimal performance.
To address this limitation, we introduce HQUIC, designed to exploit
underwater-image-specific features for enhanced compression efficiency. HQUIC
employs an ALTC module to adaptively predict the attenuation coefficients and
global light information of the images, which effectively mitigates the issues
caused by the differences in lighting and tone existing in underwater images.
Subsequently, HQUIC employs a codebook as an auxiliary branch to extract the
common objects within underwater images and enhances the performance of the
main branch. Furthermore, HQUIC dynamically weights multi-scale frequency
components, prioritizing information critical for distortion quality while
discarding redundant details. Extensive evaluations on diverse underwater
datasets demonstrate that HQUIC outperforms state-of-the-art compression
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Yu, Bin Huang, Minghui Zhang, Weiwen Wu, Shaoyu Wang, Qiegen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score-based diffusion models have shown significant promise in the field of
sparse-view CT reconstruction. However, the projection dataset is large and
riddled with redundancy. Consequently, applying the diffusion model to
unprocessed data results in lower learning effectiveness and higher learning
difficulty, frequently leading to reconstructed images that lack fine details.
To address these issues, we propose the ordered-subsets multi-diffusion model
(OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT
projection data into equal subsets and employs multi-subsets diffusion model
(MSDM) to learn from each subset independently. This targeted learning approach
reduces complexity and enhances the reconstruction of fine details.
Furthermore, the integration of one-whole diffusion model (OWDM) with complete
sinogram data acts as a global information constraint, which can reduce the
possibility of generating erroneous or inconsistent sinogram information.
Moreover, the OSMM's unsupervised learning framework provides strong robustness
and generalizability, adapting seamlessly to varying sparsity levels of CT
sinograms. This ensures consistent and reliable performance across different
clinical scenarios. Experimental results demonstrate that OSMM outperforms
traditional diffusion models in terms of image quality and noise resilience,
offering a powerful and versatile solution for advanced CT imaging in
sparse-view scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of
  Airborne LiDAR Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gao, Shaobo Xia, Sheng Nie, Cheng Wang, Xiaohuan Xi, Bisheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Airborne laser scanning (ALS) point cloud segmentation is a fundamental task
for large-scale 3D scene understanding. In real-world applications, models are
typically fixed after training. However, domain shifts caused by changes in the
environment, sensor types, or sensor degradation often lead to a decline in
model performance. Continuous Test-Time Adaptation (CTTA) offers a solution by
adapting a source-pretrained model to evolving, unlabeled target domains.
Despite its potential, research on ALS point clouds remains limited, facing
challenges such as the absence of standardized datasets and the risk of
catastrophic forgetting and error accumulation during prolonged adaptation. To
tackle these challenges, we propose APCoTTA, the first CTTA method tailored for
ALS point cloud semantic segmentation. We propose a dynamic trainable layer
selection module. This module utilizes gradient information to select
low-confidence layers for training, and the remaining layers are kept frozen,
mitigating catastrophic forgetting. To further reduce error accumulation, we
propose an entropy-based consistency loss. By losing such samples based on
entropy, we apply consistency loss only to the reliable samples, enhancing
model stability. In addition, we propose a random parameter interpolation
mechanism, which randomly blends parameters from the selected trainable layers
with those of the source model. This approach helps balance target adaptation
and source knowledge retention, further alleviating forgetting. Finally, we
construct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA
benchmarks for ALS point cloud segmentation. Experimental results demonstrate
that APCoTTA achieves the best performance on two benchmarks, with mIoU
improvements of approximately 9% and 14% over direct inference. The new
benchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages,12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liqian Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial expression recognition (FER) in the wild remains a challenging task
due to the subtle and localized nature of expression-related features, as well
as the complex variations in facial appearance. In this paper, we introduce a
novel framework that explicitly focuses on Texture Key Driver Factors (TKDF),
localized texture regions that exhibit strong discriminative power across
emotional categories. By carefully observing facial image patterns, we identify
that certain texture cues, such as micro-changes in skin around the brows,
eyes, and mouth, serve as primary indicators of emotional dynamics. To
effectively capture and leverage these cues, we propose a FER architecture
comprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual
Information Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced
with multi-branch attention to extract fine-grained texture representations,
while DCIF refines these features by filtering context through adaptive pooling
and attention mechanisms. Experimental results on RAF-DB and KDEF datasets
demonstrate that our method achieves state-of-the-art performance, verifying
the effectiveness and robustness of incorporating TKDFs into FER pipelines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier
  Refinement for Diffusion-Based Disease Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yang, Tao Tan, Shuai Tan, Weiqin Yang, Kunyan Cai, Calvin Chen, Yue Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modelling disease progression in precision medicine requires capturing
complex spatio-temporal dynamics while preserving anatomical integrity.
Existing methods often struggle with longitudinal dependencies and structural
consistency in progressive disorders. To address these limitations, we
introduce MambaControl, a novel framework that integrates selective state-space
modelling with diffusion processes for high-fidelity prediction of medical
image trajectories. To better capture subtle structural changes over time while
maintaining anatomical consistency, MambaControl combines Mamba-based
long-range modelling with graph-guided anatomical control to more effectively
represent anatomical correlations. Furthermore, we introduce Fourier-enhanced
spectral graph representations to capture spatial coherence and multiscale
detail, enabling MambaControl to achieve state-of-the-art performance in
Alzheimer's disease prediction. Quantitative and regional evaluations
demonstrate improved progression prediction quality and anatomical fidelity,
highlighting its potential for personalised prognosis and clinical decision
support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSPENet: Contour-Aware and Saliency Priors Embedding Network for
  Infrared Small Target Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiakun Deng, Kexuan Li, Xingye Cui, Jiaxuan Li, Chang Long, Tian Pu, Zhenming Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared small target detection (ISTD) plays a critical role in a wide range
of civilian and military applications. Existing methods suffer from
deficiencies in the localization of dim targets and the perception of contour
information under dense clutter environments, severely limiting their detection
performance. To tackle these issues, we propose a contour-aware and saliency
priors embedding network (CSPENet) for ISTD. We first design a
surround-convergent prior extraction module (SCPEM) that effectively captures
the intrinsic characteristic of target contour pixel gradients converging
toward their center. This module concurrently extracts two collaborative
priors: a boosted saliency prior for accurate target localization and
multi-scale structural priors for comprehensively enriching contour detail
representation. Building upon this, we propose a dual-branch priors embedding
architecture (DBPEA) that establishes differentiated feature fusion pathways,
embedding these two priors at optimal network positions to achieve performance
enhancement. Finally, we develop an attention-guided feature enhancement module
(AGFEM) to refine feature representations and improve saliency estimation
accuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and
NUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art
methods in detection performance. The code is available at
https://github.com/IDIP2025/CSPENet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Registration Change Detection: A Novel Change Detection Task and
  Benchmark <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Shan, Lei Zhou, Liu Mao, Shaofan Chen, Chuanqiu Ren, Xia Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a novel remote sensing change detection task,
non-registration change detection, to address the increasing number of
emergencies such as natural disasters, anthropogenic accidents, and military
strikes. First, in light of the limited discourse on the issue of
non-registration change detection, we systematically propose eight scenarios
that could arise in the real world and potentially contribute to the occurrence
of non-registration problems. Second, we develop distinct image transformation
schemes tailored to various scenarios to convert the available registration
change detection dataset into a non-registration version. Finally, we
demonstrate that non-registration change detection can cause catastrophic
damage to the state-of-the-art methods. Our code and dataset are available at
https://github.com/ShanZard/NRCD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IGARSS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VRU-CIPI: Crossing Intention Prediction at Intersections for Improving
  Vulnerable Road Users Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed S. Abdelrahman, Mohamed Abdel-Aty, Quoc Dai Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and predicting human behavior in-thewild, particularly at urban
intersections, remains crucial for enhancing interaction safety between road
users. Among the most critical behaviors are crossing intentions of Vulnerable
Road Users (VRUs), where misinterpretation may result in dangerous conflicts
with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a
sequential attention-based model designed to predict VRU crossing intentions at
intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal
dynamics in VRU movements, combined with a multi-head Transformer
self-attention mechanism to encode contextual and spatial dependencies critical
for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed
achieves state-of-the-art performance with an accuracy of 96.45% and achieving
real-time inference speed reaching 33 frames per second. Furthermore, by
integrating with Infrastructure-to-Vehicles (I2V) communication, our approach
can proactively enhance intersection safety through timely activation of
crossing signals and providing early warnings to connected vehicles, ensuring
smoother and safer interactions for all road users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDFP: Data-dependent Frequency <span class="highlight-title">Prompt</span> for Source Free Domain Adaptation
  of Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Yin, Shaolei Liu, Manning Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptation addresses the challenge of model performance degradation
caused by domain gaps. In the typical setup for unsupervised domain adaptation,
labeled data from a source domain and unlabeled data from a target domain are
used to train a target model. However, access to labeled source domain data,
particularly in medical datasets, can be restricted due to privacy policies. As
a result, research has increasingly shifted to source-free domain adaptation
(SFDA), which requires only a pretrained model from the source domain and
unlabeled data from the target domain data for adaptation. Existing SFDA
methods often rely on domain-specific image style translation and
self-supervision techniques to bridge the domain gap and train the target
domain model. However, the quality of domain-specific style-translated images
and pseudo-labels produced by these methods still leaves room for improvement.
Moreover, training the entire model during adaptation can be inefficient under
limited supervision. In this paper, we propose a novel SFDA framework to
address these challenges. Specifically, to effectively mitigate the impact of
domain gap in the initial training phase, we introduce preadaptation to
generate a preadapted model, which serves as an initialization of target model
and allows for the generation of high-quality enhanced pseudo-labels without
introducing extra parameters. Additionally, we propose a data-dependent
frequency prompt to more effectively translate target domain images into a
source-like style. To further enhance adaptation, we employ a style-related
layer fine-tuning strategy, specifically designed for SFDA, to train the target
model using the prompted target domain images and pseudo-labels. Extensive
experiments on cross-modality abdominal and cardiac SFDA segmentation tasks
demonstrate that our proposed method outperforms existing state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin-Bin Gao, Yue Zhu, Jiangtao Yan, Yuezhi Cai, Weixi Zhang, Meng Wang, Jun Liu, Yong Liu, Lei Wang, Chengjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universal visual anomaly detection aims to identify anomalies from novel or
unseen vision domains without additional fine-tuning, which is critical in open
scenarios. Recent studies have demonstrated that pre-trained vision-language
models like CLIP exhibit strong generalization with just zero or a few normal
images. However, existing methods struggle with designing prompt templates,
complex token interactions, or requiring additional fine-tuning, resulting in
limited flexibility. In this work, we present a simple yet effective method
called AdaptCLIP based on two key insights. First, adaptive visual and textual
representations should be learned alternately rather than jointly. Second,
comparative learning between query and normal image prompt should incorporate
both contextual and aligned residual features, rather than relying solely on
residual features. AdaptCLIP treats CLIP models as a foundational service,
adding only three simple adapters, visual adapter, textual adapter, and
prompt-query adapter, at its input or output ends. AdaptCLIP supports
zero-/few-shot generalization across domains and possesses a training-free
manner on target domains once trained on a base dataset. AdaptCLIP achieves
state-of-the-art performance on 12 anomaly detection benchmarks from industrial
and medical domains, significantly outperforming existing competitive methods.
We will make the code and model of AdaptCLIP available at
https://github.com/gaobb/AdaptCLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 15 figures, 22 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large-Scale Gaussian Splatting SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Xin, Chenyang Wu, Penghui Huang, Yanyong Zhang, Yinian Mao, Guoquan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS) have shown encouraging and impressive results for visual SLAM.
However, most representative methods require RGBD sensors and are only
available for indoor environments. The robustness of reconstruction in
large-scale outdoor scenarios remains unexplored. This paper introduces a
large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The
proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses
under large view changes. In tracking, we introduce feature-alignment warping
constraints to alleviate the adverse effects of appearance similarity in
rendering losses. For the scalability of large-scale scenarios, we introduce
continuous Gaussian Splatting submaps to tackle unbounded scenes with limited
memory. Loops are detected between GS submaps by place recognition and the
relative pose between looped keyframes is optimized utilizing rendering and
feature warping losses. After the global optimization of camera poses and
Gaussian points, a structure refinement module enhances the reconstruction
quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM
achieves superior performance over existing Neural, 3DGS-based, and even
traditional approaches. Project page: https://lsg-slam.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniSkill: Imitating Human Videos via Cross-Embodiment Skill
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanjung Kim, Jaehyun Kang, Hyolim Kang, Meedeum Cho, Seon Joo Kim, Youngwoon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mimicry is a fundamental learning mechanism in humans, enabling individuals
to learn new tasks by observing and imitating experts. However, applying this
ability to robots presents significant challenges due to the inherent
differences between human and robot embodiments in both their visual appearance
and physical capabilities. While previous methods bridge this gap using
cross-embodiment datasets with shared scenes and tasks, collecting such aligned
data between humans and robots at scale is not trivial. In this paper, we
propose UniSkill, a novel framework that learns embodiment-agnostic skill
representations from large-scale cross-embodiment video data without any
labels, enabling skills extracted from human video prompts to effectively
transfer to robot policies trained only on robot data. Our experiments in both
simulation and real-world environments show that our cross-embodiment skills
successfully guide robots in selecting appropriate actions, even with unseen
video prompts. The project website can be found at:
https://kimhanjung.github.io/UniSkill.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://kimhanjung.github.io/UniSkill/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A portable diagnosis model for Keratoconus using a smartphone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Li, Peter Ho, Jo Woon Chong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keratoconus (KC) is a corneal disorder that results in blurry and distorted
vision. Traditional diagnostic tools, while effective, are often bulky, costly,
and require professional operation. In this paper, we present a portable and
innovative methodology for diagnosing. Our proposed approach first captures the
image reflected on the eye's cornea when a smartphone screen-generated Placido
disc sheds its light on an eye, then utilizes a two-stage diagnosis for
identifying the KC cornea and pinpointing the location of the KC on the cornea.
The first stage estimates the height and width of the Placido disc extracted
from the captured image to identify whether it has KC. In this KC
identification, k-means clustering is implemented to discern statistical
characteristics, such as height and width values of extracted Placido discs,
from non-KC (control) and KC-affected groups. The second stage involves the
creation of a distance matrix, providing a precise localization of KC on the
cornea, which is critical for efficient treatment planning. The analysis of
these distance matrices, paired with a logistic regression model and robust
statistical analysis, reveals a clear distinction between control and KC
groups. The logistic regression model, which classifies small areas on the
cornea as either control or KC-affected based on the corresponding inter-disc
distances in the distance matrix, reported a classification accuracy of 96.94%,
which indicates that we can effectively pinpoint the protrusion caused by KC.
This comprehensive, smartphone-based method is expected to detect KC and
streamline timely treatment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Learning-Driven Inhalation Injury Grading Assistant Using
  Bronchoscopy Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Li, Alan W Pang, Jo Woon Chong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inhalation injuries present a challenge in clinical diagnosis and grading due
to Conventional grading methods such as the Abbreviated Injury Score (AIS)
being subjective and lacking robust correlation with clinical parameters like
mechanical ventilation duration and patient mortality. This study introduces a
novel deep learning-based diagnosis assistant tool for grading inhalation
injuries using bronchoscopy images to overcome subjective variability and
enhance consistency in severity assessment. Our approach leverages data
augmentation techniques, including graphic transformations, Contrastive
Unpaired Translation (CUT), and CycleGAN, to address the scarcity of medical
imaging data. We evaluate the classification performance of two deep learning
models, GoogLeNet and Vision Transformer (ViT), across a dataset significantly
expanded through these augmentation methods. The results demonstrate GoogLeNet
combined with CUT as the most effective configuration for grading inhalation
injuries through bronchoscopy images and achieves a classification accuracy of
97.8%. The histograms and frequency analysis evaluations reveal variations
caused by the augmentation CUT with distribution changes in the histogram and
texture details of the frequency spectrum. PCA visualizations underscore the
CUT substantially enhances class separability in the feature space. Moreover,
Grad-CAM analyses provide insight into the decision-making process; mean
intensity for CUT heatmaps is 119.6, which significantly exceeds 98.8 of the
original datasets. Our proposed tool leverages mechanical ventilation periods
as a novel grading standard, providing comprehensive diagnostic support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An unsupervised method for MRI recovery: Deep image prior with
  structured sparsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Ahmad Sultan, Chong Chen, Yingmin Liu, Katarzyna Gil, Karolina Zareba, Rizwan Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: To propose and validate an unsupervised MRI reconstruction method
that does not require fully sampled k-space data. Materials and Methods: The
proposed method, deep image prior with structured sparsity (DISCUS), extends
the deep image prior (DIP) by introducing group sparsity to frame-specific code
vectors, enabling the discovery of a low-dimensional manifold for capturing
temporal variations. \discus was validated using four studies: (I) simulation
of a dynamic Shepp-Logan phantom to demonstrate its manifold discovery
capabilities, (II) comparison with compressed sensing and DIP-based methods
using simulated single-shot late gadolinium enhancement (LGE) image series from
six distinct digital cardiac phantoms in terms of normalized mean square error
(NMSE) and structural similarity index measure (SSIM), (III) evaluation on
retrospectively undersampled single-shot LGE data from eight patients, and (IV)
evaluation on prospectively undersampled single-shot LGE data from eight
patients, assessed via blind scoring from two expert readers. Results: DISCUS
outperformed competing methods, demonstrating superior reconstruction quality
in terms of NMSE and SSIM (Studies I--III) and expert reader scoring (Study
IV). Discussion: An unsupervised image reconstruction method is presented and
validated on simulated and measured data. These developments can benefit
applications where acquiring fully sampled data is challenging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Magn Reson Mater Phy (2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S2-Track: A Simple yet Strong Approach for End-to-End 3D Multi-Object
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02147v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02147v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Tang, Lijun Zhou, Pengkun Hao, Zihang He, Kalok Ho, Shuo Gu, Zhihui Hao, Haiyang Sun, Kun Zhan, Peng Jia, XianPeng Lang, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D multiple object tracking (MOT) plays a crucial role in autonomous driving
perception. Recent end-to-end query-based trackers simultaneously detect and
track objects, which have shown promising potential for the 3D MOT task.
However, existing methods are still in the early stages of development and lack
systematic improvements, failing to track objects in certain complex scenarios,
like occlusions and the small size of target object's situations. In this
paper, we first summarize the current end-to-end 3D MOT framework by
decomposing it into three constituent parts: query initialization, query
propagation, and query matching. Then we propose corresponding improvements,
which lead to a strong yet simple tracker: S2-Track. Specifically, for query
initialization, we present 2D-Prompted Query Initialization, which leverages
predicted 2D object and depth information to prompt an initial estimate of the
object's 3D location. For query propagation, we introduce an Uncertainty-aware
Probabilistic Decoder to capture the uncertainty of complex environment in
object prediction with probabilistic attention. For query matching, we propose
a Hierarchical Query Denoising strategy to enhance training robustness and
convergence. As a result, our S2-Track achieves state-of-the-art performance on
nuScenes benchmark, i.e., 66.3% AMOTA on test split, surpassing the previous
best end-to-end solution by a significant margin of 8.9% AMOTA. We achieve 1st
place on the nuScenes tracking task leaderboard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DINO-X: A Unified Vision Model for Open-World Object Detection and
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14347v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14347v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, Xingyu Chen, Zhuheng Song, Yuhong Zhang, Hongjie Huang, Han Gao, Shilong Liu, Hao Zhang, Feng Li, Kent Yu, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce DINO-X, which is a unified object-centric vision
model developed by IDEA Research with the best open-world object detection
performance to date. DINO-X employs the same Transformer-based encoder-decoder
architecture as Grounding DINO 1.5 to pursue an object-level representation for
open-world object understanding. To make long-tailed object detection easy,
DINO-X extends its input options to support text prompt, visual prompt, and
customized prompt. With such flexible prompt options, we develop a universal
object prompt to support prompt-free open-world detection, making it possible
to detect anything in an image without requiring users to provide any prompt.
To enhance the model's core grounding capability, we have constructed a
large-scale dataset with over 100 million high-quality grounding samples,
referred to as Grounding-100M, for advancing the model's open-vocabulary
detection performance. Pre-training on such a large-scale grounding dataset
leads to a foundational object-level representation, which enables DINO-X to
integrate multiple perception heads to simultaneously support multiple object
perception and understanding tasks, including detection, segmentation, pose
estimation, object captioning, object-based QA, etc. Experimental results
demonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro
model achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and
LVIS-val zero-shot object detection benchmarks, respectively. Notably, it
scores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val
benchmarks, improving the previous SOTA performance by 5.8 AP and 5.0 AP. Such
a result underscores its significantly improved capacity for recognizing
long-tailed objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Examining the Source of Defects from a Mechanical Perspective for 3D
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhe Liang, Aoran Wang, Jie Zhou, Xin Jin, Can Gao, Jinbao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore a novel approach to 3D anomaly detection (AD) that
goes beyond merely identifying anomalies based on structural characteristics.
Our primary perspective is that most anomalies arise from unpredictable
defective forces originating from both internal and external sources. To
address these anomalies, we seek out opposing forces that can help correct
them. Therefore, we introduce the Mechanics Complementary Model-based Framework
for the 3D-AD task (MC4AD), which generates internal and external corrective
forces for each point. We first propose a Diverse Anomaly-Generation (DA-Gen)
module designed to simulate various types of anomalies. Next, we present the
Corrective Force Prediction Network (CFP-Net), which uses complementary
representations for point-level analysis to simulate the different
contributions from internal and external corrective forces. To ensure the
corrective forces are constrained effectively, we have developed a combined
loss function that includes a new symmetric loss and an overall loss. Notably,
we implement a Hierarchical Quality Control (HQC) strategy based on a three-way
decision process and contribute a dataset titled Anomaly-IntraVariance, which
incorporates intraclass variance to evaluate our model. As a result, the
proposed MC4AD has been proven effective through theory and experimentation.
The experimental results demonstrate that our approach yields nine
state-of-the-art performances, achieving optimal results with minimal
parameters and the fastest inference speed across five existing datasets, in
addition to the proposed Anomaly-IntraVariance dataset. The source is available
at https://github.com/hzzzzzhappy/MC4AD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PEP-GS: Perceptually-Enhanced Precise Structured 3D Gaussians for
  View-Adaptive Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05731v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05731v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junxi Jin, Xiulai Li, Haiping Huang, Lianjun Liu, Yujie Sun, Logan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, 3D Gaussian Splatting (3D-GS) has achieved significant success in
real-time, high-quality 3D scene rendering. However, it faces several
challenges, including Gaussian redundancy, limited ability to capture
view-dependent effects, and difficulties in handling complex lighting and
specular reflections. Additionally, methods that use spherical harmonics for
color representation often struggle to effectively capture anisotropic
components, especially when modeling view-dependent colors under complex
lighting conditions, leading to insufficient contrast and unnatural color
saturation. To address these limitations, we introduce PEP-GS, a
perceptually-enhanced framework that dynamically predicts Gaussian attributes,
including opacity, color, and covariance. We replace traditional spherical
harmonics with a Hierarchical Granular-Structural Attention mechanism, which
enables more accurate modeling of complex view-dependent color effects. By
employing a stable and interpretable framework for opacity and covariance
estimation, PEP-GS avoids the removal of essential Gaussians prematurely,
ensuring a more accurate scene representation. Furthermore, perceptual
optimization is applied to the final rendered images, enhancing perceptual
consistency across different views and ensuring high-quality renderings with
improved texture fidelity and fine-scale detail preservation. Experimental
results demonstrate that PEP-GS outperforms state-of-the-art methods,
particularly in challenging scenarios involving view-dependent effects and
fine-scale details.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage
  Estimation in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16147v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16147v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannik Elsäßer, Laura Weihl, Veronika Cheplygina, Lisbeth Tangaa Nielsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seagrass meadows play a crucial role in marine ecosystems, providing benefits
such as carbon sequestration, water quality improvement, and habitat provision.
Monitoring the distribution and abundance of seagrass is essential for
environmental impact assessments and conservation efforts. However, the current
manual methods of analyzing underwater video data to assess seagrass coverage
are time-consuming and subjective. This work explores the use of deep learning
models to automate the process of seagrass detection and coverage estimation
from underwater video data. We create a new dataset of over 8,300 annotated
underwater images, and subsequently evaluate several deep learning
architectures, including ResNet, InceptionNetV3, DenseNet, and Vision
Transformer for the task of binary classification on the presence and absence
of seagrass by transfer learning. The results demonstrate that deep learning
models, particularly Vision Transformers, can achieve high performance in
predicting eelgrass presence, with AUROC scores exceeding 0.95 on the final
test dataset. The application of underwater image enhancement further improved
the models' prediction capabilities. Furthermore, we introduce a novel approach
for estimating seagrass coverage from video data, showing promising preliminary
results that align with expert manual labels, and indicating potential for
consistent and scalable monitoring. The proposed methodology allows for the
efficient processing of large volumes of video data, enabling the acquisition
of much more detailed information on seagrass distributions in comparison to
current manual methods. This information is crucial for environmental impact
assessments and monitoring programs, as seagrasses are important indicators of
coastal ecosystem health. This project demonstrates the value that deep
learning can bring to the field of marine ecology and environmental monitoring.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TactileNet: Bridging the Accessibility Gap with AI-Generated Tactile
  Graphics for Individuals with Vision Impairment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adnan Khan, Alireza Choubineh, Mai A. Shaaban, Abbas Akkasi, Majid Komeili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile graphics are essential for providing access to visual information for
the 43 million people globally living with vision loss. Traditional methods for
creating these graphics are labor-intensive and cannot meet growing demand. We
introduce TactileNet, the first comprehensive dataset and AI-driven framework
for generating embossing-ready 2D tactile templates using text-to-image Stable
Diffusion (SD) models. By integrating Low-Rank Adaptation (LoRA) and
DreamBooth, our method fine-tunes SD models to produce high-fidelity,
guideline-compliant graphics while reducing computational costs. Quantitative
evaluations with tactile experts show 92.86% adherence to accessibility
standards. Structural fidelity analysis revealed near-human design similarity,
with an SSIM of 0.538 between generated graphics and expert-designed tactile
images. Notably, our method preserves object silhouettes better than human
designs (SSIM = 0.259 vs. 0.215 for binary masks), addressing a key limitation
of manual tactile abstraction. The framework scales to 32,000 images (7,050
high-quality) across 66 classes, with prompt editing enabling customizable
outputs (e.g., adding or removing details). By automating the 2D template
generation step-compatible with standard embossing workflows-TactileNet
accelerates production while preserving design flexibility. This work
demonstrates how AI can augment (not replace) human expertise to bridge the
accessibility gap in education and beyond. Code, data, and models will be
publicly released to foster further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at
  Intermediate Resolution with Structure-Aware Multimodal U-Nets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenwei Zhang, Khanh Dao Duc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at
intermediate resolution (4-8 {\AA}) is crucial in protein structure
determination. Recent advances in deep learning have led to the development of
automated approaches for enhancing experimental cryo-EM density maps. Yet,
these methods are not optimized for intermediate-resolution maps and rely on
map density features alone. To address this, we propose CryoSAMU, a novel
method designed to enhance 3D cryo-EM density maps of protein structures using
structure-aware multimodal U-Nets and trained on curated
intermediate-resolution density maps. We comprehensively evaluate CryoSAMU
across various metrics and demonstrate its competitive performance compared to
state-of-the-art methods. Notably, CryoSAMU achieves significantly faster
processing speed, showing promise for future practical applications. Our code
is available at https://github.com/chenwei-zhang/CryoSAMU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 main figures, 2 supplementary figures, 3 main tables, 4
  supplementary tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Scalable IoT Deployment for Visual Anomaly Detection via
  Efficient Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arianna Stropeni, Francesco Borsatti, Manuel Barusco, Davide Dalle Pezze, Marco Fabris, Gian Antonio Susto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Anomaly Detection (VAD) is a key task in industrial settings, where
minimizing operational costs is essential. Deploying deep learning models
within Internet of Things (IoT) environments introduces specific challenges due
to limited computational power and bandwidth of edge devices. This study
investigates how to perform VAD effectively under such constraints by
leveraging compact, efficient processing strategies. We evaluate several data
compression techniques, examining the tradeoff between system latency and
detection accuracy. Experiments on the MVTec AD benchmark demonstrate that
significant compression can be achieved with minimal loss in anomaly detection
performance compared to uncompressed data. Current results show up to 80%
reduction in end-to-end inference time, including edge processing,
transmission, and server computation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Teaching Humans Subtle Differences with DIFFusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mia Chiquier, Orr Avrech, Yossi Gandelsman, Berthy Feng, Katherine Bouman, Carl Vondrick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific expertise often requires recognizing subtle visual differences
that remain challenging to articulate even for domain experts. We present a
system that leverages generative models to automatically discover and visualize
minimal discriminative features between categories while preserving instance
identity. Our method generates counterfactual visualizations with subtle,
targeted transformations between classes, performing well even in domains where
data is sparse, examples are unpaired, and category boundaries resist verbal
description. Experiments across six domains, including black hole simulations,
butterfly taxonomy, and medical imaging, demonstrate accurate transitions with
limited training data, highlighting both established discriminative features
and novel subtle distinctions that measurably improved category
differentiation. User studies confirm our generated counterfactuals
significantly outperform traditional approaches in teaching humans to correctly
differentiate between fine-grained classes, showing the potential of generative
models to advance visual learning and scientific research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Highly Efficient 3D Human Pose Tracking from Events with Spiking
  Spatiotemporal <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09681v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09681v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Zou, Yuxuan Mu, Wei Ji, Zi-An Wang, Xinxin Zuo, Sen Wang, Weixin Si, Li Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event camera, as an asynchronous vision sensor capturing scene dynamics,
presents new opportunities for highly efficient 3D human pose tracking.
Existing approaches typically adopt modern-day Artificial Neural Networks
(ANNs), such as CNNs or Transformer, where sparse events are converted into
dense images or paired with additional gray-scale images as input. Such
practices, however, ignore the inherent sparsity of events, resulting in
redundant computations, increased energy consumption, and potentially degraded
performance. Motivated by these observations, we introduce the first sparse
Spiking Neural Networks (SNNs) framework for 3D human pose tracking based
solely on events. Our approach eliminates the need to convert sparse data to
dense formats or incorporate additional images, thereby fully exploiting the
innate sparsity of input events. Central to our framework is a novel Spiking
Spatiotemporal Transformer, which enables bi-directional spatiotemporal fusion
of spike pose features and provides a guaranteed similarity measurement between
binary spike features in spiking attention. Moreover, we have constructed a
large-scale synthetic dataset, SynEventHPD, that features a broad and diverse
set of 3D human motions, as well as much longer hours of event streams.
Empirical experiments demonstrate the superiority of our approach over existing
state-of-the-art (SOTA) ANN-based methods, requiring only 19.1% FLOPs and 3.6%
energy cost. Furthermore, our approach outperforms existing SNN-based
benchmarks in this task, highlighting the effectiveness of our proposed SNN
framework. The dataset will be released upon acceptance, and code can be found
at https://github.com/JimmyZou/HumanPoseTracking_SNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TCSVT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WildFireCan-MMD: A Multimodal <span class="highlight-title">Dataset</span> for Classification of
  User-Generated Content During Wildfires in Canada 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Braeden Sherritt, Isar Nejadgholi, Marzieh Amini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid information access is vital during wildfires, yet traditional data
sources are slow and costly. Social media offers real-time updates, but
extracting relevant insights remains a challenge. We present WildFireCan-MMD, a
new multimodal dataset of X posts from recent Canadian wildfires, annotated
across twelve key themes. Evaluating both vision-language models and
custom-trained classifiers, we show that while zero-shot prompting offers quick
deployment, even simple trained models outperform them when labelled data is
available. Our best-performing transformer-based fine-tuned model reaches 83%
f-score, outperforming gpt4 by 23%. As a use case, we demonstrate how this
model can be used to uncover trends during wildfires. Our findings highlight
the enduring importance of tailored datasets and task-specific training.
Importantly, such datasets should be localized, as disaster response
requirements vary across regions and contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pose Priors from Language Models <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjay Subramanian, Evonne Ng, Lea Müller, Dan Klein, Shiry Ginosar, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language is often used to describe physical interaction, yet most 3D human
pose estimation methods overlook this rich source of information. We bridge
this gap by leveraging large multimodal models (LMMs) as priors for
reconstructing contact poses, offering a scalable alternative to traditional
methods that rely on human annotations or motion capture data. Our approach
extracts contact-relevant descriptors from an LMM and translates them into
tractable losses to constrain 3D human pose optimization. Despite its
simplicity, our method produces compelling reconstructions for both two-person
interactions and self-contact scenarios, accurately capturing the semantics of
physical and social interactions. Our results demonstrate that LMMs can serve
as powerful tools for contact prediction and pose estimation, offering an
alternative to costly manual human annotations or motion capture data. Our code
is publicly available at https://prosepose.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Student Behavioral Engagement using Histogram of Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Abdelkawy, Aly Farag, Islam Alkabbany, Asem Ali, Chris Foreman, Thomas Tretter, Nicholas Hindy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel technique for measuring behavioral
engagement through students' actions recognition. The proposed approach
recognizes student actions then predicts the student behavioral engagement
level. For student action recognition, we use human skeletons to model student
postures and upper body movements. To learn the dynamics of student upper body,
a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actions
within every 2minute video segment then these actions are used to build a
histogram of actions which encodes the student actions and their frequencies.
This histogram is utilized as an input to SVM classifier to classify whether
the student is engaged or disengaged. To evaluate the proposed framework, we
build a dataset consisting of 1414 2-minute video segments annotated with 13
actions and 112 video segments annotated with two engagement levels.
Experimental results indicate that student actions can be recognized with top 1
accuracy 83.63% and the proposed framework can capture the average engagement
of the class.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating the Diameter at Breast Height of Trees in a Forest With a
  Single 360 Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siming He, Zachary Osman, Fernando Cladera, Dexter Ong, Nitant Rai, Patrick Corey Green, Vijay Kumar, Pratik Chaudhari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forest inventories rely on accurate measurements of the diameter at breast
height (DBH) for ecological monitoring, resource management, and carbon
accounting. While LiDAR-based techniques can achieve centimeter-level
precision, they are cost-prohibitive and operationally complex. We present a
low-cost alternative that only needs a consumer-grade 360 video camera. Our
semi-automated pipeline comprises of (i) a dense point cloud reconstruction
using Structure from Motion (SfM) photogrammetry software called Agisoft
Metashape, (ii) semantic trunk segmentation by projecting Grounded Segment
Anything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based
technique to estimate cross section shape and DBH. We introduce an interactive
visualization tool for inspecting segmented trees and their estimated DBH. On
61 acquisitions of 43 trees under a variety of conditions, our method attains
median absolute relative errors of 5-9% with respect to "ground-truth" manual
measurements. This is only 2-4% higher than LiDAR-based estimates, while
employing a single 360 camera that costs orders of magnitude less, requires
minimal setup, and is widely available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Fine-Grained Control via Aggregation of Multiple Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01262v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01262v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Conghan Yue, Zhengwei Peng, Shiyan Du, Zhi Ji, Chuangjian Cai, Le Wan, Dongyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While many diffusion models perform well when controlling for particular
aspect among style, character, and interaction, they struggle with fine-grained
control due to dataset limitations and intricate model architecture design.
This paper first introduces a novel training-free algorithm in fine-grained
generation, Aggregation of Multiple Diffusion Models (AMDM), which integrates
features from multiple diffusion models into a specified model to activate
specific features and enable fine-grained control. Experimental results
demonstrate that AMDM significantly improves fine-grained control without
training, validating its effectiveness. Additionally, it reveals that diffusion
models initially focus on features such as position, attributes, and style,
with later stages improving generation quality and consistency. AMDM offers a
new perspective for tackling the challenges of fine-grained conditional control
generation in diffusion models: We can fully utilize existing or develop new
conditional diffusion models that control specific aspects, and then aggregate
them using AMDM algorithm. This eliminates the need for constructing complex
datasets, designing intricate model architectures, and incurring high training
costs. Code is available at: https://github.com/Hammour-steak/AMDM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniCAD: Efficient and Extendable Architecture for Multi-Task
  Computer-Aided Diagnosis System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitao Zhu, Yuan Yin, Zhenrong Shen, Zihao Zhao, Haiyu Song, Sheng Wang, Dinggang Shen, Qian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing complexity and scale of visual model pre-training have made
developing and deploying multi-task computer-aided diagnosis (CAD) systems
increasingly challenging and resource-intensive. Furthermore, the medical
imaging community lacks an open-source CAD platform to enable the rapid
creation of efficient and extendable diagnostic models. To address these
issues, we propose UniCAD, a unified architecture that leverages the robust
capabilities of pre-trained vision foundation models to seamlessly handle both
2D and 3D medical images while requiring only minimal task-specific parameters.
UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation
strategy is employed to adapt a pre-trained visual model to the medical image
domain, achieving performance on par with fully fine-tuned counterparts while
introducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular
architecture that combines a frozen foundation model with multiple
plug-and-play experts, enabling diverse tasks and seamless functionality
expansion. Building on this unified CAD architecture, we establish an
open-source platform where researchers can share and access lightweight CAD
experts, fostering a more equitable and efficient research ecosystem.
Comprehensive experiments across 12 diverse medical datasets demonstrate that
UniCAD consistently outperforms existing methods in both accuracy and
deployment efficiency. The source code and project page are available at
https://mii-laboratory.github.io/UniCAD/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Action <span class="highlight-title">Pretrain</span>ing from Videos <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, Lars Liden, Kimin Lee, Jianfeng Gao, Luke Zettlemoyer, Dieter Fox, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Latent Action Pretraining for general Action models (LAPA), an
unsupervised method for pretraining Vision-Language-Action (VLA) models without
ground-truth robot action labels. Existing Vision-Language-Action models
require action labels typically collected by human teleoperators during
pretraining, which significantly limits possible data sources and scale. In
this work, we propose a method to learn from internet-scale videos that do not
have robot action labels. We first train an action quantization model
leveraging VQ-VAE-based objective to learn discrete latent actions between
image frames, then pretrain a latent VLA model to predict these latent actions
from observations and task descriptions, and finally finetune the VLA on
small-scale robot manipulation data to map from latent to robot actions.
Experimental results demonstrate that our method significantly outperforms
existing techniques that train robot manipulation policies from large-scale
videos. Furthermore, it outperforms the state-of-the-art VLA model trained with
robotic action labels on real-world manipulation tasks that require language
conditioning, generalization to unseen objects, and semantic generalization to
unseen instructions. Training only on human manipulation videos also shows
positive transfer, opening up the potential for leveraging web-scale data for
robotics foundation model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Website: https://latentactionpretraining.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Quantum Convolutional Neural Networks for Image
  Classification: Overcoming Hardware Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Röseler, Oliver Schaudt, Helmut Berg, Christian Bauckhage, Matthias Koch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While classical convolutional neural networks (CNNs) have revolutionized
image classification, the emergence of quantum computing presents new
opportunities for enhancing neural network architectures. Quantum CNNs (QCNNs)
leverage quantum mechanical properties and hold potential to outperform
classical approaches. However, their implementation on current noisy
intermediate-scale quantum (NISQ) devices remains challenging due to hardware
limitations. In our research, we address this challenge by introducing an
encoding scheme that significantly reduces the input dimensionality. We
demonstrate that a primitive QCNN architecture with 49 qubits is sufficient to
directly process $28\times 28$ pixel MNIST images, eliminating the need for
classical dimensionality reduction pre-processing. Additionally, we propose an
automated framework based on expressibility, entanglement, and complexity
characteristics to identify the building blocks of QCNNs, parameterized quantum
circuits (PQCs). Our approach demonstrates advantages in accuracy and
convergence speed with a similar parameter count compared to both hybrid QCNNs
and classical CNNs. We validated our experiments on IBM's Heron r2 quantum
processor, achieving $96.08\%$ classification accuracy, surpassing the
$71.74\%$ benchmark of traditional approaches under identical training
conditions. These results represent one of the first implementations of image
classifications on real quantum hardware and validate the potential of quantum
computing in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative <span class="highlight-title">Pre-train</span>ed Autoregressive Diffusion <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhang, Jiacheng Jiang, Guoqing Ma, Zhiying Lu, Haoyang Huang, Jianlong Yuan, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present GPDiT, a Generative Pre-trained Autoregressive
Diffusion Transformer that unifies the strengths of diffusion and
autoregressive modeling for long-range video synthesis, within a continuous
latent space. Instead of predicting discrete tokens, GPDiT autoregressively
predicts future latent frames using a diffusion loss, enabling natural modeling
of motion dynamics and semantic consistency across frames. This continuous
autoregressive framework not only enhances generation quality but also endows
the model with representation capabilities. Additionally, we introduce a
lightweight causal attention variant and a parameter-free rotation-based
time-conditioning mechanism, improving both the training and inference
efficiency. Extensive experiments demonstrate that GPDiT achieves strong
performance in video generation quality, video representation ability, and
few-shot learning tasks, highlighting its potential as an effective framework
for video modeling in continuous space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CreativeSynth: Cross-Art-Attention for Artistic Image Synthesis with
  Multimodal Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14066v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14066v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nisha Huang, Weiming Dong, Yuxin Zhang, Fan Tang, Ronghui Li, Chongyang Ma, Xiu Li, Tong-Yee Lee, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although remarkable progress has been made in image style transfer, style is
just one of the components of artistic paintings. Directly transferring
extracted style features to natural images often results in outputs with
obvious synthetic traces. This is because key painting attributes including
layout, perspective, shape, and semantics often cannot be conveyed and
expressed through style transfer. Large-scale pretrained text-to-image
generation models have demonstrated their capability to synthesize a vast
amount of high-quality images. However, even with extensive textual
descriptions, it is challenging to fully express the unique visual properties
and details of paintings. Moreover, generic models often disrupt the overall
artistic effect when modifying specific areas, making it more complicated to
achieve a unified aesthetic in artworks. Our main novel idea is to integrate
multimodal semantic information as a synthesis guide into artworks, rather than
transferring style to the real world. We also aim to reduce the disruption to
the harmony of artworks while simplifying the guidance conditions.
Specifically, we propose an innovative multi-task unified framework called
CreativeSynth, based on the diffusion model with the ability to coordinate
multimodal inputs. CreativeSynth combines multimodal features with customized
attention mechanisms to seamlessly integrate real-world semantic content into
the art domain through Cross-Art-Attention for aesthetic maintenance and
semantic fusion. We demonstrate the results of our method across a wide range
of different art categories, proving that CreativeSynth bridges the gap between
generative models and artistic expression. Code and results are available at
https://github.com/haha-lisa/CreativeSynth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Charm: The Missing Piece in ViT fine-tuning for Image Aesthetic
  Assessment <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.02522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.02522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Behrad, Tinne Tuytelaars, Johan Wagemans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capacity of Vision transformers (ViTs) to handle variable-sized inputs is
often constrained by computational complexity and batch processing limitations.
Consequently, ViTs are typically trained on small, fixed-size images obtained
through downscaling or cropping. While reducing computational burden, these
methods result in significant information loss, negatively affecting tasks like
image aesthetic assessment. We introduce Charm, a novel tokenization approach
that preserves Composition, High-resolution, Aspect Ratio, and Multi-scale
information simultaneously. Charm prioritizes high-resolution details in
specific regions while downscaling others, enabling shorter fixed-size input
sequences for ViTs while incorporating essential information. Charm is designed
to be compatible with pre-trained ViTs and their learned positional embeddings.
By providing multiscale input and introducing variety to input tokens, Charm
improves ViT performance and generalizability for image aesthetic assessment.
We avoid cropping or changing the aspect ratio to further preserve information.
Extensive experiments demonstrate significant performance improvements on
various image aesthetic and quality assessment datasets (up to 8.1 %) using a
lightweight ViT backbone. Code and pre-trained models are available at
https://github.com/FBehrad/Charm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IntrinsicEdit: Precise generative image manipulation in intrinsic space <span class="chip">SIGGRAPH 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linjie Lyu, Valentin Deschaintre, Yannick Hold-Geoffroy, Miloš Hašan, Jae Shin Yoon, Thomas Leimkühler, Christian Theobalt, Iliyan Georgiev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative diffusion models have advanced image editing with high-quality
results and intuitive interfaces such as prompts and semantic drawing. However,
these interfaces lack precise control, and the associated methods typically
specialize on a single editing task. We introduce a versatile, generative
workflow that operates in an intrinsic-image latent space, enabling semantic,
local manipulation with pixel precision for a range of editing operations.
Building atop the RGB-X diffusion framework, we address key challenges of
identity preservation and intrinsic-channel entanglement. By incorporating
exact diffusion inversion and disentangled channel manipulation, we enable
precise, efficient editing with automatic resolution of global illumination
effects -- all without additional data collection or model fine-tuning. We
demonstrate state-of-the-art performance across a variety of tasks on complex
images, including color and texture adjustments, object insertion and removal,
global relighting, and their combinations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH 2025 Journal track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07547v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07547v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungho Lee, Dogyoon Lee, Minhyeok Lee, Donghyung Kim, Sangyoun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields (NeRF) has attracted considerable attention for their
exceptional ability in synthesizing novel views with high fidelity. However,
the presence of motion blur, resulting from slight camera movements during
extended shutter exposures, poses a significant challenge, potentially
compromising the quality of the reconstructed 3D scenes. To effectively handle
this issue, we propose sequential motion understanding radiance fields (SMURF),
a novel approach that models continuous camera motion and leverages the
explicit volumetric representation method for robustness to motion-blurred
input images. The core idea of the SMURF is continuous motion blurring kernel
(CMBK), a module designed to model a continuous camera movements for processing
blurry inputs. Our model is evaluated against benchmark datasets and
demonstrates state-of-the-art performance both quantitatively and
qualitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPRW 2025, Neural Fields Beyond Conventional Cameras, Project Page:
  https://jho-yonsei.github.io/SMURF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Illegal Waste Detection in Remote Sensing Images: A Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06607v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06607v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Gibellini, Piero Fraternali, Giacomo Boracchi, Luca Morandini, Thomas Martinoli, Andrea Diecidue, Simona Malegori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environmental crime is the third largest criminal activity worldwide, with
significant revenues coming from illegal management of solid waste. Thanks to
the increasing availability and the decreasing cost of Very High Resolution
Remote Sensing (VHR RS) images, the fight against environmental crime can
nowadays rely on modern image-analysis tools to support photo-interpretation
for scanning vast territories in search of illegal waste disposal sites. This
paper illustrates a semi-automatic waste detection pipeline, developed in
collaboration with a regional environmental protection agency, for detecting
candidate illegal dumping sites in VHR RS images. To optimize the effectiveness
of the waste detector, extensive experiments evaluate such design choices as
the network architecture, the ground resolution and geographic span of the
input images, as well as the pretraining procedures. The best model attains
remarkable performance, achieving 92.02% F1-Score and 94.56% Accuracy. A
generalization study assesses the performance variation when the detector
processes images from a territory substantially different from the one used
during training, incurring only a moderate performance loss, i.e., 6.5%
decrease in the F1-Score. Finally, an exercise in which photo interpreters
compare the territory scanning effort with and without the support of the waste
detector assesses the concrete benefit of using a computer-aided image analysis
tool in a professional environment protection agency. Results show that a
reduction up to 30% of the time spent for waste site detection can be attained.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Multi-Modal Information to Enhance <span class="highlight-title">Dataset</span> Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08605v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08605v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Li, Hadrien Reynaud, Bernhard Kainz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation aims to create a compact and highly representative
synthetic dataset that preserves the knowledge of a larger real dataset. While
existing methods primarily focus on optimizing visual representations,
incorporating additional modalities and refining object-level information can
significantly improve the quality of distilled datasets. In this work, we
introduce two key enhancements to dataset distillation: caption-guided
supervision and object-centric masking. To integrate textual information, we
propose two strategies for leveraging caption features: the feature
concatenation, where caption embeddings are fused with visual features at the
classification stage, and caption matching, which introduces a caption-based
alignment loss during training to ensure semantic coherence between real and
synthetic data. Additionally, we apply segmentation masks to isolate target
objects and remove background distractions, introducing two loss functions
designed for object-centric learning: masked feature alignment loss and masked
gradient matching loss. Comprehensive evaluations demonstrate that integrating
caption-based guidance and object-centric masking enhances dataset
distillation, leading to synthetic datasets that achieve superior performance
on downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Laws for Black box Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan Liu, Huanran Chen, Yichi Zhang, Yinpeng Dong, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial examples usually exhibit good cross-model transferability,
enabling attacks on black-box models with limited information about their
architectures and parameters, which are highly threatening in commercial
black-box scenarios. Model ensembling is an effective strategy to improve the
transferability of adversarial examples by attacking multiple surrogate models.
However, since prior studies usually adopt few models in the ensemble, there
remains an open question of whether scaling the number of models can further
improve black-box attacks. Inspired by the scaling law of large foundation
models, we investigate the scaling laws of black-box adversarial attacks in
this work. Through theoretical analysis and empirical evaluations, we conclude
with clear scaling laws that using more surrogate models enhances adversarial
transferability. Comprehensive experiments verify the claims on standard image
classifiers, diverse defended models and multimodal large language models using
various adversarial attack methods. Specifically, by scaling law, we achieve
90%+ transfer attack success rate on even proprietary models like GPT-4o.
Further visualization indicates that there is also a scaling law on the
interpretability and semantics of adversarial perturbations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from
  Defocused Images <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungho Lee, Suhwan Cho, Taeoh Kim, Ho-Deok Jang, Minhyeok Lee, Geonho Cha, Dongyoon Wee, Dogyoon Lee, Sangyoun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has attracted significant attention for its
high-quality novel view rendering, inspiring research to address real-world
challenges. While conventional methods depend on sharp images for accurate
scene reconstruction, real-world scenarios are often affected by defocus blur
due to finite depth of field, making it essential to account for realistic 3D
scene representation. In this study, we propose CoCoGaussian, a Circle of
Confusion-aware Gaussian Splatting that enables precise 3D scene representation
using only defocused images. CoCoGaussian addresses the challenge of defocus
blur by modeling the Circle of Confusion (CoC) through a physically grounded
approach based on the principles of photographic defocus. Exploiting 3D
Gaussians, we compute the CoC diameter from depth and learnable aperture
information, generating multiple Gaussians to precisely capture the CoC shape.
Furthermore, we introduce a learnable scaling factor to enhance robustness and
provide more flexibility in handling unreliable depth in scenes with reflective
or refractive surfaces. Experiments on both synthetic and real-world datasets
demonstrate that CoCoGaussian achieves state-of-the-art performance across
multiple benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025, Project Page: https://Jho-Yonsei.github.io/CoCoGaussian/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Single View Garment Reconstruction Using Diffusion Mapping Via Pattern
  Coordinates <span class="chip">SIGGRAPH 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ren Li, Cong Cao, Corentin Dumery, Yingxuan You, Hao Li, Pascal Fua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing 3D clothed humans from images is fundamental to applications
like virtual try-on, avatar creation, and mixed reality. While recent advances
have enhanced human body recovery, accurate reconstruction of garment geometry
-- especially for loose-fitting clothing -- remains an open challenge. We
present a novel method for high-fidelity 3D garment reconstruction from single
images that bridges 2D and 3D representations. Our approach combines Implicit
Sewing Patterns (ISP) with a generative diffusion model to learn rich garment
shape priors in a 2D UV space. A key innovation is our mapping model that
establishes correspondences between 2D image pixels, UV pattern coordinates,
and 3D geometry, enabling joint optimization of both 3D garment meshes and the
corresponding 2D patterns by aligning learned priors with image observations.
Despite training exclusively on synthetically simulated cloth data, our method
generalizes effectively to real-world images, outperforming existing approaches
on both tight- and loose-fitting garments. The reconstructed garments maintain
physical plausibility while capturing fine geometric details, enabling
downstream applications including garment retargeting and texture manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-R1: Reinforcing Video Reasoning in MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21776v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21776v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, Xiangyu Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by DeepSeek-R1's success in eliciting reasoning abilities through
rule-based reinforcement learning (RL), we introduce Video-R1 as the first
attempt to systematically explore the R1 paradigm for incentivizing video
reasoning within multimodal large language models (MLLMs). However, directly
applying RL training with the GRPO algorithm to video reasoning presents two
primary challenges: (i) a lack of temporal modeling for video reasoning, and
(ii) the scarcity of high-quality video-reasoning data. To address these
issues, we first propose the T-GRPO algorithm, which encourages models to
utilize temporal information in videos for reasoning. Additionally, instead of
relying solely on video data, we incorporate high-quality image-reasoning data
into the training process. We have constructed two datasets: Video-R1-CoT-165k
for SFT cold start and Video-R1-260k for RL training, both comprising image and
video data. Experimental results demonstrate that Video-R1 achieves significant
improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as
well as on general video benchmarks including MVBench and TempCompass, etc.
Notably, Video-R1-7B attains a 37.1% accuracy on video spatial reasoning
benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All
code, models, and data are released in: https://github.com/tulerfeng/Video-R1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/tulerfeng/Video-R1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoGenAV: Versatile Audio-Visual Representation Learning via
  Contrastive-Generative Synchronization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Detao Bai, Zhiheng Ma, Xihan Wei, Liefeng Bo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inherent synchronization between a speaker's lip movements, voice, and
the underlying linguistic content offers a rich source of information for
improving speech processing tasks, especially in challenging conditions where
traditional audio-only systems falter. We introduce CoGenAV, a powerful and
data-efficient model designed to learn versatile audio-visual representations
applicable across a wide range of speech and audio-visual tasks. CoGenAV is
trained by optimizing a dual objective derived from natural audio-visual
synchrony, contrastive feature alignment and generative text prediction, using
only 223 hours of labeled data from the LRS2 dataset. This
contrastive-generative synchronization strategy effectively captures
fundamental cross-modal correlations. We showcase the effectiveness and
versatility of the learned CoGenAV representations on multiple benchmarks. When
utilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these
representations contribute to achieving a state-of-the-art Word Error Rate
(WER) of 1.27. They also enable strong performance in Visual Speech Recognition
(VSR) with a WER of 20.5 on LRS2, and significantly improve performance in
noisy environments by over 70%. Furthermore, CoGenAV representations benefit
speech reconstruction tasks, boosting performance in Speech Enhancement and
Separation, and achieve competitive results in audio-visual synchronization
tasks like Active Speaker Detection (ASD). Our model will be open-sourced to
facilitate further development and collaboration within both academia and
industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19159v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19159v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Ding, Rui Sun, Yunjian Zhang, Xiu Yan, Yueqi Zhou, Kaihao Huang, Suzhong Fu, Angelica I Aviles-Rivero, Chuanlong Xie, Yao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to width-wise pruning, depth-wise pruning can significantly
accelerate inference in resource-constrained scenarios. However, treating the
entire Transformer layer as the minimum pruning unit may degrade model
performance by indiscriminately discarding the entire information of the layer.
This paper reveals the ``Patch-like'' feature relationship between layers in
large language models by analyzing the correlation of the outputs of different
layers in the reproducing kernel Hilbert space. Building on this observation,
we propose a sliding layer merging method that dynamically selects and fuses
consecutive layers from top to bottom according to a pre-defined similarity
threshold, thereby simplifying the model structure while maintaining its
performance. Extensive experiments on LLMs with various architectures and
different parameter scales show that our method outperforms existing pruning
techniques in both zero-shot inference performance and retraining recovery
quality after pruning. In particular, in the experiment with 35% pruning on the
Vicuna-7B model, our method achieved a 1.654% improvement in average
performance on zero-shot tasks compared to the existing method. Moreover, we
further reveal the potential of combining depth pruning with width pruning to
enhance the pruning effect. Our codes are available at
https://github.com/920927/SLM-a-sliding-layer-merging-method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Convolutional Neural Networks for Rice Grain Classification:
  An Explainable AI Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Junaid Asif, Hamza Khan, Rabia Tehseen, Syed Tahir Hussain Rizvi, Mujtaba Asad, Shazia Saqib, Rana Fayyaz Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rice is an essential staple food worldwide that is important in promoting
international trade, economic growth, and nutrition. Asian countries such as
China, India, Pakistan, Thailand, Vietnam, and Indonesia are notable for their
significant contribution to the cultivation and utilization of rice. These
nations are also known for cultivating different rice grains, including short
and long grains. These sizes are further classified as basmati, jasmine, kainat
saila, ipsala, arborio, etc., catering to diverse culinary preferences and
cultural traditions. For both local and international trade, inspecting and
maintaining the quality of rice grains to satisfy customers and preserve a
country's reputation is necessary. Manual quality check and classification is
quite a laborious and time-consuming process. It is also highly prone to
mistakes. Therefore, an automatic solution must be proposed for the effective
and efficient classification of different varieties of rice grains. This
research paper presents an automatic framework based on a convolutional neural
network (CNN) for classifying different varieties of rice grains. We evaluated
the proposed model based on performance metrics such as accuracy, recall,
precision, and F1-Score. The CNN model underwent rigorous training and
validation, achieving a remarkable accuracy rate and a perfect area under each
class's Receiver Operating Characteristic (ROC) curve. The confusion matrix
analysis confirmed the model's effectiveness in distinguishing between the
different rice varieties, indicating minimal misclassifications. Additionally,
the integration of explainability techniques such as LIME (Local Interpretable
Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) provided
valuable insights into the model's decision-making process, revealing how
specific features of the rice grains influenced classification outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified theory for joint covariance properties under geometric image
  transformations for spatio-temporal receptive fields according to the
  generalized Gaussian derivative model for visual receptive fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10543v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10543v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Lindeberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The influence of natural image transformations on receptive field responses
is crucial for modelling visual operations in computer vision and biological
vision. In this regard, covariance properties with respect to geometric image
transformations in the earliest layers of the visual hierarchy are essential
for expressing robust image operations, and for formulating invariant visual
operations at higher levels.
  This paper defines and proves a set of joint covariance properties for
spatio-temporal receptive fields in terms of spatio-temporal derivative
operators applied to spatio-temporally smoothed image data under compositions
of spatial scaling transformations, spatial affine transformations, Galilean
transformations and temporal scaling transformations. Specifically, the derived
relations show how the parameters of the receptive fields need to be
transformed, in order to match the output from spatio-temporal receptive fields
under composed spatio-temporal image transformations.
  For this purpose, we also fundamentally extend the notion of scale-normalized
derivatives to affine-normalized derivatives, that are computed based on
spatial smoothing with affine Gaussian kernels, and analyze the covariance
properties of the resulting affine-normalized derivatives for the affine group
as well as for important subgroups thereof.
  We conclude with a geometric analysis, showing how the derived joint
covariance properties make it possible to relate or match spatio-temporal
receptive field responses, when observing, possibly moving, local surface
patches from different views, under locally linearized perspective or
projective transformations, as well as when observing different instances of
spatio-temporal events, that may occur either faster or slower between
different views of similar spatio-temporal events.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages, 19 figures. Note: From version 4, this paper considers a
  different form of joint composition of the geometric image transformations
  than in the earlier versions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EndoMamba: An Efficient Foundation Model for Endoscopic Videos via
  Hierarchical <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Dongdong Lei, Sebastien Ourselin, Hongbin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Endoscopic video-based tasks, such as visual navigation and surgical phase
recognition, play a crucial role in minimally invasive surgeries by providing
real-time assistance. While recent video foundation models have shown promise,
their applications are hindered by (1) computational inefficiencies and (2)
suboptimal performance caused by limited data for pre-training in endoscopy. To
address these issues, we present EndoMamba, a foundation model designed for
real-time inference while learning generalized spatiotemporal representations.
First, to mitigate computational inefficiencies, we propose the EndoMamba
backbone, optimized for real-time inference. Inspired by recent advancements in
state space models, EndoMamba integrates Bidirectional Mamba blocks for spatial
modeling within individual frames and vanilla Mamba blocks for past-to-present
reasoning across the temporal domain. This design enables both strong
spatiotemporal modeling and efficient inference in online video streams.
Second, we propose a self-supervised hierarchical pre-training diagram to
enhance EndoMamba's representation learning using endoscopic videos and
incorporating general video domain knowledge. Specifically, our approach
combines masked reconstruction with auxiliary supervision, leveraging low-level
reconstruction to capture spatial-temporal structures and high-level alignment
to transfer broader knowledge from a pretrained general-video domain foundation
model. Extensive experiments on four downstream tasks--classification,
segmentation, surgical phase recognition, and localization--demonstrate that
EndoMamba outperforms existing foundation models and task-specific methods
while maintaining real-time inference speed. The source code is available at
https://github.com/TianCuteQY/EndoMamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Translating Electrocardiograms to Cardiac Magnetic Resonance Imaging
  Useful for Cardiac Assessment and Disease Screening: A Multi-Center Study AI
  for ECG to CMR Translation Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13602v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13602v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyao Ding, Ziyu Li, Yujian Hu, Youyao Xu, Chengchen Zhao, Yiheng Mao, Haitao Li, Zhikang Li, Qian Li, Jing Wang, Yue Chen, Mengjia Chen, Longbo Wang, Xuesen Chu, Weichao Pan, Ziyi Liu, Fei Wu, Hongkun Zhang, Ting Chen, Zhengxing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiovascular diseases (CVDs) are the leading cause of global mortality,
necessitating accessible and accurate diagnostic tools. While cardiac magnetic
resonance imaging (CMR) provides gold-standard insights into cardiac structure
and function, its clinical utility is limited by high cost and complexity. In
contrast, electrocardiography (ECG) is inexpensive and widely available but
lacks the granularity of CMR. We propose CardioNets, a deep learning framework
that translates 12-lead ECG signals into CMR-level functional parameters and
synthetic images, enabling scalable cardiac assessment. CardioNets integrates
cross-modal contrastive learning and generative pretraining, aligning ECG with
CMR-derived cardiac phenotypes and synthesizing high-resolution CMR images via
a masked autoregressive model. Trained on 159,819 samples from five cohorts,
including the UK Biobank (n=42,483) and MIMIC-IV-ECG (n=164,550), and
externally validated on independent clinical datasets (n=3,767), CardioNets
achieved strong performance across disease screening and phenotype estimation
tasks. In the UK Biobank, it improved cardiac phenotype regression R2 by 24.8%
and cardiomyopathy AUC by up to 39.3% over baseline models. In MIMIC, it
increased AUC for pulmonary hypertension detection by 5.6%. Generated CMR
images showed 36.6% higher SSIM and 8.7% higher PSNR than prior approaches. In
a reader study, ECG-only CardioNets achieved 13.9% higher accuracy than human
physicians using both ECG and real CMR. These results suggest that CardioNets
offers a promising, low-cost alternative to CMR for large-scale CVD screening,
particularly in resource-limited settings. Future efforts will focus on
clinical deployment and regulatory validation of ECG-based synthetic imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards user-centered interactive medical image segmentation in VR with
  an assistive AI agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07214v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07214v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Spiegler, Arash Harirpoush, Yiming Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crucial in disease analysis and surgical planning, manual segmentation of
volumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and
challenging to master, while fully automatic algorithms can benefit from user
feedback. Therefore, with the complementary power of the latest radiological AI
foundation models and virtual reality (VR)'s intuitive data interaction, we
propose SAMIRA, a novel conversational AI agent that assists users with
localizing, segmenting, and visualizing 3D medical concepts in VR. Through
speech-based interaction, the agent helps users understand radiological
features, locate clinical targets, and generate segmentation masks that can be
refined with just a few point prompts. The system also supports true-to-scale
3D visualization of segmented pathology to enhance patient-specific anatomical
understanding. Furthermore, to determine the optimal interaction paradigm under
near-far attention-switching for refining segmentation masks in an immersive,
human-in-the-loop workflow, we compare VR controller pointing, head pointing,
and eye tracking as input modes. With a user study, evaluations demonstrated a
high usability score (SUS=90.0 $\pm$ 9.0), low overall task load, as well as
strong support for the proposed VR system's guidance, training potential, and
integration of AI in radiological segmentation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cyclic 2.5D Perceptual Loss for Cross-Modal 3D Medical Image Synthesis:
  T1w MRI to Tau PET 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junho Moon, Symac Kim, Haejun Chung, Ikbeom Jang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a demand for medical image synthesis or translation to generate
synthetic images of missing modalities from available data. This need stems
from challenges such as restricted access to high-cost imaging devices,
government regulations, or failure to follow up with patients or study
participants. In medical imaging, preserving high-level semantic features is
often more critical than achieving pixel-level accuracy. Perceptual loss
functions are widely employed to train medical image synthesis or translation
models, as they quantify differences in high-level image features using a
pre-trained feature extraction network. While 3D and 2.5D perceptual losses are
used in 3D medical image synthesis, they face challenges, such as the lack of
pre-trained 3D models or difficulties in balancing loss reduction across
different planes. In this work, we focus on synthesizing 3D tau PET images from
3D T1-weighted MR images. We propose a cyclic 2.5D perceptual loss that
sequentially computes the 2D average perceptual loss for each of the axial,
coronal, and sagittal planes over epochs, with the cycle duration gradually
decreasing. Additionally, we process tau PET images using by-manufacturer
standardization to enhance the preservation of high-SUVR regions indicative of
tau pathology and mitigate SUVR variability caused by inter-manufacturer
differences. We combine the proposed loss with SSIM and MSE losses and
demonstrate its effectiveness in improving both quantitative and qualitative
performance across various generative models, including U-Net, UNETR,
SwinUNETR, CycleGAN, and Pix2Pix.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Saliency-Motion Guided Trunk-Collateral Network for Unsupervised Video
  Object Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Zheng, Wanyun Li, Songcheng He, Jianping Fan, Xiaoqiang Li, We Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent mainstream unsupervised video object segmentation (UVOS)
motion-appearance approaches use either the bi-encoder structure to separately
encode motion and appearance features, or the uni-encoder structure for joint
encoding. However, these methods fail to properly balance the motion-appearance
relationship. Consequently, even with complex fusion modules for
motion-appearance integration, the extracted suboptimal features degrade the
models' overall performance. Moreover, the quality of optical flow varies
across scenarios, making it insufficient to rely solely on optical flow to
achieve high-quality segmentation results. To address these challenges, we
propose the Saliency-Motion guided Trunk-Collateral Network (SMTC-Net), which
better balances the motion-appearance relationship and incorporates model's
intrinsic saliency information to enhance segmentation performance.
Specifically, considering that optical flow maps are derived from RGB images,
they share both commonalities and differences. Accordingly, we propose a novel
Trunk-Collateral structure for motion-appearance UVOS. The shared trunk
backbone captures the motion-appearance commonality, while the collateral
branch learns the uniqueness of motion features. Furthermore, an Intrinsic
Saliency guided Refinement Module (ISRM) is devised to efficiently leverage the
model's intrinsic saliency information to refine high-level features, and
provide pixel-level guidance for motion-appearance fusion, thereby enhancing
performance without additional input. Experimental results show that SMTC-Net
achieved state-of-the-art performance on three UVOS datasets ( 89.2% J&F on
DAVIS-16, 76% J on YouTube-Objects, 86.4% J on FBMS ) and four standard video
salient object detection (VSOD) benchmarks with the notable increase,
demonstrating its effectiveness and superiority over previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Behind Maya: Building a Multilingual Vision Language Model <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nahid Alam, Karthik Reddy Kanjula, Surya Guthikonda, Timothy Chung, Bala Krishna S Vegesna, Abhipsha Das, Anthony Susevski, Ryan Sze-Yin Chan, S M Iftekhar Uddin, Shayekh Bin Islam, Roshan Santhosh, Snegha A, Drishti Sharma, Chen Liu, Isha Chaturvedi, Genta Indra Winata, Ashvanth. S, Snehanshu Mukherjee, Alham Fikri Aji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent times, we have seen a rapid development of large Vision-Language
Models (VLMs). They have shown impressive results on academic benchmarks,
primarily in widely spoken languages but lack performance on low-resource
languages and varied cultural contexts. To address these limitations, we
introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a
multilingual image-text pretraining dataset in eight languages, based on the
LLaVA pretraining dataset; and 2) a multilingual image-text model supporting
these languages, enhancing cultural and linguistic comprehension in
vision-language tasks. Code available at https://github.com/nahidalam/maya.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at VLMs4ALL CVPR 2025 Workshop; corrected workshop name
  spelling</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Trust-Guided Approach to MR Image Reconstruction with Side Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03021v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03021v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arda Atalık, Sumit Chopra, Daniel K. Sodickson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reducing MRI scan times can improve patient care and lower healthcare costs.
Many acceleration methods are designed to reconstruct diagnostic-quality images
from sparse k-space data, via an ill-posed or ill-conditioned linear inverse
problem (LIP). To address the resulting ambiguities, it is crucial to
incorporate prior knowledge into the optimization problem, e.g., in the form of
regularization. Another form of prior knowledge less commonly used in medical
imaging is the readily available auxiliary data (a.k.a. side information)
obtained from sources other than the current acquisition. In this paper, we
present the Trust- Guided Variational Network (TGVN), an end-to-end deep
learning framework that effectively and reliably integrates side information
into LIPs. We demonstrate its effectiveness in multi-coil, multi-contrast MRI
reconstruction, where incomplete or low-SNR measurements from one contrast are
used as side information to reconstruct high-quality images of another contrast
from heavily under-sampled data. TGVN is robust across different contrasts,
anatomies, and field strengths. Compared to baselines utilizing side
information, TGVN achieves superior image quality while preserving subtle
pathological features even at challenging acceleration levels, drastically
speeding up acquisition while minimizing hallucinations. Source code and
dataset splits are available on github.com/sodicksonlab/TGVN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learned Image Compression with Dictionary-based Entropy Model <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.00496v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.00496v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingbo Lu, Leheng Zhang, Xingyu Zhou, Mu Li, Wen Li, Shuhang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned image compression methods have attracted great research interest and
exhibited superior rate-distortion performance to the best classical image
compression standards of the present. The entropy model plays a key role in
learned image compression, which estimates the probability distribution of the
latent representation for further entropy coding. Most existing methods
employed hyper-prior and auto-regressive architectures to form their entropy
models. However, they only aimed to explore the internal dependencies of latent
representation while neglecting the importance of extracting prior from
training data. In this work, we propose a novel entropy model named
Dictionary-based Cross Attention Entropy model, which introduces a learnable
dictionary to summarize the typical structures occurring in the training
dataset to enhance the entropy model. Extensive experimental results have
demonstrated that the proposed model strikes a better balance between
performance and latency, achieving state-of-the-art results on various
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OSMLoc: Single Image-Based Visual Localization in OpenStreetMap with
  Fused Geometric and Semantic Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youqi Liao, Xieyuanli Chen, Shuhao Kang, Jianping Li, Zhen Dong, Hongchao Fan, Bisheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OpenStreetMap (OSM), a rich and versatile source of volunteered geographic
information (VGI), facilitates human self-localization and scene understanding
by integrating nearby visual observations with vectorized map data. However,
the disparity in modalities and perspectives poses a major challenge for
effectively matching camera imagery with compact map representations, thereby
limiting the full potential of VGI data in real-world localization
applications.
  Inspired by the fact that the human brain relies on the fusion of geometric
and semantic understanding for spatial localization tasks, we propose the
OSMLoc in this paper. OSMLoc is a brain-inspired visual localization approach
based on first-person-view images against the OSM maps. It integrates semantic
and geometric guidance to significantly improve accuracy, robustness, and
generalization capability. First, we equip the OSMLoc with the visual
foundational model to extract powerful image features. Second, a
geometry-guided depth distribution adapter is proposed to bridge the monocular
depth estimation and camera-to-BEV transform. Thirdly, the semantic embeddings
from the OSM data are utilized as auxiliary guidance for image-to-OSM feature
matching. To validate the proposed OSMLoc, we collect a worldwide cross-area
and cross-condition (CC) benchmark for extensive evaluation. Experiments on the
MGL dataset, CC validation benchmark, and KITTI dataset have demonstrated the
superiority of our method. Code, pre-trained models, CC validation benchmark,
and additional results are available at: https://github.com/WHU-USI3DV/OSMLoc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiECVC: Gated Diversification of Bidirectional Contexts for Learned
  Video Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jiang, Junru Li, Kai Zhang, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent forward prediction-based learned video compression (LVC) methods have
achieved impressive results, even surpassing VVC reference software VTM under
the Low Delay B (LDB) configuration. In contrast, learned bidirectional video
compression (BVC) remains underexplored and still lags behind its forward-only
counterparts. This performance gap is mainly due to the limited ability to
extract diverse and accurate contexts: most existing BVCs primarily exploit
temporal motion while neglecting non-local correlations across frames.
Moreover, they lack the adaptability to dynamically suppress harmful contexts
arising from fast motion or occlusion. To tackle these challenges, we propose
BiECVC, a BVC framework that incorporates diversified local and non-local
context modeling along with adaptive context gating. For local context
enhancement, BiECVC reuses high-quality features from lower layers and aligns
them using decoded motion vectors without introducing extra motion overhead. To
model non-local dependencies efficiently, we adopt a linear attention mechanism
that balances performance and complexity. To further mitigate the impact of
inaccurate context prediction, we introduce Bidirectional Context Gating,
inspired by data-dependent decay in recent autoregressive language models, to
dynamically filter contextual information based on conditional coding results.
Extensive experiments demonstrate that BiECVC achieves state-of-the-art
performance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2
under the Random Access (RA) configuration with intra periods of 32 and 64,
respectively. To our knowledge, BiECVC is the first learned video codec to
surpass VTM 13.2 RA across all standard test datasets. Code will be available
at https://github.com/JiangWeibeta/ECVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first learned video codec that surpasses VTM 13.2 RA across all
  standard test datasets. Code will be available at
  https://github.com/JiangWeibeta/ECVC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.06512v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.06512v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Wang, Zhi-Qi Cheng, Chenhao Lin, Chao Shen, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image synthesis has progressed to the point where models can generate
visually compelling images from natural language prompts. Yet, existing methods
often fail to reconcile high-level semantic fidelity with explicit spatial
control, particularly in scenes involving multiple objects, nuanced relations,
or complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal
Alignment (HCMA) framework for grounded text-to-image generation. HCMA
integrates two alignment modules into each diffusion sampling step: a global
module that continuously aligns latent representations with textual
descriptions to ensure scene-level coherence, and a local module that employs
bounding-box layouts to anchor objects at specified locations, enabling
fine-grained spatial control. Extensive experiments on the MS-COCO 2014
validation set show that HCMA surpasses state-of-the-art baselines, achieving a
0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP
Score. These results demonstrate HCMA's effectiveness in faithfully capturing
intricate textual semantics while adhering to user-defined spatial constraints,
offering a robust solution for semantically grounded image generation. Our code
is available at https://github.com/hwang-cs-ime/HCMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs Memorize Recommendation <span class="highlight-title">Dataset</span>s? A Preliminary Study on
  MovieLens-1M 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dario Di Palma, Felice Antonio Merra, Maurizio Sfilio, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become increasingly central to
recommendation scenarios due to their remarkable natural language understanding
and generation capabilities. Although significant research has explored the use
of LLMs for various recommendation tasks, little effort has been dedicated to
verifying whether they have memorized public recommendation dataset as part of
their training data. This is undesirable because memorization reduces the
generalizability of research findings, as benchmarking on memorized datasets
does not guarantee generalization to unseen datasets. Furthermore, memorization
can amplify biases, for example, some popular items may be recommended more
frequently than others.
  In this work, we investigate whether LLMs have memorized public
recommendation datasets. Specifically, we examine two model families (GPT and
Llama) across multiple sizes, focusing on one of the most widely used dataset
in recommender systems: MovieLens-1M. First, we define dataset memorization as
the extent to which item attributes, user profiles, and user-item interactions
can be retrieved by prompting the LLMs. Second, we analyze the impact of
memorization on recommendation performance. Lastly, we examine whether
memorization varies across model families and model sizes. Our results reveal
that all models exhibit some degree of memorization of MovieLens-1M, and that
recommendation performance is related to the extent of memorization. We have
made all the code publicly available at:
https://github.com/sisinflab/LLM-MemoryInspector
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Text-to-Chart Retrieval through Training with Synthesized
  Semantic Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wu, Lutao Yan, Yizhang Zhu, Yinan Mei, Jiannan Wang, Nan Tang, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Charts are crucial for data analysis and decision-making.Text-to-chart
retrieval systems have become increasingly important for Business Intelligence
(BI), where users need to find relevant charts that match their analytical
needs. These needs can be categorized into precise queries that are
well-specified and fuzzy queries that are more exploratory -- both require
understanding the semantics and context of the charts. However, existing
text-to-chart retrieval solutions often fail to capture the semantic content
and contextual information of charts, primarily due to the lack of
comprehensive metadata (or semantic insights). To address this limitation, we
propose a training data development pipeline that automatically synthesizes
hierarchical semantic insights for charts, covering visual patterns
(visual-oriented), statistical properties (statistics-oriented), and practical
applications (task-oriented), which produces 207,498 semantic insights for
69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to
learn better representations of charts for text-to-chart retrieval. Our method
leverages rich semantic insights during the training phase to develop a model
that understands both visual and semantic aspects of charts.To evaluate
text-to-chart retrieval performance, we curate the first benchmark, CRBench,
for this task with 21,862 charts and 326 text queries from real-world BI
applications, with ground-truth labels verified by the crowd
workers.Experiments show that ChartFinder significantly outperforms existing
methods in text-to-chart retrieval tasks across various settings. For precise
queries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than
state-of-the-art models. In fuzzy query tasks, our method also demonstrates
consistent improvements, with an average increase of 5% across nearly all
metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiwei Peng, Robert Moro, Michal Gregor, Ivan Srba, Simon Ostermann, Marian Simko, Juraj Podroužek, Matúš Mesarčík, Jaroslav Kopčan, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid spread of online disinformation presents a global challenge, and
machine learning has been widely explored as a potential solution. However,
multilingual settings and low-resource languages are often neglected in this
field. To address this gap, we conducted a shared task on multilingual claim
retrieval at SemEval 2025, aimed at identifying fact-checked claims that match
newly encountered claims expressed in social media posts across different
languages. The task includes two subtracks: (1) a monolingual track, where
social posts and claims are in the same language, and (2) a crosslingual track,
where social posts and claims might be in different languages. A total of 179
participants registered for the task contributing to 52 test submissions. 23
out of 31 teams have submitted their system papers. In this paper, we report
the best-performing systems as well as the most common and the most effective
approaches across both subtracks. This shared task, along with its dataset and
participating systems, provides valuable insights into multilingual claim
retrieval and automated fact-checking, supporting future research in this
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GSPRec: Temporal-Aware Graph Spectral Filtering for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Bin Rabiah, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based recommendation systems are effective at modeling collaborative
patterns but often suffer from two limitations: overreliance on low-pass
filtering, which suppresses user-specific signals, and omission of sequential
dynamics in graph construction. We introduce GSPRec, a graph spectral model
that integrates temporal transitions through sequentially-informed graph
construction and applies frequency-aware filtering in the spectral domain.
GSPRec encodes item transitions via multi-hop diffusion to enable the use of
symmetric Laplacians for spectral processing. To capture user preferences, we
design a dual-filtering mechanism: a Gaussian bandpass filter to extract
mid-frequency, user-level patterns, and a low-pass filter to retain global
trends. Extensive experiments on four public datasets show that GSPRec
consistently outperforms baselines, with an average improvement of 6.77% in
NDCG@10. Ablation studies show the complementary benefits of both sequential
graph augmentation and bandpass filtering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedPCL-CDR: A Federated Prototype-based Contrastive Learning Framework
  for Privacy-Preserving Cross-domain Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Wang, Qiang Wu, Min Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-domain recommendation (CDR) aims to improve recommendation accuracy in
sparse domains by transferring knowledge from data-rich domains. However,
existing CDR approaches often assume that user-item interaction data across
domains is publicly available, neglecting user privacy concerns. Additionally,
they experience performance degradation with sparse overlapping users due to
their reliance on a large number of fully shared users for knowledge transfer.
To address these challenges, we propose a Federated Prototype-based Contrastive
Learning (CL) framework for Privacy Preserving CDR, called FedPCL-CDR. This
approach utilizes non-overlapping user information and differential prototypes
to improve model performance within a federated learning framework. FedPCL-CDR
comprises two key modules: local domain (client) learning and global server
aggregation. In the local domain, FedPCL-CDR first clusters all user data and
utilizes local differential privacy (LDP) to learn differential prototypes,
effectively utilizing non-overlapping user information and protecting user
privacy. It then conducts knowledge transfer by employing both local and global
prototypes returned from the server in a CL manner. Meanwhile, the global
server aggregates differential prototypes sent from local domains to learn both
local and global prototypes. Extensive experiments on four CDR tasks across
Amazon and Douban datasets demonstrate that FedPCL-CDR surpasses SOTA
baselines. We release our code at https://github.com/Lili1013/FedPCL CDR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal
  Perspective <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19458v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19458v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taoyu Su, Jiawei Sheng, Duohe Ma, Xiaodong Li, Juwei Yue, Mengxiao Song, Yingkai Tang, Tingwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from
different Multi-Modal Knowledge Graphs (MMKGs), a critical information
retrieval task. Existing studies have explored various fusion paradigms and
consistency constraints to improve the alignment of equivalent entities, while
overlooking that the visual modality may not always contribute positively.
Empirically, entities with low-similarity images usually generate
unsatisfactory performance, highlighting the limitation of overly relying on
visual features. We believe the model can be biased toward the visual modality,
leading to a shortcut image-matching task. To address this, we propose a
counterfactual debiasing framework for MMEA, termed CDMEA, which investigates
visual modality bias from a causal perspective. Our approach aims to leverage
both visual and graph modalities to enhance MMEA while suppressing the direct
causal effect of the visual modality on model predictions. By estimating the
Total Effect (TE) of both modalities and excluding the Natural Direct Effect
(NDE) of the visual modality, we ensure that the model predicts based on the
Total Indirect Effect (TIE), effectively utilizing both modalities and reducing
visual modality bias. Extensive experiments on 9 benchmark datasets show that
CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,
high-noise, and low-resource data scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2025, 11 pages, 10 figures, 4 tables,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAKR: Enhancing Retrieval-Augmented Generation via Streaming Algorithm
  and K-Means Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21300v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21300v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Kang, Yuzhou Zhu, Yukun Zhong, Ke Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has achieved significant success in
information retrieval to assist large language models LLMs because it builds an
external knowledge database. However, it also has many problems, it consumes a
lot of memory because of the enormous database, and it cannot update the
established index database in time when confronted with massive streaming data.
To reduce the memory required for building the database and maintain accuracy
simultaneously, we proposed a new approach integrating a streaming algorithm
with k-means clustering into RAG. Our approach applied a streaming algorithm to
update the index dynamically and reduce memory consumption. Additionally, the
k-means algorithm clusters highly similar documents, and the query time would
be shortened. We conducted comparative experiments on four methods, and the
results indicated that RAG with streaming algorithm and k-means clusters
outperforms traditional RAG in accuracy and memory, particularly when dealing
with large-scale streaming data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shallow AutoEncoding Recommender with Cold Start Handling via Side
  Features <span class="chip">CIKM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.02288v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.02288v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward DongBo Cui, Lu Zhang, William Ping-hsun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User and item cold starts present significant challenges in industrial
applications of recommendation systems. Supplementing user-item interaction
data with metadata is a common solution-but often at the cost of introducing
additional biases. In this work, we introduce an augmented EASE model that
seamlessly integrates both user and item side information to address these cold
start issues. Our straightforward, autoencoder-based method produces a
closed-form solution that leverages rich content signals for cold items while
refining user representations in data-sparse environments. Importantly, our
method strikes a balance by effectively recommending cold start items and
handling cold start users without incurring extra bias, and it maintains strong
performance in warm settings. Experimental results demonstrate improved
recommendation accuracy and robustness compared to previous collaborative
filtering approaches. Moreover, our model serves as a strong baseline for
future comparative studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preparing submission to CIKM 2025; 2 Figures; 4 Tables; 13 pages;
  Python code implementation example</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Recommender Models and the Illusion of Progress: A Concerning
  Study of Reproducibility and a Conceptual Mismatch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Benigni, Maurizio Ferrari Dacrema, Dietmar Jannach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Countless new machine learning models are published every year and are
reported to significantly advance the state-of-the-art in \emph{top-n}
recommendation. However, earlier reproducibility studies indicate that progress
in this area may be quite limited. Specifically, various widespread
methodological issues, e.g., comparisons with untuned baseline models, have led
to an \emph{illusion of progress}. In this work, our goal is to examine whether
these problems persist in today's research. To this end, we aim to reproduce
the latest advancements reported from applying modern Denoising Diffusion
Probabilistic Models to recommender systems, focusing on four models published
at the top-ranked SIGIR conference in 2023 and 2024. Our findings are
concerning, revealing persistent methodological problems. Alarmingly, through
experiments, we find that the latest recommendation techniques based on
diffusion models, despite their computational complexity and substantial carbon
footprint, are consistently outperformed by simpler existing models.
Furthermore, we identify key mismatches between the characteristics of
diffusion models and those of the traditional \emph{top-n} recommendation task,
raising doubts about their suitability for recommendation. We also note that,
in the papers we analyze, the generative capabilities of these models are
constrained to a minimum. Overall, our results and continued methodological
issues call for greater scientific rigor and a disruptive change in the
research and publication culture in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamics of Adversarial Attacks on Large Language Model-Based Search
  Engines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00745v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00745v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiyang Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing integration of Large Language Model (LLM) based search engines
has transformed the landscape of information retrieval. However, these systems
are vulnerable to adversarial attacks, especially ranking manipulation attacks,
where attackers craft webpage content to manipulate the LLM's ranking and
promote specific content, gaining an unfair advantage over competitors. In this
paper, we study the dynamics of ranking manipulation attacks. We frame this
problem as an Infinitely Repeated Prisoners' Dilemma, where multiple players
strategically decide whether to cooperate or attack. We analyze the conditions
under which cooperation can be sustained, identifying key factors such as
attack costs, discount rates, attack success rates, and trigger strategies that
influence player behavior. We identify tipping points in the system dynamics,
demonstrating that cooperation is more likely to be sustained when players are
forward-looking. However, from a defense perspective, we find that simply
reducing attack success probabilities can, paradoxically, incentivize attacks
under certain conditions. Furthermore, defensive measures to cap the upper
bound of attack success rates may prove futile in some scenarios. These
insights highlight the complexity of securing LLM-based systems. Our work
provides a theoretical foundation and practical insights for understanding and
mitigating their vulnerabilities, while emphasizing the importance of adaptive
security strategies and thoughtful ecosystem design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in
  Conversational Search <span class="chip">SIGIR '25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14609v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14609v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Lupart, Mohammad Aliannejadi, Evangelos Kanoulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Search (CS) involves retrieving relevant documents from a
corpus while considering the conversational context, integrating retrieval with
context modeling. Recent advancements in Large Language Models (LLMs) have
significantly enhanced CS by enabling query rewriting based on conversational
context. However, employing LLMs during inference poses efficiency challenges.
Existing solutions mitigate this issue by distilling embeddings derived from
human-rewritten queries, focusing primarily on learning the context modeling
task. These methods, however, often separate the contrastive retrieval task
from the distillation process, treating it as an independent loss term. To
overcome these limitations, we introduce DiSCo (Distillation of Sparse
Conversational retrieval), a novel approach that unifies retrieval and context
modeling through a relaxed distillation objective. Instead of relying
exclusively on representation learning, our method distills similarity scores
between conversations and documents, providing more freedom in the
representation space and better leveraging the contrastive nature of document
relevance. Extensive experiments on Learned Sparse Retrieval (LSR) across five
CS datasets demonstrate that DiSCo achieves substantial improvements in both
in-domain and out-of-domain retrieval tasks, achieving up to a six-point gain
in recall for out-of-domain datasets over state-of-the-art methods.
Additionally, DiSCo employs a multi-teacher distillation strategy, using
multiple LLMs as teachers, further enhancing performance and surpassing the
individual teachers in in-domain settings. Furthermore, analysis of model
sparsity reveals that DiSCo allows for more effective control over the sparsity
of the trained models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures. SIGIR '25 Proceedings of the 48th International
  ACM SIGIR Conference on Research and Development in Information Retrieval
  July 13--18, 2025 Padua, Italy</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-14T00:00:00Z">2025-05-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">64</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EdgeAI Drone for Autonomous Construction Site Demonstrator <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emre Girgin, Arda Taha Candan, Coşkun Anıl Zaman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fields of autonomous systems and robotics are receiving considerable
attention in civil applications such as construction, logistics, and
firefighting. Nevertheless, the widespread adoption of these technologies is
hindered by the necessity for robust processing units to run AI models. Edge-AI
solutions offer considerable promise, enabling low-power, cost-effective
robotics that can automate civil services, improve safety, and enhance
sustainability. This paper presents a novel Edge-AI-enabled drone-based
surveillance system for autonomous multi-robot operations at construction
sites. Our system integrates a lightweight MCU-based object detection model
within a custom-built UAV platform and a 5G-enabled multi-agent coordination
infrastructure. We specifically target the real-time obstacle detection and
dynamic path planning problem in construction environments, providing a
comprehensive dataset specifically created for MCU-based edge applications.
Field experiments demonstrate practical viability and identify optimal
operational parameters, highlighting our approach's scalability and
computational efficiency advantages compared to existing UAV solutions. The
present and future roles of autonomous vehicles on construction sites are also
discussed, as well as the effectiveness of edge-AI solutions. We share our
dataset publicly at github.com/egirgin/storaige-b950
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper presented at the 4th Workshop on Future of Construction: Safe,
  Reliable, and Precise Robots in Construction Environments, ICRA 2025,
  Atlanta, GA, United States</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Rock Pushability on Rough Planetary Terrain <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuba Girgin, Emre Girgin, Cagri Kilic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of mobile navigation in unstructured environments, the
predominant approach entails the avoidance of obstacles. The prevailing path
planning algorithms are contingent upon deviating from the intended path for an
indefinite duration and returning to the closest point on the route after the
obstacle is left behind spatially. However, avoiding an obstacle on a path that
will be used repeatedly by multiple agents can hinder long-term efficiency and
lead to a lasting reliance on an active path planning system. In this study, we
propose an alternative approach to mobile navigation in unstructured
environments by leveraging the manipulation capabilities of a robotic
manipulator mounted on top of a mobile robot. Our proposed framework integrates
exteroceptive and proprioceptive feedback to assess the push affordance of
obstacles, facilitating their repositioning rather than avoidance. While our
preliminary visual estimation takes into account the characteristics of both
the obstacle and the surface it relies on, the push affordance estimation
module exploits the force feedback obtained by interacting with the obstacle
via a robotic manipulator as the guidance signal. The objective of our
navigation approach is to enhance the efficiency of routes utilized by multiple
agents over extended periods by reducing the overall time spent by a fleet in
environments where autonomous infrastructure development is imperative, such as
lunar or Martian surfaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper presented at the Workshop on Field Robotics, ICRA 2025,
  Atlanta, GA, United States</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Inertial Odometry from Lie Events 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Royina Karegoudra Jayanth, Yinshuang Xu, Evangelos Chatzipantazis, Kostas Daniilidis, Daniel Gehrig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural displacement priors (NDP) can reduce the drift in inertial odometry
and provide uncertainty estimates that can be readily fused with off-the-shelf
filters. However, they fail to generalize to different IMU sampling rates and
trajectory profiles, which limits their robustness in diverse settings. To
address this challenge, we replace the traditional NDP inputs comprising raw
IMU data with Lie events that are robust to input rate changes and have
favorable invariances when observed under different trajectory profiles. Unlike
raw IMU data sampled at fixed rates, Lie events are sampled whenever the norm
of the IMU pre-integration change, mapped to the Lie algebra of the SE(3)
group, exceeds a threshold. Inspired by event-based vision, we generalize the
notion of level-crossing on 1D signals to level-crossings on the Lie algebra
and generalize binary polarities to normalized Lie polarities within this
algebra. We show that training NDPs on Lie events incorporating these
polarities reduces the trajectory error of off-the-shelf downstream inertial
odometry methods by up to 21% with only minimal preprocessing. We conjecture
that many more sensors than IMUs or cameras can benefit from an event-based
sampling paradigm and that this work makes an important first step in this
direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at RSS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grasp EveryThing (GET): 1-DoF, 3-Fingered Gripper with Tactile Sensing
  for Robust Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Burgess, Edward H. Adelson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Grasp EveryThing (GET) gripper, a novel 1-DoF, 3-finger
design for securely grasping objects of many shapes and sizes. Mounted on a
standard parallel jaw actuator, the design features three narrow, tapered
fingers arranged in a two-against-one configuration, where the two fingers
converge into a V-shape. The GET gripper is more capable of conforming to
object geometries and forming secure grasps than traditional designs with two
flat fingers. Inspired by the principle of self-similarity, these V-shaped
fingers enable secure grasping across a wide range of object sizes. Further to
this end, fingers are parametrically designed for convenient resizing and
interchangeability across robotic embodiments with a parallel jaw gripper.
Additionally, we incorporate a rigid fingernail to enhance small object
manipulation. Tactile sensing can be integrated into the standalone finger via
an externally-mounted camera. A neural network was trained to estimate normal
force from tactile images with an average validation error of 1.3~N across a
diverse set of geometries. In grasping 15 objects and performing 3 tasks via
teleoperation, the GET fingers consistently outperformed standard flat fingers.
Finger designs for use with multiple robotic embodiments are available on
GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Associative Skill Memories for safer robotics and modelling human
  sensorimotor repertoires 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Mahajan, Mufeng Tang, T. Ed Li, Ioannis Havoutis, Ben Seymour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern robots face challenges shared by humans, where machines must learn
multiple sensorimotor skills and express them adaptively. Equipping robots with
a human-like memory of how it feels to do multiple stereotypical movements can
make robots more aware of normal operational states and help develop
self-preserving safer robots. Associative Skill Memories (ASMs) aim to address
this by linking movement primitives to sensory feedback, but existing
implementations rely on hard-coded libraries of individual skills. A key
unresolved problem is how a single neural network can learn a repertoire of
skills while enabling fault detection and context-aware execution. Here we
introduce Neural Associative Skill Memories (ASMs), a framework that utilises
self-supervised predictive coding for temporal prediction to unify skill
learning and expression, using biologically plausible learning rules. Unlike
traditional ASMs which require explicit skill selection, Neural ASMs implicitly
recognize and express skills through contextual inference, enabling fault
detection across learned behaviours without an explicit skill selection
mechanism. Compared to recurrent neural networks trained via backpropagation
through time, our model achieves comparable qualitative performance in skill
memory expression while using local learning rules and predicts a biologically
relevant speed-accuracy trade-off during skill memory expression. This work
advances the field of neurorobotics by demonstrating how predictive coding
principles can model adaptive robot control and human motor preparation. By
unifying fault detection, reactive control, skill memorisation and expression
into a single energy-based architecture, Neural ASMs contribute to safer
robotics and provide a computational lens to study biological sensorimotor
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trailblazer: Learning offroad costmaps for long range planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kasi Viswanath, Felix Sanchez, Timothy Overbye, Jason M. Gregory, Srikanth Saripalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous navigation in off-road environments remains a significant
challenge in field robotics, particularly for Unmanned Ground Vehicles (UGVs)
tasked with search and rescue, exploration, and surveillance. Effective
long-range planning relies on the integration of onboard perception systems
with prior environmental knowledge, such as satellite imagery and LiDAR data.
This work introduces Trailblazer, a novel framework that automates the
conversion of multi-modal sensor data into costmaps, enabling efficient path
planning without manual tuning. Unlike traditional approaches, Trailblazer
leverages imitation learning and a differentiable A* planner to learn costmaps
directly from expert demonstrations, enhancing adaptability across diverse
terrains. The proposed methodology was validated through extensive real-world
testing, achieving robust performance in dynamic and complex environments,
demonstrating Trailblazer's potential for scalable, efficient autonomous
navigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ General Dynamic Goal Recognition <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osher Elhadad, Reuth Mirsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding an agent's intent through its behavior is essential in
human-robot interaction, interactive AI systems, and multi-agent
collaborations. This task, known as Goal Recognition (GR), poses significant
challenges in dynamic environments where goals are numerous and constantly
evolving. Traditional GR methods, designed for a predefined set of goals, often
struggle to adapt to these dynamic scenarios. To address this limitation, we
introduce the General Dynamic GR problem - a broader definition of GR - aimed
at enabling real-time GR systems and fostering further research in this area.
Expanding on this foundation, this paper employs a model-free goal-conditioned
RL approach to enable fast adaptation for GR across various changing tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at Generalization in Planning (GenPlan) as
  part of AAAI 2025 workshops</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Risk-Aware Safe Reinforcement Learning for Control of Stochastic Linear
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Babak Esmaeili, Nariman Niknejad, Hamidreza Modares
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a risk-aware safe reinforcement learning (RL) control
design for stochastic discrete-time linear systems. Rather than using a safety
certifier to myopically intervene with the RL controller, a risk-informed safe
controller is also learned besides the RL controller, and the RL and safe
controllers are combined together. Several advantages come along with this
approach: 1) High-confidence safety can be certified without relying on a
high-fidelity system model and using limited data available, 2) Myopic
interventions and convergence to an undesired equilibrium can be avoided by
deciding on the contribution of two stabilizing controllers, and 3) highly
efficient and computationally tractable solutions can be provided by optimizing
over a scalar decision variable and linear programming polyhedral sets. To
learn safe controllers with a large invariant set, piecewise affine controllers
are learned instead of linear controllers. To this end, the closed-loop system
is first represented using collected data, a decision variable, and noise. The
effect of the decision variable on the variance of the safe violation of the
closed-loop system is formalized. The decision variable is then designed such
that the probability of safety violation for the learned closed-loop system is
minimized. It is shown that this control-oriented approach reduces the data
requirements and can also reduce the variance of safety violations. Finally, to
integrate the safe and RL controllers, a new data-driven interpolation
technique is introduced. This method aims to maintain the RL agent's optimal
implementation while ensuring its safety within environments characterized by
noise. The study concludes with a simulation example that serves to validate
the theoretical results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Asian Journal of Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unfettered Forceful Skill Acquisition with Physical Reasoning and
  Coordinate Frame Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Xie, Max Conway, Yutong Zhang, Nikolaus Correll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision language models (VLMs) exhibit vast knowledge of the physical world,
including intuition of physical and spatial properties, affordances, and
motion. With fine-tuning, VLMs can also natively produce robot trajectories. We
demonstrate that eliciting wrenches, not trajectories, allows VLMs to
explicitly reason about forces and leads to zero-shot generalization in a
series of manipulation tasks without pretraining. We achieve this by overlaying
a consistent visual representation of relevant coordinate frames on
robot-attached camera images to augment our query. First, we show how this
addition enables a versatile motion control framework evaluated across four
tasks (opening and closing a lid, pushing a cup or chair) spanning prismatic
and rotational motion, an order of force and position magnitude, different
camera perspectives, annotation schemes, and two robot platforms over 220
experiments, resulting in 51% success across the four tasks. Then, we
demonstrate that the proposed framework enables VLMs to continually reason
about interaction feedback to recover from task failure or incompletion, with
and without human supervision. Finally, we observe that prompting schemes with
visual annotation and embodied reasoning can bypass VLM safeguards. We
characterize prompt component contribution to harmful behavior elicitation and
discuss its implications for developing embodied reasoning. Our code, videos,
and data are available at: https://scalingforce.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EnerVerse-AC: Envisioning Embodied Environments with Action Condition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Jiang, Shengcong Chen, Siyuan Huang, Liliang Chen, Pengfei Zhou, Yue Liao, Xindong He, Chiming Liu, Hongsheng Li, Maoqing Yao, Guanghui Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic imitation learning has advanced from solving static tasks to
addressing dynamic interaction scenarios, but testing and evaluation remain
costly and challenging due to the need for real-time interaction with dynamic
environments. We propose EnerVerse-AC (EVAC), an action-conditional world model
that generates future visual observations based on an agent's predicted
actions, enabling realistic and controllable robotic inference. Building on
prior architectures, EVAC introduces a multi-level action-conditioning
mechanism and ray map encoding for dynamic multi-view image generation while
expanding training data with diverse failure trajectories to improve
generalization. As both a data engine and evaluator, EVAC augments
human-collected trajectories into diverse datasets and generates realistic,
action-conditioned video observations for policy testing, eliminating the need
for physical robots or complex simulations. This approach significantly reduces
costs while maintaining high fidelity in robotic manipulation evaluation.
Extensive experiments validate the effectiveness of our method. Code,
checkpoints, and datasets can be found at
<https://annaj2178.github.io/EnerverseAC.github.io>.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://annaj2178.github.io/EnerverseAC.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ManipBench: Benchmarking Vision-Language Models for Low-Level Robot
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enyu Zhao, Vedant Raval, Hejia Zhang, Jiageng Mao, Zeyu Shangguan, Stefanos Nikolaidis, Yue Wang, Daniel Seita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have revolutionized artificial intelligence and
robotics due to their commonsense reasoning capabilities. In robotic
manipulation, VLMs are used primarily as high-level planners, but recent work
has also studied their lower-level reasoning ability, which refers to making
decisions about precise robot movements. However, the community currently lacks
a clear and common benchmark that can evaluate how well VLMs can aid low-level
reasoning in robotics. Consequently, we propose a novel benchmark, ManipBench,
to evaluate the low-level robot manipulation reasoning capabilities of VLMs
across various dimensions, including how well they understand object-object
interactions and deformable object manipulation. We extensively test 33
representative VLMs across 10 model families on our benchmark, including
variants to test different model sizes. Our evaluation shows that the
performance of VLMs significantly varies across tasks, and there is a strong
correlation between this performance and trends in our real-world manipulation
tasks. It also shows that there remains a significant gap between these models
and human-level understanding. See our website at:
https://manipbench.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 29 figures. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied
  World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hu Yue, Siyuan Huang, Yue Liao, Shengcong Chen, Pengfei Zhou, Liliang Chen, Maoqing Yao, Guanghui Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in creative AI have enabled the synthesis of high-fidelity
images and videos conditioned on language instructions. Building on these
developments, text-to-video diffusion models have evolved into embodied world
models (EWMs) capable of generating physically plausible scenes from language
commands, effectively bridging vision and action in embodied AI applications.
This work addresses the critical challenge of evaluating EWMs beyond general
perceptual metrics to ensure the generation of physically grounded and
action-consistent behaviors. We propose the Embodied World Model Benchmark
(EWMBench), a dedicated framework designed to evaluate EWMs based on three key
aspects: visual scene consistency, motion correctness, and semantic alignment.
Our approach leverages a meticulously curated dataset encompassing diverse
scenes and motion patterns, alongside a comprehensive multi-dimensional
evaluation toolkit, to assess and compare candidate models. The proposed
benchmark not only identifies the limitations of existing video generation
models in meeting the unique requirements of embodied tasks but also provides
valuable insights to guide future advancements in the field. The dataset and
evaluation tools are publicly available at
https://github.com/AgibotTech/EWMBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://github.com/AgibotTech/EWMBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DataMIL: Selecting Data for Robot Imitation Learning with Datamodels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivin Dass, Alaa Khaddaj, Logan Engstrom, Aleksander Madry, Andrew Ilyas, Roberto Martín-Martín
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the robotics community has amassed ever larger and more diverse
datasets to train generalist robot policies. However, while these policies
achieve strong mean performance across a variety of tasks, they often
underperform on individual, specialized tasks and require further tuning on
newly acquired task-specific data. Combining task-specific data with carefully
curated subsets of large prior datasets via co-training can produce better
specialized policies, but selecting data naively may actually harm downstream
performance. To address this, we introduce DataMIL, a policy-driven data
selection framework built on the datamodels paradigm that reasons about data
selection in an end-to-end manner, using the policy itself to identify which
data points will most improve performance. Unlike standard practices that
filter data using human notions of quality (e.g., based on semantic or visual
similarity), DataMIL directly optimizes data selection for task success,
allowing us to select data that enhance the policy while dropping data that
degrade it. To avoid performing expensive rollouts in the environment during
selection, we use a novel surrogate loss function on task-specific data,
allowing us to use DataMIL in the real world without degrading performance. We
validate our approach on a suite of more than 60 simulation and real-world
manipulation tasks - most notably showing successful data selection from the
Open X-Embodiment datasets-demonstrating consistent gains in success rates and
superior performance over multiple baselines. Our results underscore the
importance of end-to-end, performance-aware data selection for unlocking the
potential of large prior datasets in robotics. More information at
https://robin-lab.cs.utexas.edu/datamodels4imitation/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or
  Robot Hardware 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Yu, Letian Fu, Huang Huang, Karim El-Refai, Rares Andrei Ambrus, Richard Cheng, Muhammad Zubair Irshad, Ken Goldberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling robot learning requires vast and diverse datasets. Yet the prevailing
data collection paradigm-human teleoperation-remains costly and constrained by
manual effort and physical robot access. We introduce Real2Render2Real (R2R2R),
a novel approach for generating robot training data without relying on object
dynamics simulation or teleoperation of robot hardware. The input is a
smartphone-captured scan of one or more objects and a single video of a human
demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic
demonstrations by reconstructing detailed 3D object geometry and appearance,
and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to
enable flexible asset generation and trajectory synthesis for both rigid and
articulated objects, converting these representations to meshes to maintain
compatibility with scalable rendering engines like IsaacLab but with collision
modeling off. Robot demonstration data generated by R2R2R integrates directly
with models that operate on robot proprioceptive states and image observations,
such as vision-language-action models (VLA) and imitation learning policies.
Physical experiments suggest that models trained on R2R2R data from a single
human demonstration can match the performance of models trained on 150 human
teleoperation demonstrations. Project page: https://real2render2real.com
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VTLA: Vision-Tactile-Language-Action Model with Preference Learning for
  Insertion Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaofan Zhang, Peng Hao, Xiaoge Cao, Xiaoshuai Hao, Shaowei Cui, Shuo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While vision-language models have advanced significantly, their application
in language-conditioned robotic manipulation is still underexplored, especially
for contact-rich tasks that extend beyond visually dominant pick-and-place
scenarios. To bridge this gap, we introduce Vision-Tactile-Language-Action
model, a novel framework that enables robust policy generation in
contact-intensive scenarios by effectively integrating visual and tactile
inputs through cross-modal language grounding. A low-cost, multi-modal dataset
has been constructed in a simulation environment, containing
vision-tactile-action-instruction pairs specifically designed for the fingertip
insertion task. Furthermore, we introduce Direct Preference Optimization (DPO)
to offer regression-like supervision for the VTLA model, effectively bridging
the gap between classification-based next token prediction loss and continuous
robotic tasks. Experimental results show that the VTLA model outperforms
traditional imitation learning methods (e.g., diffusion policies) and existing
multi-modal baselines (TLA/VLA), achieving over 90% success rates on unseen peg
shapes. Finally, we conduct real-world peg-in-hole experiments to demonstrate
the exceptional Sim2Real performance of the proposed VTLA model. For
supplementary videos and results, please visit our project website:
https://sites.google.com/view/vtla
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Long-Context Diffusion Policies via Past-Token Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Torne, Andy Tang, Yuejiang Liu, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning over long sequences of observations and actions is essential for
many robotic tasks. Yet, learning effective long-context policies from
demonstrations remains challenging. As context length increases, training
becomes increasingly expensive due to rising memory demands, and policy
performance often degrades as a result of spurious correlations. Recent methods
typically sidestep these issues by truncating context length, discarding
historical information that may be critical for subsequent decisions. In this
paper, we propose an alternative approach that explicitly regularizes the
retention of past information. We first revisit the copycat problem in
imitation learning and identify an opposite challenge in recent diffusion
policies: rather than over-relying on prior actions, they often fail to capture
essential dependencies between past and future actions. To address this, we
introduce Past-Token Prediction (PTP), an auxiliary task in which the policy
learns to predict past action tokens alongside future ones. This regularization
significantly improves temporal modeling in the policy head, with minimal
reliance on visual representations. Building on this observation, we further
introduce a multistage training strategy: pre-train the visual encoder with
short contexts, and fine-tune the policy head using cached long-context
embeddings. This strategy preserves the benefits of PTP while greatly reducing
memory and computational overhead. Finally, we extend PTP into a
self-verification mechanism at test time, enabling the policy to score and
select candidates consistent with past actions during inference. Experiments
across four real-world and six simulated tasks demonstrate that our proposed
method improves the performance of long-context diffusion policies by 3x and
accelerates policy training by more than 10x.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Videos are available at https://long-context-dp.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Realizable Students from Unrealizable Teachers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Kim, Nathaniel Chin, Arnav Vasudev, Sanjiban Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study policy distillation under privileged information, where a student
policy with only partial observations must learn from a teacher with full-state
access. A key challenge is information asymmetry: the student cannot directly
access the teacher's state space, leading to distributional shifts and policy
degradation. Existing approaches either modify the teacher to produce
realizable but sub-optimal demonstrations or rely on the student to explore
missing information independently, both of which are inefficient. Our key
insight is that the student should strategically interact with the teacher
--querying only when necessary and resetting from recovery states --to stay on
a recoverable path within its own observation space. We introduce two methods:
(i) an imitation learning approach that adaptively determines when the student
should query the teacher for corrections, and (ii) a reinforcement learning
approach that selects where to initialize training for efficient exploration.
We validate our methods in both simulated and real-world robotic tasks,
demonstrating significant improvements over standard teacher-student baselines
in training efficiency and final performance. The project website is available
at : https://portal-cornell.github.io/CritiQ_ReTRy/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design of a Formation Control System to Assist Human Operators in Flying
  a Swarm of Robotic Blimps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianfu Wu, Jiaqi Fu, Wugang Meng, Sungjin Cho, Huanzhe Zhan, Fumin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Formation control is essential for swarm robotics, enabling coordinated
behavior in complex environments. In this paper, we introduce a novel formation
control system for an indoor blimp swarm using a specialized leader-follower
approach enhanced with a dynamic leader-switching mechanism. This strategy
allows any blimp to take on the leader role, distributing maneuvering demands
across the swarm and enhancing overall formation stability. Only the leader
blimp is manually controlled by a human operator, while follower blimps use
onboard monocular cameras and a laser altimeter for relative position and
altitude estimation. A leader-switching scheme is proposed to assist the human
operator to maintain stability of the swarm, especially when a sharp turn is
performed. Experimental results confirm that the leader-switching mechanism
effectively maintains stable formations and adapts to dynamic indoor
environments while assisting human operator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deploying Foundation Model-Enabled Air and Ground Robots in the Field:
  Challenges and Opportunities <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Ravichandran, Fernando Cladera, Jason Hughes, Varun Murali, M. Ani Hsieh, George J. Pappas, Camillo J. Taylor, Vijay Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of foundation models (FMs) into robotics has enabled robots
to understand natural language and reason about the semantics in their
environments. However, existing FM-enabled robots primary operate in
closed-world settings, where the robot is given a full prior map or has a full
view of its workspace. This paper addresses the deployment of FM-enabled robots
in the field, where missions often require a robot to operate in large-scale
and unstructured environments. To effectively accomplish these missions, robots
must actively explore their environments, navigate obstacle-cluttered terrain,
handle unexpected sensor inputs, and operate with compute constraints. We
discuss recent deployments of SPINE, our LLM-enabled autonomy framework, in
field robotic settings. To the best of our knowledge, we present the first
demonstration of large-scale LLM-enabled robot planning in unstructured
environments with several kilometers of missions. SPINE is agnostic to a
particular LLM, which allows us to distill small language models capable of
running onboard size, weight and power (SWaP) limited platforms. Via
preliminary model distillation work, we then present the first language-driven
UAV planner using on-device language models. We conclude our paper by proposing
several promising directions for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the IEEE ICRA Workshop on Field Robotics 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ aUToPath: Unified Planning and Control for Autonomous Vehicles in Urban
  Environments Using Hybrid Lattice and Free-Space Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanmay P. Patel, Connor Wilson, Ellina R. Zhang, Morgan Tran, Chang Keun Paik, Steven L. Waslander, Timothy D. Barfoot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents aUToPath, a unified online framework for global
path-planning and control to address the challenge of autonomous navigation in
cluttered urban environments. A key component of our framework is a novel
hybrid planner that combines pre-computed lattice maps with dynamic free-space
sampling to efficiently generate optimal driveable corridors in cluttered
scenarios. Our system also features sequential convex programming (SCP)-based
model predictive control (MPC) to refine the corridors into smooth, dynamically
consistent trajectories. A single optimization problem is used to both generate
a trajectory and its corresponding control commands; this addresses limitations
of decoupled approaches by guaranteeing a safe and feasible path. Simulation
results of the novel planner on randomly generated obstacle-rich scenarios
demonstrate the success rate of a free-space Adaptively Informed Trees*
(AIT*)-based planner, and runtimes comparable to a lattice-based planner.
Real-world experiments of the full system on a Chevrolet Bolt EUV further
validate performance in dense obstacle fields, demonstrating no violations of
traffic, kinematic, or vehicle constraints, and a 100% success rate across
eight trials.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures. Tanmay P. Patel, Connor Wilson, and Ellina R.
  Zhang contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Streaming Multi-agent Pathfinding <span class="chip">IJCAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingkai Tang, Lu Gan, Kaichen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of the multi-agent pathfinding (MAPF) problem is to navigate a team
of agents from their start point to the goal points. However, this setup is
unsuitable in the assembly line scenario, which is periodic with a long working
hour. To address this issue, the study formalizes the streaming MAPF (S-MAPF)
problem, which assumes that the agents in the same agent stream have a periodic
start time and share the same action sequence. The proposed solution, Agent
Stream Conflict-Based Search (ASCBS), is designed to tackle this problem by
incorporating a cyclic vertex/edge constraint to handle conflicts.
Additionally, this work explores the potential usage of the disjoint splitting
strategy within ASCBS. Experimental results indicate that ASCBS surpasses
traditional MAPF solvers in terms of runtime for scenarios with prolonged
working hours.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in IJCAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Train a Multi-Task Diffusion Policy on RLBench-18 in One Day with One
  GPU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Hu, Pinhao Song, Kehan Wen, Renaud Detry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for training multi-task vision-language robotic diffusion
policies that reduces training time and memory usage by an order of magnitude.
This improvement arises from a previously underexplored distinction between
action diffusion and the image diffusion techniques that inspired it: image
generation targets are high-dimensional, while robot actions lie in a much
lower-dimensional space. Meanwhile, the vision-language conditions for action
generation remain high-dimensional. Our approach, Mini-Diffuser, exploits this
asymmetry by introducing Level-2 minibatching, which pairs multiple noised
action samples with each vision-language condition, instead of the conventional
one-to-one sampling strategy. To support this batching scheme, we introduce
architectural adaptations to the diffusion transformer that prevent information
leakage across samples while maintaining full conditioning access. In RLBench
simulations, Mini-Diffuser achieves 95\% of the performance of state-of-the-art
multi-task diffusion policies, while using only 5\% of the training time and
7\% of the memory. Real-world experiments further validate that Mini-Diffuser
preserves the key strengths of diffusion-based policies, including the ability
to model multimodal action distributions and produce behavior conditioned on
diverse perceptual inputs. Code available at
github.com/utomm/mini-diffuse-actor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Pose-Guided Imitation Learning for Robotic Precise Insertion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Sun, Yizhao Wang, Zhenning Zhou, Shuai Wang, Haibo Yang, Jingyuan Sun, Qixin Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have proved that imitation learning shows strong potential in
the field of robotic manipulation. However, existing methods still struggle
with precision manipulation task and rely on inefficient image/point cloud
observations. In this paper, we explore to introduce SE(3) object pose into
imitation learning and propose the pose-guided efficient imitation learning
methods for robotic precise insertion task. First, we propose a precise
insertion diffusion policy which utilizes the relative SE(3) pose as the
observation-action pair. The policy models the source object SE(3) pose
trajectory relative to the target object. Second, we explore to introduce the
RGBD data to the pose-guided diffusion policy. Specifically, we design a
goal-conditioned RGBD encoder to capture the discrepancy between the current
state and the goal state. In addition, a pose-guided residual gated fusion
method is proposed, which takes pose features as the backbone, and the RGBD
features selectively compensate for pose feature deficiencies through an
adaptive gating mechanism. Our methods are evaluated on 6 robotic precise
insertion tasks, demonstrating competitive performance with only 7-10
demonstrations. Experiments demonstrate that the proposed methods can
successfully complete precision insertion tasks with a clearance of about 0.01
mm. Experimental results highlight its superior efficiency and generalization
capability compared to existing baselines. Code will be available at
https://github.com/sunhan1997/PoseInsert.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strategic Jenga Play via Graph Based Dynamics Modeling <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavya Puthuveetil, Xinyi Zhang, Kazuto Yokoyama, Tetsuya Narita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controlled manipulation of multiple objects whose dynamics are closely linked
is a challenging problem within contact-rich manipulation, requiring an
understanding of how the movement of one will impact the others. Using the
Jenga game as a testbed to explore this problem, we graph-based modeling to
tackle two different aspects of the task: 1) block selection and 2) block
extraction. For block selection, we construct graphs of the Jenga tower and
attempt to classify, based on the tower's structure, whether removing a given
block will cause the tower to collapse. For block extraction, we train a
dynamics model that predicts how all the blocks in the tower will move at each
timestep in an extraction trajectory, which we then use in a sampling-based
model predictive control loop to safely pull blocks out of the tower with a
general-purpose parallel-jaw gripper. We train and evaluate our methods in
simulation, demonstrating promising results towards block selection and block
extraction on a challenging set of full-sized Jenga towers, even at advanced
stages of the game.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, Oral Spotlight at ICRA 2025 Workshop "Learning Meets
  Model-Based Methods for Contact-Rich Manipulation"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Corner Cutting Constraints for Mixed-Integer Motion Planning of
  a Differential Drive Micro-Mobility Vehicle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelo Caregnato-Neto, Janito Vaqueiro Ferreira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of motion planning for differential drive
micro-mobility platforms. This class of vehicle is designed to perform
small-distance transportation of passengers and goods in structured
environments. Our approach leverages mixed-integer linear programming (MILP) to
compute global optimal collision-free trajectories taking into account the
kinematics and dynamics of the vehicle. We propose novel constraints for
intersample collision avoidance and demonstrate its effectiveness using pick-up
and delivery missions and statistical analysis of Monte Carlo simulations. The
results show that the novel formulation provides the best trajectories in terms
of time expenditure and control effort when compared to two state-of-the-art
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ APR-<span class="highlight-title">Transformer</span>: Initial Pose Estimation for Localization in Complex
  Environments through Absolute Pose Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srinivas Ravuri, Yuan Xu, Martin Ludwig Zehetner, Ketan Motlag, Sahin Albayrak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise initialization plays a critical role in the performance of
localization algorithms, especially in the context of robotics, autonomous
driving, and computer vision. Poor localization accuracy is often a consequence
of inaccurate initial poses, particularly noticeable in GNSS-denied
environments where GPS signals are primarily relied upon for initialization.
Recent advances in leveraging deep neural networks for pose regression have led
to significant improvements in both accuracy and robustness, especially in
estimating complex spatial relationships and orientations. In this paper, we
introduce APR-Transformer, a model architecture inspired by state-of-the-art
methods, which predicts absolute pose (3D position and 3D orientation) using
either image or LiDAR data. We demonstrate that our proposed method achieves
state-of-the-art performance on established benchmark datasets such as the
Radar Oxford Robot-Car and DeepLoc datasets. Furthermore, we extend our
experiments to include our custom complex APR-BeIntelli dataset. Additionally,
we validate the reliability of our approach in GNSS-denied environments by
deploying the model in real-time on an autonomous test vehicle. This showcases
the practical feasibility and effectiveness of our approach. The source code is
available at:https://github.com/GT-ARC/APR-Transformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages with 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransDiffuser: End-to-end Trajectory Generation with Decorrelated
  Multi-modal Representation for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuefeng Jiang, Yuan Ma, Pengxiang Li, Leimeng Xu, Xin Wen, Kun Zhan, Zhongpu Xia, Peng Jia, XianPeng Lang, Sheng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, diffusion model has shown its potential across diverse
domains from vision generation to language modeling. Transferring its
capabilities to modern autonomous driving systems has also emerged as a
promising direction.In this work, we propose TransDiffuser, an encoder-decoder
based generative trajectory planning model for end-to-end autonomous driving.
The encoded scene information serves as the multi-modal conditional input of
the denoising decoder. To tackle the mode collapse dilemma in generating
high-quality diverse trajectories, we introduce a simple yet effective
multi-modal representation decorrelation optimization mechanism during the
training process.TransDiffuser achieves PDMS of 94.85 on the NAVSIM benchmark,
surpassing previous state-of-the-art methods without any anchor-based prior
trajectories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A drone that learns to efficiently find objects in agricultural fields:
  from simulation to the real world <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rick van Essen, Gert Kootstra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drones are promising for data collection in precision agriculture, however,
they are limited by their battery capacity. Efficient path planners are
therefore required. This paper presents a drone path planner trained using
Reinforcement Learning (RL) on an abstract simulation that uses object
detections and uncertain prior knowledge. The RL agent controls the flight
direction and can terminate the flight. By using the agent in combination with
the drone's flight controller and a detection network to process camera images,
it is possible to evaluate the performance of the agent on real-world data. In
simulation, the agent yielded on average a 78% shorter flight path compared to
a full coverage planner, at the cost of a 14% lower recall. On real-world data,
the agent showed a 72% shorter flight path compared to a full coverage planner,
however, at the cost of a 25% lower recall. The lower performance on real-world
data was attributed to the real-world object distribution and the lower
accuracy of prior knowledge, and shows potential for improvement. Overall, we
concluded that for applications where it is not crucial to find all objects,
such as weed detection, the learned-based path planner is suitable and
efficient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Novel Approaches for Precision Agriculture and
  Forestry with Autonomous Robots IEEE ICRA Workshop - 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ethical Aspects of the Use of Social Robots in Elderly Care -- A
  Systematic Qualitative <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marianne Leineweber, Clara Victoria Keusgen, Marc Bubeck, Joschka Haltaufderheide, Robert Ranisch, Corinna Klingler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: The use of social robotics in elderly care is increasingly
discussed as one way of meeting emerging care needs due to scarce resources.
While many potential benefits are associated with robotic care technologies,
there is a variety of ethical challenges. To support steps towards a
responsible implementation and use, this review develops an overview on ethical
aspects of the use of social robots in elderly care from a decision-makers'
perspective.
  Methods: Electronic databases were queried using a comprehensive search
strategy based on the key concepts of "ethical aspects", "social robotics" and
"elderly care". Abstract and title screening was conducted by two authors
independently. Full-text screening was conducted by one author following a
joint consolidation phase. Data was extracted using MAXQDA24 by one author,
based on a consolidated coding framework. Analysis was performed through
modified qualitative content analysis.
  Results: A total of 1,518 publications were screened, and 248 publications
were included. We have organized our analysis in a scheme of ethical hazards,
ethical opportunities and unsettled questions, identifying at least 60 broad
ethical aspects affecting three different stakeholder groups. While some
ethical issues are well-known and broadly discussed our analysis shows a
plethora of potentially relevant aspects, often only marginally recognized,
that are worthy of consideration from a practical perspective.
  Discussion: The findings highlight the need for a contextual and detailed
evaluation of implementation scenarios. To make use of the vast knowledge of
the ethical discourse, we hypothesize that decision-makers need to understand
the specific nature of this discourse to be able to engage in careful ethical
deliberation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>93 pages, 1 figure, 5 tables, 3 suplements</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robot-Assisted Drone Recovery on a Wavy Surface Using Error-State Kalman
  Filter and Receding Horizon Model Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimou Wu, Mingyang Liang, Ruoyu Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering a drone on a disturbed water surface remains a significant
challenge in maritime robotics. In this paper, we propose a unified framework
for Robot-Assisted Drone Recovery on a Wavy Surface that addresses two major
tasks: Firstly, accurate prediction of a moving drone's position under
wave-induced disturbances using an Error-State Kalman Filter (ESKF), and
secondly, effective motion planning for a manipulator via Receding Horizon
Control (RHC). Specifically, the ESKF predicts the drone's future position 0.5s
ahead, while the manipulator plans a capture trajectory in real time, thus
overcoming not only wave-induced base motions but also limited torque
constraints. We provide a system design that comprises a manipulator subsystem
and a UAV subsystem. On the UAV side, we detail how position control and
suspended payload strategies are implemented. On the manipulator side, we show
how an RHC scheme outperforms traditional low-level control algorithms.
Simulation and real-world experiments - using wave-disturbed motion data -
demonstrate that our approach achieves a high success rate - above 95% and
outperforms conventional baseline methods by up to 10% in efficiency and 20% in
precision. The results underscore the feasibility and robustness of our system,
which achieves state-of-the-art (SOTA) performance and offers a practical
solution for maritime drone operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Theory of Mind: A Decentralized Diffusion Architecture for
  Cooperative Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyang He, Gadiel Sznaier Camps, Xu Liu, Mac Schwager, Guillaume Sartoretti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Latent Theory of Mind (LatentToM), a decentralized diffusion
policy architecture for collaborative robot manipulation. Our policy allows
multiple manipulators with their own perception and computation to collaborate
with each other towards a common task goal with or without explicit
communication. Our key innovation lies in allowing each agent to maintain two
latent representations: an ego embedding specific to the robot, and a consensus
embedding trained to be common to both robots, despite their different sensor
streams and poses. We further let each robot train a decoder to infer the other
robot's ego embedding from their consensus embedding, akin to theory of mind in
latent space. Training occurs centrally, with all the policies' consensus
encoders supervised by a loss inspired by sheaf theory, a mathematical theory
for clustering data on a topological manifold. Specifically, we introduce a
first-order cohomology loss to enforce sheaf-consistent alignment of the
consensus embeddings. To preserve the expressiveness of the consensus
embedding, we further propose structural constraints based on theory of mind
and a directional consensus mechanism. Execution can be fully distributed,
requiring no explicit communication between policies. In which case, the
information is exchanged implicitly through each robot's sensor stream by
observing the actions of the other robots and their effects on the scene.
Alternatively, execution can leverage direct communication to share the robots'
consensus embeddings, where the embeddings are shared once during each
inference step and are aligned using the sheaf Laplacian. In our hardware
experiments, LatentToM outperforms a naive decentralized diffusion baseline,
and shows comparable performance with a state-of-the-art centralized diffusion
policy for bi-manual manipulation. Project website:
https://stanfordmsl.github.io/LatentToM/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Identification Adaptive Control with $ρ$-POMDP Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michelle Ho, Arec Jamgochian, Mykel J. Kochenderfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate system modeling is crucial for safe, effective control, as
misidentification can lead to accumulated errors, especially under partial
observability. We address this problem by formulating informative input design
(IID) and model identification adaptive control (MIAC) as belief space planning
problems, modeled as partially observable Markov decision processes with
belief-dependent rewards ($\rho$-POMDPs). We treat system parameters as hidden
state variables that must be localized while simultaneously controlling the
system. We solve this problem with an adapted belief-space iterative Linear
Quadratic Regulator (BiLQR). We demonstrate it on fully and partially
observable tasks for cart-pole and steady aircraft flight domains. Our method
outperforms baselines such as regression, filtering, and local optimal control
methods, even under instantaneous disturbances to system parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CoDIT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding
  via Keypoint-Driven Asset and Demonstration Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxing Chen, Bowen Xiao, He Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the deformability of garments, generating a large amount of
high-quality data for robotic garment manipulation tasks is highly challenging.
In this paper, we present a synthetic garment dataset that can be used for
robotic garment folding. We begin by constructing geometric garment templates
based on keypoints and applying generative models to generate realistic texture
patterns. Leveraging these keypoint annotations, we generate folding
demonstrations in simulation and train folding policies via closed-loop
imitation learning. To improve robustness, we propose KG-DAgger, which uses a
keypoint-based strategy to generate demonstration data for recovering from
failures. KG-DAgger significantly improves the model performance, boosting the
real-world success rate by 25\%. After training with 15K trajectories (about 2M
image-action pairs), the model achieves a 75\% success rate in the real world.
Experiments in both simulation and real-world settings validate the
effectiveness of our proposed framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Air-Ground Collaboration for Language-Specified Missions in Unknown
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Cladera, Zachary Ravichandran, Jason Hughes, Varun Murali, Carlos Nieto-Granda, M. Ani Hsieh, George J. Pappas, Camillo J. Taylor, Vijay Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As autonomous robotic systems become increasingly mature, users will want to
specify missions at the level of intent rather than in low-level detail.
Language is an expressive and intuitive medium for such mission specification.
However, realizing language-guided robotic teams requires overcoming
significant technical hurdles. Interpreting and realizing language-specified
missions requires advanced semantic reasoning. Successful heterogeneous robots
must effectively coordinate actions and share information across varying
viewpoints. Additionally, communication between robots is typically
intermittent, necessitating robust strategies that leverage communication
opportunities to maintain coordination and achieve mission objectives. In this
work, we present a first-of-its-kind system where an unmanned aerial vehicle
(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively
accomplish missions specified in natural language while reacting to changes in
specification on the fly. We leverage a Large Language Model (LLM)-enabled
planner to reason over semantic-metric maps that are built online and
opportunistically shared between an aerial and a ground robot. We consider
task-driven navigation in urban and rural areas. Our system must infer
mission-relevant semantics and actively acquire information via semantic
mapping. In both ground and air-ground teaming experiments, we demonstrate our
system on seven different natural-language specifications at up to
kilometer-scale navigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 24 figures, 7 tables. Submitted to T-FR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imitation Learning for Adaptive Control of a Virtual Soft Exoglove 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shirui Lyu, Vittorio Caggiano, Matteo Leonetti, Dario Farina, Letizia Gionfrida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of wearable robots has been widely adopted in rehabilitation training
for patients with hand motor impairments. However, the uniqueness of patients'
muscle loss is often overlooked. Leveraging reinforcement learning and a
biologically accurate musculoskeletal model in simulation, we propose a
customized wearable robotic controller that is able to address specific muscle
deficits and to provide compensation for hand-object manipulation tasks. Video
data of a same subject performing human grasping tasks is used to train a
manipulation model through learning from demonstration. This manipulation model
is subsequently fine-tuned to perform object-specific interaction tasks. The
muscle forces in the musculoskeletal manipulation model are then weakened to
simulate neurological motor impairments, which are later compensated by the
actuation of a virtual wearable robotics glove. Results shows that integrating
the virtual wearable robotic glove provides shared assistance to support the
hand manipulator with weakened muscle forces. The learned exoglove controller
achieved an average of 90.5\% of the original manipulation proficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenLKA: An Open <span class="highlight-title">Dataset</span> of Lane Keeping Assist from Recent Car Models
  under Real-world Driving Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Wang, Abdulaziz Alhuraish, Shengming Yuan, Hao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its
real-world performance remains underexplored due to proprietary systems and
limited data access. This paper presents OpenLKA, the first open, large-scale
dataset for LKA evaluation and improvement. It includes 400 hours of driving
data from 50+ production vehicle models, collected through extensive road
testing in Tampa, Florida and global contributions from the Comma.ai driving
community. The dataset spans a wide range of challenging scenarios, including
complex road geometries, degraded lane markings, adverse weather, lighting
conditions and surrounding traffic. The dataset is multimodal, comprising: i)
full CAN bus streams, decoded using custom reverse-engineered DBC files to
extract key LKA events (e.g., system disengagements, lane detection failures);
ii) synchronized high-resolution dash-cam video; iii) real-time outputs from
Openpilot, providing accurate estimates of road curvature and lane positioning;
iv) enhanced scene annotations generated by Vision Language Models, describing
lane visibility, pavement quality, weather, lighting, and traffic conditions.
By integrating vehicle-internal signals with high-fidelity perception and rich
semantic context, OpenLKA provides a comprehensive platform for benchmarking
the real-world performance of production LKA systems, identifying
safety-critical operational scenarios, and assessing the readiness of current
road infrastructure for autonomous driving. The dataset is publicly available
at: https://github.com/OpenLKA/OpenLKA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deployable and Generalizable Motion Prediction: Taxonomy, Open
  Challenges and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Letian Wang, Marc-Antoine Lavoie, Sandro Papais, Barza Nisar, Yuxiao Chen, Wenhao Ding, Boris Ivanovic, Hao Shao, Abulikemu Abuduweili, Evan Cook, Yang Zhou, Peter Karkus, Jiachen Li, Changliu Liu, Marco Pavone, Steven Waslander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion prediction, the anticipation of future agent states or scene
evolution, is rooted in human cognition, bridging perception and
decision-making. It enables intelligent systems, such as robots and
self-driving cars, to act safely in dynamic, human-involved environments, and
informs broader time-series reasoning challenges. With advances in methods,
representations, and datasets, the field has seen rapid progress, reflected in
quickly evolving benchmark results. Yet, when state-of-the-art methods are
deployed in the real world, they often struggle to generalize to open-world
conditions and fall short of deployment standards. This reveals a gap between
research benchmarks, which are often idealized or ill-posed, and real-world
complexity.
  To address this gap, this survey revisits the generalization and
deployability of motion prediction models, with an emphasis on the applications
of robotics, autonomous driving, and human motion. We first offer a
comprehensive taxonomy of motion prediction methods, covering representations,
modeling strategies, application domains, and evaluation protocols. We then
study two key challenges: (1) how to push motion prediction models to be
deployable to realistic deployment standards, where motion prediction does not
act in a vacuum, but functions as one module of closed-loop autonomy stacks -
it takes input from the localization and perception, and informs downstream
planning and control. 2) how to generalize motion prediction models from
limited seen scenarios/datasets to the open-world settings. Throughout the
paper, we highlight critical open challenges to guide future work, aiming to
recalibrate the community's efforts, fostering progress that is not only
measurable but also meaningful for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Initial draft, 162 pages, 40 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel 6-axis Force/Torque Sensor Using Inductance Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyun-Bin Kim, Kyung-Soo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel six-axis force/torque (F/T) sensor based on
inductive sensing technology. Unlike conventional strain gauge-based sensors
that require direct contact and external amplification, the proposed sensor
utilizes non-contact inductive measurements to estimate force via displacement
of a conductive target. A compact, fully integrated architecture is achieved by
incorporating a CAN-FD based signal processing module directly onto the PCB,
enabling high-speed data acquisition at up to 4~kHz without external DAQ
systems. The sensing mechanism is modeled and calibrated through a rational
function fitting approach, which demonstrated superior performance in terms of
root mean square error (RMSE), coefficient of determination ($R^2$), and
linearity error compared to other nonlinear models. Static and repeatability
experiments validate the sensor's accuracy, achieving a resolution of 0.03~N
and quantization levels exceeding 55,000 steps, surpassing that of commercial
sensors. The sensor also exhibits low crosstalk, high sensitivity, and robust
noise characteristics. Its performance and structure make it suitable for
precision robotic applications, especially in scenarios where compactness,
non-contact operation, and integrated processing are essential.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving Reach- and Stabilize-Avoid Problems Using Discounted
  Reachability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyang Li, Zheng Gong, Sylvia Herbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we consider the infinite-horizon reach-avoid (RA) and
stabilize-avoid (SA) zero-sum game problems for general nonlinear
continuous-time systems, where the goal is to find the set of states that can
be controlled to reach or stabilize to a target set, without violating
constraints even under the worst-case disturbance. Based on the Hamilton-Jacobi
reachability method, we address the RA problem by designing a new Lipschitz
continuous RA value function, whose zero sublevel set exactly characterizes the
RA set. We establish that the associated Bellman backup operator is contractive
and that the RA value function is the unique viscosity solution of a
Hamilton-Jacobi variational inequality. Finally, we develop a two-step
framework for the SA problem by integrating our RA strategies with a recently
proposed Robust Control Lyapunov-Value Function, thereby ensuring both target
reachability and long-term stability. We numerically verify our RA and SA
frameworks on a 3D Dubins car system to demonstrate the efficacy of the
proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reach-Avoid-Stabilize Using Admissible Control Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Gong, Boyang Li, Sylvia Herbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hamilton-Jacobi Reachability (HJR) analysis has been successfully used in
many robotics and control tasks, and is especially effective in computing
reach-avoid sets and control laws that enable an agent to reach a goal while
satisfying state constraints. However, the original HJR formulation provides no
guarantees of safety after a) the prescribed time horizon, or b) goal
satisfaction. The reach-avoid-stabilize (RAS) problem has therefore gained a
lot of focus: find the set of initial states (the RAS set), such that the
trajectory can reach the target, and stabilize to some point of interest (POI)
while avoiding obstacles. Solving RAS problems using HJR usually requires
defining a new value function, whose zero sub-level set is the RAS set. The
existing methods do not consider the problem when there are a series of targets
to reach and/or obstacles to avoid. We propose a method that uses the idea of
admissible control sets; we guarantee that the system will reach each target
while avoiding obstacles as prescribed by the given time series. Moreover, we
guarantee that the trajectory ultimately stabilizes to the POI. The proposed
method provides an under-approximation of the RAS set, guaranteeing safety.
Numerical examples are provided to validate the theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, submitted to 64th IEEE Conference on Decision and
  Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RT-cache: Efficient Robot Trajectory Retrieval System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Owen Kwon, Abraham George, Alison Bartsch, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces RT-cache, a novel trajectorymemory pipeline that
accelerates real-world robot inference by leveraging big-data retrieval and
learning from experience. While modern Vision-Language-Action (VLA) models can
handle diverse robotic tasks, they often incur high per-step inference costs,
resulting in significant latency, sometimes minutes per task. In contrast,
RT-cache stores a large-scale Memory of previously successful robot
trajectories and retrieves relevant multistep motion snippets, drastically
reducing inference overhead. By integrating a Memory Builder with a Trajectory
Retrieval, we develop an efficient retrieval process that remains tractable
even for extremely large datasets. RT-cache flexibly accumulates real-world
experiences and replays them whenever the current scene matches past states,
adapting quickly to new or unseen environments with only a few additional
samples. Experiments on the Open-X Embodiment Dataset and other real-world data
demonstrate that RT-cache completes tasks both faster and more successfully
than a baseline lacking retrieval, suggesting a practical, data-driven solution
for real-time manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures. Submitted to an IEEE robotics conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CRADMap: Applied Distributed Volumetric Mapping with 5G-Connected
  Multi-Robots and 4D Radar Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maaz Qureshi, Alexander Werner, Zhenan Liu, Amir Khajepour, George Shaker, William Melek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse and feature SLAM methods provide robust camera pose estimation.
However, they often fail to capture the level of detail required for inspection
and scene awareness tasks. Conversely, dense SLAM approaches generate richer
scene reconstructions but impose a prohibitive computational load to create 3D
maps. We present a novel distributed volumetric mapping framework designated as
CRADMap that addresses these issues by extending the state-of-the-art (SOTA)
ORBSLAM3 system with the COVINS on the backend for global optimization. Our
pipeline for volumetric reconstruction fuses dense keyframes at a centralized
server via 5G connectivity, aggregating geometry, and occupancy information
from multiple autonomous mobile robots (AMRs) without overtaxing onboard
resources. This enables each AMR to independently perform mapping while the
backend constructs high-fidelity real-time 3D maps. To operate Beyond the
Visible (BtV) and overcome the limitations of standard visual sensors, we
automated a standalone 4D mmWave radar module that functions independently
without sensor fusion with SLAM. The BtV system enables the detection and
mapping of occluded metallic objects in cluttered environments, enhancing
situational awareness in inspection scenarios. Experimental validation in
Section~\ref{sec:IV} demonstrates the effectiveness of our framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, IEEE, ICARM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physical synchronization of soft self-oscillating limbs for fast and
  autonomous locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Comoretto, Harmannus A. H. Schomaker, Johannes T. B. Overvelde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animals achieve robust locomotion by offloading regulation from the brain to
physical couplings within the body. In contrast, locomotion in artificial
systems often depends on centralized processors. We introduce a rapid and
autonomous locomotion strategy with synchronized gaits emerging through
physical interactions between self-oscillating limbs and the environment,
without control signals. Each limb is a single soft tube that only requires
constant flow of air to perform cyclic stepping motions at frequencies reaching
300 hertz. By combining several of these self-oscillating limbs, their physical
synchronization enables locomotion speeds that are orders of magnitude faster
than comparable state-of-the-art. Through body-environment dynamics, these
seemingly simple devices exhibit autonomy, including obstacle avoidance,
amphibious gait transitions, and phototaxis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On-Robot Reinforcement Learning with Goal-Contrastive Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ondrej Biza, Thomas Weng, Lingfeng Sun, Karl Schmeckpeper, Tarik Kelestemur, Yecheng Jason Ma, Robert Platt, Jan-Willem van de Meent, Lawson L. S. Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has the potential to enable robots to learn from
their own actions in the real world. Unfortunately, RL can be prohibitively
expensive, in terms of on-robot runtime, due to inefficient exploration when
learning from a sparse reward signal. Designing dense reward functions is
labour-intensive and requires domain expertise. In our work, we propose GCR
(Goal-Contrastive Rewards), a dense reward function learning method that can be
trained on passive video demonstrations. By using videos without actions, our
method is easier to scale, as we can use arbitrary videos. GCR combines two
loss functions, an implicit value loss function that models how the reward
increases when traversing a successful trajectory, and a goal-contrastive loss
that discriminates between successful and failed trajectories. We perform
experiments in simulated manipulation environments across RoboMimic and
MimicGen tasks, as well as in the real world using a Franka arm and a Spot
quadruped. We find that GCR leads to a more-sample efficient RL, enabling
model-free RL to solve about twice as many tasks as our baseline reward
learning methods. We also demonstrate positive cross-embodiment transfer from
videos of people and of other robots performing a task. Website:
https://gcr-robot.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Hierarchical World Models as Visual Whole-Body Humanoid Controllers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18418v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18418v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicklas Hansen, Jyothir S V, Vlad Sobal, <span class="highlight-author">Yann LeCun</span>, Xiaolong Wang, Hao Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole-body control for humanoids is challenging due to the high-dimensional
nature of the problem, coupled with the inherent instability of a bipedal
morphology. Learning from visual observations further exacerbates this
difficulty. In this work, we explore highly data-driven approaches to visual
whole-body humanoid control based on reinforcement learning, without any
simplifying assumptions, reward design, or skill primitives. Specifically, we
propose a hierarchical world model in which a high-level agent generates
commands based on visual observations for a low-level agent to execute, both of
which are trained with rewards. Our approach produces highly performant control
policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing
motions that are broadly preferred by humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and videos at https://nicklashansen.com/rlpuppeteer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiFlex: A Passive Bimodal Stiffness Flexible Wrist for Manipulation in
  Unstructured Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08706v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08706v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gu-Cheol Jeong, Stefano Dalla Gasperina, Ashish D. Deshpande, Lillian Chin, Roberto Martín-Martín
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic manipulation in unstructured, humancentric environments poses a dual
challenge: achieving the precision need for delicate free-space operation while
ensuring safety during unexpected contact events. Traditional wrists struggle
to balance these demands, often relying on complex control schemes or
complicated mechanical designs to mitigate potential damage from force
overload. In response, we present BiFlex, a flexible robotic wrist that uses a
soft buckling honeycomb structure to provides a natural bimodal stiffness
response. The higher stiffness mode enables precise household object
manipulation, while the lower stiffness mode provides the compliance needed to
adapt to external forces. We design BiFlex to maintain a fingertip deflection
of less than 1 cm while supporting loads up to 500g and create a BiFlex wrist
for many grippers, including Panda, Robotiq, and BaRiFlex. We validate BiFlex
under several real-world experimental evaluations, including surface wiping,
precise pick-and-place, and grasping under environmental constraints. We
demonstrate that BiFlex simplifies control while maintaining precise object
manipulation and enhanced safety in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pitch-axis supermanoeuvrability in a biomimetic morphing-wing UAV 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.09431v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.09431v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arion Pons, Fehmi Cirak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Birds and bats are extremely adept flyers: whether in hunting prey, or
evading predators, post-stall manoeuvrability is a characteristic of vital
importance. Their performance, in this regard, greatly exceeds that of uncrewed
aerial vehicles (UAVs) of similar scale. Attempts to attain post-stall
manoeuvrability, or supermanoeuvrability, in UAVs have typically focused on
thrust-vectoring technology. Here we show that biomimetic wing morphing offers
an additional pathway to classical supermanoeuvrability, as well as novel forms
of bioinspired post-stall manoeuvrability. Using a state-of-the-art flight
simulator, equipped with a multibody model of lifting surface motion and a
delay differential equation (Goman-Khrabrov) dynamic stall model for all
lifting surfaces, we demonstrate the capability of a biomimetic morphing-wing
UAV for two post-stall manoeuvres: a classical rapid nose-pointing-and-shooting
(RaNPAS) manoeuvre; and a wall landing manoeuvre inspired by biological
ballistic transitions. We develop a guidance method for these manoeuvres, based
on parametric variation of nonlinear longitudinal stability profiles, which
allows efficient exploration of the space of post-stall manoeuvres in these
types of UAVs; and yields insight into effective morphing kinematics to enable
these manoeuvres. Our results demonstrate the capability of biomimetic
morphing, and morphing control of nonlinear longitudinal stability, to enable
advanced forms of transient supermanoeuvrability in UAVs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guaranteed Rejection-free Sampling Method Using Past Behaviours for
  Motion Planning of Autonomous Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.14687v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.14687v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas T. Enevoldsen, Roberto Galeazzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents a novel learning-based sampling strategy that guarantees
rejection-free sampling of the free space under both biased and approximately
uniform conditions, leveraging multivariate kernel densities. Historical data
from a given autonomous system is leveraged to estimate a non-parametric
probabilistic description of the domain, which also describes the free space
where feasible solutions of the motion planning problem are likely to be found.
The tuning parameters of the kernel density estimator, the bandwidth and the
kernel, are used to alter the description of the free space so that no samples
can fall outside the originally defined space.The proposed method is
demonstrated in two real-life case studies: An autonomous surface vessel (2D)
and an autonomous drone (3D). Two planning problems are solved, showing that
the proposed approximately uniform sampling scheme is capable of guaranteeing
rejection-free samples of the considered workspace. Furthermore, the
effectiveness of the proposed method is statistically validated using Monte
Carlo simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Robotics and Autonomous Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open X-Embodiment: Robotic Learning <span class="highlight-title">Dataset</span>s and RT-X Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08864v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08864v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Open X-Embodiment Collaboration, Abby O'Neill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Andrey Kolobov, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homanga Bharadhwaj, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jay Vakil, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi "Jim" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Muhammad Zubair Irshad, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick "Tree" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shubham Tulsiani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vikash Kumar, Vincent Vanhoucke, Vitor Guizilini, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiangyu Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yansong Pang, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yongqiang Dou, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, Zipeng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large, high-capacity models trained on diverse datasets have shown remarkable
successes on efficiently tackling downstream applications. In domains from NLP
to Computer Vision, this has led to a consolidation of pretrained models, with
general pretrained backbones serving as a starting point for many applications.
Can such a consolidation happen in robotics? Conventionally, robotic learning
methods train a separate model for every application, every robot, and even
every environment. Can we instead train generalist X-robot policy that can be
adapted efficiently to new robots, tasks, and environments? In this paper, we
provide datasets in standardized data formats and models to make it possible to
explore this possibility in the context of robotic manipulation, alongside
experimental results that provide an example of effective X-robot policies. We
assemble a dataset from 22 different robots collected through a collaboration
between 21 institutions, demonstrating 527 skills (160266 tasks). We show that
a high-capacity model trained on this data, which we call RT-X, exhibits
positive transfer and improves the capabilities of multiple robots by
leveraging experience from other platforms. More details can be found on the
project website https://robotics-transformer-x.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://robotics-transformer-x.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Navigation in Uncertain Crowded Environments Using Risk Adaptive
  CVaR Barrier Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Wang, Taekyung Kim, Bardh Hoxha, Georgios Fainekos, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot navigation in dynamic, crowded environments poses a significant
challenge due to the inherent uncertainties in the obstacle model. In this
work, we propose a risk-adaptive approach based on the Conditional
Value-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically
adjusted to accept the minimum necessary risk, achieving a good performance in
terms of safety and optimization feasibility under uncertainty. Additionally,
we introduce a dynamic zone-based barrier function which characterizes the
collision likelihood by evaluating the relative state between the robot and the
obstacle. By integrating risk adaptation with this new function, our approach
adaptively expands the safety margin, enabling the robot to proactively avoid
obstacles in highly dynamic environments. Comparisons and ablation studies
demonstrate that our method outperforms existing social navigation approaches,
and validate the effectiveness of our proposed framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ON as ALC: Active Loop Closing Object Goal Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daiki Iwata, Kanji Tanaka, Shoya Miyazaki, Kouki Terashima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In simultaneous localization and mapping, active loop closing (ALC) is an
active vision problem that aims to visually guide a robot to maximize the
chances of revisiting previously visited points, thereby resetting the drift
errors accumulated in the incrementally built map during travel. However,
current mainstream navigation strategies that leverage such incomplete maps as
workspace prior knowledge often fail in modern long-term autonomy long-distance
travel scenarios where map accumulation errors become significant. To address
these limitations of map-based navigation, this paper is the first to explore
mapless navigation in the embodied AI field, in particular, to utilize
object-goal navigation (commonly abbreviated as ON, ObjNav, or OGN) techniques
that efficiently explore target objects without using such a prior map.
Specifically, in this work, we start from an off-the-shelf mapless ON planner,
extend it to utilize a prior map, and further show that the performance in
long-distance ALC (LD-ALC) can be maximized by minimizing ``ALC loss" and ``ON
loss". This study highlights a simple and effective approach, called ALC-ON
(ALCON), to accelerate the progress of challenging long-distance ALC technology
by leveraging the growing frontier-guided, data-driven, and LLM-guided ON
technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Draft version of a conference paper with 7 pages, 5 figures, and 1
  table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Autonomy: Off-Road Navigation Enhanced by Human Input 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akhil Nagariya, Dimitar Filev, Srikanth Saripalli, Gaurav Pandey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the area of autonomous driving, navigating off-road terrains presents a
unique set of challenges, from unpredictable surfaces like grass and dirt to
unexpected obstacles such as bushes and puddles. In this work, we present a
novel learning-based local planner that addresses these challenges by directly
capturing human driving nuances from real-world demonstrations using only a
monocular camera. The key features of our planner are its ability to navigate
in challenging off-road environments with various terrain types and its fast
learning capabilities. By utilizing minimal human demonstration data (5-10
mins), it quickly learns to navigate in a wide array of off-road conditions.
The local planner significantly reduces the real world data required to learn
human driving preferences. This allows the planner to apply learned behaviors
to real-world scenarios without the need for manual fine-tuning, demonstrating
quick adjustment and adaptability in off-road autonomous driving technology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ F$^3$Loc: Fusion and Filtering for Floorplan Localization <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changan Chen, Rui Wang, Christoph Vogel, Marc Pollefeys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose an efficient data-driven solution to
self-localization within a floorplan. Floorplan data is readily available,
long-term persistent and inherently robust to changes in the visual appearance.
Our method does not require retraining per map and location or demand a large
database of images of the area of interest. We propose a novel probabilistic
model consisting of an observation and a novel temporal filtering module.
Operating internally with an efficient ray-based representation, the
observation module consists of a single and a multiview module to predict
horizontal depth from images and fuses their results to benefit from advantages
offered by either methodology. Our method operates on conventional consumer
hardware and overcomes a common limitation of competing methods that often
demand upright images. Our full system meets real-time requirements, while
outperforming the state-of-the-art by a significant margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figure, accepted to CVPR 2024 (fixed typo eq.8: s_x,s_y,
  s_phi -> x, y, phi)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Soft Arm-Motor Thrust Characterization for a Pneumatically Actuated Soft
  Morphing Quadrotor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12716v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12716v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vidya Sumathy, Jakub Haluska, George Nikolakopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, an experimental characterization of the configuration space of
a soft, pneumatically actuated morphing quadrotor is presented, with a focus on
precise thrust characterization of its flexible arms, considering the effect of
downwash. Unlike traditional quadrotors, the soft drone has pneumatically
actuated arms, introducing complex, nonlinear interactions between motor thrust
and arm deformation, which make precise control challenging. The silicone arms
are actuated using differential pressure to achieve flexibility and thus have a
variable workspace compared to their fixed counter-parts. The deflection of the
soft arms during compression and expansion is controlled throughout the flight.
However, in real time, the downwash from the motor attached at the tip of the
soft arm generates a significant and random disturbance on the arm. This
disturbance affects both the desired deflection of the arm and the overall
stability of the system. To address this factor, an experimental
characterization of the effect of downwash on the deflection angle of the arm
is conducted.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This extended abstract was accepted for RoboSoft Conference, 2025 but
  later withdrawn</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Brain: A Neuroscience-inspired Framework for Embodied Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Liu, Xiongtao Shi, Thai Duy Nguyen, Haitian Zhang, Tianxiang Zhang, Wei Sun, Yanjie Li, Athanasios V. Vasilakos, Giovanni Iacca, Arshad Ali Khan, Arvind Kumar, Jae Won Cho, Ajmal Mian, Lihua Xie, Erik Cambria, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of artificial intelligence (AI) has shifted from static,
data-driven models to dynamic systems capable of perceiving and interacting
with real-world environments. Despite advancements in pattern recognition and
symbolic reasoning, current AI systems, such as large language models, remain
disembodied, unable to physically engage with the world. This limitation has
driven the rise of embodied AI, where autonomous agents, such as humanoid
robots, must navigate and manipulate unstructured environments with human-like
adaptability. At the core of this challenge lies the concept of Neural Brain, a
central intelligence system designed to drive embodied agents with human-like
adaptability. A Neural Brain must seamlessly integrate multimodal sensing and
perception with cognitive capabilities. Achieving this also requires an
adaptive memory system and energy-efficient hardware-software co-design,
enabling real-time action in dynamic environments. This paper introduces a
unified framework for the Neural Brain of embodied agents, addressing two
fundamental challenges: (1) defining the core components of Neural Brain and
(2) bridging the gap between static AI models and the dynamic adaptability
required for real-world deployment. To this end, we propose a biologically
inspired architecture that integrates multimodal active sensing,
perception-cognition-action function, neuroplasticity-based memory storage and
updating, and neuromorphic hardware/software optimization. Furthermore, we also
review the latest research on embodied agents across these four aspects and
analyze the gap between current AI systems and human intelligence. By
synthesizing insights from neuroscience, we outline a roadmap towards the
development of generalizable, autonomous agents capable of human-level
intelligence in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages, 17 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIMPPI: Enhancing Model Predictive Path Integral Control with
  Variational Integration for Underactuated Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Alentev, Lev Kozlov, Ivan Domrachev, Simeon Nedelchev, Jee-Hwan Ryu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents VIMPPI, a novel control approach for underactuated double
pendulum systems developed for the AI Olympics competition. We enhance the
Model Predictive Path Integral framework by incorporating variational
integration techniques, enabling longer planning horizons without additional
computational cost. Operating at 500-700 Hz with control interpolation and
disturbance detection mechanisms, VIMPPI substantially outperforms both
baseline methods and alternative MPPI implementations
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Linear Feedback on Smoothed Dynamics Sufficient for Stabilizing
  Contact-Rich Plans? <span class="chip">ICRA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06542v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06542v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Shirai, Tong Zhao, H. J. Terry Suh, Huaijiang Zhu, Xinpei Ni, Jiuguang Wang, Max Simchowitz, Tao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing planners and controllers for contact-rich manipulation is extremely
challenging as contact violates the smoothness conditions that many
gradient-based controller synthesis tools assume. Contact smoothing
approximates a non-smooth system with a smooth one, allowing one to use these
synthesis tools more effectively. However, applying classical control synthesis
methods to smoothed contact dynamics remains relatively under-explored. This
paper analyzes the efficacy of linear controller synthesis using differential
simulators based on contact smoothing. We introduce natural baselines for
leveraging contact smoothing to compute (a) open-loop plans robust to uncertain
conditions and/or dynamics, and (b) feedback gains to stabilize around
open-loop plans. Using robotic bimanual whole-body manipulation as a testbed,
we perform extensive empirical experiments on over 300 trajectories and analyze
why LQR seems insufficient for stabilizing contact-rich plans. The video
summarizing this paper and hardware experiments is found here:
https://youtu.be/HLaKi6qbwQg?si=_zCAmBBD6rGSitm9.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Extractor or Decision Maker: Rethinking the Role of Visual
  Encoders in Visuomotor Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyu Wang, Zheyu Zhuang, Shutong Jin, Nils Ingelhag, Danica Kragic, Florian T. Pokorny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An end-to-end (E2E) visuomotor policy is typically treated as a unified
whole, but recent approaches using out-of-domain (OOD) data to pretrain the
visual encoder have cleanly separated the visual encoder from the network, with
the remainder referred to as the policy. We propose Visual Alignment Testing,
an experimental framework designed to evaluate the validity of this functional
separation. Our results indicate that in E2E-trained models, visual encoders
actively contribute to decision-making resulting from motor data supervision,
contradicting the assumed functional separation. In contrast, OOD-pretrained
models, where encoders lack this capability, experience an average performance
drop of 42\% in our benchmark results, compared to the state-of-the-art
performance achieved by E2E policies. We believe this initial exploration of
visual encoders' role can provide a first step towards guiding future
pretraining methods to address their decision-making ability, such as
developing task-conditioned or context-aware encoders.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ METDrive: Multi-modal End-to-end Autonomous Driving with Temporal
  Guidance <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12667v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12667v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziang Guo, Xinhao Lin, Zakhar Yagudin, Artem Lykov, Yong Wang, Yanqiang Li, Dzmitry Tsetserukou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal end-to-end autonomous driving has shown promising advancements in
recent work. By embedding more modalities into end-to-end networks, the
system's understanding of both static and dynamic aspects of the driving
environment is enhanced, thereby improving the safety of autonomous driving. In
this paper, we introduce METDrive, an end-to-end system that leverages temporal
guidance from the embedded time series features of ego states, including
rotation angles, steering, throttle signals, and waypoint vectors. The
geometric features derived from perception sensor data and the time series
features of ego state data jointly guide the waypoint prediction with the
proposed temporal guidance loss function. We evaluated METDrive on the CARLA
leaderboard benchmarks, achieving a driving score of 70%, a route completion
score of 94%, and an infraction score of 0.78.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdaWorld: Learning Adaptable World Models with Latent Actions <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.18938v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.18938v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenyuan Gao, Siyuan Zhou, Yilun Du, Jun Zhang, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models aim to learn action-controlled future prediction and have proven
essential for the development of intelligent agents. However, most existing
world models rely heavily on substantial action-labeled data and costly
training, making it challenging to adapt to novel environments with
heterogeneous actions through limited interactions. This limitation can hinder
their applicability across broader domains. To overcome this limitation, we
propose AdaWorld, an innovative world model learning approach that enables
efficient adaptation. The key idea is to incorporate action information during
the pretraining of world models. This is achieved by extracting latent actions
from videos in a self-supervised manner, capturing the most critical
transitions between frames. We then develop an autoregressive world model that
conditions on these latent actions. This learning paradigm enables highly
adaptable world models, facilitating efficient transfer and learning of new
actions even with limited interactions and finetuning. Our comprehensive
experiments across multiple environments demonstrate that AdaWorld achieves
superior performance in both simulation quality and visual planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025. Project page: https://adaptable-world-model.github.io/,
  code: https://github.com/Little-Podi/AdaWorld, model:
  https://huggingface.co/Little-Podi/AdaWorld</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UAV-VLPA*: A Vision-Language-Path-Action System for Optimal Route
  Generation on a Large Scales 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02454v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02454v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleg Sautenkov, Aibek Akhmetkazy, Yasheerah Yaqoot, Muhammad Ahsan Mustafa, Grik Tadevosyan, Artem Lykov, Dzmitry Tsetserukou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The UAV-VLPA* (Visual-Language-Planning-and-Action) system represents a
cutting-edge advancement in aerial robotics, designed to enhance communication
and operational efficiency for unmanned aerial vehicles (UAVs). By integrating
advanced planning capabilities, the system addresses the Traveling Salesman
Problem (TSP) to optimize flight paths, reducing the total trajectory length by
18.5\% compared to traditional methods. Additionally, the incorporation of the
A* algorithm enables robust obstacle avoidance, ensuring safe and efficient
navigation in complex environments. The system leverages satellite imagery
processing combined with the Visual Language Model (VLM) and GPT's natural
language processing capabilities, allowing users to generate detailed flight
plans through simple text commands. This seamless fusion of visual and
linguistic analysis empowers precise decision-making and mission planning,
making UAV-VLPA* a transformative tool for modern aerial operations. With its
unmatched operational efficiency, navigational safety, and user-friendly
functionality, UAV-VLPA* sets a new standard in autonomous aerial robotics,
paving the way for future innovations in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2501.05014</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Morphological-Symmetry-Equivariant Heterogeneous Graph Neural Network
  for Robotic Dynamics Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01297v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01297v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengze Xie, Sizhe Wei, Yue Song, Yisong Yue, Lu Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a morphological-symmetry-equivariant heterogeneous graph neural
network, namely MS-HGNN, for robotic dynamics learning, that integrates robotic
kinematic structures and morphological symmetries into a single graph network.
These structural priors are embedded into the learning architecture as
constraints, ensuring high generalizability, sample and model efficiency. The
proposed MS-HGNN is a versatile and general architecture that is applicable to
various multi-body dynamic systems and a wide range of dynamics learning
problems. We formally prove the morphological-symmetry-equivariant property of
our MS-HGNN and validate its effectiveness across multiple quadruped robot
learning problems using both real-world and simulated data. Our code is made
publicly available at https://github.com/lunarlab-gatech/MorphSym-HGNN/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dexterous Contact-Rich Manipulation via the Contact Trust Region 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.02291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.02291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. J. Terry Suh, Tao Pang, Tong Zhao, Russ Tedrake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What is a good local description of contact dynamics for contact-rich
manipulation, and where can we trust this local description? While many
approaches often rely on the Taylor approximation of dynamics with an
ellipsoidal trust region, we argue that such approaches are fundamentally
inconsistent with the unilateral nature of contact. As a remedy, we present the
Contact Trust Region (CTR), which captures the unilateral nature of contact
while remaining efficient for computation. With CTR, we first develop a
Model-Predictive Control (MPC) algorithm capable of synthesizing local
contact-rich plans. Then, we extend this capability to plan globally by
stitching together local MPC plans, enabling efficient and dexterous
contact-rich manipulation. To verify the performance of our method, we perform
comprehensive evaluations, both in high-fidelity simulation and on hardware, on
two contact-rich systems: a planar IiwaBimanual system and a 3D AllegroHand
system. On both systems, our method offers a significantly lower-compute
alternative to existing RL-based approaches to contact-rich manipulation. In
particular, our Allegro in-hand manipulation policy, in the form of a roadmap,
takes fewer than 10 minutes to build offline on a standard laptop using just
its CPU, with online inference taking just a few seconds. Experiment data,
video and code are available at ctr.theaiinstitute.com.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RobotMover: Learning to Move Large Objects From Human Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Li, Joanne Truong, Jimmy Yang, Alexander Clegg, Akshara Rai, Sehoon Ha, Xavier Puig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Moving large objects, such as furniture or appliances, is a critical
capability for robots operating in human environments. This task presents
unique challenges, including whole-body coordination to avoid collisions and
managing the dynamics of bulky, heavy objects. In this work, we present
RobotMover, a learning-based system for large object manipulation that uses
human-object interaction demonstrations to train robot control policies.
RobotMover formulates the manipulation problem as imitation learning using a
simplified spatial representation called the Interaction Chain, which captures
essential interaction dynamics in a way that generalizes across different robot
bodies. We incorporate this Interaction Chain into a reward function and train
policies in simulation using domain randomization to enable zero-shot transfer
to real-world robots. The resulting policies allow a Spot robot to manipulate
various large objects, including chairs, tables, and standing lamps. Through
extensive experiments in both simulation and the real world, we show that
RobotMover achieves strong performance in terms of capability, robustness, and
controllability, outperforming both learned and teleoperation baselines. The
system also supports practical applications by combining learned policies with
simple planning modules to perform long-horizon object transport and
rearrangement tasks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">36</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Learning of Visual Compositional Concepts through Probabilistic
  Schema Induction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Jun Lee, Taylor Webb, Trevor Bihl, Keith Holyoak, Hongjing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to learn new visual concepts from limited examples is a hallmark
of human cognition. While traditional category learning models represent each
example as an unstructured feature vector, compositional concept learning is
thought to depend on (1) structured representations of examples (e.g., directed
graphs consisting of objects and their relations) and (2) the identification of
shared relational structure across examples through analogical mapping. Here,
we introduce Probabilistic Schema Induction (PSI), a prototype model that
employs deep learning to perform analogical mapping over structured
representations of only a handful of examples, forming a compositional concept
called a schema. In doing so, PSI relies on a novel conception of similarity
that weighs object-level similarity and relational similarity, as well as a
mechanism for amplifying relations relevant to classification, analogous to
selective attention parameters in traditional models. We show that PSI produces
human-like learning performance and outperforms two controls: a prototype model
that uses unstructured feature vectors extracted from a deep learning model,
and a variant of PSI with weaker structured representations. Notably, we find
that PSI's human-like performance is driven by an adaptive strategy that
increases relational similarity over object-level similarity and upweights the
contribution of relations that distinguish classes. These findings suggest that
structured representations and analogical mapping are critical to modeling
rapid human-like learning of compositional visual concepts, and demonstrate how
deep learning can be leveraged to create psychological models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Lee, A. J., Webb, T., Bihl, T., Holyoak, K. J., & Lu, H. (2025).
  Few-shot learning of visual compositional concepts through probabilistic
  schema induction. In A. Ruggeri, D. Barner, C. Walker, & N. Bramley (Eds.),
  Proceedings of the 47th Annual Conference of the Cognitive Science Society.
  Cognitive Science Society</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mission Balance: Generating Under-represented Class Samples using Video
  Diffusion Models <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danush Kumar Venkatesh, Isabel Funke, Micha Pfeiffer, Fiona Kolbinger, Hanna Maria Schmeiser, Juergen Weitz, Marius Distler, Stefanie Speidel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer-assisted interventions can improve intra-operative guidance,
particularly through deep learning methods that harness the spatiotemporal
information in surgical videos. However, the severe data imbalance often found
in surgical video datasets hinders the development of high-performing models.
In this work, we aim to overcome the data imbalance by synthesizing surgical
videos. We propose a unique two-stage, text-conditioned diffusion-based method
to generate high-fidelity surgical videos for under-represented classes. Our
approach conditions the generation process on text prompts and decouples
spatial and temporal modeling by utilizing a 2D latent diffusion model to
capture spatial content and then integrating temporal attention layers to
ensure temporal consistency. Furthermore, we introduce a rejection sampling
strategy to select the most suitable synthetic samples, effectively augmenting
existing datasets to address class imbalance. We evaluate our method on two
downstream tasks-surgical action recognition and intra-operative event
prediction-demonstrating that incorporating synthetic videos from our approach
substantially enhances model performance. We open-source our implementation at
https://gitlab.com/nct_tso_public/surgvgen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Early accept at MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImplicitStainer: Data-Efficient Medical Image Translation for Virtual
  Antibody-based Tissue Staining Using Local Implicit Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tushar Kataria, Beatrice Knudsen, Shireen Y. Elhabian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hematoxylin and eosin (H&E) staining is a gold standard for microscopic
diagnosis in pathology. However, H&E staining does not capture all the
diagnostic information that may be needed. To obtain additional molecular
information, immunohistochemical (IHC) stains highlight proteins that mark
specific cell types, such as CD3 for T-cells or CK8/18 for epithelial cells.
While IHC stains are vital for prognosis and treatment guidance, they are
typically only available at specialized centers and time consuming to acquire,
leading to treatment delays for patients. Virtual staining, enabled by deep
learning-based image translation models, provides a promising alternative by
computationally generating IHC stains from H&E stained images. Although many
GAN and diffusion based image to image (I2I) translation methods have been used
for virtual staining, these models treat image patches as independent data
points, which results in increased and more diverse data requirements for
effective generation. We present ImplicitStainer, a novel approach that
leverages local implicit functions to improve image translation, specifically
virtual staining performance, by focusing on pixel-level predictions. This
method enhances robustness to variations in dataset sizes, delivering
high-quality results even with limited data. We validate our approach on two
datasets using a comprehensive set of metrics and benchmark it against over
fifteen state-of-the-art GAN- and diffusion based models. Full Code and models
trained will be released publicly via Github upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image
  Segmentation Performance for Low Data Regimes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tushar Kataria, Shireen Y. Elhabian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obtaining large-scale medical data, annotated or unannotated, is challenging
due to stringent privacy regulations and data protection policies. In addition,
annotating medical images requires that domain experts manually delineate
anatomical structures, making the process both time-consuming and costly. As a
result, semi-supervised methods have gained popularity for reducing annotation
costs. However, the performance of semi-supervised methods is heavily dependent
on the availability of unannotated data, and their effectiveness declines when
such data are scarce or absent. To overcome this limitation, we propose a
simple, yet effective and computationally efficient approach for medical image
segmentation that leverages only existing annotations. We propose BoundarySeg ,
a multi-task framework that incorporates organ boundary prediction as an
auxiliary task to full organ segmentation, leveraging consistency between the
two task predictions to provide additional supervision. This strategy improves
segmentation accuracy, especially in low data regimes, allowing our method to
achieve performance comparable to or exceeding state-of-the-art semi supervised
approaches all without relying on unannotated data or increasing computational
demands. Code will be released upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dyadic Mamba: Long-term Dyadic Human Motion Synthesis <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Tanke, Takashi Shibuya, Kengo Uchida, Koichi Saito, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating realistic dyadic human motion from text descriptions presents
significant challenges, particularly for extended interactions that exceed
typical training sequence lengths. While recent transformer-based approaches
have shown promising results for short-term dyadic motion synthesis, they
struggle with longer sequences due to inherent limitations in positional
encoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach
that leverages State-Space Models (SSMs) to generate high-quality dyadic human
motion of arbitrary length. Our method employs a simple yet effective
architecture that facilitates information flow between individual motion
sequences through concatenation, eliminating the need for complex
cross-attention mechanisms. We demonstrate that Dyadic Mamba achieves
competitive performance on standard short-term benchmarks while significantly
outperforming transformer-based approaches on longer sequences. Additionally,
we propose a new benchmark for evaluating long-term motion synthesis quality,
providing a standardized framework for future research. Our results demonstrate
that SSM-based architectures offer a promising direction for addressing the
challenging task of long-term dyadic human motion synthesis from text
descriptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025 HuMoGen Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Feedback of Pattern Separability Improves Myoelectric Decoding
  Performance of Upper Limb Prostheses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichen Yang, György M. Lévay, Christopher L. Hunt, Dániel Czeiner, Megan C. Hodgson, Damini Agarwal, Rahul R. Kaliki, Nitish V. Thakor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art upper limb myoelectric prostheses often use pattern
recognition (PR) control systems that translate electromyography (EMG) signals
into desired movements. As prosthesis movement complexity increases, users
often struggle to produce sufficiently distinct EMG patterns for reliable
classification. Existing training typically involves heuristic, trial-and-error
user adjustments to static decoder boundaries. Goal: We introduce the Reviewer,
a 3D visual interface projecting EMG signals directly into the decoder's
classification space, providing intuitive, real-time insight into PR algorithm
behavior. This structured feedback reduces cognitive load and fosters mutual,
data-driven adaptation between user-generated EMG patterns and decoder
boundaries. Methods: A 10-session study with 12 able-bodied participants
compared PR performance after motor-based training and updating using the
Reviewer versus conventional virtual arm visualization. Performance was
assessed using a Fitts law task that involved the aperture of the cursor and
the control of orientation. Results: Participants trained with the Reviewer
achieved higher completion rates, reduced overshoot, and improved path
efficiency and throughput compared to the standard visualization group.
Significance: The Reviewer introduces decoder-informed motor training,
facilitating immediate and consistent PR-based myoelectric control
improvements. By iteratively refining control through real-time feedback, this
approach reduces reliance on trial-and-error recalibration, enabling a more
adaptive, self-correcting training framework. Conclusion: The 3D visual
feedback significantly improves PR control in novice operators through
structured training, enabling feedback-driven adaptation and reducing reliance
on extensive heuristic adjustments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the
  Left Atrium 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xabier Morales, Ayah Elsayed, Debbie Zhao, Filip Loncaric, Ainhoa Aguado, Mireia Masias, Gina Quill, Marc Ramos, Ada Doltra, Ana Garcia, Marta Sitges, David Marlevi, Alistair Young, Martyn Nash, Bart Bijnens, Oscar Camara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The left atrium (LA) plays a pivotal role in modulating left ventricular
filling, but our comprehension of its hemodynamics is significantly limited by
the constraints of conventional ultrasound analysis. 4D flow magnetic resonance
imaging (4D Flow MRI) holds promise for enhancing our understanding of atrial
hemodynamics. However, the low velocities within the LA and the limited spatial
resolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,
the absence of dedicated computational frameworks, combined with diverse
acquisition protocols and vendors, complicates gathering large cohorts for
studying the prognostic value of hemodynamic parameters provided by 4D Flow
MRI. In this study, we introduce the first open-source computational framework
tailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive
qualitative and quantitative analysis of advanced hemodynamic parameters. Our
framework proves robust to data from different centers of varying quality,
producing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95
$<$ 3 mm), even with limited training data. Additionally, we conducted the
first comprehensive assessment of energy, vorticity, and pressure parameters in
the LA across a spectrum of disorders to investigate their potential as
prognostic biomarkers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yung-Hsuan Lai, Janek Ebbers, Yu-Chiang Frank Wang, François Germain, Michael Jeffrey Jones, Moitreya Chatterjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing
both uni-modal events (i.e., those occurring exclusively in either the visual
or acoustic modality of a video) and multi-modal events (i.e., those occurring
in both modalities concurrently). Moreover, the prohibitive cost of annotating
training data with the class labels of all these events, along with their start
and end times, imposes constraints on the scalability of AVVP techniques unless
they can be trained in a weakly-supervised setting, where only
modality-agnostic, video-level labels are available in the training data. To
this end, recently proposed approaches seek to generate segment-level
pseudo-labels to better guide model training. However, the absence of
inter-segment dependencies when generating these pseudo-labels and the general
bias towards predicting labels that are absent in a segment limit their
performance. This work proposes a novel approach towards overcoming these
weaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video
Parsing (UWAV). Additionally, our innovative approach factors in the
uncertainty associated with these estimated pseudo-labels and incorporates a
feature mixup based training regularization for improved training. Empirical
results show that UWAV outperforms state-of-the-art methods for the AVVP task
on multiple metrics, across two different datasets, attesting to its
effectiveness and generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LightLab: Controlling Light Sources in Images with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Magar, Amir Hertz, Eric Tabellion, Yael Pritch, Alex Rav-Acha, Ariel Shamir, Yedid Hoshen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple, yet effective diffusion-based method for fine-grained,
parametric control over light sources in an image. Existing relighting methods
either rely on multiple input views to perform inverse rendering at inference
time, or fail to provide explicit control over light changes. Our method
fine-tunes a diffusion model on a small set of real raw photograph pairs,
supplemented by synthetically rendered images at scale, to elicit its
photorealistic prior for relighting. We leverage the linearity of light to
synthesize image pairs depicting controlled light changes of either a target
light source or ambient illumination. Using this data and an appropriate
fine-tuning scheme, we train a model for precise illumination changes with
explicit control over light intensity and color. Lastly, we show how our method
can achieve compelling light editing results, and outperforms existing methods
based on user preference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://nadmag.github.io/LightLab/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Visual Question Answering <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Jan Wieczorek, Nathalie Daun, Mohammad Emtiyaz Khan, Marcus Rohrbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite remarkable progress in multimodal models for Visual Question
Answering (VQA), there remain major reliability concerns because the models can
often be overconfident and miscalibrated, especially in out-of-distribution
(OOD) settings. Plenty has been done to address such issues for unimodal
models, but little work exists for multimodal cases. Here, we address
unreliability in multimodal models by proposing a Variational VQA approach.
Specifically, instead of fine-tuning vision-language models by using AdamW, we
employ a recently proposed variational algorithm called IVON, which yields a
posterior distribution over model parameters. Through extensive experiments, we
show that our approach improves calibration and abstentions without sacrificing
the accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce
Expected Calibration Error by more than 50% compared to the AdamW baseline and
raise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of
distribution shifts, the performance gain is even higher, achieving 8% Coverage
(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we
present variational learning as a viable option to enhance the reliability of
multimodal models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 16 figures, under review at ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Forget your Inverse DDIM for Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillermo Gomez-Trenado, Pablo Mesejo, Oscar Cordón, Stéphane Lathuilière
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of text-to-image generation has undergone significant advancements
with the introduction of diffusion models. Nevertheless, the challenge of
editing real images persists, as most methods are either computationally
intensive or produce poor reconstructions. This paper introduces SAGE
(Self-Attention Guidance for image Editing) - a novel technique leveraging
pre-trained diffusion models for image editing. SAGE builds upon the DDIM
algorithm and incorporates a novel guidance mechanism utilizing the
self-attention layers of the diffusion U-Net. This mechanism computes a
reconstruction objective based on attention maps generated during the inverse
DDIM process, enabling efficient reconstruction of unedited regions without the
need to precisely reconstruct the entire input image. Thus, SAGE directly
addresses the key challenges in image editing. The superiority of SAGE over
other methods is demonstrated through quantitative and qualitative evaluations
and confirmed by a statistically validated comprehensive user study, in which
all 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE
ranks as the top-performing method in seven out of 10 quantitative analyses and
secures second and third places in the remaining three.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 12 figures, code available at
  https://guillermogotre.github.io/sage/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,
  Training and <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, Ran Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unifying image understanding and generation has gained growing attention in
recent research on multimodal models. Although design choices for image
understanding have been extensively studied, the optimal model architecture and
training recipe for a unified framework with image generation remain
underexplored. Motivated by the strong potential of autoregressive and
diffusion models for high-quality generation and scalability, we conduct a
comprehensive study of their use in unified multimodal settings, with emphasis
on image representations, modeling objectives, and training strategies.
Grounded in these investigations, we introduce a novel approach that employs a
diffusion transformer to generate semantically rich CLIP image features, in
contrast to conventional VAE-based representations. This design yields both
higher training efficiency and improved generative quality. Furthermore, we
demonstrate that a sequential pretraining strategy for unified models-first
training on image understanding and subsequently on image generation-offers
practical advantages by preserving image understanding capability while
developing strong image generation ability. Finally, we carefully curate a
high-quality instruction-tuning dataset BLIP3o-60k for image generation by
prompting GPT-4o with a diverse set of captions covering various scenes,
objects, human gestures, and more. Building on our innovative model design,
training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art
unified multimodal models. BLIP3-o achieves superior performance across most of
the popular benchmarks spanning both image understanding and generation tasks.
To facilitate future research, we fully open-source our models, including code,
model weights, training scripts, and pretraining and instruction tuning
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using
  Implicit Neural Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maik Dannecker, Thomas Sanchez, Meritxell Bach Cuadra, Özgün Turgut, Anthony N. Price, Lucilio Cordero-Grande, Vanessa Kyriakopoulou, Joseph V. Hajnal, Daniel Rueckert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution slice-to-volume reconstruction (SVR) from multiple
motion-corrupted low-resolution 2D slices constitutes a critical step in
image-based diagnostics of moving subjects, such as fetal brain Magnetic
Resonance Imaging (MRI). Existing solutions struggle with image artifacts and
severe subject motion or require slice pre-alignment to achieve satisfying
reconstruction performance. We propose a novel SVR method to enable fast and
accurate MRI reconstruction even in cases of severe image and motion
corruption. Our approach performs motion correction, outlier handling, and
super-resolution reconstruction with all operations being entirely based on
implicit neural representations. The model can be initialized with
task-specific priors through fully self-supervised meta-learning on either
simulated or real-world data. In extensive experiments including over 480
reconstructions of simulated and clinical MRI brain data from different
centers, we prove the utility of our method in cases of severe subject motion
and image artifacts. Our results demonstrate improvements in reconstruction
quality, especially in the presence of severe motion, compared to
state-of-the-art methods, and up to 50% reduction in reconstruction time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D
  Cardiac CT Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anne-Marie Rickmann, Stephanie L. Thorn, Shawn S. Ahn, Supum Lee, Selen Uman, Taras Lysyy, Rachel Burns, Nicole Guerrera, Francis G. Spinale, Jason A. Burdick, Albert J. Sinusas, James S. Duncan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac image segmentation is an important step in many cardiac image
analysis and modeling tasks such as motion tracking or simulations of cardiac
mechanics. While deep learning has greatly advanced segmentation in clinical
settings, there is limited work on pre-clinical imaging, notably in porcine
models, which are often used due to their anatomical and physiological
similarity to humans. However, differences between species create a domain
shift that complicates direct model transfer from human to pig data.
  Recently, foundation models trained on large human datasets have shown
promise for robust medical image segmentation; yet their applicability to
porcine data remains largely unexplored. In this work, we investigate whether
foundation models can generate sufficiently accurate pseudo-labels for pig
cardiac CT and propose a simple self-training approach to iteratively refine
these labels. Our method requires no manually annotated pig data, relying
instead on iterative updates to improve segmentation quality. We demonstrate
that this self-training process not only enhances segmentation accuracy but
also smooths out temporal inconsistencies across consecutive frames. Although
our results are encouraging, there remains room for improvement, for example by
incorporating more sophisticated self-training strategies and by exploring
additional foundation models and other cardiac imaging technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at FIMH 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through
  Differentiable Object Shapes <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Marinello, Simen Cassiman, Jonas Heylen, Marc Proesmans, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles need a complete map of their surroundings to plan and
act. This has sparked research into the tasks of 3D occupancy prediction, 3D
scene completion, and 3D panoptic scene completion, which predict a dense map
of the ego vehicle's surroundings as a voxel grid. Scene completion extends
occupancy prediction by predicting occluded regions of the voxel grid, and
panoptic scene completion further extends this task by also distinguishing
object instances within the same class; both aspects are crucial for path
planning and decision-making. However, 3D panoptic scene completion is
currently underexplored. This work introduces a novel framework for 3D panoptic
scene completion that extends existing 3D semantic scene completion models. We
propose an Object Module and Panoptic Module that can easily be integrated with
3D occupancy and scene completion methods presented in the literature. Our
approach leverages the available annotations in occupancy benchmarks, allowing
individual object shapes to be learned as a differentiable problem. The code is
available at https://github.com/nicolamarinello/OffsetOcc .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025 Workshop on Autonomous Driving</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contactless Cardiac Pulse Monitoring Using Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Moustafa, Joseph Lemley, Peter Corcoran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time event cameras are a novel technology for recording scene information at
extremely low latency and with low power consumption. Event cameras output a
stream of events that encapsulate pixel-level light intensity changes within
the scene, capturing information with a higher dynamic range and temporal
resolution than traditional cameras. This study investigates the contact-free
reconstruction of an individual's cardiac pulse signal from time event
recording of their face using a supervised convolutional neural network (CNN)
model. An end-to-end model is trained to extract the cardiac signal from a
two-dimensional representation of the event stream, with model performance
evaluated based on the accuracy of the calculated heart rate. The experimental
results confirm that physiological cardiac information in the facial region is
effectively preserved within the event stream, showcasing the potential of this
novel sensor for remote heart rate monitoring. The model trained on event
frames achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm)
compared to the RMSE of 2.92 bpm achieved by the baseline model trained on
standard camera frames. Furthermore, models trained on event frames generated
at 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an
RMSE of 2.54 and 2.13 bpm, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is a preprint of a paper submitted to IEEE Access and is
  currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal Bounds on Full-Reference Image Quality for Imaging Inverse
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey Wen, Rizwan Ahmad, Philip Schniter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In imaging inverse problems, we would like to know how close the recovered
image is to the true image in terms of full-reference image quality (FRIQ)
metrics like PSNR, SSIM, LPIPS, etc. This is especially important in
safety-critical applications like medical imaging, where knowing that, say, the
SSIM was poor could potentially avoid a costly misdiagnosis. But since we don't
know the true image, computing FRIQ is non-trivial. In this work, we combine
conformal prediction with approximate posterior sampling to construct bounds on
FRIQ that are guaranteed to hold up to a user-specified error probability. We
demonstrate our approach on image denoising and accelerated magnetic resonance
imaging (MRI) problems. Code is available at
https://github.com/jwen307/quality_uq.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI
  Reconstruction based on Multi-directional Time-Frequency Convolutional
  Attention Encoder and Vision-Mamba U-Net 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyi He, Shiyang Li, Bin Jiang, He Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution functional magnetic resonance imaging (fMRI) is essential for
mapping human brain activity; however, it remains costly and logistically
challenging. If comparable volumes could be generated directly from widely
available scalp electroencephalography (EEG), advanced neuroimaging would
become significantly more accessible. Existing EEG-to-fMRI generators rely on
plain CNNs that fail to capture cross-channel time-frequency cues or on heavy
transformer/GAN decoders that strain memory and stability. We propose
Spec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts
these issues via a Multi-directional Time-Frequency Convolutional Attention
Encoder, stacking temporal, spectral and joint convolutions with
self-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space
blocks enable efficient long-range spatial modelling. Trained end-to-end with a
hybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on
three public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball
and 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9%
respectively over previous best SSIM scores. Furthermore, it achieves
competitive PSNR scores, particularly excelling on the CN-EPFL dataset with a
4.6% improvement over the previous best PSNR, thus striking a better balance in
reconstruction quality. The proposed model is lightweight and efficient, making
it suitable for real-time applications in clinical and research settings. The
code is available at https://github.com/hdy6438/Spec2VolCAMU-Net.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low
  Latency and High Throughput 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Zhang, Shuo Li, Runhe Tian, Yang Yang, Jixin Tang, Jinhao Zhou, Lin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Flash-VL 2B, a novel approach to optimizing
Vision-Language Models (VLMs) for real-time applications, targeting ultra-low
latency and high throughput without sacrificing accuracy. Leveraging advanced
architectural enhancements and efficient computational strategies, Flash-VL 2B
is designed to maximize throughput by reducing processing time while
maintaining competitive performance across multiple vision-language benchmarks.
Our approach includes tailored architectural choices, token compression
mechanisms, data curation, training schemes, and a novel image processing
technique called implicit semantic stitching that effectively balances
computational load and model performance. Through extensive evaluations on 11
standard VLM benchmarks, we demonstrate that Flash-VL 2B achieves
state-of-the-art results in both speed and accuracy, making it a promising
solution for deployment in resource-constrained environments and large-scale
real-time applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Denoising and Alignment: Rethinking Domain Generalization for Multimodal
  Face Anti-Spoofing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingjie Ma, Xun Lin, Zitong Yu, Xin Liu, Xiaochen Yuan, Weicheng Xie, Linlin Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face Anti-Spoofing (FAS) is essential for the security of facial recognition
systems in diverse scenarios such as payment processing and surveillance.
Current multimodal FAS methods often struggle with effective generalization,
mainly due to modality-specific biases and domain shifts. To address these
challenges, we introduce the \textbf{M}ulti\textbf{m}odal \textbf{D}enoising
and \textbf{A}lignment (\textbf{MMDA}) framework. By leveraging the zero-shot
generalization capability of CLIP, the MMDA framework effectively suppresses
noise in multimodal data through denoising and alignment mechanisms, thereby
significantly enhancing the generalization performance of cross-modal
alignment. The \textbf{M}odality-\textbf{D}omain Joint \textbf{D}ifferential
\textbf{A}ttention (\textbf{MD2A}) module in MMDA concurrently mitigates the
impacts of domain and modality noise by refining the attention mechanism based
on extracted common noise features. Furthermore, the \textbf{R}epresentation
\textbf{S}pace \textbf{S}oft (\textbf{RS2}) Alignment strategy utilizes the
pre-trained CLIP model to align multi-domain multimodal data into a generalized
representation space in a flexible manner, preserving intricate representations
and enhancing the model's adaptability to various unseen conditions. We also
design a \textbf{U}-shaped \textbf{D}ual \textbf{S}pace \textbf{A}daptation
(\textbf{U-DSA}) module to enhance the adaptability of representations while
maintaining generalization performance. These improvements not only enhance the
framework's generalization capabilities but also boost its ability to represent
complex representations. Our experimental results on four benchmark datasets
under different evaluation protocols demonstrate that the MMDA framework
outperforms existing state-of-the-art methods in terms of cross-domain
generalization and multimodal detection accuracy. The code will be released
soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A 2D Semantic-Aware Position Encoding for Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Chen, Shiyang Zhou, Muqi Huang, Jiaxu Feng, Yun Xiong, Kun Zhou, Biao Yang, Yuhui Zhang, Huishuai Bao, Sijia Peng, Chuan Li, Feng Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers have demonstrated significant advantages in computer
vision tasks due to their ability to capture long-range dependencies and
contextual relationships through self-attention. However, existing position
encoding techniques, which are largely borrowed from natural language
processing, fail to effectively capture semantic-aware positional relationships
between image patches. Traditional approaches like absolute position encoding
and relative position encoding primarily focus on 1D linear position
relationship, often neglecting the semantic similarity between distant yet
contextually related patches. These limitations hinder model generalization,
translation equivariance, and the ability to effectively handle repetitive or
structured patterns in images. In this paper, we propose 2-Dimensional
Semantic-Aware Position Encoding ($\text{SaPE}^2$), a novel position encoding
method with semantic awareness that dynamically adapts position representations
by leveraging local content instead of fixed linear position relationship or
spatial coordinates. Our method enhances the model's ability to generalize
across varying image resolutions and scales, improves translation equivariance,
and better aggregates features for visually similar but spatially distant
patches. By integrating $\text{SaPE}^2$ into vision transformers, we bridge the
gap between position encoding and perceptual similarity, thereby improving
performance on computer vision tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Pixels: Leveraging the Language of Soccer to Improve
  Spatio-Temporal Action Detection in Broadcast Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremie Ochin, Raphael Chekroun, Bogdan Stanciulescu, Sotiris Manitsaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art spatio-temporal action detection (STAD) methods show
promising results for extracting soccer events from broadcast videos. However,
when operated in the high-recall, low-precision regime required for exhaustive
event coverage in soccer analytics, their lack of contextual understanding
becomes apparent: many false positives could be resolved by considering a
broader sequence of actions and game-state information. In this work, we
address this limitation by reasoning at the game level and improving STAD
through the addition of a denoising sequence transduction task. Sequences of
noisy, context-free player-centric predictions are processed alongside clean
game state information using a Transformer-based encoder-decoder model. By
modeling extended temporal context and reasoning jointly over team-level
dynamics, our method leverages the "language of soccer" - its tactical
regularities and inter-player dependencies - to generate "denoised" sequences
of actions. This approach improves both precision and recall in low-confidence
regimes, enabling more reliable event extraction from broadcast video and
complementing existing pixel-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, submitted to Advanced Concepts for Intelligent Vision
  Systems 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating
  Motion during Ultrasound-Guided Aspiration Biopsy <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuelin Zhang, Qingpeng Ding, Long Lei, Yongxuan Feng, Raymond Shing-Yan Tang, Shing Shin Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally
invasive diagnostic procedure. However, an aspiration needle tracker addressing
rapid reciprocating motion is still missing. MrTrack, an aspiration needle
tracker with a mamba-based register mechanism, is proposed. MrTrack leverages a
Mamba-based register extractor to sequentially distill global context from each
historical search map, storing these temporal cues in a register bank. The
Mamba-based register retriever then retrieves temporal prompts from the
register bank to provide external cues when current vision features are
temporarily unusable due to rapid reciprocating motion and imaging degradation.
A self-supervised register diversify loss is proposed to encourage feature
diversity and dimension independence within the learned register, mitigating
feature collapse. Comprehensive experiments conducted on both motorized and
manual aspiration datasets demonstrate that MrTrack not only outperforms
state-of-the-art trackers in accuracy and robustness but also achieves superior
inference efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Early Accepted by MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Endo-CLIP: Progressive <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-train</span>ing on Raw Colonoscopy
  Records <span class="chip">MICCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yili He, Yan Zhu, Peiyao Fu, Ruijie Yang, Tianyi Chen, Zhihua Wang, Quanlin Li, Pinghong Zhou, Xian Yang, Shuo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training on image-text colonoscopy records offers substantial potential
for improving endoscopic image analysis, but faces challenges including
non-informative background images, complex medical terminology, and ambiguous
multi-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised
framework that enhances Contrastive Language-Image Pre-training (CLIP) for this
domain. Endo-CLIP's three-stage framework--cleansing, attunement, and
unification--addresses these challenges by (1) removing background frames, (2)
leveraging large language models to extract clinical attributes for
fine-grained contrastive learning, and (3) employing patient-level
cross-attention to resolve multi-polyp ambiguities. Extensive experiments
demonstrate that Endo-CLIP significantly outperforms state-of-the-art
pre-training methods in zero-shot and few-shot polyp detection and
classification, paving the way for more accurate and clinically relevant
endoscopic analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Early accepted to MICCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient LiDAR Reflectance Compression via Scanning Serialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Zhu, Kang You, Dandan Ding, Zhan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reflectance attributes in LiDAR point clouds provide essential information
for downstream tasks but remain underexplored in neural compression methods. To
address this, we introduce SerLiC, a serialization-based neural compression
framework to fully exploit the intrinsic characteristics of LiDAR reflectance.
SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order
serialization, offering a device-centric perspective for reflectance analysis.
Each point is then tokenized into a contextual representation comprising its
sensor scanning index, radial distance, and prior reflectance, for effective
dependencies exploration. For efficient sequential modeling, Mamba is
incorporated with a dual parallelization scheme, enabling simultaneous
autoregressive dependency capture and fast processing. Extensive experiments
demonstrate that SerLiC attains over 2x volume reduction against the original
reflectance data, outperforming the state-of-the-art method by up to 22%
reduction of compressed bits while using only 2% of its parameters. Moreover, a
lightweight version of SerLiC achieves > 10 fps (frames per second) with just
111K parameters, which is attractive for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Video Highlight Detection by Learning from Audio and Visual
  Recurrence <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahidul Islam, Sujoy Paul, Mrigank Rochan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the exponential growth of video content, the need for automated video
highlight detection to extract key moments or highlights from lengthy videos
has become increasingly pressing. This technology has the potential to enhance
user experiences by allowing quick access to relevant content across diverse
domains. Existing methods typically rely either on expensive manually labeled
frame-level annotations, or on a large external dataset of videos for weak
supervision through category information. To overcome this, we focus on
unsupervised video highlight detection, eliminating the need for manual
annotations. We propose a novel unsupervised approach which capitalizes on the
premise that significant moments tend to recur across multiple videos of the
similar category in both audio and visual modalities. Surprisingly, audio
remains under-explored, especially in unsupervised algorithms, despite its
potential to detect key moments. Through a clustering technique, we identify
pseudo-categories of videos and compute audio pseudo-highlight scores for each
video by measuring the similarities of audio features among audio clips of all
the videos within each pseudo-category. Similarly, we also compute visual
pseudo-highlight scores for each video using visual features. Then, we combine
audio and visual pseudo-highlights to create the audio-visual pseudo
ground-truth highlight of each video for training an audio-visual highlight
detection network. Extensive experiments and ablation studies on three
benchmarks showcase the superior performance of our method over prior work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2025 IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StyleMorpheus: A Style-Based 3D-Aware Morphable Face Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peizhi Yan, Rabab K. Ward, Dan Wang, Qiang Tang, Shan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For 3D face modeling, the recently developed 3D-aware neural rendering
methods are able to render photorealistic face images with arbitrary viewing
directions. The training of the parametric controllable 3D-aware face models,
however, still relies on a large-scale dataset that is lab-collected. To
address this issue, this paper introduces "StyleMorpheus", the first
style-based neural 3D Morphable Face Model (3DMM) that is trained on
in-the-wild images. It inherits 3DMM's disentangled controllability (over face
identity, expression, and appearance) but without the need for accurately
reconstructed explicit 3D shapes. StyleMorpheus employs an auto-encoder
structure. The encoder aims at learning a representative disentangled
parametric code space and the decoder improves the disentanglement using shape
and appearance-related style codes in the different sub-modules of the network.
Furthermore, we fine-tune the decoder through style-based generative
adversarial learning to achieve photorealistic 3D rendering quality. The
proposed style-based design enables StyleMorpheus to achieve state-of-the-art
3D-aware face reconstruction results, while also allowing disentangled control
of the reconstructed face. Our model achieves real-time rendering speed,
allowing its use in virtual reality applications. We also demonstrate the
capability of the proposed style-based design in face editing applications such
as style mixing and color editing. Project homepage:
https://github.com/ubc-3d-vision-lab/StyleMorpheus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, work was completed in 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HaHeAE: Learning Generalisable Joint Representations of Human Hand and
  Head Movements in Extended Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiming Hu, Guanhua Zhang, Zheming Yin, Daniel Haeufle, Syn Schmitt, Andreas Bulling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human hand and head movements are the most pervasive input modalities in
extended reality (XR) and are significant for a wide range of applications.
However, prior works on hand and head modelling in XR only explored a single
modality or focused on specific applications. We present HaHeAE - a novel
self-supervised method for learning generalisable joint representations of hand
and head movements in XR. At the core of our method is an autoencoder (AE) that
uses a graph convolutional network-based semantic encoder and a diffusion-based
stochastic encoder to learn the joint semantic and stochastic representations
of hand-head movements. It also features a diffusion-based decoder to
reconstruct the original signals. Through extensive evaluations on three public
XR datasets, we show that our method 1) significantly outperforms commonly used
self-supervised methods by up to 74.0% in terms of reconstruction quality and
is generalisable across users, activities, and XR environments, 2) enables new
applications, including interpretable hand-head cluster identification and
variable hand-head movement generation, and 3) can serve as an effective
feature extractor for downstream tasks. Together, these results demonstrate the
effectiveness of our method and underline the potential of self-supervised
methods for jointly modelling hand-head behaviours in extended reality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Link: https://zhiminghu.net/hu25_haheae</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Hierarchical World Models as Visual Whole-Body Humanoid Controllers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18418v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18418v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicklas Hansen, Jyothir S V, Vlad Sobal, <span class="highlight-author">Yann LeCun</span>, Xiaolong Wang, Hao Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole-body control for humanoids is challenging due to the high-dimensional
nature of the problem, coupled with the inherent instability of a bipedal
morphology. Learning from visual observations further exacerbates this
difficulty. In this work, we explore highly data-driven approaches to visual
whole-body humanoid control based on reinforcement learning, without any
simplifying assumptions, reward design, or skill primitives. Specifically, we
propose a hierarchical world model in which a high-level agent generates
commands based on visual observations for a low-level agent to execute, both of
which are trained with rewards. Our approach produces highly performant control
policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing
motions that are broadly preferred by humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and videos at https://nicklashansen.com/rlpuppeteer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for
  Embodied Interactive Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, Weiming Lu, Peng Li, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep thinking models have demonstrated remarkable
reasoning capabilities on mathematical and coding tasks. However, their
effectiveness in embodied domains which require continuous interaction with
environments through image action interleaved trajectories remains largely
-unexplored. We present Embodied Reasoner, a model that extends o1 style
reasoning to interactive embodied search tasks. Unlike mathematical reasoning
that relies primarily on logical deduction, embodied scenarios demand spatial
understanding, temporal reasoning, and ongoing self-reflection based on
interaction history. To address these challenges, we synthesize 9.3k coherent
Observation-Thought-Action trajectories containing 64k interactive images and
90k diverse thinking processes (analysis, spatial reasoning, reflection,
planning, and verification). We develop a three-stage training pipeline that
progressively enhances the model's capabilities through imitation learning,
self-exploration via rejection sampling, and self-correction through reflection
tuning. The evaluation shows that our model significantly outperforms those
advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and
Claude-3.7 by +9\%, 24\%, and +13\%. Analysis reveals our model exhibits fewer
repeated searches and logical inconsistencies, with particular advantages in
complex long-horizon tasks. Real-world environments also show our superiority
while exhibiting fewer repeated searches and logical inconsistency cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/zwq2018/embodied_reasoner Dataset:
  https://huggingface.co/datasets/zwq2018/embodied_reasoner</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal-state Dynamics Estimation for Physics-based Human Motion Capture
  from Videos <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07795v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07795v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cuong Le, Viktor Johansson, Manon Kok, Bastian Wandt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion capture from monocular videos has made significant progress in
recent years. However, modern approaches often produce temporal artifacts, e.g.
in form of jittery motion and struggle to achieve smooth and physically
plausible motions. Explicitly integrating physics, in form of internal forces
and exterior torques, helps alleviating these artifacts. Current
state-of-the-art approaches make use of an automatic PD controller to predict
torques and reaction forces in order to re-simulate the input kinematics, i.e.
the joint angles of a predefined skeleton. However, due to imperfect physical
models, these methods often require simplifying assumptions and extensive
preprocessing of the input kinematics to achieve good performance. To this end,
we propose a novel method to selectively incorporate the physics models with
the kinematics observations in an online setting, inspired by a neural
Kalman-filtering approach. We develop a control loop as a meta-PD controller to
predict internal joint torques and external reaction forces, followed by a
physics-based motion simulation. A recurrent neural network is introduced to
realize a Kalman filter that attentively balances the kinematics input and
simulated motion, resulting in an optimal-state dynamics prediction. We show
that this filtering step is crucial to provide an online supervision that helps
balancing the shortcoming of the respective input motions, thus being important
for not only capturing accurate global motion trajectories but also producing
physically plausible human poses. The proposed approach excels in the
physics-based human pose estimation task and demonstrates the physical
plausibility of the predictive dynamics, compared to state of the art. The code
is available on https://github.com/cuongle1206/OSDCap
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figure, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Call to Arms: AI Should be Critical for Social Media Analysis of
  Conflict Zones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00810v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00810v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afia Abedin, Abdul Bais, Cody Buntain, Laura Courchesne, Brian McQuinn, Matthew E. Taylor, Muhib Ullah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The massive proliferation of social media data represents a transformative
opportunity for conflict studies and for tracking the proliferation and use of
weaponry, as conflicts are increasingly documented in these online spaces. At
the same time, the scale and types of data available are problematic for
traditional open-source intelligence. This paper focuses on identifying
specific weapon systems and the insignias of the armed groups using them as
documented in the Ukraine war, as these tasks are critical to operational
intelligence and tracking weapon proliferation, especially given the scale of
international military aid given to Ukraine. The large scale of social media
makes manual assessment difficult, however, so this paper presents early work
that uses computer vision models to support this task. We demonstrate that
these models can both identify weapons embedded in images shared in social
media and how the resulting collection of military-relevant images and their
post times interact with the offline, real-world conflict. Not only can we then
track changes in the prevalence of images of tanks, land mines, military
trucks, etc., we find correlations among time series data associated with these
images and the daily fatalities in this conflict. This work shows substantial
opportunity for examining similar online documentation of conflict contexts,
and we also point to future avenues where computer vision can be further
improved for these open-source intelligence tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ State-of-the-Art Periorbital Distance Prediction and Disease
  Classification Using Periorbital Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18769v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18769v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George R. Nahass, Sasha Hubschman, Jeffrey C. Peterson, Ghasem Yazdanpanah, Nicholas Tomaras, Madison Cheung, Alex Palacios, Kevin Heinze, Chad A. Purnell, Pete Setabutr, Ann Q. Tran, Darvin Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Periorbital distances are critical markers for diagnosing and monitoring a
range of oculoplastic and craniofacial conditions. Manual measurement, however,
is subjective and prone to intergrader variability. Automated methods have been
developed but remain limited by standardized imaging requirements, small
datasets, and a narrow focus on individual measurements. We developed a
segmentation pipeline trained on a domain-specific dataset of healthy eyes and
compared its performance against the Segment Anything Model (SAM) and the prior
benchmark, PeriorbitAI. Segmentation accuracy was evaluated across multiple
disease classes and imaging conditions. We further investigated the use of
predicted periorbital distances as features for disease classification under
in-distribution (ID) and out-of-distribution (OOD) settings, comparing shallow
classifiers, CNNs, and fusion models. Our segmentation model achieved
state-of-the-art accuracy across all datasets, with error rates within
intergrader variability and superior performance relative to SAM and
PeriorbitAI. In classification tasks, models trained on periorbital distances
matched CNN performance on ID data (77--78\% accuracy) and substantially
outperformed CNNs under OOD conditions (63--68\% accuracy vs. 14\%). Fusion
models achieved the highest ID accuracy (80\%) but were sensitive to degraded
CNN features under OOD shifts. Segmentation-derived periorbital distances
provide robust, explainable features for disease classification and generalize
better under domain shift than CNN image classifiers. These results establish a
new benchmark for periorbital distance prediction and highlight the potential
of anatomy-based AI pipelines for real-world deployment in oculoplastic and
craniofacial care.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 12 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HybridMQA: Exploring Geometry-Texture Interactions for Colored Mesh
  Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armin Shafiee Sarvestani, Sheyang Tang, Zhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mesh quality assessment (MQA) models play a critical role in the design,
optimization, and evaluation of mesh operation systems in a wide variety of
applications. Current MQA models, whether model-based methods using
topology-aware features or projection-based approaches working on rendered 2D
projections, often fail to capture the intricate interactions between texture
and 3D geometry. We introduce HybridMQA, a first-of-its-kind hybrid
full-reference colored MQA framework that integrates model-based and
projection-based approaches, capturing complex interactions between textural
information and 3D structures for enriched quality representations. Our method
employs graph learning to extract detailed 3D representations, which are then
projected to 2D using a novel feature rendering process that precisely aligns
them with colored projections. This enables the exploration of geometry-texture
interactions via cross-attention, producing comprehensive mesh quality
representations. Extensive experiments demonstrate HybridMQA's superior
performance across diverse datasets, highlighting its ability to effectively
leverage geometry-texture interactions for a thorough understanding of mesh
quality. Our implementation will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MGPATH: Vision-Language Model with Multi-Granular <span class="highlight-title">Prompt</span> Learning for
  Few-Shot WSI Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07409v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07409v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh-Tien Nguyen, Duy Minh Ho Nguyen, Nghiem Tuong Diep, Trung Quoc Nguyen, Nhat Ho, Jacqueline Michelle Metsch, Miriam Cindy Maurer, Daniel Sonntag, Hanibal Bohnenberger, Anne-Christin Hauschild
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole slide pathology image classification presents challenges due to
gigapixel image sizes and limited annotation labels, hindering model
generalization. This paper introduces a prompt learning method to adapt large
vision-language models for few-shot pathology classification. We first extend
the Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology
image tiles, into a vision-language model by adding adaptors and aligning it
with medical text encoders via contrastive learning on 923K image-text pairs.
The model is then used to extract visual features and text embeddings from
few-shot annotations and fine-tunes with learnable prompt embeddings. Unlike
prior methods that combine prompts with frozen features using prefix embeddings
or self-attention, we propose multi-granular attention that compares
interactions between learnable prompts with individual image patches and groups
of them. This approach improves the model's ability to capture both
fine-grained details and broader context, enhancing its recognition of complex
patterns across sub-regions. To further improve accuracy, we leverage
(unbalanced) optimal transport-based visual-text distance to secure model
robustness by mitigating perturbations that might occur during the data
augmentation process. Empirical experiments on lung, kidney, and breast
pathology modalities validate the effectiveness of our approach; thereby, we
surpass several of the latest competitors and consistently improve performance
across diverse architectures, including CLIP, PLIP, and Prov-GigaPath
integrated PLIP. We release our implementations and pre-trained models at this
MGPATH.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ F$^3$Loc: Fusion and Filtering for Floorplan Localization <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changan Chen, Rui Wang, Christoph Vogel, Marc Pollefeys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose an efficient data-driven solution to
self-localization within a floorplan. Floorplan data is readily available,
long-term persistent and inherently robust to changes in the visual appearance.
Our method does not require retraining per map and location or demand a large
database of images of the area of interest. We propose a novel probabilistic
model consisting of an observation and a novel temporal filtering module.
Operating internally with an efficient ray-based representation, the
observation module consists of a single and a multiview module to predict
horizontal depth from images and fuses their results to benefit from advantages
offered by either methodology. Our method operates on conventional consumer
hardware and overcomes a common limitation of competing methods that often
demand upright images. Our full system meets real-time requirements, while
outperforming the state-of-the-art by a significant margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figure, accepted to CVPR 2024 (fixed typo eq.8: s_x,s_y,
  s_phi -> x, y, phi)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">21</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiDDA: Data Driven Attribution at LinkedIn 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Bencina, Erkut Aykutlug, Yue Chen, Zerui Zhang, Stephanie Sorenson, Shao Tang, Changshuai Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data Driven Attribution, which assigns conversion credits to marketing
interactions based on causal patterns learned from data, is the foundation of
modern marketing intelligence and vital to any marketing businesses and
advertising platform. In this paper, we introduce a unified transformer-based
attribution approach that can handle member-level data, aggregate-level data,
and integration of external macro factors. We detail the large scale
implementation of the approach at LinkedIn, showcasing significant impact. We
also share learning and insights that are broadly applicable to the marketing
and ad tech fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Predictive Optimization and Generation for Business AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyang Zhao, Olurotimi Seton, Himadeep Reddy Reddivari, Suvendu Jena, Shadow Zhao, Rachit Kumar, Changshuai Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sales process involves sales functions converting leads or opportunities
to customers and selling more products to existing customers. The optimization
of the sales process thus is key to success of any B2B business. In this work,
we introduce a principled approach to sales optimization and business AI,
namely the Causal Predictive Optimization and Generation, which includes three
layers: 1) prediction layer with causal ML 2) optimization layer with
constraint optimization and contextual bandit 3) serving layer with Generative
AI and feedback-loop for system enhancement. We detail the implementation and
deployment of the system in LinkedIn, showcasing significant wins over legacy
systems and sharing learning and insight broadly applicable to this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Pairwise Learning-To-Rank At Airbnb 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malay Haldar, Daochen Zha, Huiji Gao, Liwei He, Sanjeev Katariya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are three fundamental asks from a ranking algorithm: it should scale to
handle a large number of items, sort items accurately by their utility, and
impose a total order on the items for logical consistency. But here's the
catch-no algorithm can achieve all three at the same time. We call this
limitation the SAT theorem for ranking algorithms. Given the dilemma, how can
we design a practical system that meets user needs? Our current work at Airbnb
provides an answer, with a working solution deployed at scale. We start with
pairwise learning-to-rank (LTR) models-the bedrock of search ranking tech
stacks today. They scale linearly with the number of items ranked and perform
strongly on metrics like NDCG by learning from pairwise comparisons. They are
at a sweet spot of performance vs. cost, making them an ideal choice for
several industrial applications. However, they have a drawback-by ignoring
interactions between items, they compromise on accuracy. To improve accuracy,
we create a "true" pairwise LTR model-one that captures interactions between
items during pairwise comparisons. But accuracy comes at the expense of
scalability and total order, and we discuss strategies to counter these
challenges. For greater accuracy, we take each item in the search result, and
compare it against the rest of the items along two dimensions: (1) Superiority:
How strongly do searchers prefer the given item over the remaining ones? (2)
Similarity: How similar is the given item to all the other items? This forms
the basis of our "all-pairwise" LTR framework, which factors in interactions
across all items at once. Looking at items on the search result page all
together-superiority and similarity combined-gives us a deeper understanding of
what searchers truly want. We quantify the resulting improvements in searcher
experience through offline and online experiments at Airbnb.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Large Language Models in Multimodal Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejo Lopez-Avila, Jinhua Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal recommender systems (MRS) integrate heterogeneous user and item
data, such as text, images, and structured information, to enhance
recommendation performance. The emergence of large language models (LLMs)
introduces new opportunities for MRS by enabling semantic reasoning, in-context
learning, and dynamic input handling. Compared to earlier pre-trained language
models (PLMs), LLMs offer greater flexibility and generalisation capabilities
but also introduce challenges related to scalability and model accessibility.
This survey presents a comprehensive review of recent work at the intersection
of LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and
data adaptation techniques. We propose a novel taxonomy to characterise
integration patterns, identify transferable techniques from related
recommendation domains, provide an overview of evaluation metrics and datasets,
and point to possible future directions. We aim to clarify the emerging role of
LLMs in multimodal recommendation and support future research in this rapidly
evolving field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of International Collaborations with Highly Publishing
  Countries in Computer Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Gomez Espes, Michael Faerber, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper analyzes international collaborations in Computer Science,
focusing on three major players: China, the European Union, and the United
States. Drawing from a comprehensive literature review, we examine
collaboration patterns, research impact, retraction rates, and the role of the
Development Index in shaping research outcomes. Our findings show that while
China, the EU, and the US lead global research efforts, other regions are
narrowing the gap in publication volume. Collaborations involving these key
regions tend to have lower retraction rates, reflecting stronger adherence to
scientific standards. We also find that countries with a Very High Development
Index contribute to research with higher citation rates and fewer retractions.
Overall, this study highlights the value of international collaboration and the
importance of inclusive, ethical practices in advancing global research in
Computer Science.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distance-aware Self-adaptive Graph Convolution for Fine-grained
  Hierarchical Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Huang, Yihong Chen, Wei Fan, Wei Zhou, Junhao Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Convolutional Networks (GCNs) are widely used to improve recommendation
accuracy and performance by effectively learning the representations of user
and item nodes. However, two major challenges remain: (1) the lack of further
optimization in the graph representation structure and (2) insufficient
attention given to the varying contributions of different convolutional
layers.This paper proposes SAGCN, a distance-based adaptive hierarchical
aggregation method that refines the aggregation process through differentiated
representation metrics. SAGCN introduces a detailed approach to multilayer
information aggregation and representation space optimization, enabling the
model to learn hierarchical embedding weights based on the distance between
hierarchical representations. This innovation allows for more precise
cross-layer information aggregation, improves the model's ability to capture
hierarchical embeddings, and optimizes the representation space structure.
Additionally, the objective loss function is refined to better align with
recommendation tasks.Extensive experiments conducted on four real-world
datasets demonstrate significant improvements, including over a 5% increase on
Yelp and a 5.58% increase in Recall@10 on the ML_1M dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GlobalMood: A cross-cultural benchmark for music emotion recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harin Lee, Elif Çelen, Peter Harrison, Manuel Anglada-Tort, Pol van Rijn, Minsu Park, Marc Schönwiesner, Nori Jacoby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human annotations of mood in music are essential for music generation and
recommender systems. However, existing datasets predominantly focus on Western
songs with mood terms derived from English, which may limit generalizability
across diverse linguistic and cultural backgrounds. To address this, we
introduce `GlobalMood', a novel cross-cultural benchmark dataset comprising
1,180 songs sampled from 59 countries, with large-scale annotations collected
from 2,519 individuals across five culturally and linguistically distinct
locations: U.S., France, Mexico, S. Korea, and Egypt. Rather than imposing
predefined mood categories, we implement a bottom-up, participant-driven
approach to organically elicit culturally specific music-related mood terms. We
then recruit another pool of human participants to collect 988,925 ratings for
these culture-specific descriptors. Our analysis confirms the presence of a
valence-arousal structure shared across cultures, yet also reveals significant
divergences in how certain mood terms, despite being dictionary equivalents,
are perceived cross-culturally. State-of-the-art multimodal models benefit
substantially from fine-tuning on our cross-culturally balanced dataset, as
evidenced by improved alignment with human evaluations - particularly in
non-English contexts. More broadly, our findings inform the ongoing debate on
the universality versus cultural specificity of emotional descriptors, and our
methodology can contribute to other multimodal and cross-lingual research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CXMArena: Unified <span class="highlight-title">Dataset</span> to benchmark performance in realistic CXM
  Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Garg, Kapil Sharma, Karan Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) hold immense potential for revolutionizing
Customer Experience Management (CXM), particularly in contact center
operations. However, evaluating their practical utility in complex operational
environments is hindered by data scarcity (due to privacy concerns) and the
limitations of current benchmarks. Existing benchmarks often lack realism,
failing to incorporate deep knowledge base (KB) integration, real-world noise,
or critical operational tasks beyond conversational fluency. To bridge this
gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset
specifically designed for evaluating AI in operational CXM contexts. Given the
diversity in possible contact center features, we have developed a scalable
LLM-powered pipeline that simulates the brand's CXM entities that form the
foundation of our datasets-such as knowledge articles including product
specifications, issue taxonomies, and contact center conversations. The
entities closely represent real-world distribution because of controlled noise
injection (informed by domain experts) and rigorous automated validation.
Building on this, we release CXMArena, which provides dedicated benchmarks
targeting five important operational tasks: Knowledge Base Refinement, Intent
Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with
Integrated Tools. Our baseline experiments underscore the benchmark's
difficulty: even state of the art embedding and generation models achieve only
68% accuracy on article search, while standard embedding methods yield a low F1
score of 0.3 for knowledge base refinement, highlighting significant challenges
for current models necessitating complex pipelines and solutions over
conventional techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FACTors: A New <span class="highlight-title">Dataset</span> for Studying the Fact-checking Ecosystem <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enes Altuncu, Can Başkent, Sanjay Bhattacherjee, Shujun Li, Dwaipayan Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our fight against false information is spearheaded by fact-checkers. They
investigate the veracity of claims and document their findings as fact-checking
reports. With the rapid increase in the amount of false information circulating
online, the use of automation in fact-checking processes aims to strengthen
this ecosystem by enhancing scalability. Datasets containing fact-checked
claims play a key role in developing such automated solutions. However, to the
best of our knowledge, there is no fact-checking dataset at the ecosystem
level, covering claims from a sufficiently long period of time and sourced from
a wide range of actors reflecting the entire ecosystem that admittedly follows
widely-accepted codes and principles of fact-checking. We present a new dataset
FACTors, the first to fill this gap by presenting ecosystem-level data on
fact-checking. It contains 118,112 claims from 117,993 fact-checking reports in
English (co-)authored by 1,953 individuals and published during the period of
1995-2025 by 39 fact-checking organisations that are active signatories of the
IFCN (International Fact-Checking Network) and/or EFCSN (European Fact-Checking
Standards Network). It contains 7,327 overlapping claims investigated by
multiple fact-checking organisations, corresponding to 2,977 unique claims. It
allows to conduct new ecosystem-level studies of the fact-checkers
(organisations and individuals). To demonstrate the usefulness of FACTors, we
present three example applications, including a first-of-its-kind statistical
analysis of the fact-checking ecosystem, examining the political inclinations
of the fact-checking organisations, and attempting to assign a credibility
score to each organisation based on the findings of the statistical analysis
and political leanings. Our methods for constructing FACTors are generic and
can be used to maintain a live dataset that can be updated dynamically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 48th International ACM SIGIR Conference on Research
  and Development in Information Retrieval (SIGIR '25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Recommender Models and the Illusion of Progress: A Concerning
  Study of Reproducibility and a Conceptual Mismatch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Benigni, Maurizio Ferrari Dacrema, Dietmar Jannach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Countless new machine learning models are published every year and are
reported to significantly advance the state-of-the-art in \emph{top-n}
recommendation. However, earlier reproducibility studies indicate that progress
in this area may be quite limited. Specifically, various widespread
methodological issues, e.g., comparisons with untuned baseline models, have led
to an \emph{illusion of progress}. In this work, our goal is to examine whether
these problems persist in today's research. To this end, we aim to reproduce
the latest advancements reported from applying modern Denoising Diffusion
Probabilistic Models to recommender systems, focusing on four models published
at the top-ranked SIGIR conference in 2023 and 2024. Our findings are
concerning, revealing persistent methodological problems. Alarmingly, through
experiments, we find that the latest recommendation techniques based on
diffusion models, despite their computational complexity and substantial carbon
footprint, are consistently outperformed by simpler existing models.
Furthermore, we identify key mismatches between the characteristics of
diffusion models and those of the traditional \emph{top-n} recommendation task,
raising doubts about their suitability for recommendation. We also note that,
in the papers we analyze, the generative capabilities of these models are
constrained to a minimum. Overall, our results and continued methodological
issues call for greater scientific rigor and a disruptive change in the
research and publication culture in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scent of Knowledge: Optimizing Search-Enhanced Reasoning with
  Information Foraging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjin Qian, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmenting large language models (LLMs) with external retrieval has become a
standard method to address their inherent knowledge cutoff limitations.
However, traditional retrieval-augmented generation methods employ static,
pre-inference retrieval strategies, making them inadequate for complex tasks
involving ambiguous, multi-step, or evolving information needs. Recent advances
in test-time scaling techniques have demonstrated significant potential in
enabling LLMs to dynamically interact with external tools, motivating the shift
toward adaptive inference-time retrieval. Inspired by Information Foraging
Theory (IFT), we propose InForage, a reinforcement learning framework that
formalizes retrieval-augmented reasoning as a dynamic information-seeking
process. Unlike existing approaches, InForage explicitly rewards intermediate
retrieval quality, encouraging LLMs to iteratively gather and integrate
information through adaptive search behaviors. To facilitate training, we
construct a human-guided dataset capturing iterative search and reasoning
trajectories for complex, real-world web tasks. Extensive evaluations across
general question answering, multi-hop reasoning tasks, and a newly developed
real-time web QA dataset demonstrate InForage's superior performance over
baseline methods. These results highlight InForage's effectiveness in building
robust, adaptive, and efficient reasoning agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Focus, Merge, Rank: Improved Question Answering Based on Semi-structured
  Knowledge Bases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Derian Boer, Stephen Roth, Stefan Kramer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world settings, machine learning models and interactive systems
have access to both structured knowledge, e.g., knowledge graphs or tables, and
unstructured content, e.g., natural language documents. However, most rely on
either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking
unstructured content to nodes within structured data, thereby enabling new
strategies for knowledge access and use. In this work, we present
FocusedRetriever, a modular SKB-based framework for multi-hop question
answering. It integrates components (VSS-based entity search, LLM-based
generation of Cypher queries and pairwise re-ranking) in a way that enables it
to outperform state-of-the-art methods across all three STaRK benchmark test
sets, covering diverse domains and multiple performance metrics. The average
first-hit rate exceeds that of the second-best method by 25.7%.
FocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to
extract relational facts and entity attributes from unstructured text, (2) node
set joins to filter answer candidates based on these extracted triplets and
constraints, (3) vector similarity search to retrieve and rank relevant
unstructured content, and (4) the contextual capabilities of LLMs to finally
rank the top-k answers. For generality, we only incorporate base LLMs in
FocusedRetriever in our evaluation. However, our analysis of intermediate
results highlights several opportunities for further upgrades including
finetuning. The source code is publicly available at
https://github.com/kramerlab/FocusedRetriever .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HMamba: Hyperbolic Mamba for Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianru Zhang, Honggang Wen, Wei Yuan, Crystal Chen, Menglin Yang, Siu-Ming Yiu, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation systems have become a cornerstone of personalized
services, adept at modeling the temporal evolution of user preferences by
capturing dynamic interaction sequences. Existing approaches predominantly rely
on traditional models, including RNNs and Transformers. Despite their success
in local pattern recognition, Transformer-based methods suffer from quadratic
computational complexity and a tendency toward superficial attention patterns,
limiting their ability to infer enduring preference hierarchies in sequential
recommendation data. Recent advances in Mamba-based sequential models introduce
linear-time efficiency but remain constrained by Euclidean geometry, failing to
leverage the intrinsic hyperbolic structure of recommendation data. To bridge
this gap, we propose Hyperbolic Mamba, a novel architecture that unifies the
efficiency of Mamba's selective state space mechanism with hyperbolic
geometry's hierarchical representational power. Our framework introduces (1) a
hyperbolic selective state space that maintains curvature-aware sequence
modeling and (2) stabilized Riemannian operations to enable scalable training.
Experiments across four benchmarks demonstrate that Hyperbolic Mamba achieves
3-11% improvement while retaining Mamba's linear-time efficiency, enabling
real-world deployment. This work establishes a new paradigm for efficient,
hierarchy-aware sequential modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Display Content, Display Methods and Evaluation Methods of the HCI in
  Explainable Recommender Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiqing Li, Yue Xu, Yuefeng Li, Yinghui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Recommender Systems (XRS) aim to provide users with
understandable reasons for the recommendations generated by these systems,
representing a crucial research direction in artificial intelligence (AI).
Recent research has increasingly focused on the algorithms, display, and
evaluation methodologies of XRS. While current research and reviews primarily
emphasize the algorithmic aspects, with fewer studies addressing the
Human-Computer Interaction (HCI) layer of XRS. Additionally, existing reviews
lack a unified taxonomy for XRS and there is insufficient attention given to
the emerging area of short video recommendations. In this study, we synthesize
existing literature and surveys on XRS, presenting a unified framework for its
research and development. The main contributions are as follows: 1) We adopt a
lifecycle perspective to systematically summarize the technologies and methods
used in XRS, addressing challenges posed by the diversity and complexity of
algorithmic models and explanation techniques. 2) For the first time, we
highlight the application of multimedia, particularly video-based explanations,
along with its potential, technical pathways, and challenges in XRS. 3) We
provide a structured overview of evaluation methods from both qualitative and
quantitative dimensions. These findings provide valuable insights for the
systematic design, progress, and testing of XRS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 Tables, 29 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Item Level Exploration Traffic Allocation in Large-scale Recommendation
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Wang, Junyi Jiao, Arnab Bhadury, Yaping Zhang, Mingyan Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper contributes to addressing the item cold start problem in
large-scale recommender systems, focusing on how to efficiently gain initial
visibility for newly ingested content. We propose an exploration system
designed to efficiently allocate impressions to these fresh items. Our approach
leverages a learned probabilistic model to predict an item's discoverability,
which then informs a scalable and adaptive traffic allocation strategy. This
system intelligently distributes exploration budgets, optimizing for the
long-term benefit of the recommendation platform. The impact is a demonstrably
more efficient cold-start process, leading to a significant increase in the
discoverability of new content and ultimately enriching the item corpus
available for exploitation, as evidenced by its successful deployment in a
large-scale production environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by the 18th ACM Recsys Large Recsys Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TARGET: Benchmarking Table Retrieval for Generative Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Ji, Parker Glenn, Aditya G. Parameswaran, Madelon Hulsebos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The data landscape is rich with structured data, often of high value to
organizations, driving important applications in data analysis and machine
learning. Recent progress in representation learning and generative models for
such data has led to the development of natural language interfaces to
structured data, including those leveraging text-to-SQL. Contextualizing
interactions, either through conversational interfaces or agentic components,
in structured data through retrieval-augmented generation can provide
substantial benefits in the form of freshness, accuracy, and comprehensiveness
of answers. The key question is: how do we retrieve the right table(s) for the
analytical query or task at hand? To this end, we introduce TARGET: a benchmark
for evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the
retrieval performance of different retrievers in isolation, as well as their
impact on downstream tasks. We find that dense embedding-based retrievers far
outperform a BM25 baseline which is less effective than it is for retrieval
over unstructured text. We also surface the sensitivity of retrievers across
various metadata (e.g., missing table titles), and demonstrate a stark
variation of retrieval performance across datasets and tasks. TARGET is
available at https://target-benchmark.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Goal-Conditioned Supervised Learning for Multi-Objective Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08911v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08911v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijun Li, Hilaf Hasson, Jing Hu, Joydeep Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective learning endeavors to concurrently optimize multiple
objectives using a single model, aiming to achieve high and balanced
performance across diverse objectives. However, this often entails a more
complex optimization problem, particularly when navigating potential conflicts
between objectives, leading to solutions with higher memory requirements and
computational complexity. This paper introduces a Multi-Objective
Goal-Conditioned Supervised Learning (MOGCSL) framework for automatically
learning to achieve multiple objectives from offline sequential data. MOGCSL
extends the conventional GCSL method to multi-objective scenarios by redefining
goals from one-dimensional scalars to multi-dimensional vectors. It benefits
from naturally eliminating the need for complex architectures and optimization
constraints. Moreover, MOGCSL effectively filters out uninformative or noisy
instances that fail to achieve desirable long-term rewards across multiple
objectives. We also introduces a novel goal-selection algorithm for MOGCSL to
model and identify "high" achievable goals for inference.
  While MOGCSL is quite general, we focus on its application to the next action
prediction problem in commercial-grade recommender systems. In this context,
any viable solution needs to be reasonably scalable and also be robust to large
amounts of noisy data that is characteristic of this application space. We show
that MOGCSL performs admirably on both counts by extensive experiments on
real-world recommendation datasets. Also, analysis and experiments are included
to explain its strength in discounting the noisier portions of training data in
recommender systems with multiple objectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OAEI-LLM-T: A TBox Benchmark <span class="highlight-title">Dataset</span> for Understanding Large Language
  Model Hallucinations in Ontology Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21813v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21813v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Kerry Taylor, Weiqing Wang, Jing Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations are often inevitable in downstream tasks using large language
models (LLMs). To tackle the substantial challenge of addressing hallucinations
for LLM-based ontology matching (OM) systems, we introduce a new benchmark
dataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the
Ontology Alignment Evaluation Initiative (OAEI), capturing hallucinations of
ten different LLMs performing OM tasks. These OM-specific hallucinations are
organised into two primary categories and six sub-categories. We showcase the
usefulness of the dataset in constructing an LLM leaderboard for OM tasks and
for fine-tuning LLMs used in OM tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, 4 tables, 2 prompt templates</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Overlap Ratio in Defocused Electron Ptychography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00762v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00762v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirafshar Moshtaghpour, Angus I. Kirkland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Four-dimensional Scanning Transmission Electron Microscopy (4D STEM) with
data acquired using a defocused electron probe is a promising tool for
characterising complex biological specimens and materials through a phase
retrieval process known as Electron Ptychography (EP). The efficacy of 4D STEM
acquisition and the resulting quality of EP reconstruction depends on the
overlap ratio of adjacent illuminated areas. This paper demonstrates how the
overlap ratio impacts the data redundancy and the quality of the EP
reconstruction. We define two quantities as a function of the overlap ratio
that are independent of both the object and the EP algorithm. Subsequently, we
evaluate an EP algorithm for varying overlap ratios using simulated 4D STEM
datasets. Notably, a 40% or greater overlap ratio yields stable, high-quality
reconstructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deconstructing Jazz Piano Style Using Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huw Cheston, Reuben Bance, Peter M. C. Harrison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artistic style has been studied for centuries, and recent advances in machine
learning create new possibilities for understanding it computationally.
However, ensuring that machine-learning models produce insights aligned with
the interests of practitioners and critics remains a significant challenge.
Here, we focus on musical style, which benefits from a rich theoretical and
mathematical analysis tradition. We train a variety of supervised-learning
models to identify 20 iconic jazz musicians across a carefully curated dataset
of 84 hours of recordings, and interpret their decision-making processes. Our
models include a novel multi-input architecture that enables four musical
domains (melody, harmony, rhythm, and dynamics) to be analysed separately.
These models enable us to address fundamental questions in music theory and
also advance the state-of-the-art in music performer identification (94%
accuracy across 20 classes). We release open-source implementations of our
models and an accompanying web application for exploring musical styles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper: 40 pages, 11 figures, 1 table; added information on training
  time + computation cost, corrections to Table 1. Supplementary material: 33
  pages, 48 figures, 6 tables; corrections to Table S.5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Signed Latent Factors for Spamming Activity Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.13814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.13814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuli Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the increasing trend of performing spamming activities (e.g., Web
spam, deceptive reviews, fake followers, etc.) on various online platforms to
gain undeserved benefits, spam detection has emerged as a hot research issue.
Previous attempts to combat spam mainly employ features related to metadata,
user behaviors, or relational ties. These studies have made considerable
progress in understanding and filtering spamming campaigns. However, this
problem remains far from fully solved. Almost all the proposed features focus
on a limited number of observed attributes or explainable phenomena, making it
difficult for existing methods to achieve further improvement. To broaden the
vision about solving the spam problem and address long-standing challenges
(class imbalance and graph incompleteness) in the spam detection area, we
propose a new attempt of utilizing signed latent factors to filter fraudulent
activities. The spam-contaminated relational datasets of multiple online
applications in this scenario are interpreted by the unified signed network.
Two competitive and highly dissimilar algorithms of latent factors mining (LFM)
models are designed based on multi-relational likelihoods estimation (LFM-MRLE)
and signed pairwise ranking (LFM-SPR), respectively. We then explore how to
apply the mined latent factors to spam detection tasks. Experiments on
real-world datasets of different kinds of Web applications (social media and
Web forum) indicate that LFM models outperform state-of-the-art baselines in
detecting spamming activities. By specifically manipulating experimental data,
the effectiveness of our methods in dealing with incomplete and imbalanced
challenges is validated.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-05-13T00:00:00Z">2025-05-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">42</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JcvPCA and JsvCRP : a set of metrics to evaluate changes in joint
  coordination strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Océane Dubois, Agnès Roby-Brami, Ross Parry, Nathanaël Jarrassé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characterizing changes in inter-joint coordination presents significant
challenges, as it necessitates the examination of relationships between
multiple degrees of freedom during movements and their temporal evolution.
Existing metrics are inadequate in providing physiologically coherent results
that document both the temporal and spatial aspects of inter-joint
coordination. In this article, we introduce two novel metrics to enhance the
analysis of inter-joint coordination. The first metric, Joint Contribution
Variation based on Principal Component Analysis (JcvPCA), evaluates the
variation in each joint's contribution during series of movements. The second
metric, Joint Synchronization Variation based on Continuous Relative Phase
(JsvCRP), measures the variation in temporal synchronization among joints
between two movement datasets. We begin by presenting each metric and
explaining their derivation. We then demonstrate the application of these
metrics using simulated and experimental datasets involving identical movement
tasks performed with distinct coordination strategies. The results show that
these metrics can successfully differentiate between unique coordination
strategies, providing meaningful insights into joint collaboration during
movement. These metrics hold significant potential for fields such as
ergonomics and clinical rehabilitation, where a precise understanding of the
evolution of inter-joint coordination strategies is crucial. Potential
applications include evaluating the effects of upper limb exoskeletons in
industrial settings or monitoring the progress of patients undergoing
neurological rehabilitation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ardian Selmonaj, Oleg Szehr, Giacomo Del Rio, Alessandro Antonucci, Adrian Schneider, Michael Rüegsegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a Hierarchical Multi-Agent Reinforcement Learning
framework for analyzing simulated air combat scenarios involving heterogeneous
agents. The objective is to identify effective Courses of Action that lead to
mission success within preset simulations, thereby enabling the exploration of
real-world defense scenarios at low cost and in a safe-to-fail setting.
Applying deep Reinforcement Learning in this context poses specific challenges,
such as complex flight dynamics, the exponential size of the state and action
spaces in multi-agent systems, and the capability to integrate real-time
control of individual units with look-ahead planning. To address these
challenges, the decision-making process is split into two levels of
abstraction: low-level policies control individual units, while a high-level
commander policy issues macro commands aligned with the overall mission
targets. This hierarchical structure facilitates the training process by
exploiting policy symmetries of individual agents and by separating control
from command tasks. The low-level policies are trained for individual combat
control in a curriculum of increasing complexity. The high-level commander is
then trained on mission targets given pre-trained control policies. The
empirical validation confirms the advantages of the proposed framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as journal chapter in Deep Learning Applications, Vol. 1,
  by Taylor & Francis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control
  for Delicate, Irregular Bio-products Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirreza Davar, Zhengtong Xu, Siavash Mahmoudi, Pouya Sohrabipour, Chaitanya Pallerla, Yu She, Wan Shou, Philip Crandall, Dongyi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated poultry processing lines still rely on humans to lift slippery,
easily bruised carcasses onto a shackle conveyor. Deformability, anatomical
variance, and strict hygiene rules make conventional suction and scripted
motions unreliable. We present ChicGrasp, an end--to--end hardware--software
co-design for this task. An independently actuated dual-jaw pneumatic gripper
clamps both chicken legs, while a conditional diffusion-policy controller,
trained from only 50 multi--view teleoperation demonstrations (RGB +
proprioception), plans 5 DoF end--effector motion, which includes jaw commands
in one shot. On individually presented raw broiler carcasses, our system
achieves a 40.6\% grasp--and--lift success rate and completes the pick to
shackle cycle in 38 s, whereas state--of--the--art implicit behaviour cloning
(IBC) and LSTM-GMM baselines fail entirely. All CAD, code, and datasets will be
open-source. ChicGrasp shows that imitation learning can bridge the gap between
rigid hardware and variable bio--products, offering a reproducible benchmark
and a public dataset for researchers in agricultural engineering and robot
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for journal review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-step manipulation task and motion planning guided by video
  demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kateryna Zorina, David Kovar, Mederic Fourmy, Florent Lamiraux, Nicolas Mansard, Justin Carpentier, Josef Sivic, Vladimir Petrik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work aims to leverage instructional video to solve complex multi-step
task-and-motion planning tasks in robotics. Towards this goal, we propose an
extension of the well-established Rapidly-Exploring Random Tree (RRT) planner,
which simultaneously grows multiple trees around grasp and release states
extracted from the guiding video. Our key novelty lies in combining contact
states and 3D object poses extracted from the guiding video with a traditional
planning algorithm that allows us to solve tasks with sequential dependencies,
for example, if an object needs to be placed at a specific location to be
grasped later. We also investigate the generalization capabilities of our
approach to go beyond the scene depicted in the instructional video. To
demonstrate the benefits of the proposed video-guided planning approach, we
design a new benchmark with three challenging tasks: (I) 3D re-arrangement of
multiple objects between a table and a shelf, (ii) multi-step transfer of an
object through a tunnel, and (iii) transferring objects using a tray similar to
a waiter transfers dishes. We demonstrate the effectiveness of our planning
algorithm on several robots, including the Franka Emika Panda and the KUKA KMR
iiwa. For a seamless transfer of the obtained plans to the real robot, we
develop a trajectory refinement approach formulated as an optimal control
problem (OCP).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest
  Floor Segmentation from UAV Imagery <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Wasil, Ahmad Drak, Brennan Penfold, Ludovico Scarton, Maximilian Johenneken, Alexander Asteroth, Sebastian Houben
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation and
forest monitoring, including seed dispersal in hard-to-reach terrains. However,
a detailed understanding of the forest floor remains a challenge due to high
natural variability, quickly changing environmental parameters, and ambiguous
annotations due to unclear definitions. To address this issue, we adapt the
Segment Anything Model (SAM), a vision foundation model with strong
generalization capabilities, to segment forest floor objects such as tree
stumps, vegetation, and woody debris. To this end, we employ
parameter-efficient fine-tuning (PEFT) to fine-tune a small subset of
additional model parameters while keeping the original weights fixed. We adjust
SAM's mask decoder to generate masks corresponding to our dataset categories,
allowing for automatic segmentation without manual prompting. Our results show
that the adapter-based PEFT method achieves the highest mean intersection over
union (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers a
lightweight alternative for resource-constrained UAV platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Novel Approaches for Precision Agriculture and
  Forestry with Autonomous Robots IEEE ICRA Workshop - 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep reinforcement learning-based longitudinal control strategy for
  automated vehicles at signalised intersections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pankaj Kumar, Aditya Mishra, Pranamesh Chakraborty, Subrahmanya Swamy Peruru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing an autonomous vehicle control strategy for signalised
intersections (SI) is one of the challenging tasks due to its inherently
complex decision-making process. This study proposes a Deep Reinforcement
Learning (DRL) based longitudinal vehicle control strategy at SI. A
comprehensive reward function has been formulated with a particular focus on
(i) distance headway-based efficiency reward, (ii) decision-making criteria
during amber light, and (iii) asymmetric acceleration/ deceleration response,
along with the traditional safety and comfort criteria. This reward function
has been incorporated with two popular DRL algorithms, Deep Deterministic
Policy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the
continuous action space of acceleration/deceleration. The proposed models have
been trained on the combination of real-world leader vehicle (LV) trajectories
and simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process.
The overall performance of the proposed models has been tested using Cumulative
Distribution Function (CDF) plots and compared with the real-world trajectory
data. The results show that the RL models successfully maintain lower distance
headway (i.e., higher efficiency) and jerk compared to human-driven vehicles
without compromising safety. Further, to assess the robustness of the proposed
models, we evaluated the model performance on diverse safety-critical
scenarios, in terms of car-following and traffic signal compliance. Both DDPG
and SAC models successfully handled the critical scenarios, while the DDPG
model showed smoother action profiles compared to the SAC model. Overall, the
results confirm that DRL-based longitudinal vehicle control strategy at SI can
help to improve traffic safety, efficiency, and comfort.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Capable Learning-based Visual Tool Pose Correction via
  Differentiable Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyuan Yang, Zonghe Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomy in Minimally Invasive Robotic Surgery (MIRS) has the potential to
reduce surgeon cognitive and task load, thereby increasing procedural
efficiency. However, implementing accurate autonomous control can be difficult
due to poor end-effector proprioception, a limitation of their cable-driven
mechanisms. Although the robot may have joint encoders for the end-effector
pose calculation, various non-idealities make the entire kinematics chain
inaccurate. Modern vision-based pose estimation methods lack real-time
capability or can be hard to train and generalize. In this work, we demonstrate
a real-time capable, vision transformer-based pose estimation approach that is
trained using end-to-end differentiable kinematics and rendering in simulation.
We demonstrate the potential of this method to correct for noisy pose estimates
in simulation, with the longer term goal of verifying the sim-to-real
transferability of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative AI for Autonomous Driving: Frontiers and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuping Wang, Shuo Xing, Cui Can, Renjie Li, Hongyuan Hua, Kexin Tian, Zhaobin Mo, Xiangbo Gao, Keshu Wu, Sulong Zhou, Hengxu You, Juntong Peng, Junge Zhang, Zehao Wang, Rui Song, Mingxuan Yan, Walter Zimmer, Xingcheng Zhou, Peiran Li, Zhaohan Lu, Chia-Ju Chen, Yue Huang, Ryan A. Rossi, Lichao Sun, Hongkai Yu, Zhiwen Fan, Frank Hao Yang, Yuhao Kang, Ross Greer, Chenxi Liu, Eun Hak Lee, Xuan Di, Xinyue Ye, Liu Ren, Alois Knoll, Xiaopeng Li, Shuiwang Ji, Masayoshi Tomizuka, Marco Pavone, Tianbao Yang, Jing Du, Ming-Hsuan Yang, Hua Wei, Ziran Wang, Yang Zhou, Jiachen Li, Zhengzhong Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Artificial Intelligence (GenAI) constitutes a transformative
technological wave that reconfigures industries through its unparalleled
capabilities for content creation, reasoning, planning, and multimodal
understanding. This revolutionary force offers the most promising path yet
toward solving one of engineering's grandest challenges: achieving reliable,
fully autonomous driving, particularly the pursuit of Level 5 autonomy. This
survey delivers a comprehensive and critical synthesis of the emerging role of
GenAI across the autonomous driving stack. We begin by distilling the
principles and trade-offs of modern generative modeling, encompassing VAEs,
GANs, Diffusion Models, and Large Language Models (LLMs). We then map their
frontier applications in image, LiDAR, trajectory, occupancy, video generation
as well as LLM-guided reasoning and decision making. We categorize practical
applications, such as synthetic data workflows, end-to-end driving strategies,
high-fidelity digital twin systems, smart transportation networks, and
cross-domain transfer to embodied AI. We identify key obstacles and
possibilities such as comprehensive generalization across rare cases,
evaluation and safety checks, budget-limited implementation, regulatory
compliance, ethical concerns, and environmental effects, while proposing
research plans across theoretical assurances, trust metrics, transport
integration, and socio-technical influence. By unifying these threads, the
survey provides a forward-looking reference for researchers, engineers, and
policymakers navigating the convergence of generative AI and advanced
autonomous mobility. An actively maintained repository of cited works is
available at https://github.com/taco-group/GenAI4AD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficiently Manipulating Clutter via Learning and Search-Based Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baichuan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis presents novel algorithms to advance robotic object
rearrangement, a critical task for autonomous systems in applications like
warehouse automation and household assistance. Addressing challenges of
high-dimensional planning, complex object interactions, and computational
demands, our work integrates deep learning for interaction prediction, tree
search for action sequencing, and parallelized computation for efficiency. Key
contributions include the Deep Interaction Prediction Network (DIPN) for
accurate push motion forecasting (over 90% accuracy), its synergistic
integration with Monte Carlo Tree Search (MCTS) for effective non-prehensile
object retrieval (100% completion in specific challenging scenarios), and the
Parallel MCTS with Batched Simulations (PMBS) framework, which achieves
substantial planning speed-up while maintaining or improving solution quality.
The research further explores combining diverse manipulation primitives,
validated extensively through simulated and real-world experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD Thesis of Baichuan Huang, written under the direction of Prof.
  Jingjin Yu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Trajectory Planning with Collision Avoidance for Autonomous
  Vehicle Maneuvering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Zalev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To perform autonomous driving maneuvers, such as parallel or perpendicular
parking, a vehicle requires continual speed and steering adjustments to follow
a generated path. In consequence, the path's quality is a limiting factor of
the vehicle maneuver's performance. While most path planning approaches include
finding a collision-free route, optimal trajectory planning involves solving
the best transition from initial to final states, minimizing the action over
all paths permitted by a kinematic model. Here we propose a novel method based
on sequential convex optimization, which permits flexible and efficient optimal
trajectory generation. The objective is to achieve the fastest time, shortest
distance, and fewest number of path segments to satisfy motion requirements,
while avoiding sensor blind-spots. In our approach, vehicle kinematics are
represented by a discretized Dubins model. To avoid collisions, each waypoint
is constrained by linear inequalities representing closest distance of
obstacles to a polygon specifying the vehicle's extent. To promote smooth and
valid trajectories, the solved kinematic state and control variables are
constrained and regularized by penalty terms in the model's cost function,
which enforces physical restrictions including limits for steering angle,
acceleration and speed. In this paper, we analyze trajectories obtained for
several parking scenarios. Results demonstrate efficient and collision-free
motion generated by the proposed technique.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIAM Conference on Control and Its Applications, July 28-30th, 2025,
  Montreal, Canada</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged
  Information Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhe Cai, Jiaqi Peng, Yuqiang Yang, Yujian Zhang, Meng Wei, Hanqing Wang, Yilun Chen, Tai Wang, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning navigation in dynamic open-world environments is an important yet
challenging skill for robots. Most previous methods rely on precise
localization and mapping or learn from expensive real-world demonstrations. In
this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end
framework trained solely in simulation and can zero-shot transfer to different
embodiments in diverse real-world environments. The key ingredient of NavDP's
network is the combination of diffusion-based trajectory generation and a
critic function for trajectory selection, which are conditioned on only local
observation tokens encoded from a shared policy transformer. Given the
privileged information of the global environment in simulation, we scale up the
demonstrations of good quality to train the diffusion policy and formulate the
critic value function targets with contrastive negative samples. Our
demonstration generation approach achieves about 2,500 trajectories/GPU per
day, 20$\times$ more efficient than real-world data collection, and results in
a large-scale navigation dataset with 363.2km trajectories across 1244 scenes.
Trained with this simulation dataset, NavDP achieves state-of-the-art
performance and consistently outstanding generalization capability on
quadruped, wheeled, and humanoid robots in diverse indoor and outdoor
environments. In addition, we present a preliminary attempt at using Gaussian
Splatting to make in-domain real-to-sim fine-tuning to further bridge the
sim-to-real gap. Experiments show that adding such real-to-sim data can improve
the success rate by 30\% without hurting its generalization capability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Social Robot with Inner Speech for Dietary Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valerio Belcamino, Alessandro Carfì, Valeria Seidita, Fulvio Mastrogiovanni, Antonio Chella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the use of inner speech as a mechanism to enhance transparency and
trust in social robots for dietary advice. In humans, inner speech structures
thought processes and decision-making; in robotics, it improves explainability
by making reasoning explicit. This is crucial in healthcare scenarios, where
trust in robotic assistants depends on both accurate recommendations and
human-like dialogue, which make interactions more natural and engaging.
Building on this, we developed a social robot that provides dietary advice, and
we provided the architecture with inner speech capabilities to validate user
input, refine reasoning, and generate clear justifications. The system
integrates large language models for natural language understanding and a
knowledge graph for structured dietary information. By making decisions more
transparent, our approach strengthens trust and improves human-robot
interaction in healthcare. We validated this by measuring the computational
efficiency of our architecture and conducting a small user study, which
assessed the reliability of inner speech in explaining the robot's behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of Human Activity Recognition: Motion, Tactile, and
  multi-modal Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valerio Belcamino, Nhat Minh Dinh Le, Quan Khanh Luu, Alessandro Carfì, Van Anh Ho, Fulvio Mastrogiovanni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human activity recognition (HAR) is essential for effective Human-Robot
Collaboration (HRC), enabling robots to interpret and respond to human actions.
This study evaluates the ability of a vision-based tactile sensor to classify
15 activities, comparing its performance to an IMU-based data glove.
Additionally, we propose a multi-modal framework combining tactile and motion
data to leverage their complementary strengths. We examined three approaches:
motion-based classification (MBC) using IMU data, tactile-based classification
(TBC) with single or dual video streams, and multi-modal classification (MMC)
integrating both. Offline validation on segmented datasets assessed each
configuration's accuracy under controlled conditions, while online validation
on continuous action sequences tested online performance. Results showed the
multi-modal approach consistently outperformed single-modality methods,
highlighting the potential of integrating tactile and motion sensing to enhance
HAR systems for collaborative robotics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Holly Dinkel, Marcel Büsching, Alberta Longhini, Brian Coltin, Trey Smith, Danica Kragic, Mårten Björkman, Timothy Bretl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents DLO-Splatting, an algorithm for estimating the 3D shape of
Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state
information through prediction-update filtering. The DLO-Splatting algorithm
uses a position-based dynamics model with shape smoothness and rigidity
dampening corrections to predict the object shape. Optimization with a 3D
Gaussian Splatting-based rendering loss iteratively renders and refines the
prediction to align it with the visual observations in the update step. Initial
experiments demonstrate promising results in a knot tying scenario, which is
challenging for existing vision-only methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, presented at the 2025 5th Workshop: Reflections
  on Representations and Manipulating Deformable Objects at the IEEE
  International Conference on Robotics and Automation. RMDO workshop
  (https://deformable-workshop.github.io/icra2025/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmented Reality for RObots (ARRO): Pointing Visuomotor Policies
  Towards Visual Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reihaneh Mirjalili, Tobias Jülg, Florian Walter, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visuomotor policies trained on human expert demonstrations have recently
shown strong performance across a wide range of robotic manipulation tasks.
However, these policies remain highly sensitive to domain shifts stemming from
background or robot embodiment changes, which limits their generalization
capabilities. In this paper, we present ARRO, a novel calibration-free visual
representation that leverages zero-shot open-vocabulary segmentation and object
detection models to efficiently mask out task-irrelevant regions of the scene
without requiring additional training. By filtering visual distractors and
overlaying virtual guides during both training and inference, ARRO improves
robustness to scene variations and reduces the need for additional data
collection. We extensively evaluate ARRO with Diffusion Policy on several
tabletop manipulation tasks in both simulation and real-world environments, and
further demonstrate its compatibility and effectiveness with generalist robot
policies, such as Octo and OpenVLA. Across all settings in our evaluation, ARRO
yields consistent performance gains, allows for selective masking to choose
between different objects, and shows robustness even to challenging
segmentation conditions. Videos showcasing our results are available at:
augmented-reality-for-robots.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Predefined Actions: Integrating Behavior Trees and Dynamic
  Movement Primitives for Robot Learning from Demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Cáceres Domínguez, Erik Schaffernicht, Todor Stoyanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretable policy representations like Behavior Trees (BTs) and Dynamic
Motion Primitives (DMPs) enable robot skill transfer from human demonstrations,
but each faces limitations: BTs require expert-crafted low-level actions, while
DMPs lack high-level task logic. We address these limitations by integrating
DMP controllers into a BT framework, jointly learning the BT structure and DMP
actions from single demonstrations, thereby removing the need for predefined
actions. Additionally, by combining BT decision logic with DMP motion
generation, our method enhances policy interpretability, modularity, and
adaptability for autonomous systems. Our approach readily affords both learning
to replicate low-level motions and combining partial demonstrations into a
coherent and easy-to-modify overall policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures, accepted (not yet published) at IAS19 2025
  conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost Function Estimation Using Inverse Reinforcement Learning with
  Minimal Observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarmad Mehrdad, Avadesh Meduri, Ludovic Righetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an iterative inverse reinforcement learning algorithm to infer
optimal cost functions in continuous spaces. Based on a popular maximum entropy
criteria, our approach iteratively finds a weight improvement step and proposes
a method to find an appropriate step size that ensures learned cost function
features remain similar to the demonstrated trajectory features. In contrast to
similar approaches, our algorithm can individually tune the effectiveness of
each observation for the partition function and does not need a large sample
set, enabling faster learning. We generate sample trajectories by solving an
optimal control problem instead of random sampling, leading to more informative
trajectories. The performance of our method is compared to two state of the art
algorithms to demonstrate its benefits in several simulated environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MC-Swarm: Minimal-Communication Multi-Agent Trajectory Planning and
  Deadlock Resolution for Quadrotor Swarm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunwoo Lee, Jungwon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For effective multi-agent trajectory planning, it is important to consider
lightweight communication and its potential asynchrony. This paper presents a
distributed trajectory planning algorithm for a quadrotor swarm that operates
asynchronously and requires no communication except during the initial planning
phase. Moreover, our algorithm guarantees no deadlock under asynchronous
updates and absence of communication during flight. To effectively ensure these
points, we build two main modules: coordination state updater and trajectory
optimizer. The coordination state updater computes waypoints for each agent
toward its goal and performs subgoal optimization while considering deadlocks,
as well as safety constraints with respect to neighbor agents and obstacles.
Then, the trajectory optimizer generates a trajectory that ensures collision
avoidance even with the asynchronous planning updates of neighboring agents. We
provide a theoretical guarantee of collision avoidance with deadlock resolution
and evaluate the effectiveness of our method in complex simulation
environments, including random forests and narrow-gap mazes. Additionally, to
reduce the total mission time, we design a faster coordination state update
using lightweight communication. Lastly, our approach is validated through
extensive simulations and real-world experiments with cluttered environment
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MESSI: A Multi-Elevation Semantic Segmentation Image <span class="highlight-title">Dataset</span> of an Urban
  Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barak Pinkovich, Boaz Matalon, Ehud Rivlin, Hector Rotstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a Multi-Elevation Semantic Segmentation Image (MESSI)
dataset comprising 2525 images taken by a drone flying over dense urban
environments. MESSI is unique in two main features. First, it contains images
from various altitudes, allowing us to investigate the effect of depth on
semantic segmentation. Second, it includes images taken from several different
urban regions (at different altitudes). This is important since the variety
covers the visual richness captured by a drone's 3D flight, performing
horizontal and vertical maneuvers. MESSI contains images annotated with
location, orientation, and the camera's intrinsic parameters and can be used to
train a deep neural network for semantic segmentation or other applications of
interest (e.g., localization, navigation, and tracking). This paper describes
the dataset and provides annotation details. It also explains how semantic
segmentation was performed using several neural network models and shows
several relevant statistics. MESSI will be published in the public domain to
serve as an evaluation benchmark for semantic segmentation using images
captured by a drone or similar vehicle flying over a dense urban environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Multi-Task Policy Learning from NMPC for Quadruped Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anudeep Sajja, Shahram Khorshidi, Sebastian Houben, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadruped robots excel in traversing complex, unstructured environments where
wheeled robots often fail. However, enabling efficient and adaptable locomotion
remains challenging due to the quadrupeds' nonlinear dynamics, high degrees of
freedom, and the computational demands of real-time control. Optimization-based
controllers, such as Nonlinear Model Predictive Control (NMPC), have shown
strong performance, but their reliance on accurate state estimation and high
computational overhead makes deployment in real-world settings challenging. In
this work, we present a Multi-Task Learning (MTL) framework in which expert
NMPC demonstrations are used to train a single neural network to predict
actions for multiple locomotion behaviors directly from raw proprioceptive
sensor inputs. We evaluate our approach extensively on the quadruped robot Go1,
both in simulation and on real hardware, demonstrating that it accurately
reproduces expert behavior, allows smooth gait switching, and simplifies the
control pipeline for real-time deployment. Our MTL architecture enables
learning diverse gaits within a unified policy, achieving high $R^{2}$ scores
for predicted joint targets across all tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Seeing to Doing: Bridging Reasoning and Decision for Robotic
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Yuan, Haiqin Cui, Yibin Chen, Zibin Dong, Fei Ni, Longxin Kou, Jinyi Liu, Pengyi Li, Yan Zheng, Jianye Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving generalization in robotic manipulation remains a critical
challenge, particularly for unseen scenarios and novel tasks. Current
Vision-Language-Action (VLA) models, while building on top of general
Vision-Language Models (VLMs), still fall short of achieving robust zero-shot
performance due to the scarcity and heterogeneity prevalent in embodied
datasets. To address these limitations, we propose FSD (From Seeing to Doing),
a novel vision-language model that generates intermediate representations
through spatial relationship reasoning, providing fine-grained guidance for
robotic manipulation. Our approach combines a hierarchical data pipeline for
training with a self-consistency mechanism that aligns spatial coordinates with
visual signals. Through extensive experiments, we comprehensively validated
FSD's capabilities in both "seeing" and "doing," achieving outstanding
performance across 8 benchmarks for general spatial reasoning and embodied
reference abilities, as well as on our proposed more challenging benchmark
VABench. We also verified zero-shot capabilities in robot manipulation,
demonstrating significant performance improvements over baseline methods in
both SimplerEnv and real robot settings. Experimental results show that FSD
achieves 54.1% success rate in SimplerEnv and 72% success rate across 8
real-world tasks, outperforming the strongest baseline by 30%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Early version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FOCI: Trajectory Optimization on Gaussian Splats 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario Gomez Andreu, Maximum Wilder-Smith, Victor Klemm, Vaishakh Patil, Jesus Tordesillas, Marco Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has recently gained popularity as a faster
alternative to Neural Radiance Fields (NeRFs) in 3D reconstruction and view
synthesis methods. Leveraging the spatial information encoded in 3DGS, this
work proposes FOCI (Field Overlap Collision Integral), an algorithm that is
able to optimize trajectories directly on the Gaussians themselves. FOCI
leverages a novel and interpretable collision formulation for 3DGS using the
notion of the overlap integral between Gaussians. Contrary to other approaches,
which represent the robot with conservative bounding boxes that underestimate
the traversability of the environment, we propose to represent the environment
and the robot as Gaussian Splats. This not only has desirable computational
properties, but also allows for orientation-aware planning, allowing the robot
to pass through very tight and narrow spaces. We extensively test our algorithm
in both synthetic and real Gaussian Splats, showcasing that collision-free
trajectories for the ANYmal legged robot that can be computed in a few seconds,
even with hundreds of thousands of Gaussians making up the environment. The
project page and code are available at
https://rffr.leggedrobotics.com/works/foci/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures, Mario Gomez Andreu and Maximum Wilder-Smith
  contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Achieving Scalable Robot Autonomy via neurosymbolic planning using
  lightweight local LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Attolino, Alessio Capitanelli, Fulvio Mastrogiovanni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PDDL-based symbolic task planning remains pivotal for robot autonomy yet
struggles with dynamic human-robot collaboration due to scalability,
re-planning demands, and delayed plan availability. Although a few
neurosymbolic frameworks have previously leveraged LLMs such as GPT-3 to
address these challenges, reliance on closed-source, remote models with limited
context introduced critical constraints: third-party dependency, inconsistent
response times, restricted plan length and complexity, and multi-domain
scalability issues. We present Gideon, a novel framework that enables the
transition to modern, smaller, local LLMs with extended context length. Gideon
integrates a novel problem generator to systematically generate large-scale
datasets of realistic domain-problem-plan tuples for any domain, and adapts
neurosymbolic planning for local LLMs, enabling on-device execution and
extended context for multi-domain support. Preliminary experiments in
single-domain scenarios performed on Qwen-2.5 1.5B and trained on 8k-32k
samples, demonstrate a valid plan percentage of 66.1% (32k model) and show that
the figure can be further scaled through additional data. Multi-domain tests on
16k samples yield an even higher 70.6% planning validity rate, proving
extensibility across domains and signaling that data variety can have a
positive effect on learning efficiency. Although long-horizon planning and
reduced model size make Gideon training much less efficient than baseline
models based on larger LLMs, the results are still significant considering that
the trained model is about 120x smaller than baseline and that significant
advantages can be achieved in inference efficiency, scalability, and
multi-domain adaptability, all critical factors in human-robot collaboration.
Training inefficiency can be mitigated by Gideon's streamlined data generation
pipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 3 figures, 4 tables, accepted at IAS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Sim-to-Real Reinforcement Learning for Fruit Harvesting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emlyn Williams, Athanasios Polydoros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive sim-to-real pipeline for autonomous
strawberry picking from dense clusters using a Franka Panda robot. Our approach
leverages a custom Mujoco simulation environment that integrates domain
randomization techniques. In this environment, a deep reinforcement learning
agent is trained using the dormant ratio minimization algorithm. The proposed
pipeline bridges low-level control with high-level perception and decision
making, demonstrating promising performance in both simulation and in a real
laboratory environment, laying the groundwork for successful transfer to
real-world autonomous fruit harvesting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter Estimation using Reinforcement Learning Causal Curiosity:
  Limits and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Arana-Catania, Weisi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal understanding is important in many disciplines of science and
engineering, where we seek to understand how different factors in the system
causally affect an experiment or situation and pave a pathway towards creating
effective or optimising existing models. Examples of use cases are autonomous
exploration and modelling of unknown environments or assessing key variables in
optimising large complex systems. In this paper, we analyse a Reinforcement
Learning approach called Causal Curiosity, which aims to estimate as accurately
and efficiently as possible, without directly measuring them, the value of
factors that causally determine the dynamics of a system. Whilst the idea
presents a pathway forward, measurement accuracy is the foundation of
methodology effectiveness. Focusing on the current causal curiosity's robotic
manipulator, we present for the first time a measurement accuracy analysis of
the future potentials and current limitations of this technique and an analysis
of its sensitivity and confounding factor disentanglement capability - crucial
for causal analysis. As a result of our work, we promote proposals for an
improved and efficient design of Causal Curiosity methods to be applied to
real-world complex scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 10 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbolically-Guided Visual Plan Inference from Uncurated Video Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyan Yang, Ahmet Tikna, Yi Zhao, Yuying Zhang, Luigi Palopoli, Marco Roveri, Joni Pajarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual planning, by offering a sequence of intermediate visual subgoals to a
goal-conditioned low-level policy, achieves promising performance on
long-horizon manipulation tasks. To obtain the subgoals, existing methods
typically resort to video generation models but suffer from model hallucination
and computational cost. We present Vis2Plan, an efficient, explainable and
white-box visual planning framework powered by symbolic guidance. From raw,
unlabeled play data, Vis2Plan harnesses vision foundation models to
automatically extract a compact set of task symbols, which allows building a
high-level symbolic transition graph for multi-goal, multi-stage planning. At
test time, given a desired task goal, our planner conducts planning at the
symbolic level and assembles a sequence of physically consistent intermediate
sub-goal images grounded by the underlying symbolic representation. Our
Vis2Plan outperforms strong diffusion video generation-based visual planners by
delivering 53\% higher aggregate success rate in real robot settings while
generating visual plans 35$\times$ faster. The results indicate that Vis2Plan
is able to generate physically consistent image goals while offering fully
inspectable reasoning steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HMR-ODTA: Online Diverse Task Allocation for a Team of Heterogeneous
  Mobile Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashish Verma, Avinash Gautam, Tanishq Duhan, V. S. Shekhawat, Sudeept Mohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coordinating time-sensitive deliveries in environments like hospitals poses a
complex challenge, particularly when managing multiple online pickup and
delivery requests within strict time windows using a team of heterogeneous
robots. Traditional approaches fail to address dynamic rescheduling or diverse
service requirements, typically restricting robots to single-task types. This
paper tackles the Multi-Pickup and Delivery Problem with Time Windows (MPDPTW),
where autonomous mobile robots are capable of handling varied service requests.
The objective is to minimize late delivery penalties while maximizing task
completion rates. To achieve this, we propose a novel framework leveraging a
heterogeneous robot team and an efficient dynamic scheduling algorithm that
supports dynamic task rescheduling. Users submit requests with specific time
constraints, and our decentralized algorithm, Heterogeneous Mobile Robots
Online Diverse Task Allocation (HMR-ODTA), optimizes task assignments to ensure
timely service while addressing delays or task rejections. Extensive
simulations validate the algorithm's effectiveness. For smaller task sets
(40-160 tasks), penalties were reduced by nearly 63%, while for larger sets
(160-280 tasks), penalties decreased by approximately 50%. These results
highlight the algorithm's effectiveness in improving task scheduling and
coordination in multi-robot systems, offering a robust solution for enhancing
delivery performance in structured, time-critical environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffOG: Differentiable Policy Trajectory Optimization with
  Generalizability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13807v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13807v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengtong Xu, Zichen Miao, Qiang Qiu, Zhe Zhang, Yu She
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning-based visuomotor policies excel at manipulation tasks but
often produce suboptimal action trajectories compared to model-based methods.
Directly mapping camera data to actions via neural networks can result in jerky
motions and difficulties in meeting critical constraints, compromising safety
and robustness in real-world deployment. For tasks that require high robustness
or strict adherence to constraints, ensuring trajectory quality is crucial.
However, the lack of interpretability in neural networks makes it challenging
to generate constraint-compliant actions in a controlled manner. This paper
introduces differentiable policy trajectory optimization with generalizability
(DiffOG), a learning-based trajectory optimization framework designed to
enhance visuomotor policies. By leveraging the proposed differentiable
formulation of trajectory optimization with transformer, DiffOG seamlessly
integrates policies with a generalizable optimization layer. DiffOG refines
action trajectories to be smoother and more constraint-compliant while
maintaining alignment with the original demonstration distribution, thus
avoiding degradation in policy performance. We evaluated DiffOG across 11
simulated tasks and 2 real-world tasks. The results demonstrate that DiffOG
significantly enhances the trajectory quality of visuomotor policies while
having minimal impact on policy performance, outperforming trajectory
processing baselines such as greedy constraint clipping and penalty-based
trajectory optimization. Furthermore, DiffOG achieves superior performance
compared to existing constrained visuomotor policy. Please visit the project
website for more details: https://zhengtongxu.github.io/diffog-website/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafeNav: Safe Path Navigation using Landmark Based Localization in a
  GPS-denied Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.01956v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.01956v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganesh Sapkota, Sanjay Madria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In battlefield environments, adversaries frequently disrupt GPS signals,
requiring alternative localization and navigation methods. Traditional
vision-based approaches like Simultaneous Localization and Mapping (SLAM) and
Visual Odometry (VO) involve complex sensor fusion and high computational
demand, whereas range-free methods like DV-HOP face accuracy and stability
challenges in sparse, dynamic networks. This paper proposes LanBLoc-BMM, a
navigation approach using landmark-based localization (LanBLoc) combined with a
battlefield-specific motion model (BMM) and Extended Kalman Filter (EKF). Its
performance is benchmarked against three state-of-the-art visual localization
algorithms integrated with BMM and Bayesian filters, evaluated on synthetic and
real-imitated trajectory datasets using metrics including Average Displacement
Error (ADE), Final Displacement Error (FDE), and a newly introduced Average
Weighted Risk Score (AWRS). LanBLoc-BMM (with EKF) demonstrates superior
performance in ADE, FDE, and AWRS on real-imitated datasets. Additionally, two
safe navigation methods, SafeNav-CHull and SafeNav-Centroid, are introduced by
integrating LanBLoc-BMM(EKF) with a novel Risk-Aware RRT* (RAw-RRT*) algorithm
for obstacle avoidance and risk exposure minimization. Simulation results in
battlefield scenarios indicate SafeNav-Centroid excels in accuracy, risk
exposure, and trajectory efficiency, while SafeNav-CHull provides superior
computational speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, conference paper. arXiv admin note: text overlap with
  arXiv:2402.14280</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffCloud: Real-to-Sim from Point Clouds with Differentiable Simulation
  and Rendering of Deformable Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.03139v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.03139v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priya Sundaresan, Rika Antonova, Jeannette Bohg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in manipulation of deformable objects is typically conducted on a
limited range of scenarios, because handling each scenario on hardware takes
significant effort. Realistic simulators with support for various types of
deformations and interactions have the potential to speed up experimentation
with novel tasks and algorithms. However, for highly deformable objects it is
challenging to align the output of a simulator with the behavior of real
objects. Manual tuning is not intuitive, hence automated methods are needed. We
view this alignment problem as a joint perception-inference challenge and
demonstrate how to use recent neural network architectures to successfully
perform simulation parameter inference from real point clouds. We analyze the
performance of various architectures, comparing their data and training
requirements. Furthermore, we propose to leverage differentiable point cloud
sampling and differentiable simulation to significantly reduce the time to
achieve the alignment. We employ an efficient way to propagate gradients from
point clouds to simulated meshes and further through to the physical simulation
parameters, such as mass and stiffness. Experiments with highly deformable
objects show that our method can achieve comparable or better alignment with
real object behavior, while reducing the time needed to achieve this by more
than an order of magnitude. Videos and supplementary material are available at
https://diffcloud.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Certifiably Correct Range-Aided SLAM <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Thoms, Alan Papalia, Jared Velasquez, David M. Rosen, Sriram Narasimhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable simultaneous localization and mapping (SLAM) algorithms are
necessary for safety-critical autonomous navigation. In the
communication-constrained multi-agent setting, navigation systems increasingly
use point-to-point range sensors as they afford measurements with low bandwidth
requirements and known data association. The state estimation problem for these
systems takes the form of range-aided (RA) SLAM. However, distributed
algorithms for solving the RA-SLAM problem lack formal guarantees on the
quality of the returned estimate. To this end, we present the first distributed
algorithm for RA-SLAM that can efficiently recover certifiably globally optimal
solutions. Our algorithm, distributed certifiably correct RA-SLAM (DCORA),
achieves this via the Riemannian Staircase method, where computational
procedures developed for distributed certifiably correct pose graph
optimization are generalized to the RA-SLAM problem. We demonstrate DCORA's
efficacy on real-world multi-agent datasets by achieving absolute trajectory
errors comparable to those of a state-of-the-art centralized certifiably
correct RA-SLAM algorithm. Additionally, we perform a parametric study on the
structure of the RA-SLAM problem using synthetic data, revealing how common
parameters affect DCORA's performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, accepted to the 2025 International Conference on
  Robotics and Automation (ICRA). This version includes minor clerical edits to
  the published version in the conference proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal navigation of magnetic artificial microswimmers in blood
  capillaries with deep reinforcement learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Amoudruz, Sergey Litvinov, Petros Koumoutsakos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomedical applications such as targeted drug delivery, microsurgery, and
sensing rely on reaching precise areas within the body in a minimally invasive
way. Artificial bacterial flagella (ABFs) have emerged as potential tools for
this task by navigating through the circulatory system with the help of
external magnetic fields. While their swimming characteristics are well
understood in simple settings, their controlled navigation through realistic
capillary networks remains a significant challenge due to the complexity of
blood flow and the high computational cost of detailed simulations. We address
this challenge by conducting numerical simulations of ABFs in retinal
capillaries, propelled by an external magnetic field. The simulations are based
on a validated blood model that predicts the dynamics of individual red blood
cells and their hydrodynamic interactions with ABFs. The magnetic field follows
a control policy that brings the ABF to a prescribed target. The control policy
is learned with an actor-critic, off-policy reinforcement learning algorithm
coupled with a reduced-order model of the system. We show that the same policy
robustly guides the ABF to a prescribed target in both the reduced-order model
and the fine-grained blood simulations. This approach is suitable for designing
robust control policies for personalized medicine at moderate computational
cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPOT: SE(3) Pose Trajectory Diffusion for Object-Centric Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00965v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00965v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-Chun Hsu, Bowen Wen, Jie Xu, Yashraj Narang, Xiaolong Wang, Yuke Zhu, Joydeep Biswas, Stan Birchfield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SPOT, an object-centric imitation learning framework. The key
idea is to capture each task by an object-centric representation, specifically
the SE(3) object pose trajectory relative to the target. This approach
decouples embodiment actions from sensory inputs, facilitating learning from
various demonstration types, including both action-based and action-less human
hand demonstrations, as well as cross-embodiment generalization. Additionally,
object pose trajectories inherently capture planning constraints from
demonstrations without the need for manually-crafted rules. To guide the robot
in executing the task, the object trajectory is used to condition a diffusion
policy. We systematically evaluate our method on simulation and real-world
tasks. In real-world evaluation, using only eight demonstrations shot on an
iPhone, our approach completed all tasks while fully complying with task
constraints. Project page: https://nvlabs.github.io/object_centric_diffusion
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory
  Planner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10060v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10060v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kota Kondo, Claudius T. Tewari, Andrea Tagliabue, Jesus Tordesillas, Parker C. Lusk, Mason B. Peterson, Jonathan P. How
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In decentralized multiagent trajectory planners, agents need to communicate
and exchange their positions to generate collision-free trajectories. However,
due to localization errors/uncertainties, trajectory deconfliction can fail
even if trajectories are perfectly shared between agents. To address this
issue, we first present PARM and PARM*, perception-aware, decentralized,
asynchronous multiagent trajectory planners that enable a team of agents to
navigate uncertain environments while deconflicting trajectories and avoiding
obstacles using perception information. PARM* differs from PARM as it is less
conservative, using more computation to find closer-to-optimal solutions. While
these methods achieve state-of-the-art performance, they suffer from high
computational costs as they need to solve large optimization problems onboard,
making it difficult for agents to replan at high rates. To overcome this
challenge, we present our second key contribution, PRIMER, a learning-based
planner trained with imitation learning (IL) using PARM* as the expert
demonstrator. PRIMER leverages the low computational requirements at deployment
of neural networks and achieves a computation speed up to 5500 times faster
than optimization-based approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Imitation Enables Contextual Humanoid Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03729v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03729v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Allshire, Hongsuk Choi, Junyi Zhang, David McAllister, Anthony Zhang, Chung Min Kim, Trevor Darrell, Pieter Abbeel, Jitendra Malik, Angjoo Kanazawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we teach humanoids to climb staircases and sit on chairs using the
surrounding environment context? Arguably, the simplest way is to just show
them-casually capture a human motion video and feed it to humanoids. We
introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday
videos, jointly reconstructs the humans and the environment, and produces
whole-body control policies for humanoid robots that perform the corresponding
skills. We demonstrate the results of our pipeline on real humanoid robots,
showing robust, repeatable contextual control such as staircase ascents and
descents, sitting and standing from chairs and benches, as well as other
dynamic whole-body skills-all from a single policy, conditioned on the
environment and global root commands. VIDEOMIMIC offers a scalable path towards
teaching humanoids to operate in diverse real-world environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://www.videomimic.net/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMPERROR: A Flexible Generative Perception Error Model for Probing
  Self-Driving Planners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07719v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07719v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Hanselmann, Simon Doll, Marius Cordts, Hendrik P. A. Lensch, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To handle the complexities of real-world traffic, learning planners for
self-driving from data is a promising direction. While recent approaches have
shown great progress, they typically assume a setting in which the ground-truth
world state is available as input. However, when deployed, planning needs to be
robust to the long-tail of errors incurred by a noisy perception system, which
is often neglected in evaluation. To address this, previous work has proposed
drawing adversarial samples from a perception error model (PEM) mimicking the
noise characteristics of a target object detector. However, these methods use
simple PEMs that fail to accurately capture all failure modes of detection. In
this paper, we present EMPERROR, a novel transformer-based generative PEM,
apply it to stress-test an imitation learning (IL)-based planner and show that
it imitates modern detectors more faithfully than previous work. Furthermore,
it is able to produce realistic noisy inputs that increase the planner's
collision rate by up to 85%, demonstrating its utility as a valuable tool for a
more complete evaluation of self-driving planners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://lasnik.github.io/emperror/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aerial Path Online Planning for Urban Scene Updation <span class="chip">SIGGRAPH 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.01486v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.01486v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingfeng Tang, Ningna Wang, Ziyuan Xie, Jianwei Hu, Ke Xie, Xiaohu Guo, Hui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first scene-update aerial path planning algorithm specifically
designed for detecting and updating change areas in urban environments. While
existing methods for large-scale 3D urban scene reconstruction focus on
achieving high accuracy and completeness, they are inefficient for scenarios
requiring periodic updates, as they often re-explore and reconstruct entire
scenes, wasting significant time and resources on unchanged areas. To address
this limitation, our method leverages prior reconstructions and change
probability statistics to guide UAVs in detecting and focusing on areas likely
to have changed. Our approach introduces a novel changeability heuristic to
evaluate the likelihood of changes, driving the planning of two flight paths: a
prior path informed by static priors and a dynamic real-time path that adapts
to newly detected changes. The framework integrates surface sampling and
candidate view generation strategies, ensuring efficient coverage of change
areas with minimal redundancy. Extensive experiments on real-world urban
datasets demonstrate that our method significantly reduces flight time and
computational overhead, while maintaining high-quality updates comparable to
full-scene re-exploration and reconstruction. These contributions pave the way
for efficient, scalable, and adaptive UAV-based scene updates in complex urban
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM SIGGRAPH 2025 (Patent Protected); Project page:
  https://vcc.tech/research/2025/DroneUpdate ; Video:
  https://youtu.be/3nNBoh2syTU?si=LQ04NlJNyPosdm8F</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nano Drone-based Indoor Crime Scene Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Cooney, Sivadinesh Ponrajan, Fernando Alonso-Fernandez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Technologies such as robotics, Artificial Intelligence (AI), and Computer
Vision (CV) can be applied to crime scene analysis (CSA) to help protect lives,
facilitate justice, and deter crime, but an overview of the tasks that can be
automated has been lacking. Here we follow a speculative prototyping approach:
First, the STAIR tool is used to rapidly review the literature and identify
tasks that seem to have not received much attention, like accessing crime
scenes through a window, mapping/gathering evidence, and analyzing blood
smears. Secondly, we present a prototype of a small drone that implements these
three tasks with 75%, 85%, and 80% performance, to perform a minimal analysis
of an indoor crime scene. Lessons learned are reported, toward guiding next
work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 3 tables, accepted at ARSO 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimization-free Smooth Control Barrier Function for Polygonal
  Collision Avoidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shizhen Wu, Yongchun Fang, Ning Sun, Biao Lu, Xiao Liang, Yiming Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polygonal collision avoidance (PCA) is short for the problem of collision
avoidance between two polygons (i.e., polytopes in planar) that own their
dynamic equations. This problem suffers the inherent difficulty in dealing with
non-smooth boundaries and recently optimization-defined metrics, such as signed
distance field (SDF) and its variants, have been proposed as control barrier
functions (CBFs) to tackle PCA problems. In contrast, we propose an
optimization-free smooth CBF method in this paper, which is computationally
efficient and proved to be nonconservative. It is achieved by three main steps:
a lower bound of SDF is expressed as a nested Boolean logic composition first,
then its smooth approximation is established by applying the latest log-sum-exp
method, after which a specified CBF-based safety filter is proposed to address
this class of problems. To illustrate its wide applications, the
optimization-free smooth CBF method is extended to solve distributed collision
avoidance of two underactuated nonholonomic vehicles and drive an underactuated
container crane to avoid a moving obstacle respectively, for which numerical
simulations are also performed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Anytime Optical Flow Estimation with Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05033v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05033v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaozu Ye, Hao Shi, Kailun Yang, Ze Wang, Xiaoting Yin, Lei Sun, Yaonan Wang, Kaiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras respond to changes in log-brightness at the millisecond level,
making them ideal for optical flow estimation. However, existing datasets from
event cameras provide only low frame rate ground truth for optical flow,
limiting the research potential of event-driven optical flow. To address this
challenge, we introduce a low-latency event representation, Unified Voxel Grid,
and propose EVA-Flow, an EVent-based Anytime Flow estimation network to produce
high-frame-rate event optical flow with only low-frame-rate optical flow ground
truth for supervision. Furthermore, we propose the Rectified Flow Warp Loss
(RFWL) for the unsupervised assessment of intermediate optical flow. A
comprehensive variety of experiments on MVSEC, DESC, and our EVA-FlowSet
demonstrates that EVA-Flow achieves competitive performance, super-low-latency
(5ms), time-dense motion estimation (200Hz), and strong generalization. Our
code will be available at https://github.com/Yaozhuwa/EVA-Flow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Sensors. Our code will be available at
  https://github.com/Yaozhuwa/EVA-Flow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for
  Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12514v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12514v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, Feifei Feng, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language-Action (VLA) models have shown remarkable potential in
visuomotor control and instruction comprehension through end-to-end learning
processes. However, current VLA models face significant challenges: they are
slow during inference and require extensive pre-training on large amounts of
robotic data, making real-world deployment difficult. In this paper, we
introduce a new family of compact vision-language-action models, called
TinyVLA, which offers two key advantages over existing VLA models: (1) faster
inference speeds, and (2) improved data efficiency, eliminating the need for
pre-training stage. Our framework incorporates two essential components to
build TinyVLA: (1) initializing the policy backbone with robust, high-speed
multimodal models, and (2) integrating a diffusion policy decoder during
fine-tuning to enable precise robot actions. We conducted extensive evaluations
of TinyVLA in both simulation and on real robots, demonstrating that our
approach significantly outperforms the state-of-the-art VLA model, OpenVLA, in
terms of speed and data efficiency, while delivering comparable or superior
performance. Additionally, TinyVLA exhibits strong generalization capabilities
across various dimensions, including language instructions, novel objects,
unseen positions, changes in object appearance, background variations, and
environmental shifts, often matching or exceeding the performance of OpenVLA.
We believe that \methodname offers an interesting perspective on utilizing
pre-trained multimodal models for policy learning. Our project is at
https://tiny-vla.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>add more citations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General
  Robot Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, Feifei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enabling robots to perform diverse tasks across varied environments is a
central challenge in robot learning. While vision-language-action (VLA) models
have shown promise for generalizable robot skills, realizing their full
potential requires addressing limitations in action representation and
efficient training. Current VLA models often focus on scaling the
vision-language model (VLM) component, while the action space representation
remains a critical bottleneck. This paper introduces DexVLA, a novel framework
designed to enhance the efficiency and generalization capabilities of VLAs for
complex, long-horizon tasks across diverse robot embodiments. DexVLA features a
novel diffusion-based action expert, scaled to one billion parameters, designed
for cross-embodiment learning. A novel embodiment curriculum learning strategy
facilitates efficient training: (1) pre-training the diffusion expert that is
separable from the VLA on cross-embodiment data, (2) aligning the VLA model to
specific embodiments, and (3) post-training for rapid adaptation to new tasks.
We conduct comprehensive experiments across multiple embodiments, including
single-arm, bimanual, and dexterous hand, demonstrating DexVLA's adaptability
to challenging tasks without task-specific adaptation, its ability to learn
dexterous skills on novel embodiments with limited data, and its capacity to
complete complex, long-horizon tasks using only direct language prompting, such
as laundry folding. In all settings, our method demonstrates superior
performance compared to state-of-the-art models like Octo, OpenVLA, and
Diffusion Policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The webpage is at https://dex-vla.github.io/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Securing RAG: A Risk Assessment and Mitigation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Ammann, Sara Ott, Christoph R. Landolt, Marco P. Lehmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) has emerged as the de facto industry
standard for user-facing NLP applications, offering the ability to integrate
data without re-training or fine-tuning Large Language Models (LLMs). This
capability enhances the quality and accuracy of responses but also introduces
novel security and privacy challenges, particularly when sensitive data is
integrated. With the rapid adoption of RAG, securing data and services has
become a critical priority. This paper first reviews the vulnerabilities of RAG
pipelines, and outlines the attack surface from data pre-processing and data
storage management to integration with LLMs. The identified risks are then
paired with corresponding mitigations in a structured overview. In a second
step, the paper develops a framework that combines RAG-specific security
considerations, with existing general security guidelines, industry standards,
and best practices. The proposed framework aims to guide the implementation of
robust, compliant, secure, and trustworthy RAG systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, Sara Ott and Lukas Ammann contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interest Changes: Considering User Interest Life Cycle in Recommendation
  System <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinjiang Cai, Jiangpan Hou, Yangping Zhu, Yuan Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recommendation systems, user interests are always in a state of constant
flux. Typically, a user interest experiences a emergent phase, a stable phase,
and a declining phase, which are referred to as the "user interest life-cycle".
Recent papers on user interest modeling have primarily focused on how to
compute the correlation between the target item and user's historical
behaviors, without thoroughly considering the life-cycle features of user
interest. In this paper, we propose an effective method called Deep Interest
Life-cycle Network (DILN), which not only captures the interest life-cycle
features efficiently, but can also be easily integrated to existing ranking
models. DILN contains two key components: Interest Life-cycle Encoder Module
constructs historical activity histograms of the user interest and then encodes
them into dense representation. Interest Life-cycle Fusion Module injects the
encoded dense representation into multiple expert networks, with the aim of
enabling the specific phase of interest life-cycle to activate distinct
experts. Online A/B testing reveals that DILN achieves significant improvements
of +0.38% in CTR, +1.04% in CVR and +0.25% in duration per user, which
demonstrates its effectiveness. In addition, DILN inherently increase the
exposure of users' emergent and stable interests while decreasing the exposure
of declining interests. DILN has been deployed on the Lofter App.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lost in Transliteration: Bridging the Script Gap in Neural IR <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Chari, Iadh Ounis, Sean MacAvaney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most human languages use scripts other than the Latin alphabet. Search users
in these languages often formulate their information needs in a transliterated
-- usually Latinized -- form for ease of typing. For example, Greek speakers
might use Greeklish, and Arabic speakers might use Arabizi. This paper shows
that current search systems, including those that use multilingual dense
embeddings such as BGE-M3, do not generalise to this setting, and their
performance rapidly deteriorates when exposed to transliterated queries. This
creates a ``script gap" between the performance of the same queries when
written in their native or transliterated form. We explore whether adapting the
popular ``translate-train" paradigm to transliterations can enhance the
robustness of multilingual Information Retrieval (IR) methods and bridge the
gap between native and transliterated scripts. By exploring various
combinations of non-Latin and Latinized query text for training, we investigate
whether we can enhance the capacity of existing neural retrieval techniques and
enable them to apply to this important setting. We show that by further
fine-tuning IR models on an even mixture of native and Latinized text, they can
perform this cross-script matching at nearly the same performance as when the
query was formulated in the native script. Out-of-domain evaluation and further
qualitative analysis show that transliterations can also cause queries to lose
some of their nuances, motivating further research in this direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 tables. paper accepted at the Short Paper track of The
  48th International ACM SIGIR Conference on Research and Development in
  Information Retrieval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TikTok Search Recommendations: Governance and Research Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taylor Annabell, Robert Gorwa, Rebecca Scharlach, Jacob van de Kerkhof, Thales Bertaglia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Like other social media, TikTok is embracing its use as a search engine,
developing search products to steer users to produce searchable content and
engage in content discovery. Their recently developed product search
recommendations are preformulated search queries recommended to users on
videos. However, TikTok provides limited transparency about how search
recommendations are generated and moderated, despite requirements under
regulatory frameworks like the European Union's Digital Services Act. By
suggesting that the platform simply aggregates comments and common searches
linked to videos, it sidesteps responsibility and issues that arise from
contextually problematic recommendations, reigniting long-standing concerns
about platform liability and moderation. This position paper addresses the
novelty of search recommendations on TikTok by highlighting the challenges that
this feature poses for platform governance and offering a computational
research agenda, drawing on preliminary qualitative analysis. It sets out the
need for transparency in platform documentation, data access and research to
study search recommendations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in The 1st international Workshop on Computational
  Approaches to Content Moderation and Platform Governance (COMPASS), held at
  ICWSM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperbolic Contrastive Learning with Model-augmentation for
  Knowledge-aware Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.08157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.08157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyin Sun, Chen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benefiting from the effectiveness of graph neural networks (GNNs) and
contrastive learning, GNN-based contrastive learning has become mainstream for
knowledge-aware recommendation. However, most existing contrastive
learning-based methods have difficulties in effectively capturing the
underlying hierarchical structure within user-item bipartite graphs and
knowledge graphs. Moreover, they commonly generate positive samples for
contrastive learning by perturbing the graph structure, which may lead to a
shift in user preference learning. To overcome these limitations, we propose
hyperbolic contrastive learning with model-augmentation for knowledge-aware
recommendation. To capture the intrinsic hierarchical graph structures, we
first design a novel Lorentzian knowledge aggregation mechanism, which enables
more effective representations of users and items. Then, we propose three
model-level augmentation techniques to assist Hyperbolic contrastive learning.
Different from the classical structure-level augmentation (e.g., edge
dropping), the proposed model-augmentations can avoid preference shifts between
the augmented positive pair. Finally, we conduct extensive experiments to
demonstrate the superiority (maximum improvement of $11.03\%$) of proposed
methods over existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph RAG for Legal Norms: A Hierarchical and Temporal Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.00039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.00039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hudson de Martim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article proposes an adaptation of Graph Retrieval Augmented Generation
(Graph RAG) specifically designed for the analysis and comprehension of legal
norms, which are characterized by their predefined hierarchical structure,
extensive network of internal and external references and multiple temporal
versions. By combining structured knowledge graphs with contextually enriched
text segments, Graph RAG offers a promising solution to address the inherent
complexity and vast volume of legal data. The integration of hierarchical
structure and temporal evolution into knowledge graphs - along with the concept
of comprehensive Text Units - facilitates the construction of richer,
interconnected representations of legal knowledge. Through a detailed analysis
of Graph RAG and its application to legal norm datasets, this article aims to
advance the field of Artificial Intelligence applied to Law, creating
opportunities for more effective systems in legal research, legislative
analysis, and decision support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alleviating LLM-based Generative Retrieval Hallucination in Alipay
  Search <span class="chip">SIGIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.21098v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.21098v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yedan Shen, Kaixin Wu, Yuechen Ding, Jingyuan Wen, Hong Liu, Mingjie Zhong, Zhouhan Lin, Jia Xu, Linjian Mo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative retrieval (GR) has revolutionized document retrieval with the
advent of large language models (LLMs), and LLM-based GR is gradually being
adopted by the industry. Despite its remarkable advantages and potential,
LLM-based GR suffers from hallucination and generates documents that are
irrelevant to the query in some instances, severely challenging its credibility
in practical applications. We thereby propose an optimized GR framework
designed to alleviate retrieval hallucination, which integrates knowledge
distillation reasoning in model training and incorporate decision agent to
further improve retrieval precision. Specifically, we employ LLMs to assess and
reason GR retrieved query-document (q-d) pairs, and then distill the reasoning
data as transferred knowledge to the GR model. Moreover, we utilize a decision
agent as post-processing to extend the GR retrieved documents through retrieval
model and select the most relevant ones from multi perspectives as the final
generative retrieval result. Extensive offline experiments on real-world
datasets and online A/B tests on Fund Search and Insurance Search in Alipay
demonstrate our framework's superiority and effectiveness in improving search
quality and conversion gains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Popularity Bias Amplification in Recommender Systems
  Employed in the Entertainment Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04752v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04752v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Kowald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become an integral part of our daily online
experience by analyzing past user behavior to suggest relevant content in
entertainment domains such as music, movies, and books. Today, they are among
the most widely used applications of AI and machine learning. Consequently,
regulations and guidelines for trustworthy AI, such as the European AI Act,
which addresses issues like bias and fairness, are highly relevant to the
design, development, and evaluation of recommender systems. One particularly
important type of bias in this context is popularity bias, which results in the
unfair underrepresentation of less popular content in recommendation lists.
This work summarizes our research on investigating the amplification of
popularity bias in recommender systems within the entertainment sector.
Analyzing datasets from three entertainment domains, music, movies, and anime,
we demonstrate that an item's recommendation frequency is positively correlated
with its popularity. As a result, user groups with little interest in popular
content receive less accurate recommendations compared to those who prefer
widely popular items. Furthermore, this work contributes to a better
understanding of the connection between recommendation accuracy, calibration
quality of algorithms, and popularity bias amplification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EWAF'25, summarizes fairness and popularity bias research
  presented in Dr. Kowald's habilitation:
  https://domkowald.github.io/documents/others/2024habilitation_recsys.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clinically inspired enhance Explainability and Interpretability of an
  AI-Tool for BCC diagnosis based on expert annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iván Matas, Carmen Serrano, Francisca Silva, Amalia Serrano, Tomás Toledo-Pastrana, Begoña Acha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An AI tool has been developed to provide interpretable support for the
diagnosis of BCC via teledermatology, thus speeding up referrals and optimizing
resource utilization. The interpretability is provided in two ways: on the one
hand, the main BCC dermoscopic patterns are found in the image to justify the
BCC/Non BCC classification. Secondly, based on the common visual XAI Grad-CAM,
a clinically inspired visual explanation is developed where the relevant
features for diagnosis are located. Since there is no established ground truth
for BCC dermoscopic features, a standard reference is inferred from the
diagnosis of four dermatologists using an Expectation Maximization (EM) based
algorithm. The results demonstrate significant improvements in classification
accuracy and interpretability, positioning this approach as a valuable tool for
early BCC detection and referral to dermatologists. The BCC/non-BCC
classification achieved an accuracy rate of 90%. For Clinically-inspired XAI
results, the detection of BCC patterns useful to clinicians reaches 99%
accuracy. As for the Clinically-inspired Visual XAI results, the mean of the
Grad-CAM normalized value within the manually segmented clinical features is
0.57, while outside this region it is 0.16. This indicates that the model
struggles to accurately identify the regions of the BCC patterns. These results
prove the ability of the AI tool to provide a useful explanation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 4 tables, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From raw affiliations to organization identifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myrto Kallipoliti, Serafeim Chatzopoulos, Miriam Baglioni, Eleni Adamidi, Paris Koloveas, Thanasis Vergoulis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate affiliation matching, which links affiliation strings to
standardized organization identifiers, is critical for improving research
metadata quality, facilitating comprehensive bibliometric analyses, and
supporting data interoperability across scholarly knowledge bases. Existing
approaches fail to handle the complexity of affiliation strings that often
include mentions of multiple organizations or extraneous information. In this
paper, we present AffRo, a novel approach designed to address these challenges,
leveraging advanced parsing and disambiguation techniques. We also introduce
AffRoDB, an expert-curated dataset to systematically evaluate affiliation
matching algorithms, ensuring robust benchmarking. Results demonstrate the
effectiveness of AffRp in accurately identifying organizations from complex
affiliation strings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-level Distributionally Robust Optimization for Large Language
  Model-based Dense Retrieval <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10613v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10613v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyuan Ma, Yongliang Ma, Xing Wu, Zhenpeng Su, Ming Zhou, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model-based Dense Retrieval (LLM-DR) optimizes over numerous
heterogeneous fine-tuning collections from different domains. However, the
discussion about its training data distribution is still minimal. Previous
studies rely on empirically assigned dataset choices or sampling ratios, which
inevitably lead to sub-optimal retrieval performances. In this paper, we
propose a new task-level Distributionally Robust Optimization (tDRO) algorithm
for LLM-DR fine-tuning, targeted at improving the universal domain
generalization ability by end-to-end reweighting the data distribution of each
task. The tDRO parameterizes the domain weights and updates them with scaled
domain gradients. The optimized weights are then transferred to the LLM-DR
fine-tuning to train more robust retrievers. Experiments show optimal
improvements in large-scale retrieval benchmarks and reduce up to 30% dataset
usage after applying our optimization algorithm with a series of
different-sized LLM-DR models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI25. Source code is available at
  https://github.com/ma787639046/tdro</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlippedRAG: Black-Box Opinion Manipulation Adversarial Attacks to
  Retrieval-Augmented Generation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02968v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02968v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Chen, Jiawei Liu, Yuyang Gong, Miaokun Chen, Haotan Liu, Qikai Cheng, Fan Zhang, Wei Lu, Xiaozhong Liu, Xiaofeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) enriches LLMs by dynamically retrieving
external knowledge, reducing hallucinations and satisfying real-time
information needs. While existing research mainly targets RAG's performance and
efficiency, emerging studies highlight critical security concerns. Yet, current
adversarial approaches remain limited, mostly addressing white-box scenarios or
heuristic black-box attacks without fully investigating vulnerabilities in the
retrieval phase. Additionally, prior works mainly focus on factoid QA tasks,
their attacks lack complexity and can be easily corrected by advanced LLMs. In
this paper, we investigate a more realistic and critical threat scenario:
adversarial attacks intended for opinion manipulation against black-box RAG
models, particularly on controversial topics. Specifically, we propose
FlippedRAG, a transfer-based adversarial attack against black-box RAG systems.
We first demonstrate that the underlying retriever of a black-box RAG system
can be reverse-engineered, enabling us to train a surrogate retriever.
Leveraging the surrogate retriever, we further craft target poisoning triggers,
altering vary few documents to effectively manipulate both retrieval and
subsequent generation. Extensive empirical results show that FlippedRAG
substantially outperforms baseline methods, improving the average attack
success rate by 16.7%. FlippedRAG achieves on average a 50% directional shift
in the opinion polarity of RAG-generated responses, ultimately causing a
notable 20% shift in user cognition. Furthermore, we evaluate the performance
of several potential defensive measures, concluding that existing mitigation
strategies remain insufficient against such sophisticated manipulation attacks.
These results highlight an urgent need for developing innovative defensive
solutions to ensure the security and trustworthiness of RAG systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2407.13757</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing User Interest based on Stream Clustering and Memory Networks
  in Large-Scale Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13238v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13238v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Liu, Nian Wang, Cong Xu, Ming Zhao, Bin Wang, Yi Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender Systems (RSs) provide personalized recommendation service based
on user interest, which are widely used in various platforms. However, there
are lots of users with sparse interest due to lacking consumption behaviors,
which leads to poor recommendation results for them. This problem is widespread
in large-scale RSs and is particularly difficult to address. To solve this
challenging problem, we propose an innovative solution called User Interest
Enhancement (UIE). UIE enhances user interest including user profile and user
history behavior sequences by leveraging the enhancement vectors and
personalized enhancement vectors generated based on dynamic streaming
clustering of similar users and items from multiple perspectives, which are
stored and updated in memory networks. UIE not only remarkably improves model
performance for users with sparse interest, but also delivers notable gains for
other users. As an end-to-end solution, UIE is easy to implement on top of
existing ranking models. Furthermore, we extend our approach to long-tail items
using similar methods, which also yields excellent improvements. We conduct
extensive offline and online experiments in an industrial RS. The results
demonstrate that UIE substantially outperforms other existing approaches,
especially for users with sparse interest. UIE has been deployed in several
large-scale RSs at Tencent since 2022, which was made public on 21 May 2024. In
addition, UIE-based methods have also been successfully applied in candidate
generation, pre-ranking, and context-DNN stages. Multiple teams have developed
solutions based on UIE, focusing on updating clustering algorithms and
attention mechanisms. As far as we know, UIE has been applied in multiple RSs,
advertising systems and search engines. The thoughts of UIE, dynamic streaming
clustering and similarity enhancement, have inspired subsequent relevant works.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-05-21T05:33:31.161273877Z">
            2025-05-21 05:33:31 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
