{"2025-05-15T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2505.10552v1","updated":"2025-05-15T17:58:01Z","published":"2025-05-15T17:58:01Z","title":"Loop closure grasping: Topological transformations enable strong,\n  gentle, and versatile grasps","summary":"  Grasping mechanisms must both create and subsequently hold grasps that permit\nsafe and effective object manipulation. Existing mechanisms address the\ndifferent functional requirements of grasp creation and grasp holding using a\nsingle morphology, but have yet to achieve the simultaneous strength,\ngentleness, and versatility needed for many applications. We present \"loop\nclosure grasping\", a class of robotic grasping that addresses these different\nfunctional requirements through topological transformations between open-loop\nand closed-loop morphologies. We formalize these morphologies for grasping,\nformulate the loop closure grasping method, and present principles and a design\narchitecture that we implement using soft growing inflated beams, winches, and\nclamps. The mechanisms' initial open-loop topology enables versatile grasp\ncreation via unencumbered tip movement, and closing the loop enables strong and\ngentle holding with effectively infinite bending compliance. Loop closure\ngrasping circumvents the tradeoffs of single-morphology designs, enabling\ngrasps involving historically challenging objects, environments, and\nconfigurations.\n","authors":["Kentaro Barhydt","O. Godson Osele","Sreela Kodali","Cosima du Pasquier","Chase M. Hartquist","H. Harry Asada","Allison M. Okamura"],"pdf_url":"https://arxiv.org/pdf/2505.10552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10547v1","updated":"2025-05-15T17:55:28Z","published":"2025-05-15T17:55:28Z","title":"Real-Time Out-of-Distribution Failure Prevention via Multi-Modal\n  Reasoning","summary":"  Foundation models can provide robust high-level reasoning on appropriate\nsafety interventions in hazardous scenarios beyond a robot's training data,\ni.e. out-of-distribution (OOD) failures. However, due to the high inference\nlatency of Large Vision and Language Models, current methods rely on manually\ndefined intervention policies to enact fallbacks, thereby lacking the ability\nto plan generalizable, semantically safe motions. To overcome these challenges\nwe present FORTRESS, a framework that generates and reasons about semantically\nsafe fallback strategies in real time to prevent OOD failures. At a low\nfrequency in nominal operations, FORTRESS uses multi-modal reasoners to\nidentify goals and anticipate failure modes. When a runtime monitor triggers a\nfallback response, FORTRESS rapidly synthesizes plans to fallback goals while\ninferring and avoiding semantically unsafe regions in real time. By bridging\nopen-world, multi-modal reasoning with dynamics-aware planning, we eliminate\nthe need for hard-coded fallbacks and human safety interventions. FORTRESS\noutperforms on-the-fly prompting of slow reasoning models in safety\nclassification accuracy on synthetic benchmarks and real-world ANYmal robot\ndata, and further improves system safety and planning success in simulation and\non quadrotor hardware for urban navigation.\n","authors":["Milan Ganai","Rohan Sinha","Christopher Agia","Daniel Morton","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2505.10547v1.pdf","comment":"Website: https://milanganai.github.io/fortress/"},{"id":"http://arxiv.org/abs/2505.08787v2","updated":"2025-05-15T17:53:41Z","published":"2025-05-13T17:59:22Z","title":"UniSkill: Imitating Human Videos via Cross-Embodiment Skill\n  Representations","summary":"  Mimicry is a fundamental learning mechanism in humans, enabling individuals\nto learn new tasks by observing and imitating experts. However, applying this\nability to robots presents significant challenges due to the inherent\ndifferences between human and robot embodiments in both their visual appearance\nand physical capabilities. While previous methods bridge this gap using\ncross-embodiment datasets with shared scenes and tasks, collecting such aligned\ndata between humans and robots at scale is not trivial. In this paper, we\npropose UniSkill, a novel framework that learns embodiment-agnostic skill\nrepresentations from large-scale cross-embodiment video data without any\nlabels, enabling skills extracted from human video prompts to effectively\ntransfer to robot policies trained only on robot data. Our experiments in both\nsimulation and real-world environments show that our cross-embodiment skills\nsuccessfully guide robots in selecting appropriate actions, even with unseen\nvideo prompts. The project website can be found at:\nhttps://kimhanjung.github.io/UniSkill.\n","authors":["Hanjung Kim","Jaehyun Kang","Hyolim Kang","Meedeum Cho","Seon Joo Kim","Youngwoon Lee"],"pdf_url":"https://arxiv.org/pdf/2505.08787v2.pdf","comment":"Project Page: https://kimhanjung.github.io/UniSkill/"},{"id":"http://arxiv.org/abs/2505.10542v1","updated":"2025-05-15T17:53:11Z","published":"2025-05-15T17:53:11Z","title":"AORRTC: Almost-Surely Asymptotically Optimal Planning with RRT-Connect","summary":"  Finding high-quality solutions quickly is an important objective in motion\nplanning. This is especially true for high-degree-of-freedom robots.\nSatisficing planners have traditionally found feasible solutions quickly but\nprovide no guarantees on their optimality, while almost-surely asymptotically\noptimal (a.s.a.o.) planners have probabilistic guarantees on their convergence\ntowards an optimal solution but are more computationally expensive.\n  This paper uses the AO-x meta-algorithm to extend the satisficing RRT-Connect\nplanner to optimal planning. The resulting Asymptotically Optimal RRT-Connect\n(AORRTC) finds initial solutions in similar times as RRT-Connect and uses any\nadditional planning time to converge towards the optimal solution in an anytime\nmanner. It is proven to be probabilistically complete and a.s.a.o.\n  AORRTC was tested with the Panda (7 DoF) and Fetch (8 DoF) robotic arms on\nthe MotionBenchMaker dataset. These experiments show that AORRTC finds initial\nsolutions as fast as RRT-Connect and faster than the tested state-of-the-art\na.s.a.o. algorithms while converging to better solutions faster. AORRTC finds\nsolutions to difficult high-DoF planning problems in milliseconds where the\nother a.s.a.o. planners could not consistently find solutions in seconds. This\nperformance was demonstrated both with and without single instruction/multiple\ndata (SIMD) acceleration.\n","authors":["Tyler Wilson","Wil Thomason","Zachary Kingston","Jonathan Gammell"],"pdf_url":"https://arxiv.org/pdf/2505.10542v1.pdf","comment":"8 pages, 4 figures, 1 table. A video of AORRTC can be found at\n  https://www.youtube.com/watch?v=j1itxP3KuiM . Information on the\n  implementation of AORRTC is available at https://robotic-esp.com/code/aorrtc/"},{"id":"http://arxiv.org/abs/2505.10522v1","updated":"2025-05-15T17:30:29Z","published":"2025-05-15T17:30:29Z","title":"Knowledge capture, adaptation and composition (KCAC): A framework for\n  cross-task curriculum learning in robotic manipulation","summary":"  Reinforcement learning (RL) has demonstrated remarkable potential in robotic\nmanipulation but faces challenges in sample inefficiency and lack of\ninterpretability, limiting its applicability in real world scenarios. Enabling\nthe agent to gain a deeper understanding and adapt more efficiently to diverse\nworking scenarios is crucial, and strategic knowledge utilization is a key\nfactor in this process. This paper proposes a Knowledge Capture, Adaptation,\nand Composition (KCAC) framework to systematically integrate knowledge transfer\ninto RL through cross-task curriculum learning. KCAC is evaluated using a two\nblock stacking task in the CausalWorld benchmark, a complex robotic\nmanipulation environment. To our knowledge, existing RL approaches fail to\nsolve this task effectively, reflecting deficiencies in knowledge capture. In\nthis work, we redesign the benchmark reward function by removing rigid\nconstraints and strict ordering, allowing the agent to maximize total rewards\nconcurrently and enabling flexible task completion. Furthermore, we define two\nself-designed sub-tasks and implement a structured cross-task curriculum to\nfacilitate efficient learning. As a result, our KCAC approach achieves a 40\npercent reduction in training time while improving task success rates by 10\npercent compared to traditional RL methods. Through extensive evaluation, we\nidentify key curriculum design parameters subtask selection, transition timing,\nand learning rate that optimize learning efficiency and provide conceptual\nguidance for curriculum based RL frameworks. This work offers valuable insights\ninto curriculum design in RL and robotic learning.\n","authors":["Xinrui Wang","Yan Jin"],"pdf_url":"https://arxiv.org/pdf/2505.10522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10442v1","updated":"2025-05-15T16:01:21Z","published":"2025-05-15T16:01:21Z","title":"IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy\n  Fine-Tuning","summary":"  Imitation learning (IL) and reinforcement learning (RL) each offer distinct\nadvantages for robotics policy learning: IL provides stable learning from\ndemonstrations, and RL promotes generalization through exploration. While\nexisting robot learning approaches using IL-based pre-training followed by\nRL-based fine-tuning are promising, this two-step learning paradigm often\nsuffers from instability and poor sample efficiency during the RL fine-tuning\nphase. In this work, we introduce IN-RIL, INterleaved Reinforcement learning\nand Imitation Learning, for policy fine-tuning, which periodically injects IL\nupdates after multiple RL updates and hence can benefit from the stability of\nIL and the guidance of expert data for more efficient exploration throughout\nthe entire fine-tuning process. Since IL and RL involve different optimization\nobjectives, we develop gradient separation mechanisms to prevent destructive\ninterference during \\ABBR fine-tuning, by separating possibly conflicting\ngradient updates in orthogonal subspaces. Furthermore, we conduct rigorous\nanalysis, and our findings shed light on why interleaving IL with RL stabilizes\nlearning and improves sample-efficiency. Extensive experiments on 14 robot\nmanipulation and locomotion tasks across 3 benchmarks, including\nFurnitureBench, OpenAI Gym, and Robomimic, demonstrate that \\ABBR can\nsignificantly improve sample efficiency and mitigate performance collapse\nduring online finetuning in both long- and short-horizon tasks with either\nsparse or dense rewards. IN-RIL, as a general plug-in compatible with various\nstate-of-the-art RL algorithms, can significantly improve RL fine-tuning, e.g.,\nfrom 12\\% to 88\\% with 6.3x improvement in the success rate on Robomimic\nTransport. Project page: https://github.com/ucd-dare/IN-RIL.\n","authors":["Dechen Gao","Hang Wang","Hanchu Zhou","Nejib Ammar","Shatadal Mishra","Ahmadreza Moradipari","Iman Soltani","Junshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.10442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10415v1","updated":"2025-05-15T15:35:00Z","published":"2025-05-15T15:35:00Z","title":"Internal State Estimation in Groups via Active Information Gathering","summary":"  Accurately estimating human internal states, such as personality traits or\nbehavioral patterns, is critical for enhancing the effectiveness of human-robot\ninteraction, particularly in group settings. These insights are key in\napplications ranging from social navigation to autism diagnosis. However, prior\nmethods are limited by scalability and passive observation, making real-time\nestimation in complex, multi-human settings difficult. In this work, we propose\na practical method for active human personality estimation in groups, with a\nfocus on applications related to Autism Spectrum Disorder (ASD). Our method\ncombines a personality-conditioned behavior model, based on the Eysenck\n3-Factor theory, with an active robot information gathering policy that\ntriggers human behaviors through a receding-horizon planner. The robot's belief\nabout human personality is then updated via Bayesian inference. We demonstrate\nthe effectiveness of our approach through simulations, user studies with\ntypical adults, and preliminary experiments involving participants with ASD.\nOur results show that our method can scale to tens of humans and reduce\npersonality prediction error by 29.2% and uncertainty by 79.9% in simulation.\nUser studies with typical adults confirm the method's ability to generalize\nacross complex personality distributions. Additionally, we explore its\napplication in autism-related scenarios, demonstrating that the method can\nidentify the difference between neurotypical and autistic behavior,\nhighlighting its potential for diagnosing ASD. The results suggest that our\nframework could serve as a foundation for future ASD-specific interventions.\n","authors":["Xuebo Ji","Zherong Pan","Xifeng Gao","Lei Yang","Xinxin Du","Kaiyun Li","Yongjin Liu","Wenping Wang","Changhe Tu","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2505.10415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10398v1","updated":"2025-05-15T15:21:46Z","published":"2025-05-15T15:21:46Z","title":"AutoCam: Hierarchical Path Planning for an Autonomous Auxiliary Camera\n  in Surgical Robotics","summary":"  Incorporating an autonomous auxiliary camera into robot-assisted minimally\ninvasive surgery (RAMIS) enhances spatial awareness and eliminates manual\nviewpoint control. Existing path planning methods for auxiliary cameras track\ntwo-dimensional surgical features but do not simultaneously account for camera\norientation, workspace constraints, and robot joint limits. This study presents\nAutoCam: an automatic auxiliary camera placement method to improve\nvisualization in RAMIS. Implemented on the da Vinci Research Kit, the system\nuses a priority-based, workspace-constrained control algorithm that combines\nheuristic geometric placement with nonlinear optimization to ensure robust\ncamera tracking. A user study (N=6) demonstrated that the system maintained\n99.84% visibility of a salient feature and achieved a pose error of 4.36 $\\pm$\n2.11 degrees and 1.95 $\\pm$ 5.66 mm. The controller was computationally\nefficient, with a loop time of 6.8 $\\pm$ 12.8 ms. An additional pilot study\n(N=6), where novices completed a Fundamentals of Laparoscopic Surgery training\ntask, suggests that users can teleoperate just as effectively from AutoCam's\nviewpoint as from the endoscope's while still benefiting from AutoCam's\nimproved visual coverage of the scene. These results indicate that an auxiliary\ncamera can be autonomously controlled using the da Vinci patient-side\nmanipulators to track a salient feature, laying the groundwork for new\nmulti-camera visualization methods in RAMIS.\n","authors":["Alexandre Banks","Randy Moore","Sayem Nazmuz Zaman","Alaa Eldin Abdelaal","Septimiu E. Salcudean"],"pdf_url":"https://arxiv.org/pdf/2505.10398v1.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.10359v1","updated":"2025-05-15T14:51:14Z","published":"2025-05-15T14:51:14Z","title":"NVSPolicy: Adaptive Novel-View Synthesis for Generalizable\n  Language-Conditioned Policy Learning","summary":"  Recent advances in deep generative models demonstrate unprecedented zero-shot\ngeneralization capabilities, offering great potential for robot manipulation in\nunstructured environments. Given a partial observation of a scene, deep\ngenerative models could generate the unseen regions and therefore provide more\ncontext, which enhances the capability of robots to generalize across unseen\nenvironments. However, due to the visual artifacts in generated images and\ninefficient integration of multi-modal features in policy learning, this\ndirection remains an open challenge. We introduce NVSPolicy, a generalizable\nlanguage-conditioned policy learning method that couples an adaptive novel-view\nsynthesis module with a hierarchical policy network. Given an input image,\nNVSPolicy dynamically selects an informative viewpoint and synthesizes an\nadaptive novel-view image to enrich the visual context. To mitigate the impact\nof the imperfect synthesized images, we adopt a cycle-consistent VAE mechanism\nthat disentangles the visual features into the semantic feature and the\nremaining feature. The two features are then fed into the hierarchical policy\nnetwork respectively: the semantic feature informs the high-level meta-skill\nselection, and the remaining feature guides low-level action estimation.\nMoreover, we propose several practical mechanisms to make the proposed method\nefficient. Extensive experiments on CALVIN demonstrate the state-of-the-art\nperformance of our method. Specifically, it achieves an average success rate of\n90.4\\% across all tasks, greatly outperforming the recent methods. Ablation\nstudies confirm the significance of our adaptive novel-view synthesis paradigm.\nIn addition, we evaluate NVSPolicy on a real-world robotic platform to\ndemonstrate its practical applicability.\n","authors":["Le Shi","Yifei Shi","Xin Xu","Tenglong Liu","Junhua Xi","Chengyuan Chen"],"pdf_url":"https://arxiv.org/pdf/2505.10359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10355v1","updated":"2025-05-15T14:46:19Z","published":"2025-05-15T14:46:19Z","title":"pc-dbCBS: Kinodynamic Motion Planning of Physically-Coupled Robot Teams","summary":"  Motion planning problems for physically-coupled multi-robot systems in\ncluttered environments are challenging due to their high dimensionality.\nExisting methods combining sampling-based planners with trajectory optimization\nproduce suboptimal results and lack theoretical guarantees. We propose\nPhysically-coupled discontinuity-bounded Conflict-Based Search (pc-dbCBS), an\nanytime kinodynamic motion planner, that extends discontinuity-bounded CBS to\nrigidly-coupled systems. Our approach proposes a tri-level conflict detection\nand resolution framework that includes the physical coupling between the\nrobots. Moreover, pc-dbCBS alternates iteratively between state space\nrepresentations, thereby preserving probabilistic completeness and asymptotic\noptimality while relying only on single-robot motion primitives. Across 25\nsimulated and six real-world problems involving multirotors carrying a\ncable-suspended payload and differential-drive robots linked by rigid rods,\npc-dbCBS solves up to 92% more instances than a state-of-the-art baseline and\nplans trajectories that are 50-60% faster while reducing planning time by an\norder of magnitude.\n","authors":["Khaled Wahba","Wolfgang Hönig"],"pdf_url":"https://arxiv.org/pdf/2505.10355v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2505.10258v1","updated":"2025-05-15T13:09:19Z","published":"2025-05-15T13:09:19Z","title":"Inferring Driving Maps by Deep Learning-based Trail Map Extraction","summary":"  High-definition (HD) maps offer extensive and accurate environmental\ninformation about the driving scene, making them a crucial and essential\nelement for planning within autonomous driving systems. To avoid extensive\nefforts from manual labeling, methods for automating the map creation have\nemerged. Recent trends have moved from offline mapping to online mapping,\nensuring availability and actuality of the utilized maps. While the performance\nhas increased in recent years, online mapping still faces challenges regarding\ntemporal consistency, sensor occlusion, runtime, and generalization. We propose\na novel offline mapping approach that integrates trails - informal routes used\nby drivers - into the map creation process. Our method aggregates trail data\nfrom the ego vehicle and other traffic participants to construct a\ncomprehensive global map using transformer-based deep learning models. Unlike\ntraditional offline mapping, our approach enables continuous updates while\nremaining sensor-agnostic, facilitating efficient data transfer. Our method\ndemonstrates superior performance compared to state-of-the-art online mapping\napproaches, achieving improved generalization to previously unseen environments\nand sensor configurations. We validate our approach on two benchmark datasets,\nhighlighting its robustness and applicability in autonomous driving systems.\n","authors":["Michael Hubbertz","Pascal Colling","Qi Han","Tobias Meisen"],"pdf_url":"https://arxiv.org/pdf/2505.10258v1.pdf","comment":"This paper was accepted at the CVPR WAD 2025 Workshop"},{"id":"http://arxiv.org/abs/2505.10251v1","updated":"2025-05-15T13:04:53Z","published":"2025-05-15T13:04:53Z","title":"SRT-H: A Hierarchical Framework for Autonomous Surgery via Language\n  Conditioned Imitation Learning","summary":"  Research on autonomous robotic surgery has largely focused on simple task\nautomation in controlled environments. However, real-world surgical\napplications require dexterous manipulation over extended time scales while\ndemanding generalization across diverse variations in human tissue. These\nchallenges remain difficult to address using existing logic-based or\nconventional end-to-end learning strategies. To bridge this gap, we propose a\nhierarchical framework for dexterous, long-horizon surgical tasks. Our method\nemploys a high-level policy for task planning and a low-level policy for\ngenerating task-space controls for the surgical robot. The high-level planner\nplans tasks using language, producing task-specific or corrective instructions\nthat guide the robot at a coarse level. Leveraging language as a planning\nmodality offers an intuitive and generalizable interface, mirroring how\nexperienced surgeons instruct traineers during procedures. We validate our\nframework in ex-vivo experiments on a complex minimally invasive procedure,\ncholecystectomy, and conduct ablative studies to assess key design choices. Our\napproach achieves a 100% success rate across n=8 different ex-vivo\ngallbladders, operating fully autonomously without human intervention. The\nhierarchical approach greatly improves the policy's ability to recover from\nsuboptimal states that are inevitable in the highly dynamic environment of\nrealistic surgical applications. This work represents the first demonstration\nof step-level autonomy, marking a critical milestone toward autonomous surgical\nsystems for clinical studies. By advancing generalizable autonomy in surgical\nrobotics, our approach brings the field closer to real-world deployment.\n","authors":["Ji Woong Kim","Juo-Tung Chen","Pascal Hansen","Lucy X. Shi","Antony Goldenberg","Samuel Schmidgall","Paul Maria Scheikl","Anton Deguet","Brandon M. White","De Ru Tsai","Richard Cha","Jeffrey Jopling","Chelsea Finn","Axel Krieger"],"pdf_url":"https://arxiv.org/pdf/2505.10251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10239v1","updated":"2025-05-15T12:53:32Z","published":"2025-05-15T12:53:32Z","title":"Context-aware collaborative pushing of heavy objects using\n  skeleton-based intention prediction","summary":"  In physical human-robot interaction, force feedback has been the most common\nsensing modality to convey the human intention to the robot. It is widely used\nin admittance control to allow the human to direct the robot. However, it\ncannot be used in scenarios where direct force feedback is not available since\nmanipulated objects are not always equipped with a force sensor. In this work,\nwe study one such scenario: the collaborative pushing and pulling of heavy\nobjects on frictional surfaces, a prevalent task in industrial settings. When\nhumans do it, they communicate through verbal and non-verbal cues, where body\nposes, and movements often convey more than words. We propose a novel\ncontext-aware approach using Directed Graph Neural Networks to analyze\nspatio-temporal human posture data to predict human motion intention for\nnon-verbal collaborative physical manipulation. Our experiments demonstrate\nthat robot assistance significantly reduces human effort and improves task\nefficiency. The results indicate that incorporating posture-based context\nrecognition, either together with or as an alternative to force sensing,\nenhances robot decision-making and control efficiency.\n","authors":["Gokhan Solak","Gustavo J. G. Lahr","Idil Ozdamar","Arash Ajoudani"],"pdf_url":"https://arxiv.org/pdf/2505.10239v1.pdf","comment":"Accepted to be presented at ICRA 2025 conference. Video:\n  https://youtu.be/qy7l_wGOyzo"},{"id":"http://arxiv.org/abs/2505.10228v1","updated":"2025-05-15T12:37:35Z","published":"2025-05-15T12:37:35Z","title":"Quad-LCD: Layered Control Decomposition Enables Actuator-Feasible\n  Quadrotor Trajectory Planning","summary":"  In this work, we specialize contributions from prior work on data-driven\ntrajectory generation for a quadrotor system with motor saturation constraints.\nWhen motors saturate in quadrotor systems, there is an ``uncontrolled drift\" of\nthe vehicle that results in a crash. To tackle saturation, we apply a control\ndecomposition and learn a tracking penalty from simulation data consisting of\nlow, medium and high-cost reference trajectories. Our approach reduces crash\nrates by around $49\\%$ compared to baselines on aggressive maneuvers in\nsimulation. On the Crazyflie hardware platform, we demonstrate feasibility\nthrough experiments that lead to successful flights. Motivated by the growing\ninterest in data-driven methods to quadrotor planning, we provide open-source\nlightweight code with an easy-to-use abstraction of hardware platforms.\n","authors":["Anusha Srikanthan","Hanli Zhang","Spencer Folk","Vijay Kumar","Nikolai Matni"],"pdf_url":"https://arxiv.org/pdf/2505.10228v1.pdf","comment":"4 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.10224v1","updated":"2025-05-15T12:32:46Z","published":"2025-05-15T12:32:46Z","title":"Force-Driven Validation for Collaborative Robotics in Automated Avionics\n  Testing","summary":"  ARTO is a project combining collaborative robots (cobots) and Artificial\nIntelligence (AI) to automate functional test procedures for civilian and\nmilitary aircraft certification. This paper proposes a Deep Learning (DL) and\neXplainable AI (XAI) approach, equipping ARTO with interaction analysis\ncapabilities to verify and validate the operations on cockpit components.\nDuring these interactions, forces, torques, and end effector poses are recorded\nand preprocessed to filter disturbances caused by low performance force\ncontrollers and embedded Force Torque Sensors (FTS). Convolutional Neural\nNetworks (CNNs) then classify the cobot actions as Success or Fail, while also\nidentifying and reporting the causes of failure. To improve interpretability,\nGrad CAM, an XAI technique for visual explanations, is integrated to provide\ninsights into the models decision making process. This approach enhances the\nreliability and trustworthiness of the automated testing system, facilitating\nthe diagnosis and rectification of errors that may arise during testing.\n","authors":["Pietro Dardano","Paolo Rocco","David Frisini"],"pdf_url":"https://arxiv.org/pdf/2505.10224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10219v1","updated":"2025-05-15T12:22:21Z","published":"2025-05-15T12:22:21Z","title":"Towards Safe Robot Foundation Models Using Inductive Biases","summary":"  Safety is a critical requirement for the real-world deployment of robotic\nsystems. Unfortunately, while current robot foundation models show promising\ngeneralization capabilities across a wide variety of tasks, they fail to\naddress safety, an important aspect for ensuring long-term operation. Current\nrobot foundation models assume that safe behavior should emerge by learning\nfrom a sufficiently large dataset of demonstrations. However, this approach has\ntwo clear major drawbacks. Firstly, there are no formal safety guarantees for a\nbehavior cloning policy trained using supervised learning. Secondly, without\nexplicit knowledge of any safety constraints, the policy may require an\nunreasonable number of additional demonstrations to even approximate the\ndesired constrained behavior. To solve these key issues, we show how we can\ninstead combine robot foundation models with geometric inductive biases using\nATACOM, a safety layer placed after the foundation policy that ensures safe\nstate transitions by enforcing action constraints. With this approach, we can\nensure formal safety guarantees for generalist policies without providing\nextensive demonstrations of safe behavior, and without requiring any specific\nfine-tuning for safety. Our experiments show that our approach can be\nbeneficial both for classical manipulation tasks, where we avoid unwanted\ncollisions with irrelevant objects, and for dynamic tasks, such as the robot\nair hockey environment, where we can generate fast trajectories respecting\ncomplex tasks and joint space constraints.\n","authors":["Maximilian Tölle","Theo Gruner","Daniel Palenicek","Tim Schneider","Jonas Günster","Joe Watson","Davide Tateo","Puze Liu","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2505.10219v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.03254v2","updated":"2025-05-15T12:21:59Z","published":"2024-12-04T11:52:55Z","title":"Remote Manipulation of Multiple Objects with Airflow Field Using\n  Model-Based Learning Control","summary":"  Non-contact manipulation is a promising methodology in robotics, offering a\nwide range of scientific and industrial applications. Among the proposed\napproaches, airflow stands out for its ability to project across considerable\ndistances and its flexibility in actuating objects of varying materials, sizes,\nand shapes. However, predicting airflow fields at a distance-and the motion of\nobjects within them-remains notoriously challenging due to their nonlinear and\nstochastic nature. Here, we propose a model-based learning approach using a\njet-induced airflow field for remote multi-object manipulation on a surface.\nOur approach incorporates an analytical model of the field, learned object\ndynamics, and a model-based controller. The model predicts an air velocity\nfield over an infinite surface for a specified jet orientation, while the\nobject dynamics are learned through a robust system identification algorithm.\nUsing the model-based controller, we can automatically and remotely, at\nmeter-scale distances, control the motion of single and multiple objects for\ndifferent tasks, such as path-following, aggregating, and sorting.\n","authors":["Artur Kopitca","Shahriar Haeri","Quan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.03254v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.09305v2","updated":"2025-05-15T12:17:13Z","published":"2025-05-14T11:41:55Z","title":"Embodied Intelligent Industrial Robotics: Concepts and Techniques","summary":"  In recent years, embodied intelligent robotics (EIR) has made significant\nprogress in multi-modal perception, autonomous decision-making, and physical\ninteraction. Some robots have already been tested in general-purpose scenarios\nsuch as homes and shopping malls. We aim to advance the research and\napplication of embodied intelligence in industrial scenes. However, current EIR\nlacks a deep understanding of industrial environment semantics and the\nnormative constraints between industrial operating objects. To address this\ngap, this paper first reviews the history of industrial robotics and the\nmainstream EIR frameworks. We then introduce the concept of the embodied\nintelligent industrial robotics (EIIR) and propose a knowledge-driven EIIR\ntechnology framework for industrial environments. The framework includes four\nmain modules: world model, high-level task planner, low-level skill controller,\nand simulator. We also review the current development of technologies related\nto each module and highlight recent progress in adapting them to industrial\napplications. Finally, we summarize the key challenges EIIR faces in industrial\nscenarios and suggest future research directions. We believe that EIIR\ntechnology will shape the next generation of industrial robotics. Industrial\nsystems based on embodied intelligent industrial robots offer strong potential\nfor enabling intelligent manufacturing. We will continue to track and summarize\nnew research in this area and hope this review will serve as a valuable\nreference for scholars and engineers interested in industrial embodied\nintelligence. Together, we can help drive the rapid advancement and application\nof this technology. The associated project can be found at\nhttps://github.com/jackyzengl/EIIR.\n","authors":["Chaoran Zhang","Chenhao Zhang","Zhaobo Xu","Qinghongbing Xie","Pingfa Feng","Long Zeng"],"pdf_url":"https://arxiv.org/pdf/2505.09305v2.pdf","comment":"60 pages, 11 figures. The associated project can be found at\n  https://github.com/jackyzengl/EIIR"},{"id":"http://arxiv.org/abs/2410.11758v2","updated":"2025-05-15T12:13:37Z","published":"2024-10-15T16:28:09Z","title":"Latent Action Pretraining from Videos","summary":"  We introduce Latent Action Pretraining for general Action models (LAPA), an\nunsupervised method for pretraining Vision-Language-Action (VLA) models without\nground-truth robot action labels. Existing Vision-Language-Action models\nrequire action labels typically collected by human teleoperators during\npretraining, which significantly limits possible data sources and scale. In\nthis work, we propose a method to learn from internet-scale videos that do not\nhave robot action labels. We first train an action quantization model\nleveraging VQ-VAE-based objective to learn discrete latent actions between\nimage frames, then pretrain a latent VLA model to predict these latent actions\nfrom observations and task descriptions, and finally finetune the VLA on\nsmall-scale robot manipulation data to map from latent to robot actions.\nExperimental results demonstrate that our method significantly outperforms\nexisting techniques that train robot manipulation policies from large-scale\nvideos. Furthermore, it outperforms the state-of-the-art VLA model trained with\nrobotic action labels on real-world manipulation tasks that require language\nconditioning, generalization to unseen objects, and semantic generalization to\nunseen instructions. Training only on human manipulation videos also shows\npositive transfer, opening up the potential for leveraging web-scale data for\nrobotics foundation model.\n","authors":["Seonghyeon Ye","Joel Jang","Byeongguk Jeon","Sejune Joo","Jianwei Yang","Baolin Peng","Ajay Mandlekar","Reuben Tan","Yu-Wei Chao","Bill Yuchen Lin","Lars Liden","Kimin Lee","Jianfeng Gao","Luke Zettlemoyer","Dieter Fox","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2410.11758v2.pdf","comment":"ICLR 2025 Website: https://latentactionpretraining.github.io"},{"id":"http://arxiv.org/abs/2312.01797v3","updated":"2025-05-15T11:25:13Z","published":"2023-12-04T10:37:58Z","title":"LLM A*: Human in the Loop Large Language Models Enabled A* Search for\n  Robotics","summary":"  This research focuses on how Large Language Models (LLMs) can help with\n(path) planning for mobile embodied agents such as robots, in a\nhuman-in-the-loop and interactive manner. A novel framework named LLM A*, aims\nto leverage the commonsense of LLMs, and the utility-optimal A* is proposed to\nfacilitate few-shot near-optimal path planning. Prompts are used for two main\npurposes: 1) to provide LLMs with essential information like environments,\ncosts, heuristics, etc.; 2) to communicate human feedback on intermediate\nplanning results to LLMs. This approach takes human feedback on board and\nrenders the entire planning process transparent (akin to a `white box') to\nhumans. Moreover, it facilitates code-free path planning, thereby fostering the\naccessibility and inclusiveness of artificial intelligence techniques to\ncommunities less proficient in coding. Comparative analysis against A* and RL\ndemonstrates that LLM A* exhibits greater efficiency in terms of search space\nand achieves paths comparable to A* while outperforming RL. The interactive\nnature of LLM A* also makes it a promising tool for deployment in collaborative\nhuman-robot tasks. Codes and Supplemental Materials can be found at GitHub:\nhttps://github.com/speedhawk/LLM-A-.\n","authors":["Hengjia Xiao","Peng Wang","Mingzhe Yu","Mattia Robbiani"],"pdf_url":"https://arxiv.org/pdf/2312.01797v3.pdf","comment":"7 figures, 8 pages"},{"id":"http://arxiv.org/abs/2505.06111v2","updated":"2025-05-15T10:31:45Z","published":"2025-05-09T15:11:13Z","title":"UniVLA: Learning to Act Anywhere with Task-centric Latent Actions","summary":"  A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning.\n","authors":["Qingwen Bu","Yanting Yang","Jisong Cai","Shenyuan Gao","Guanghui Ren","Maoqing Yao","Ping Luo","Hongyang Li"],"pdf_url":"https://arxiv.org/pdf/2505.06111v2.pdf","comment":"Accepted to RSS 2025. Code is available at\n  https://github.com/OpenDriveLab/UniVLA"},{"id":"http://arxiv.org/abs/2505.10151v1","updated":"2025-05-15T10:23:56Z","published":"2025-05-15T10:23:56Z","title":"Training People to Reward Robots","summary":"  Learning from demonstration (LfD) is a technique that allows expert teachers\nto teach task-oriented skills to robotic systems. However, the most effective\nway of guiding novice teachers to approach expert-level demonstrations\nquantitatively for specific teaching tasks remains an open question. To this\nend, this paper investigates the use of machine teaching (MT) to guide novice\nteachers to improve their teaching skills based on reinforcement learning from\ndemonstration (RLfD). The paper reports an experiment in which novices receive\nMT-derived guidance to train their ability to teach a given motor skill with\nonly 8 demonstrations and generalise this to previously unseen ones. Results\nindicate that the MT-guidance not only enhances robot learning performance by\n89% on the training skill but also causes a 70% improvement in robot learning\nperformance on skills not seen by subjects during training. These findings\nhighlight the effectiveness of MT-guidance in upskilling human teaching\nbehaviours, ultimately improving demonstration quality in RLfD.\n","authors":["Endong Sun","Yuqing Zhu","Matthew Howard"],"pdf_url":"https://arxiv.org/pdf/2505.10151v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2505.10105v1","updated":"2025-05-15T09:12:17Z","published":"2025-05-15T09:12:17Z","title":"EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot\n  Manipulation","summary":"  We present EmbodiedMAE, a unified 3D multi-modal representation for robot\nmanipulation. Current approaches suffer from significant domain gaps between\ntraining datasets and robot manipulation tasks, while also lacking model\narchitectures that can effectively incorporate 3D information. To overcome\nthese limitations, we enhance the DROID dataset with high-quality depth maps\nand point clouds, constructing DROID-3D as a valuable supplement for 3D\nembodied vision research. Then we develop EmbodiedMAE, a multi-modal masked\nautoencoder that simultaneously learns representations across RGB, depth, and\npoint cloud modalities through stochastic masking and cross-modal fusion.\nTrained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-art\nvision foundation models (VFMs) in both training efficiency and final\nperformance across 70 simulation tasks and 20 real-world robot manipulation\ntasks on two robot platforms. The model exhibits strong scaling behavior with\nsize and promotes effective policy learning from 3D inputs. Experimental\nresults establish EmbodiedMAE as a reliable unified 3D multi-modal VFM for\nembodied AI systems, particularly in precise tabletop manipulation settings\nwhere spatial perception is critical.\n","authors":["Zibin Dong","Fei Ni","Yifu Yuan","Yinchuan Li","Jianye Hao"],"pdf_url":"https://arxiv.org/pdf/2505.10105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09434v2","updated":"2025-05-15T08:54:18Z","published":"2025-05-14T14:42:07Z","title":"Decentralized Nonlinear Model Predictive Control-Based Flock Navigation\n  with Real-Time Obstacle Avoidance in Unknown Obstructed Environments","summary":"  This work extends our prior work on the distributed nonlinear model\npredictive control (NMPC) for navigating a robot fleet following a certain\nflocking behavior in unknown obstructed environments with a more realistic\nlocal obstacle avoidance strategy. More specifically, we integrate the local\nobstacle avoidance constraint using point clouds into the NMPC framework. Here,\neach agent relies on data from its local sensor to perceive and respond to\nnearby obstacles. A point cloud processing technique is presented for both\ntwo-dimensional and three-dimensional point clouds to minimize the\ncomputational burden during the optimization. The process consists of\ndirectional filtering and down-sampling that significantly reduce the number of\ndata points. The algorithm's performance is validated through realistic 3D\nsimulations in Gazebo, and its practical feasibility is further explored via\nhardware-in-the-loop (HIL) simulations on embedded platforms.\n","authors":["Nuthasith Gerdpratoom","Kaoru Yamamoto"],"pdf_url":"https://arxiv.org/pdf/2505.09434v2.pdf","comment":"22 pages, 14 figures, to be published in Frontiers in Robotics and AI"},{"id":"http://arxiv.org/abs/2505.10075v1","updated":"2025-05-15T08:27:16Z","published":"2025-05-15T08:27:16Z","title":"FlowDreamer: A RGB-D World Model with Flow-based Motion Representations\n  for Robot Manipulation","summary":"  This paper investigates training better visual world models for robot\nmanipulation, i.e., models that can predict future visual observations by\nconditioning on past frames and robot actions. Specifically, we consider world\nmodels that operate on RGB-D frames (RGB-D world models). As opposed to\ncanonical approaches that handle dynamics prediction mostly implicitly and\nreconcile it with visual rendering in a single model, we introduce FlowDreamer,\nwhich adopts 3D scene flow as explicit motion representations. FlowDreamer\nfirst predicts 3D scene flow from past frame and action conditions with a\nU-Net, and then a diffusion model will predict the future frame utilizing the\nscene flow. FlowDreamer is trained end-to-end despite its modularized nature.\nWe conduct experiments on 4 different benchmarks, covering both video\nprediction and visual planning tasks. The results demonstrate that FlowDreamer\nachieves better performance compared to other baseline RGB-D world models by 7%\non semantic similarity, 11% on pixel quality, and 6% on success rate in various\nrobot manipulation domains.\n","authors":["Jun Guo","Xiaojian Ma","Yikai Wang","Min Yang","Huaping Liu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2505.10075v1.pdf","comment":"Project page: see https://sharinka0715.github.io/FlowDreamer/"},{"id":"http://arxiv.org/abs/2505.10073v1","updated":"2025-05-15T08:20:57Z","published":"2025-05-15T08:20:57Z","title":"Multi-Robot Task Allocation for Homogeneous Tasks with Collision\n  Avoidance via Spatial Clustering","summary":"  In this paper, a novel framework is presented that achieves a combined\nsolution based on Multi-Robot Task Allocation (MRTA) and collision avoidance\nwith respect to homogeneous measurement tasks taking place in industrial\nenvironments. The spatial clustering we propose offers to simultaneously solve\nthe task allocation problem and deal with collision risks by cutting the\nworkspace into distinguishable operational zones for each robot. To divide task\nsites and to schedule robot routes within corresponding clusters, we use\nK-means clustering and the 2-Opt algorithm. The presented framework shows\nsatisfactory performance, where up to 93\\% time reduction (1.24s against\n17.62s) with a solution quality improvement of up to 7\\% compared to the best\nperforming method is demonstrated. Our method also completely eliminates\ncollision points that persist in comparative methods in a most significant\nsense. Theoretical analysis agrees with the claim that spatial partitioning\nunifies the apparently disjoint tasks allocation and collision avoidance\nproblems under conditions of many identical tasks to be distributed over sparse\ngeographical areas. Ultimately, the findings in this work are of substantial\nimportance for real world applications where both computational efficiency and\noperation free from collisions is of paramount importance.\n","authors":["Rathin Chandra Shit","Sharmila Subudhi"],"pdf_url":"https://arxiv.org/pdf/2505.10073v1.pdf","comment":"5 pages, 4 figures, Scheduled for presentation at an upcoming\n  conference"},{"id":"http://arxiv.org/abs/2503.07338v2","updated":"2025-05-15T08:04:19Z","published":"2025-03-10T13:50:23Z","title":"Temporal Triplane Transformers as Occupancy World Models","summary":"  World models aim to learn or construct representations of the environment\nthat enable the prediction of future scenes, thereby supporting intelligent\nmotion planning. However, existing models often struggle to produce\nfine-grained predictions and to operate in real time. In this work, we propose\nT$^3$Former, a novel 4D occupancy world model for autonomous driving.\nT$^3$Former begins by pre-training a compact {\\em triplane} representation that\nefficiently encodes 3D occupancy. It then extracts multi-scale temporal motion\nfeatures from historical triplanes and employs an autoregressive approach to\niteratively predict future triplane changes. Finally, these triplane changes\nare combined with previous states to decode future occupancy and ego-motion\ntrajectories. Experimental results show that T$^3$Former achieves 1.44$\\times$\nspeedup (26 FPS), improves mean IoU to 36.09, and reduces mean absolute\nplanning error to 1.0 meters. Demos are available in the supplementary\nmaterial.\n","authors":["Haoran Xu","Peixi Peng","Guang Tan","Yiqian Chang","Yisen Zhao","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2503.07338v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10033v1","updated":"2025-05-15T07:29:16Z","published":"2025-05-15T07:29:16Z","title":"Evaluating Robustness of Deep Reinforcement Learning for Autonomous\n  Surface Vehicle Control in Field Tests","summary":"  Despite significant advancements in Deep Reinforcement Learning (DRL) for\nAutonomous Surface Vehicles (ASVs), their robustness in real-world conditions,\nparticularly under external disturbances, remains insufficiently explored. In\nthis paper, we evaluate the resilience of a DRL-based agent designed to capture\nfloating waste under various perturbations. We train the agent using domain\nrandomization and evaluate its performance in real-world field tests, assessing\nits ability to handle unexpected disturbances such as asymmetric drag and an\noff-center payload. We assess the agent's performance under these perturbations\nin both simulation and real-world experiments, quantifying performance\ndegradation and benchmarking it against an MPC baseline. Results indicate that\nthe DRL agent performs reliably despite significant disturbances. Along with\nthe open-source release of our implementation, we provide insights into\neffective training strategies, real-world challenges, and practical\nconsiderations for deploying DRLbased ASV controllers.\n","authors":["Luis F. W. Batista","Stéphanie Aravecchia","Seth Hutchinson","Cédric Pradalier"],"pdf_url":"https://arxiv.org/pdf/2505.10033v1.pdf","comment":"Workshop on Field Robotics at ICRA 2025"},{"id":"http://arxiv.org/abs/2505.09427v2","updated":"2025-05-15T07:22:20Z","published":"2025-05-14T14:28:24Z","title":"SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation","summary":"  Large Language Models (LLMs) show growing promise in autonomous driving by\nreasoning over complex traffic scenarios to generate path plans. However, their\ntendencies toward overconfidence, and hallucinations raise critical safety\nconcerns. We introduce SafePath, a modular framework that augments LLM-based\npath planning with formal safety guarantees using conformal prediction.\nSafePath operates in three stages. In the first stage, we use an LLM that\ngenerates a set of diverse candidate paths, exploring possible trajectories\nbased on agent behaviors and environmental cues. In the second stage, SafePath\nfilters out high-risk trajectories while guaranteeing that at least one safe\noption is included with a user-defined probability, through a multiple-choice\nquestion-answering formulation that integrates conformal prediction. In the\nfinal stage, our approach selects the path with the lowest expected collision\nrisk when uncertainty is low or delegates control to a human when uncertainty\nis high. We theoretically prove that SafePath guarantees a safe trajectory with\na user-defined probability, and we show how its human delegation rate can be\ntuned to balance autonomy and safety. Extensive experiments on nuScenes and\nHighway-env show that SafePath reduces planning uncertainty by 77\\% and\ncollision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven\npath planning more safer.\n","authors":["Achref Doula","Max Mühlhäuser","Alejandro Sanchez Guinea"],"pdf_url":"https://arxiv.org/pdf/2505.09427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10028v1","updated":"2025-05-15T07:20:12Z","published":"2025-05-15T07:20:12Z","title":"Fast Heuristic Scheduling and Trajectory Planning for Robotic Fruit\n  Harvesters with Multiple Cartesian Arms","summary":"  This work proposes a fast heuristic algorithm for the coupled scheduling and\ntrajectory planning of multiple Cartesian robotic arms harvesting fruits. Our\nmethod partitions the workspace, assigns fruit-picking sequences to arms,\ndetermines tight and feasible fruit-picking schedules and vehicle travel speed,\nand generates smooth, collision-free arm trajectories. The fruit-picking\nthroughput achieved by the algorithm was assessed using synthetically generated\nfruit coordinates and a harvester design featuring up to 12 arms. The\nthroughput increased monotonically as more arms were added. Adding more arms\nwhen fruit densities were low resulted in diminishing gains because it took\nlonger to travel from one fruit to another. However, when there were enough\nfruits, the proposed algorithm achieved a linear speedup as the number of arms\nincreased.\n","authors":["Yuankai Zhu","Stavros Vougioukas"],"pdf_url":"https://arxiv.org/pdf/2505.10028v1.pdf","comment":"This work will be submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2505.10022v1","updated":"2025-05-15T07:09:23Z","published":"2025-05-15T07:09:23Z","title":"APEX: Action Priors Enable Efficient Exploration for Skill Imitation on\n  Articulated Robots","summary":"  Learning by imitation provides an effective way for robots to develop\nwell-regulated complex behaviors and directly benefit from natural\ndemonstrations. State-of-the-art imitation learning (IL) approaches typically\nleverage Adversarial Motion Priors (AMP), which, despite their impressive\nresults, suffer from two key limitations. They are prone to mode collapse,\nwhich often leads to overfitting to the simulation environment and thus\nincreased sim-to-real gap, and they struggle to learn diverse behaviors\neffectively. To overcome these limitations, we introduce APEX (Action Priors\nenable Efficient eXploration): a simple yet versatile imitation learning\nframework that integrates demonstrations directly into reinforcement learning\n(RL), maintaining high exploration while grounding behavior with\nexpert-informed priors. We achieve this through a combination of decaying\naction priors, which initially bias exploration toward expert demonstrations\nbut gradually allow the policy to explore independently. This is complemented\nby a multi-critic RL framework that effectively balances stylistic consistency\nwith task performance. Our approach achieves sample-efficient imitation\nlearning and enables the acquisition of diverse skills within a single policy.\nAPEX generalizes to varying velocities and preserves reference-like styles\nacross complex tasks such as navigating rough terrain and climbing stairs,\nutilizing only flat-terrain kinematic motion data as a prior. We validate our\nframework through extensive hardware experiments on the Unitree Go2 quadruped.\nThere, APEX yields diverse and agile locomotion gaits, inherent gait\ntransitions, and the highest reported speed for the platform to the best of our\nknowledge (peak velocity of ~3.3 m/s on hardware). Our results establish APEX\nas a compelling alternative to existing IL methods, offering better efficiency,\nadaptability, and real-world performance.\n","authors":["Shivam Sood","Laukik B Nakhwa","Yuhong Cao","Sun Ge","Guillaume Sartoretti"],"pdf_url":"https://arxiv.org/pdf/2505.10022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10020v1","updated":"2025-05-15T07:06:04Z","published":"2025-05-15T07:06:04Z","title":"Threshold Strategy for Leaking Corner-Free Hamilton-Jacobi Reachability\n  with Decomposed Computations","summary":"  Hamilton-Jacobi (HJ) Reachability is widely used to compute value functions\nfor states satisfying specific control objectives. However, it becomes\nintractable for high-dimensional problems due to the curse of dimensionality.\nDimensionality reduction approaches are essential for mitigating this\nchallenge, whereas they could introduce the ``leaking corner issue\", leading to\ninaccuracies in the results. In this paper, we define the ``leaking corner\nissue\" in terms of value functions, propose and prove a necessary condition for\nits occurrence. We then use these theoretical contributions to introduce a new\nlocal updating method that efficiently corrects inaccurate value functions\nwhile maintaining the computational efficiency of the dimensionality reduction\napproaches. We demonstrate the effectiveness of our method through numerical\nsimulations. Although we validate our method with the self-contained subsystem\ndecomposition (SCSD), our approach is applicable to other dimensionality\nreduction techniques that introduce the ``leaking corners\".\n","authors":["Chong He","Mugilan Mariappan","Keval Vora","Mo Chen"],"pdf_url":"https://arxiv.org/pdf/2505.10020v1.pdf","comment":"7 pages, Submitted to Conference on Decision and Control (CDC)"},{"id":"http://arxiv.org/abs/2505.10018v1","updated":"2025-05-15T06:59:48Z","published":"2025-05-15T06:59:48Z","title":"LEMON-Mapping: Loop-Enhanced Large-Scale Multi-Session Point Cloud\n  Merging and Optimization for Globally Consistent Mapping","summary":"  With the rapid development of robotics, multi-robot collaboration has become\ncritical and challenging. One key problem is integrating data from multiple\nrobots to build a globally consistent and accurate map for robust cooperation\nand precise localization. While traditional multi-robot pose graph optimization\n(PGO) maintains basic global consistency, it focuses primarily on pose\noptimization and ignores the geometric structure of the map. Moreover, PGO only\nuses loop closure as a constraint between two nodes, failing to fully exploit\nits capability to maintaining local consistency of multi-robot maps. Therefore,\nPGO-based multi-robot mapping methods often suffer from serious map divergence\nand blur, especially in regions with overlapping submaps. To address this\nissue, we propose Lemon-Mapping, a loop-enhanced framework for large-scale\nmulti-session point cloud map fusion and optimization, which reasonably\nutilizes loop closure and improves the geometric quality of the map. We\nre-examine the role of loops for multi-robot mapping and introduce three key\ninnovations. First, we develop a robust loop processing mechanism that\neffectively rejects outliers and a novel loop recall strategy to recover\nmistakenly removed loops. Second, we introduce a spatial bundle adjustment\nmethod for multi-robot maps that significantly reduces the divergence in\noverlapping regions and eliminates map blur. Third, we design a PGO strategy\nthat leverages the refined constraints of bundle adjustment to extend the local\naccuracy to the global map. We validate our framework on several public\ndatasets and a self-collected dataset. Experimental results demonstrate that\nour method outperforms traditional map merging approaches in terms of mapping\naccuracy and reduction of map divergence. Scalability experiments also\ndemonstrate the strong capability of our framework to handle scenarios\ninvolving numerous robots.\n","authors":["Lijie Wang","Xiaoyi Zhong","Ziyi Xu","Kaixin Chai","Anke Zhao","Tianyu Zhao","Qianhao Wang","Fei Gao"],"pdf_url":"https://arxiv.org/pdf/2505.10018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07762v2","updated":"2025-05-15T06:09:44Z","published":"2025-03-10T18:34:08Z","title":"Multi-layer Motion Planning with Kinodynamic and Spatio-Temporal\n  Constraints","summary":"  We propose a novel, multi-layered planning approach for computing paths that\nsatisfy both kinodynamic and spatiotemporal constraints. Our three-part\nframework first establishes potential sequences to meet spatial constraints,\nusing them to calculate a geometric lead path. This path then guides an\nasymptotically optimal sampling-based kinodynamic planner, which minimizes an\nSTL-robustness cost to jointly satisfy spatiotemporal and kinodynamic\nconstraints. In our experiments, we test our method with a velocity-controlled\nAckerman-car model and demonstrate significant efficiency gains compared to\nprior art. Additionally, our method is able to generate complex path maneuvers,\nsuch as crossovers, something that previous methods had not demonstrated.\n","authors":["Jeel Chatrola","Abhiroop Ajith","Kevin Leahy","Constantinos Chamzas"],"pdf_url":"https://arxiv.org/pdf/2503.07762v2.pdf","comment":"Accepted to ACM Hybrid Systems: Computation and Control (HSCC) 2025"},{"id":"http://arxiv.org/abs/2505.09988v1","updated":"2025-05-15T06:03:02Z","published":"2025-05-15T06:03:02Z","title":"Provably safe and human-like car-following behaviors: Part 2. A\n  parsimonious multi-phase model with projected braking","summary":"  Ensuring safe and human-like trajectory planning for automated vehicles\namidst real-world uncertainties remains a critical challenge. While existing\ncar-following models often struggle to consistently provide rigorous safety\nproofs alongside human-like acceleration and deceleration patterns, we\nintroduce a novel multi-phase projection-based car-following model. This model\nis designed to balance safety and performance by incorporating bounded\nacceleration and deceleration rates while emulating key human driving\nprinciples. Building upon a foundation of fundamental driving principles and a\nmulti-phase dynamical systems analysis (detailed in Part 1 of this study\n\\citep{jin2025WA20-02_Part1}), we first highlight the limitations of extending\nstandard models like Newell's with simple bounded deceleration. Inspired by\nhuman drivers' anticipatory behavior, we mathematically define and analyze\nprojected braking profiles for both leader and follower vehicles, establishing\nsafety criteria and new phase definitions based on the projected braking\nlead-vehicle problem. The proposed parsimonious model combines an extended\nNewell's model for nominal driving with a new control law for scenarios\nrequiring projected braking. Using speed-spacing phase plane analysis, we\nprovide rigorous mathematical proofs of the model's adherence to defined safe\nand human-like driving principles, including collision-free operation, bounded\ndeceleration, and acceptable safe stopping distance, under reasonable initial\nconditions. Numerical simulations validate the model's superior performance in\nachieving both safety and human-like braking profiles for the stationary\nlead-vehicle problem. Finally, we discuss the model's implications and future\nresearch directions.\n","authors":["Wen-Long Jin"],"pdf_url":"https://arxiv.org/pdf/2505.09988v1.pdf","comment":"27 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.09987v1","updated":"2025-05-15T05:56:13Z","published":"2025-05-15T05:56:13Z","title":"Provably safe and human-like car-following behaviors: Part 1. Analysis\n  of phases and dynamics in standard models","summary":"  Trajectory planning is essential for ensuring safe driving in the face of\nuncertainties related to communication, sensing, and dynamic factors such as\nweather, road conditions, policies, and other road users. Existing\ncar-following models often lack rigorous safety proofs and the ability to\nreplicate human-like driving behaviors consistently. This article applies\nmulti-phase dynamical systems analysis to well-known car-following models to\nhighlight the characteristics and limitations of existing approaches. We begin\nby formulating fundamental principles for safe and human-like car-following\nbehaviors, which include zeroth-order principles for comfort and minimum jam\nspacings, first-order principles for speeds and time gaps, and second-order\nprinciples for comfort acceleration/deceleration bounds as well as braking\nprofiles. From a set of these zeroth- and first-order principles, we derive\nNewell's simplified car-following model. Subsequently, we analyze phases within\nthe speed-spacing plane for the stationary lead-vehicle problem in Newell's\nmodel and its extensions, which incorporate both bounded acceleration and\ndeceleration. We then analyze the performance of the Intelligent Driver Model\nand the Gipps model. Through this analysis, we highlight the limitations of\nthese models with respect to some of the aforementioned principles. Numerical\nsimulations and empirical observations validate the theoretical insights.\nFinally, we discuss future research directions to further integrate safety,\nhuman-like behaviors, and vehicular automation in car-following models, which\nare addressed in Part 2 of this study \\citep{jin2025WA20-02_Part2}, where we\ndevelop a novel multi-phase projection-based car-following model that addresses\nthe limitations identified here.\n","authors":["Wen-Long Jin"],"pdf_url":"https://arxiv.org/pdf/2505.09987v1.pdf","comment":"29 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.09979v1","updated":"2025-05-15T05:32:57Z","published":"2025-05-15T05:32:57Z","title":"Learning Diverse Natural Behaviors for Enhancing the Agility of\n  Quadrupedal Robots","summary":"  Achieving animal-like agility is a longstanding goal in quadrupedal robotics.\nWhile recent studies have successfully demonstrated imitation of specific\nbehaviors, enabling robots to replicate a broader range of natural behaviors in\nreal-world environments remains an open challenge. Here we propose an\nintegrated controller comprising a Basic Behavior Controller (BBC) and a\nTask-Specific Controller (TSC) which can effectively learn diverse natural\nquadrupedal behaviors in an enhanced simulator and efficiently transfer them to\nthe real world. Specifically, the BBC is trained using a novel semi-supervised\ngenerative adversarial imitation learning algorithm to extract diverse\nbehavioral styles from raw motion capture data of real dogs, enabling smooth\nbehavior transitions by adjusting discrete and continuous latent variable\ninputs. The TSC, trained via privileged learning with depth images as input,\ncoordinates the BBC to efficiently perform various tasks. Additionally, we\nemploy evolutionary adversarial simulator identification to optimize the\nsimulator, aligning it closely with reality. After training, the robot exhibits\ndiverse natural behaviors, successfully completing the quadrupedal agility\nchallenge at an average speed of 1.1 m/s and achieving a peak speed of 3.2 m/s\nduring hurdling. This work represents a substantial step toward animal-like\nagility in quadrupedal robots, opening avenues for their deployment in\nincreasingly complex real-world environments.\n","authors":["Huiqiao Fu","Haoyu Dong","Wentao Xu","Zhehao Zhou","Guizhou Deng","Kaiqiang Tang","Daoyi Dong","Chunlin Chen"],"pdf_url":"https://arxiv.org/pdf/2505.09979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09103v2","updated":"2025-05-15T03:44:29Z","published":"2025-05-14T03:20:19Z","title":"VGC-RIO: A Tightly Integrated Radar-Inertial Odometry with Spatial\n  Weighted Doppler Velocity and Local Geometric Constrained RCS Histograms","summary":"  Recent advances in 4D radar-inertial odometry have demonstrated promising\npotential for autonomous lo calization in adverse conditions. However,\neffective handling of sparse and noisy radar measurements remains a critical\nchallenge. In this paper, we propose a radar-inertial odometry with a spatial\nweighting method that adapts to unevenly distributed points and a novel\npoint-description histogram for challenging point registration. To make full\nuse of the Doppler velocity from different spatial sections, we propose a\nweighting calculation model. To enhance the point cloud registration\nperformance under challenging scenarios, we con struct a novel point histogram\ndescriptor that combines local geometric features and radar cross-section (RCS)\nfeatures. We have also conducted extensive experiments on both public and\nself-constructed datasets. The results demonstrate the precision and robustness\nof the proposed VGC-RIO.\n","authors":["Jianguang Xiang","Xiaofeng He","Zizhuo Chen","Lilian Zhang","Xincan Luo","Jun Mao"],"pdf_url":"https://arxiv.org/pdf/2505.09103v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09919v1","updated":"2025-05-15T03:07:54Z","published":"2025-05-15T03:07:54Z","title":"Hyper Yoshimura: How a slight tweak on a classical folding pattern\n  unleashes meta-stability for deployable robots","summary":"  Deployable structures inspired by origami offer lightweight, compact, and\nreconfigurable solutions for robotic and architectural applications. We present\na geometric and mechanical framework for Yoshimura-Ori modules that supports a\ndiverse set of metastable states, including newly identified asymmetric\n\"pop-out\" and \"hyperfolded\" configurations. These states are governed by three\nparameters -- tilt angle, phase shift, and slant height -- and enable discrete,\nprogrammable transformations. Using this model, we develop forward and inverse\nkinematic strategies to stack modules into deployable booms that approximate\ncomplex 3D shapes. We validate our approach through mechanical tests and\ndemonstrate a tendon- and pneumatically-actuated Yoshimura Space Crane capable\nof object manipulation, solar tracking, and high load-bearing performance. A\nmeter-scale solar charging station further illustrates the design's\nscalability. These results establish Yoshimura-Ori structures as a promising\nplatform for adaptable, multifunctional deployable systems in both terrestrial\nand space environments.\n","authors":["Ziyang Zhou","Yogesh Phalak","Vishrut Deshpande","Ian Walker","Suyi Li"],"pdf_url":"https://arxiv.org/pdf/2505.09919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09915v1","updated":"2025-05-15T03:00:32Z","published":"2025-05-15T03:00:32Z","title":"Large-Scale Gaussian Splatting SLAM","summary":"  The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian\nSplatting (3DGS) have shown encouraging and impressive results for visual SLAM.\nHowever, most representative methods require RGBD sensors and are only\navailable for indoor environments. The robustness of reconstruction in\nlarge-scale outdoor scenarios remains unexplored. This paper introduces a\nlarge-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The\nproposed LSG-SLAM employs a multi-modality strategy to estimate prior poses\nunder large view changes. In tracking, we introduce feature-alignment warping\nconstraints to alleviate the adverse effects of appearance similarity in\nrendering losses. For the scalability of large-scale scenarios, we introduce\ncontinuous Gaussian Splatting submaps to tackle unbounded scenes with limited\nmemory. Loops are detected between GS submaps by place recognition and the\nrelative pose between looped keyframes is optimized utilizing rendering and\nfeature warping losses. After the global optimization of camera poses and\nGaussian points, a structure refinement module enhances the reconstruction\nquality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM\nachieves superior performance over existing Neural, 3DGS-based, and even\ntraditional approaches. Project page: https://lsg-slam.github.io.\n","authors":["Zhe Xin","Chenyang Wu","Penghui Huang","Yanyong Zhang","Yinian Mao","Guoquan Huang"],"pdf_url":"https://arxiv.org/pdf/2505.09915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08604v2","updated":"2025-05-15T01:34:30Z","published":"2025-03-11T16:42:36Z","title":"EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in\n  Open Environments","summary":"  Developing autonomous home robots controlled by natural language has long\nbeen a pursuit of humanity. While advancements in large language models (LLMs)\nand embodied intelligence make this goal closer, several challenges persist:\nthe lack of a unified benchmark for more complex robot tasks, limited\nevaluation methods and metrics, data incompatibility between LLMs and mobile\nmanipulation trajectories. To address these issues, we propose Embodied Mobile\nManipulation in Open Environments (EMMOE), a benchmark that requires agents to\ninterpret user instructions and execute long-horizon everyday tasks in\ncontinuous space. EMMOE seamlessly integrates high-level and low-level embodied\ntasks into a unified framework, along with three new metrics for more diverse\nassessment. Additionally, we collect~\\dataset, which features in various task\nattributes, detailed process annotations, re-plans after failures, and two\nsub-datasets for LLM training. Furthermore, we design~\\model, a sophisticated\nagent system consists of LLM with Direct Preference Optimization (DPO), light\nweighted navigation and manipulation models, and multiple error detection\nmechanisms. Finally, we demonstrate~\\model's performance and evaluations of\ndifferent models and policies.\n","authors":["Dongping Li","Tielong Cai","Tianci Tang","Wenhao Chai","Katherine Rose Driggs-Campbell","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2503.08604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09889v1","updated":"2025-05-15T01:12:48Z","published":"2025-05-15T01:12:48Z","title":"Diffusion-SAFE: Shared Autonomy Framework with Diffusion for Safe\n  Human-to-Robot Driving Handover","summary":"  Safe handover in shared autonomy for vehicle control is well-established in\nmodern vehicles. However, avoiding accidents often requires action several\nseconds in advance. This necessitates understanding human driver behavior and\nan expert control strategy for seamless intervention when a collision or unsafe\nstate is predicted. We propose Diffusion-SAFE, a closed-loop shared autonomy\nframework leveraging diffusion models to: (1) predict human driving behavior\nfor detection of potential risks, (2) generate safe expert trajectories, and\n(3) enable smooth handovers by blending human and expert policies over a short\ntime horizon. Unlike prior works which use engineered score functions to rate\ndriving performance, our approach enables both performance evaluation and\noptimal action sequence generation from demonstrations. By adjusting the\nforward and reverse processes of the diffusion-based copilot, our method\nensures a gradual transition of control authority, by mimicking the drivers'\nbehavior before intervention, which mitigates abrupt takeovers, leading to\nsmooth transitions. We evaluated Diffusion-SAFE in both simulation\n(CarRacing-v0) and real-world (ROS-based race car), measuring human-driving\nsimilarity, safety, and computational efficiency. Results demonstrate a 98.5\\%\nsuccessful handover rate, highlighting the framework's effectiveness in\nprogressively correcting human actions and continuously sampling optimal robot\nactions.\n","authors":["Yunxin Fan","Monroe Kennedy III"],"pdf_url":"https://arxiv.org/pdf/2505.09889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09887v1","updated":"2025-05-15T01:10:16Z","published":"2025-05-15T01:10:16Z","title":"Unsupervised Radar Point Cloud Enhancement via Arbitrary LiDAR Guided\n  Diffusion Prior","summary":"  In industrial automation, radar is a critical sensor in machine perception.\nHowever, the angular resolution of radar is inherently limited by the Rayleigh\ncriterion, which depends on both the radar's operating wavelength and the\neffective aperture of its antenna array.To overcome these hardware-imposed\nlimitations, recent neural network-based methods have leveraged high-resolution\nLiDAR data, paired with radar measurements, during training to enhance radar\npoint cloud resolution. While effective, these approaches require extensive\npaired datasets, which are costly to acquire and prone to calibration error.\nThese challenges motivate the need for methods that can improve radar\nresolution without relying on paired high-resolution ground-truth data. Here,\nwe introduce an unsupervised radar points enhancement algorithm that employs an\narbitrary LiDAR-guided diffusion model as a prior without the need for paired\ntraining data. Specifically, our approach formulates radar angle estimation\nrecovery as an inverse problem and incorporates prior knowledge through a\ndiffusion model with arbitrary LiDAR domain knowledge. Experimental results\ndemonstrate that our method attains high fidelity and low noise performance\ncompared to traditional regularization techniques. Additionally, compared to\npaired training methods, it not only achieves comparable performance but also\noffers improved generalization capability. To our knowledge, this is the first\napproach that enhances radar points output by integrating prior knowledge via a\ndiffusion model rather than relying on paired training data. Our code is\navailable at https://github.com/yyxr75/RadarINV.\n","authors":["Yanlong Yang","Jianan Liu","Guanxiong Luo","Hao Li","Euijoon Ahn","Mostafa Rahimi Azghadi","Tao Huang"],"pdf_url":"https://arxiv.org/pdf/2505.09887v1.pdf","comment":"19 pages, 15 figures, 4 tables"},{"id":"http://arxiv.org/abs/2505.07096v2","updated":"2025-05-15T00:43:19Z","published":"2025-05-11T19:04:00Z","title":"X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real","summary":"  Human videos offer a scalable way to train robot manipulation policies, but\nlack the action labels needed by standard imitation learning algorithms.\nExisting cross-embodiment approaches try to map human motion to robot actions,\nbut often fail when the embodiments differ significantly. We propose X-Sim, a\nreal-to-sim-to-real framework that uses object motion as a dense and\ntransferable signal for learning robot policies. X-Sim starts by reconstructing\na photorealistic simulation from an RGBD human video and tracking object\ntrajectories to define object-centric rewards. These rewards are used to train\na reinforcement learning (RL) policy in simulation. The learned policy is then\ndistilled into an image-conditioned diffusion policy using synthetic rollouts\nrendered with varied viewpoints and lighting. To transfer to the real world,\nX-Sim introduces an online domain adaptation technique that aligns real and\nsimulated observations during deployment. Importantly, X-Sim does not require\nany robot teleoperation data. We evaluate it across 5 manipulation tasks in 2\nenvironments and show that it: (1) improves task progress by 30% on average\nover hand-tracking and sim-to-real baselines, (2) matches behavior cloning with\n10x less data collection time, and (3) generalizes to new camera viewpoints and\ntest-time changes. Code and videos are available at\nhttps://portal-cornell.github.io/X-Sim/.\n","authors":["Prithwish Dan","Kushal Kedia","Angela Chao","Edward Weiyi Duan","Maximus Adrian Pace","Wei-Chiu Ma","Sanjiban Choudhury"],"pdf_url":"https://arxiv.org/pdf/2505.07096v2.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2505.10559v1","updated":"2025-05-15T17:59:22Z","published":"2025-05-15T17:59:22Z","title":"Neural Thermodynamic Laws for Large Language Model Training","summary":"  Beyond neural scaling laws, little is known about the laws underlying large\nlanguage models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new\nframework that offers fresh insights into LLM training dynamics. On the\ntheoretical side, we demonstrate that key thermodynamic quantities (e.g.,\ntemperature, entropy, heat capacity, thermal conduction) and classical\nthermodynamic principles (e.g., the three laws of thermodynamics and the\nequipartition theorem) naturally emerge under river-valley loss landscape\nassumptions. On the practical side, this scientific perspective yields\nintuitive guidelines for designing learning rate schedules.\n","authors":["Ziming Liu","Yizhou Liu","Jeff Gore","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2505.10559v1.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2505.10557v1","updated":"2025-05-15T17:59:21Z","published":"2025-05-15T17:59:21Z","title":"MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal\n  Mathematical Reasoning","summary":"  Natural language image-caption datasets, widely used for training Large\nMultimodal Models, mainly focus on natural scenarios and overlook the intricate\ndetails of mathematical figures that are critical for problem-solving,\nhindering the advancement of current LMMs in multimodal mathematical reasoning.\nTo this end, we propose leveraging code as supervision for cross-modal\nalignment, since code inherently encodes all information needed to generate\ncorresponding figures, establishing a precise connection between the two\nmodalities. Specifically, we co-develop our image-to-code model and dataset\nwith model-in-the-loop approach, resulting in an image-to-code model,\nFigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.\nFurthermore, we utilize FigCodifier to synthesize novel mathematical figures\nand then construct MM-MathInstruct-3M, a high-quality multimodal math\ninstruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with\nImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on\nMM-MathInstruct-3M for multimodal math problem solving. Our model achieves a\nnew open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and\nClaude 3.5 Sonnet in the geometry problem-solving subset of MathVista,\nachieving improvements of 8.9% and 9.2%. The dataset and models will be\nreleased at https://github.com/mathllm/MathCoder.\n","authors":["Ke Wang","Junting Pan","Linda Wei","Aojun Zhou","Weikang Shi","Zimu Lu","Han Xiao","Yunqiao Yang","Houxing Ren","Mingjie Zhan","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2505.10557v1.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2505.10551v1","updated":"2025-05-15T17:57:38Z","published":"2025-05-15T17:57:38Z","title":"Does Feasibility Matter? Understanding the Impact of Feasibility on\n  Synthetic Training Data","summary":"  With the development of photorealistic diffusion models, models trained in\npart or fully on synthetic data achieve progressively better results. However,\ndiffusion models still routinely generate images that would not exist in\nreality, such as a dog floating above the ground or with unrealistic texture\nartifacts. We define the concept of feasibility as whether attributes in a\nsynthetic image could realistically exist in the real-world domain; synthetic\nimages containing attributes that violate this criterion are considered\ninfeasible. Intuitively, infeasible images are typically considered\nout-of-distribution; thus, training on such images is expected to hinder a\nmodel's ability to generalize to real-world data, and they should therefore be\nexcluded from the training set whenever possible. However, does feasibility\nreally matter? In this paper, we investigate whether enforcing feasibility is\nnecessary when generating synthetic training data for CLIP-based classifiers,\nfocusing on three target attributes: background, color, and texture. We\nintroduce VariReal, a pipeline that minimally edits a given source image to\ninclude feasible or infeasible attributes given by the textual prompt generated\nby a large language model. Our experiments show that feasibility minimally\naffects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference\nin top-1 accuracy across three fine-grained datasets. Also, the attribute\nmatters on whether the feasible/infeasible images adversarially influence the\nclassification performance. Finally, mixing feasible and infeasible images in\ntraining datasets does not significantly impact performance compared to using\npurely feasible or infeasible datasets.\n","authors":["Yiwen Liu","Jessica Bader","Jae Myung Kim"],"pdf_url":"https://arxiv.org/pdf/2505.10551v1.pdf","comment":"CVPRW 2025"},{"id":"http://arxiv.org/abs/2505.03795v2","updated":"2025-05-15T17:57:28Z","published":"2025-05-01T18:13:20Z","title":"Modeling Human Behavior in a Strategic Network Game with Complex Group\n  Dynamics","summary":"  Human networks greatly impact important societal outcomes, including wealth\nand health inequality, poverty, and bullying. As such, understanding human\nnetworks is critical to learning how to promote favorable societal outcomes. As\na step toward better understanding human networks, we compare and contrast\nseveral methods for learning, from a small data set, models of human behavior\nin a strategic network game called the Junior High Game (JHG). These modeling\nmethods differ with respect to the assumptions they use to parameterize human\nbehavior (behavior vs. community-aware behavior) and the moments they model\n(mean vs. distribution). Results show that the highest-performing method,\ncalled hCAB, models the distribution of human behavior rather than the mean and\nassumes humans use community-aware behavior rather than behavior matching. When\napplied to small societies (6-11 individuals), the hCAB model closely mirrors\nthe population dynamics of human groups (with notable differences).\nAdditionally, in a user study, human participants were unable to distinguish\nindividual hCAB agents from other humans, thus illustrating that the hCAB model\nalso produces plausible (individual) human behavior in this strategic network\ngame.\n","authors":["Jonathan Skaggs","Jacob W. Crandall"],"pdf_url":"https://arxiv.org/pdf/2505.03795v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10547v1","updated":"2025-05-15T17:55:28Z","published":"2025-05-15T17:55:28Z","title":"Real-Time Out-of-Distribution Failure Prevention via Multi-Modal\n  Reasoning","summary":"  Foundation models can provide robust high-level reasoning on appropriate\nsafety interventions in hazardous scenarios beyond a robot's training data,\ni.e. out-of-distribution (OOD) failures. However, due to the high inference\nlatency of Large Vision and Language Models, current methods rely on manually\ndefined intervention policies to enact fallbacks, thereby lacking the ability\nto plan generalizable, semantically safe motions. To overcome these challenges\nwe present FORTRESS, a framework that generates and reasons about semantically\nsafe fallback strategies in real time to prevent OOD failures. At a low\nfrequency in nominal operations, FORTRESS uses multi-modal reasoners to\nidentify goals and anticipate failure modes. When a runtime monitor triggers a\nfallback response, FORTRESS rapidly synthesizes plans to fallback goals while\ninferring and avoiding semantically unsafe regions in real time. By bridging\nopen-world, multi-modal reasoning with dynamics-aware planning, we eliminate\nthe need for hard-coded fallbacks and human safety interventions. FORTRESS\noutperforms on-the-fly prompting of slow reasoning models in safety\nclassification accuracy on synthetic benchmarks and real-world ANYmal robot\ndata, and further improves system safety and planning success in simulation and\non quadrotor hardware for urban navigation.\n","authors":["Milan Ganai","Rohan Sinha","Christopher Agia","Daniel Morton","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2505.10547v1.pdf","comment":"Website: https://milanganai.github.io/fortress/"},{"id":"http://arxiv.org/abs/2505.10543v1","updated":"2025-05-15T17:53:47Z","published":"2025-05-15T17:53:47Z","title":"Towards a Deeper Understanding of Reasoning Capabilities in Large\n  Language Models","summary":"  While large language models demonstrate impressive performance on static\nbenchmarks, the true potential of large language models as self-learning and\nreasoning agents in dynamic environments remains unclear. This study\nsystematically evaluates the efficacy of self-reflection, heuristic mutation,\nand planning as prompting techniques to test the adaptive capabilities of\nagents. We conduct experiments with various open-source language models in\ndynamic environments and find that larger models generally outperform smaller\nones, but that strategic prompting can close this performance gap. Second, a\ntoo-long prompt can negatively impact smaller models on basic reactive tasks,\nwhile larger models show more robust behaviour. Third, advanced prompting\ntechniques primarily benefit smaller models on complex games, but offer less\nimprovement for already high-performing large language models. Yet, we find\nthat advanced reasoning methods yield highly variable outcomes: while capable\nof significantly improving performance when reasoning and decision-making\nalign, they also introduce instability and can lead to big performance drops.\nCompared to human performance, our findings reveal little evidence of true\nemergent reasoning. Instead, large language model performance exhibits\npersistent limitations in crucial areas such as planning, reasoning, and\nspatial coordination, suggesting that current-generation large language models\nstill suffer fundamental shortcomings that may not be fully overcome through\nself-reflective prompting alone. Reasoning is a multi-faceted task, and while\nreasoning methods like Chain of thought improves multi-step reasoning on math\nword problems, our findings using dynamic benchmarks highlight important\nshortcomings in general reasoning capabilities, indicating a need to move\nbeyond static benchmarks to capture the complexity of reasoning.\n","authors":["Annie Wong","Thomas Bäck","Aske Plaat","Niki van Stein","Anna V. Kononova"],"pdf_url":"https://arxiv.org/pdf/2505.10543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04689v3","updated":"2025-05-15T17:52:51Z","published":"2025-02-07T06:30:33Z","title":"ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning","summary":"  Large language models (LLMs) have demonstrated impressive capabilities on\ncomplex evaluation benchmarks, many of which are formulated as\nquestion-answering (QA) tasks. Enhancing the performance of LLMs in QA contexts\nis becoming increasingly vital for advancing their development and\napplicability. This paper introduces ARR, an intuitive, effective, and general\nQA solving method that explicitly incorporates three key steps: analyzing the\nintent of the question, retrieving relevant information, and reasoning step by\nstep. Notably, this paper is the first to introduce intent analysis in QA,\nwhich plays a vital role in ARR. Comprehensive evaluations across 10 diverse QA\ntasks demonstrate that ARR consistently outperforms the baseline methods.\nAblation and case studies further validate the positive contributions of each\nARR component. Furthermore, experiments involving variations in prompt design\nindicate that ARR maintains its effectiveness regardless of the specific prompt\nformulation. Additionally, extensive evaluations across various model sizes,\nLLM series, and generation settings solidify the effectiveness, robustness, and\ngeneralizability of ARR.\n","authors":["Yuwei Yin","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2502.04689v3.pdf","comment":"21 pages. Code: https://github.com/YuweiYin/ARR"},{"id":"http://arxiv.org/abs/2501.18901v2","updated":"2025-05-15T17:48:47Z","published":"2025-01-31T05:42:58Z","title":"Lightspeed Geometric Dataset Distance via Sliced Optimal Transport","summary":"  We introduce sliced optimal transport dataset distance (s-OTDD), a\nmodel-agnostic, embedding-agnostic approach for dataset comparison that\nrequires no training, is robust to variations in the number of classes, and can\nhandle disjoint label sets. The core innovation is Moment Transform Projection\n(MTP), which maps a label, represented as a distribution over features, to a\nreal number. Using MTP, we derive a data point projection that transforms\ndatasets into one-dimensional distributions. The s-OTDD is defined as the\nexpected Wasserstein distance between the projected distributions, with respect\nto random projection parameters. Leveraging the closed form solution of\none-dimensional optimal transport, s-OTDD achieves (near-)linear computational\ncomplexity in the number of data points and feature dimensions and is\nindependent of the number of classes. With its geometrically meaningful\nprojection, s-OTDD strongly correlates with the optimal transport dataset\ndistance while being more efficient than existing dataset discrepancy measures.\nMoreover, it correlates well with the performance gap in transfer learning and\nclassification accuracy in data augmentation.\n","authors":["Khai Nguyen","Hai Nguyen","Tuan Pham","Nhat Ho"],"pdf_url":"https://arxiv.org/pdf/2501.18901v2.pdf","comment":"Accepted to ICML 2025, 16 pages, 13 figures"},{"id":"http://arxiv.org/abs/2505.10537v1","updated":"2025-05-15T17:47:30Z","published":"2025-05-15T17:47:30Z","title":"LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps","summary":"  The O-RAN architecture is transforming cellular networks by adopting RAN\nsoftwarization and disaggregation concepts to enable data-driven monitoring and\ncontrol of the network. Such management is enabled by RICs, which facilitate\nnear-real-time and non-real-time network control through xApps and rApps.\nHowever, they face limitations, including latency overhead in data exchange\nbetween the RAN and RIC, restricting real-time monitoring, and the inability to\naccess user plain data due to privacy and security constraints, hindering use\ncases like beamforming and spectrum classification. In this paper, we leverage\nthe dApps concept to enable real-time RF spectrum classification with LibIQ, a\nnovel library for RF signals that facilitates efficient spectrum monitoring and\nsignal classification by providing functionalities to read I/Q samples as\ntime-series, create datasets and visualize time-series data through plots and\nspectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to\ndetect external RF signals, which are subsequently classified using a CNN\ninside the library. To achieve accurate spectrum analysis, we created an\nextensive dataset of time-series-based I/Q samples, representing distinct\nsignal types captured using a custom dApp running on a 5G deployment over the\nColosseum network emulator and an OTA testbed. We evaluate our model by\ndeploying LibIQ in heterogeneous scenarios with varying center frequencies,\ntime windows, and external RF signals. In real-time analysis, the model\nclassifies the processed I/Q samples, achieving an average accuracy of\napproximately 97.8\\% in identifying signal types across all scenarios. We\npledge to release both LibIQ and the dataset created as a publicly available\nframework upon acceptance.\n","authors":["Filippo Olimpieri","Noemi Giustini","Andrea Lacava","Salvatore D'Oro","Tommaso Melodia","Francesca Cuomo"],"pdf_url":"https://arxiv.org/pdf/2505.10537v1.pdf","comment":"6 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2505.10522v1","updated":"2025-05-15T17:30:29Z","published":"2025-05-15T17:30:29Z","title":"Knowledge capture, adaptation and composition (KCAC): A framework for\n  cross-task curriculum learning in robotic manipulation","summary":"  Reinforcement learning (RL) has demonstrated remarkable potential in robotic\nmanipulation but faces challenges in sample inefficiency and lack of\ninterpretability, limiting its applicability in real world scenarios. Enabling\nthe agent to gain a deeper understanding and adapt more efficiently to diverse\nworking scenarios is crucial, and strategic knowledge utilization is a key\nfactor in this process. This paper proposes a Knowledge Capture, Adaptation,\nand Composition (KCAC) framework to systematically integrate knowledge transfer\ninto RL through cross-task curriculum learning. KCAC is evaluated using a two\nblock stacking task in the CausalWorld benchmark, a complex robotic\nmanipulation environment. To our knowledge, existing RL approaches fail to\nsolve this task effectively, reflecting deficiencies in knowledge capture. In\nthis work, we redesign the benchmark reward function by removing rigid\nconstraints and strict ordering, allowing the agent to maximize total rewards\nconcurrently and enabling flexible task completion. Furthermore, we define two\nself-designed sub-tasks and implement a structured cross-task curriculum to\nfacilitate efficient learning. As a result, our KCAC approach achieves a 40\npercent reduction in training time while improving task success rates by 10\npercent compared to traditional RL methods. Through extensive evaluation, we\nidentify key curriculum design parameters subtask selection, transition timing,\nand learning rate that optimize learning efficiency and provide conceptual\nguidance for curriculum based RL frameworks. This work offers valuable insights\ninto curriculum design in RL and robotic learning.\n","authors":["Xinrui Wang","Yan Jin"],"pdf_url":"https://arxiv.org/pdf/2505.10522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10518v1","updated":"2025-05-15T17:25:03Z","published":"2025-05-15T17:25:03Z","title":"Multi-Token Prediction Needs Registers","summary":"  Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.\n","authors":["Anastasios Gerontopoulos","Spyros Gidaris","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2505.10518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10515v1","updated":"2025-05-15T17:21:54Z","published":"2025-05-15T17:21:54Z","title":"PnPXAI: A Universal XAI Framework Providing Automatic Explanations\n  Across Diverse Modalities and Models","summary":"  Recently, post hoc explanation methods have emerged to enhance model\ntransparency by attributing model outputs to input features. However, these\nmethods face challenges due to their specificity to certain neural network\narchitectures and data modalities. Existing explainable artificial intelligence\n(XAI) frameworks have attempted to address these challenges but suffer from\nseveral limitations. These include limited flexibility to diverse model\narchitectures and data modalities due to hard-coded implementations, a\nrestricted number of supported XAI methods because of the requirements for\nlayer-specific operations of attribution methods, and sub-optimal\nrecommendations of explanations due to the lack of evaluation and optimization\nphases. Consequently, these limitations impede the adoption of XAI technology\nin real-world applications, making it difficult for practitioners to select the\noptimal explanation method for their domain. To address these limitations, we\nintroduce \\textbf{PnPXAI}, a universal XAI framework that supports diverse data\nmodalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI\nautomatically detects model architectures, recommends applicable explanation\nmethods, and optimizes hyperparameters for optimal explanations. We validate\nthe framework's effectiveness through user surveys and showcase its versatility\nacross various domains, including medicine and finance.\n","authors":["Seongun Kim","Sol A Kim","Geonhyeong Kim","Enver Menadjiev","Chanwoo Lee","Seongwook Chung","Nari Kim","Jaesik Choi"],"pdf_url":"https://arxiv.org/pdf/2505.10515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02069v4","updated":"2025-05-15T17:18:12Z","published":"2024-06-04T07:51:30Z","title":"PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling","summary":"  In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance.\n","authors":["Zefan Cai","Yichi Zhang","Bofei Gao","Yuliang Liu","Yucheng Li","Tianyu Liu","Keming Lu","Wayne Xiong","Yue Dong","Junjie Hu","Wen Xiao"],"pdf_url":"https://arxiv.org/pdf/2406.02069v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13957v2","updated":"2025-05-15T17:09:21Z","published":"2025-01-21T04:05:45Z","title":"Benchmarking Generative AI for Scoring Medical Student Interviews in\n  Objective Structured Clinical Examinations (OSCEs)","summary":"  Objective Structured Clinical Examinations (OSCEs) are widely used to assess\nmedical students' communication skills, but scoring interview-based assessments\nis time-consuming and potentially subject to human bias. This study explored\nthe potential of large language models (LLMs) to automate OSCE evaluations\nusing the Master Interview Rating Scale (MIRS). We compared the performance of\nfour state-of-the-art LLMs (GPT-4o, Claude 3.5, Llama 3.1, and Gemini 1.5 Pro)\nin evaluating OSCE transcripts across all 28 items of the MIRS under the\nconditions of zero-shot, chain-of-thought (CoT), few-shot, and multi-step\nprompting. The models were benchmarked against a dataset of 10 OSCE cases with\n174 expert consensus scores available. Model performance was measured using\nthree accuracy metrics (exact, off-by-one, thresholded). Averaging across all\nMIRS items and OSCE cases, LLMs performed with low exact accuracy (0.27 to\n0.44), and moderate to high off-by-one accuracy (0.67 to 0.87) and thresholded\naccuracy (0.75 to 0.88). A zero temperature parameter ensured high intra-rater\nreliability ({\\alpha} = 0.98 for GPT-4o). CoT, few-shot, and multi-step\ntechniques proved valuable when tailored to specific assessment items. The\nperformance was consistent across MIRS items, independent of encounter phases\nand communication domains. We demonstrated the feasibility of AI-assisted OSCE\nevaluation and provided benchmarking of multiple LLMs across multiple prompt\ntechniques. Our work provides a baseline performance assessment for LLMs that\nlays a foundation for future research into automated assessment of clinical\ncommunication skills.\n","authors":["Jadon Geathers","Yann Hicke","Colleen Chan","Niroop Rajashekar","Justin Sewell","Susannah Cornes","Rene F. Kizilcec","Dennis Shung"],"pdf_url":"https://arxiv.org/pdf/2501.13957v2.pdf","comment":"12 pages + 3 pages of references, 4 figures"},{"id":"http://arxiv.org/abs/2505.10483v1","updated":"2025-05-15T16:34:50Z","published":"2025-05-15T16:34:50Z","title":"UniEval: Unified Holistic Evaluation for Unified Multimodal\n  Understanding and Generation","summary":"  The emergence of unified multimodal understanding and generation models is\nrapidly attracting attention because of their ability to enhance\ninstruction-following capabilities while minimizing model redundancy. However,\nthere is a lack of a unified evaluation framework for these models, which would\nenable an elegant, simplified, and overall evaluation. Current models conduct\nevaluations on multiple task-specific benchmarks, but there are significant\nlimitations, such as the lack of overall results, errors from extra evaluation\nmodels, reliance on extensive labeled images, benchmarks that lack diversity,\nand metrics with limited capacity for instruction-following evaluation. To\ntackle these challenges, we introduce UniEval, the first evaluation framework\ndesigned for unified multimodal models without extra models, images, or\nannotations. This facilitates a simplified and unified evaluation process. The\nUniEval framework contains a holistic benchmark, UniBench (supports both\nunified and visual generation models), along with the corresponding UniScore\nmetric. UniBench includes 81 fine-grained tags contributing to high diversity.\nExperimental results indicate that UniBench is more challenging than existing\nbenchmarks, and UniScore aligns closely with human evaluations, surpassing\ncurrent metrics. Moreover, we extensively evaluated SoTA unified and visual\ngeneration models, uncovering new insights into Univeral's unique values.\n","authors":["Yi Li","Haonan Wang","Qixiang Zhang","Boyu Xiao","Chenchang Hu","Hualiang Wang","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2505.10483v1.pdf","comment":"UniEval is the first evaluation framework designed for unified\n  multimodal models, including a holistic benchmark UniBench and the UniScore\n  metric"},{"id":"http://arxiv.org/abs/2505.10482v1","updated":"2025-05-15T16:33:44Z","published":"2025-05-15T16:33:44Z","title":"Fine-tuning Diffusion Policies with Backpropagation Through Diffusion\n  Timesteps","summary":"  Diffusion policies, widely adopted in decision-making scenarios such as\nrobotics, gaming and autonomous driving, are capable of learning diverse skills\nfrom demonstration data due to their high representation power. However, the\nsub-optimal and limited coverage of demonstration data could lead to diffusion\npolicies that generate sub-optimal trajectories and even catastrophic failures.\nWhile reinforcement learning (RL)-based fine-tuning has emerged as a promising\nsolution to address these limitations, existing approaches struggle to\neffectively adapt Proximal Policy Optimization (PPO) to diffusion models. This\nchallenge stems from the computational intractability of action likelihood\nestimation during the denoising process, which leads to complicated\noptimization objectives. In our experiments starting from randomly initialized\npolicies, we find that online tuning of Diffusion Policies demonstrates much\nlower sample efficiency compared to directly applying PPO on MLP policies\n(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework\nthat reformulates Diffusion Policy as a noise-conditioned deterministic policy.\nBy treating each denoising step as a differentiable transformation conditioned\non pre-sampled noise, NCDPO enables tractable likelihood evaluation and\ngradient backpropagation through all diffusion timesteps. Our experiments\ndemonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when\ntraining from scratch, outperforming existing methods in both sample efficiency\nand final performance across diverse benchmarks, including continuous robot\ncontrol and multi-agent game scenarios. Furthermore, our experimental results\nshow that our method is robust to the number denoising timesteps in the\nDiffusion Policy.\n","authors":["Ningyuan Yang","Jiaxuan Gao","Feng Gao","Yi Wu","Chao Yu"],"pdf_url":"https://arxiv.org/pdf/2505.10482v1.pdf","comment":"9 pages for main text, 23 pages in total, submitted to Neurips, 13\n  figures"},{"id":"http://arxiv.org/abs/2504.17671v3","updated":"2025-05-15T16:24:49Z","published":"2025-04-24T15:39:46Z","title":"Data-Driven Calibration of Prediction Sets in Large Vision-Language\n  Models Based on Inductive Conformal Prediction","summary":"  This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making.\n","authors":["Yuanchang Ye","Weiyan Wen"],"pdf_url":"https://arxiv.org/pdf/2504.17671v3.pdf","comment":"Accepted by ICIPCA 2025"},{"id":"http://arxiv.org/abs/2505.10468v1","updated":"2025-05-15T16:21:33Z","published":"2025-05-15T16:21:33Z","title":"AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and\n  Challenge","summary":"  This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications\n","authors":["Ranjan Sapkota","Konstantinos I. Roumeliotis","Manoj Karkee"],"pdf_url":"https://arxiv.org/pdf/2505.10468v1.pdf","comment":"32 pages, 14 figures, 11 tables"},{"id":"http://arxiv.org/abs/2505.10465v1","updated":"2025-05-15T16:18:13Z","published":"2025-05-15T16:18:13Z","title":"Superposition Yields Robust Neural Scaling","summary":"  The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters.\n","authors":["Yizhou liu","Ziming Liu","Jeff Gore"],"pdf_url":"https://arxiv.org/pdf/2505.10465v1.pdf","comment":"30 pages, 23 figures"},{"id":"http://arxiv.org/abs/2505.10457v1","updated":"2025-05-15T16:14:18Z","published":"2025-05-15T16:14:18Z","title":"SEAL: Searching Expandable Architectures for Incremental Learning","summary":"  Incremental learning is a machine learning paradigm where a model learns from\na sequential stream of tasks. This setting poses a key challenge: balancing\nplasticity (learning new tasks) and stability (preserving past knowledge).\nNeural Architecture Search (NAS), a branch of AutoML, automates the design of\nthe architecture of Deep Neural Networks and has shown success in static\nsettings. However, existing NAS-based approaches to incremental learning often\nrely on expanding the model at every task, making them impractical in\nresource-constrained environments. In this work, we introduce SEAL, a NAS-based\nframework tailored for data-incremental learning, a scenario where disjoint\ndata samples arrive sequentially and are not stored for future access. SEAL\nadapts the model structure dynamically by expanding it only when necessary,\nbased on a capacity estimation metric. Stability is preserved through\ncross-distillation training after each expansion step. The NAS component\njointly searches for both the architecture and the optimal expansion policy.\nExperiments across multiple benchmarks demonstrate that SEAL effectively\nreduces forgetting and enhances accuracy while maintaining a lower model size\ncompared to prior methods. These results highlight the promise of combining NAS\nand selective expansion for efficient, adaptive learning in incremental\nscenarios.\n","authors":["Matteo Gambella","Vicente Javier Castro Solar","Manuel Roveri"],"pdf_url":"https://arxiv.org/pdf/2505.10457v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.10453v1","updated":"2025-05-15T16:11:33Z","published":"2025-05-15T16:11:33Z","title":"Vision language models have difficulty recognizing virtual objects","summary":"  Vision language models (VLMs) are AI systems paired with both language and\nvision encoders to process multimodal input. They are capable of performing\ncomplex semantic tasks such as automatic captioning, but it remains an open\nquestion about how well they comprehend the visuospatial properties of scenes\ndepicted in the images they process. We argue that descriptions of virtual\nobjects -- objects that are not visually represented in an image -- can help\ntest scene comprehension in these AI systems. For example, an image that\ndepicts a person standing under a tree can be paired with the following prompt:\nimagine that a kite is stuck in the tree. VLMs that comprehend the scene should\nupdate their representations and reason sensibly about the spatial relations\nbetween all three objects. We describe systematic evaluations of\nstate-of-the-art VLMs and show that their ability to process virtual objects is\ninadequate.\n","authors":["Tyler Tran","Sangeet Khemlani","J. G. Trafton"],"pdf_url":"https://arxiv.org/pdf/2505.10453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10443v1","updated":"2025-05-15T16:04:25Z","published":"2025-05-15T16:04:25Z","title":"Are Large Language Models Robust in Understanding Code Against\n  Semantics-Preserving Mutations?","summary":"  Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding.\n  In this work, we evaluate whether state-of-the-art LLMs with up to 8B\nparameters can reason about Python programs or are simply guessing. We apply\nfive semantics-preserving code mutations: renaming variables, mirroring\ncomparison expressions, swapping if-else branches, converting for loops to\nwhile, and loop unrolling. These mutations maintain program semantics while\naltering its syntax. We evaluated six LLMs and performed a human expert\nanalysis using LiveCodeBench to assess whether the correct predictions are\nbased on sound reasoning. We also evaluated prediction stability across\ndifferent code mutations on LiveCodeBench and CruxEval. Our findings show that\nsome LLMs, such as Llama3.2, produce correct predictions based on flawed\nreasoning in up to 61% of cases. Furthermore, LLMs often change predictions in\nresponse to our code mutations, indicating limited robustness in their semantic\nunderstanding.\n","authors":["Pedro Orvalho","Marta Kwiatkowska"],"pdf_url":"https://arxiv.org/pdf/2505.10443v1.pdf","comment":"10 pages, 5 tables, 1 figure"},{"id":"http://arxiv.org/abs/2505.10442v1","updated":"2025-05-15T16:01:21Z","published":"2025-05-15T16:01:21Z","title":"IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy\n  Fine-Tuning","summary":"  Imitation learning (IL) and reinforcement learning (RL) each offer distinct\nadvantages for robotics policy learning: IL provides stable learning from\ndemonstrations, and RL promotes generalization through exploration. While\nexisting robot learning approaches using IL-based pre-training followed by\nRL-based fine-tuning are promising, this two-step learning paradigm often\nsuffers from instability and poor sample efficiency during the RL fine-tuning\nphase. In this work, we introduce IN-RIL, INterleaved Reinforcement learning\nand Imitation Learning, for policy fine-tuning, which periodically injects IL\nupdates after multiple RL updates and hence can benefit from the stability of\nIL and the guidance of expert data for more efficient exploration throughout\nthe entire fine-tuning process. Since IL and RL involve different optimization\nobjectives, we develop gradient separation mechanisms to prevent destructive\ninterference during \\ABBR fine-tuning, by separating possibly conflicting\ngradient updates in orthogonal subspaces. Furthermore, we conduct rigorous\nanalysis, and our findings shed light on why interleaving IL with RL stabilizes\nlearning and improves sample-efficiency. Extensive experiments on 14 robot\nmanipulation and locomotion tasks across 3 benchmarks, including\nFurnitureBench, OpenAI Gym, and Robomimic, demonstrate that \\ABBR can\nsignificantly improve sample efficiency and mitigate performance collapse\nduring online finetuning in both long- and short-horizon tasks with either\nsparse or dense rewards. IN-RIL, as a general plug-in compatible with various\nstate-of-the-art RL algorithms, can significantly improve RL fine-tuning, e.g.,\nfrom 12\\% to 88\\% with 6.3x improvement in the success rate on Robomimic\nTransport. Project page: https://github.com/ucd-dare/IN-RIL.\n","authors":["Dechen Gao","Hang Wang","Hanchu Zhou","Nejib Ammar","Shatadal Mishra","Ahmadreza Moradipari","Iman Soltani","Junshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.10442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10441v1","updated":"2025-05-15T16:00:31Z","published":"2025-05-15T16:00:31Z","title":"PIF: Anomaly detection via preference embedding","summary":"  We address the problem of detecting anomalies with respect to structured\npatterns. To this end, we conceive a novel anomaly detection method called PIF,\nthat combines the advantages of adaptive isolation methods with the flexibility\nof preference embedding. Specifically, we propose to embed the data in a high\ndimensional space where an efficient tree-based method, PI-Forest, is employed\nto compute an anomaly score. Experiments on synthetic and real datasets\ndemonstrate that PIF favorably compares with state-of-the-art anomaly detection\ntechniques, and confirm that PI-Forest is better at measuring arbitrary\ndistances and isolate points in the preference space.\n","authors":["Filippo Leveni","Luca Magri","Giacomo Boracchi","Cesare Alippi"],"pdf_url":"https://arxiv.org/pdf/2505.10441v1.pdf","comment":"Accepted at International Conference on Pattern Recognition (ICPR\n  2020)"},{"id":"http://arxiv.org/abs/2412.18673v2","updated":"2025-05-15T15:57:43Z","published":"2024-12-24T20:16:13Z","title":"MapExplorer: New Content Generation from Low-Dimensional Visualizations","summary":"  Low-dimensional visualizations, or \"projection maps,\" are widely used in\nscientific and creative domains to interpret large-scale and complex datasets.\nThese visualizations not only aid in understanding existing knowledge spaces\nbut also implicitly guide exploration into unknown areas. Although techniques\nsuch as t-SNE and UMAP can generate these maps, there exists no systematic\nmethod for leveraging them to generate new content. To address this, we\nintroduce MapExplorer, a novel knowledge discovery task that translates\ncoordinates within any projection map into coherent, contextually aligned\ntextual content. This allows users to interactively explore and uncover\ninsights embedded in the maps. To evaluate the performance of MapExplorer\nmethods, we propose Atometric, a fine-grained metric inspired by ROUGE that\nquantifies logical coherence and alignment between generated and reference\ntext. Experiments on diverse datasets demonstrate the versatility of\nMapExplorer in generating scientific hypotheses, crafting synthetic personas,\nand devising strategies for attacking large language models-even with simple\nbaseline methods. By bridging visualization and generation, our work highlights\nthe potential of MapExplorer to enable intuitive human-AI collaboration in\nlarge-scale data exploration.\n","authors":["Xingjian Zhang","Ziyang Xiong","Shixuan Liu","Yutong Xie","Tolga Ergen","Dongsub Shim","Hua Xu","Honglak Lee","Qiaozhu Me"],"pdf_url":"https://arxiv.org/pdf/2412.18673v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17067v2","updated":"2025-05-15T15:57:32Z","published":"2024-05-27T11:39:59Z","title":"Tokenization Matters! Degrading Large Language Models through\n  Challenging Their Tokenization","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. Nonetheless, it was also witnessed that LLMs tend\nto produce inaccurate responses to specific queries. This deficiency can be\ntraced to the tokenization step LLMs must undergo, which is an inevitable\nlimitation inherent to all LLMs. In fact, incorrect tokenization is the\ncritical point that hinders LLMs in understanding the input precisely, thus\nleading to unsatisfactory output. This defect is more obvious in Chinese\nscenarios. To demonstrate this flaw of LLMs, we construct an adversarial\ndataset, named as $\\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which\ndraws upon the vocabularies of various open-source LLMs to challenge LLMs'\ntokenization. ADT consists of two subsets: the manually constructed ADT-Human\nand the automatically generated ADT-Auto. Our empirical results reveal that our\nADT is highly effective on challenging the tokenization of leading LLMs,\nincluding GPT-4o, Llama-3, Deepseek-R1 and so on, thus degrading these LLMs'\ncapabilities. Moreover, our method of automatic data generation has been proven\nefficient and robust, which can be applied to any open-source LLMs. In this\npaper, we substantially investigate LLMs' vulnerability in terms of challenging\ntheir token segmentation, which will shed light on the subsequent research of\nimproving LLMs' capabilities through optimizing their tokenization process and\nalgorithms.\n","authors":["Dixuan Wang","Yanda Li","Junyuan Jiang","Zepeng Ding","Ziqin Luo","Guochao Jiang","Jiaqing Liang","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2405.17067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05901v2","updated":"2025-05-15T15:46:43Z","published":"2025-05-09T09:09:08Z","title":"Examining the Source of Defects from a Mechanical Perspective for 3D\n  Anomaly Detection","summary":"  In this paper, we explore a novel approach to 3D anomaly detection (AD) that\ngoes beyond merely identifying anomalies based on structural characteristics.\nOur primary perspective is that most anomalies arise from unpredictable\ndefective forces originating from both internal and external sources. To\naddress these anomalies, we seek out opposing forces that can help correct\nthem. Therefore, we introduce the Mechanics Complementary Model-based Framework\nfor the 3D-AD task (MC4AD), which generates internal and external corrective\nforces for each point. We first propose a Diverse Anomaly-Generation (DA-Gen)\nmodule designed to simulate various types of anomalies. Next, we present the\nCorrective Force Prediction Network (CFP-Net), which uses complementary\nrepresentations for point-level analysis to simulate the different\ncontributions from internal and external corrective forces. To ensure the\ncorrective forces are constrained effectively, we have developed a combined\nloss function that includes a new symmetric loss and an overall loss. Notably,\nwe implement a Hierarchical Quality Control (HQC) strategy based on a three-way\ndecision process and contribute a dataset titled Anomaly-IntraVariance, which\nincorporates intraclass variance to evaluate our model. As a result, the\nproposed MC4AD has been proven effective through theory and experimentation.\nThe experimental results demonstrate that our approach yields nine\nstate-of-the-art performances, achieving optimal results with minimal\nparameters and the fastest inference speed across five existing datasets, in\naddition to the proposed Anomaly-IntraVariance dataset. The source is available\nat https://github.com/hzzzzzhappy/MC4AD\n","authors":["Hanzhe Liang","Aoran Wang","Jie Zhou","Xin Jin","Can Gao","Jinbao Wang"],"pdf_url":"https://arxiv.org/pdf/2505.05901v2.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2505.10420v1","updated":"2025-05-15T15:37:51Z","published":"2025-05-15T15:37:51Z","title":"Learned Lightweight Smartphone ISP with Unpaired Data","summary":"  The Image Signal Processor (ISP) is a fundamental component in modern\nsmartphone cameras responsible for conversion of RAW sensor image data to RGB\nimages with a strong focus on perceptual quality. Recent work highlights the\npotential of deep learning approaches and their ability to capture details with\na quality increasingly close to that of professional cameras. A difficult and\ncostly step when developing a learned ISP is the acquisition of pixel-wise\naligned paired data that maps the raw captured by a smartphone camera sensor to\nhigh-quality reference images. In this work, we address this challenge by\nproposing a novel training method for a learnable ISP that eliminates the need\nfor direct correspondences between raw images and ground-truth data with\nmatching content. Our unpaired approach employs a multi-term loss function\nguided by adversarial training with multiple discriminators processing feature\nmaps from pre-trained networks to maintain content structure while learning\ncolor and texture characteristics from the target RGB dataset. Using\nlightweight neural network architectures suitable for mobile devices as\nbackbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm\nUltraISP datasets. Compared to paired training methods, our unpaired learning\nstrategy shows strong potential and achieves high fidelity across multiple\nevaluation metrics. The code and pre-trained models are available at\nhttps://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .\n","authors":["Andrei Arhire","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2505.10420v1.pdf","comment":"Accepted at CVPRW 2025"},{"id":"http://arxiv.org/abs/2505.10405v1","updated":"2025-05-15T15:28:32Z","published":"2025-05-15T15:28:32Z","title":"Visual Fidelity Index for Generative Semantic Communications with\n  Critical Information Embedding","summary":"  Generative semantic communication (Gen-SemCom) with large artificial\nintelligence (AI) model promises a transformative paradigm for 6G networks,\nwhich reduces communication costs by transmitting low-dimensional prompts\nrather than raw data. However, purely prompt-driven generation loses\nfine-grained visual details. Additionally, there is a lack of systematic\nmetrics to evaluate the performance of Gen-SemCom systems. To address these\nissues, we develop a hybrid Gen-SemCom system with a critical information\nembedding (CIE) framework, where both text prompts and semantically critical\nfeatures are extracted for transmissions. First, a novel approach of semantic\nfiltering is proposed to select and transmit the semantically critical features\nof images relevant to semantic label. By integrating the text prompt and\ncritical features, the receiver reconstructs high-fidelity images using a\ndiffusion-based generative model. Next, we propose the generative visual\ninformation fidelity (GVIF) metric to evaluate the visual quality of the\ngenerated image. By characterizing the statistical models of image features,\nthe GVIF metric quantifies the mutual information between the distorted\nfeatures and their original counterparts. By maximizing the GVIF metric, we\ndesign a channel-adaptive Gen-SemCom system that adaptively control the volume\nof features and compression rate according to the channel state. Experimental\nresults validate the GVIF metric's sensitivity to visual fidelity, correlating\nwith both the PSNR and critical information volume. In addition, the optimized\nsystem achieves superior performance over benchmarking schemes in terms of\nhigher PSNR and lower FID scores.\n","authors":["Jianhao Huang","Qunsong Zeng","Kaibin Huang"],"pdf_url":"https://arxiv.org/pdf/2505.10405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10402v1","updated":"2025-05-15T15:26:32Z","published":"2025-05-15T15:26:32Z","title":"Rethinking Repetition Problems of LLMs in Code Generation","summary":"  With the advent of neural language models, the performance of code generation\nhas been significantly boosted. However, the problem of repetitions during the\ngeneration process continues to linger. Previous work has primarily focused on\ncontent repetition, which is merely a fraction of the broader repetition\nproblem in code generation. A more prevalent and challenging problem is\nstructural repetition. In structural repetition, the repeated code appears in\nvarious patterns but possesses a fixed structure, which can be inherently\nreflected in grammar. In this paper, we formally define structural repetition\nand propose an efficient decoding approach called RPG, which stands for\nRepetition Penalization based on Grammar, to alleviate the repetition problems\nin code generation for LLMs. Specifically, RPG first leverages grammar rules to\nidentify repetition problems during code generation, and then strategically\ndecays the likelihood of critical tokens that contribute to repetitions,\nthereby mitigating them in code generation. To facilitate this study, we\nconstruct a new dataset CodeRepetEval to comprehensively evaluate approaches\nfor mitigating the repetition problems in code generation. Extensive\nexperimental results demonstrate that RPG substantially outperforms the\nbest-performing baselines on CodeRepetEval dataset as well as HumanEval and\nMBPP benchmarks, effectively reducing repetitions and enhancing the quality of\ngenerated code.\n","authors":["Yihong Dong","Yuchen Liu","Xue Jiang","Zhi Jin","Ge Li"],"pdf_url":"https://arxiv.org/pdf/2505.10402v1.pdf","comment":"Accepted to ACL 2025 (main)"},{"id":"http://arxiv.org/abs/2505.10399v1","updated":"2025-05-15T15:22:06Z","published":"2025-05-15T15:22:06Z","title":"Evaluating Model Explanations without Ground Truth","summary":"  There can be many competing and contradictory explanations for a single model\nprediction, making it difficult to select which one to use. Current explanation\nevaluation frameworks measure quality by comparing against ideal \"ground-truth\"\nexplanations, or by verifying model sensitivity to important inputs. We outline\nthe limitations of these approaches, and propose three desirable principles to\nground the future development of explanation evaluation strategies for local\nfeature importance explanations. We propose a ground-truth Agnostic eXplanation\nEvaluation framework (AXE) for evaluating and comparing model explanations that\nsatisfies these principles. Unlike prior approaches, AXE does not require\naccess to ideal ground-truth explanations for comparison, or rely on model\nsensitivity - providing an independent measure of explanation quality. We\nverify AXE by comparing with baselines, and show how it can be used to detect\nexplanation fairwashing. Our code is available at\nhttps://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.\n","authors":["Kaivalya Rawal","Zihao Fu","Eoin Delaney","Chris Russell"],"pdf_url":"https://arxiv.org/pdf/2505.10399v1.pdf","comment":"https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth"},{"id":"http://arxiv.org/abs/2505.10394v1","updated":"2025-05-15T15:17:09Z","published":"2025-05-15T15:17:09Z","title":"Inconsistency Handling in DatalogMTL","summary":"  In this paper, we explore the issue of inconsistency handling in DatalogMTL,\nan extension of Datalog with metric temporal operators. Since facts are\nassociated with time intervals, there are different manners to restore\nconsistency when they contradict the rules, such as removing facts or modifying\ntheir time intervals. Our first contribution is the definition of relevant\nnotions of conflicts (minimal explanations for inconsistency) and repairs\n(possible ways of restoring consistency) for this setting and the study of the\nproperties of these notions and the associated inconsistency-tolerant\nsemantics. Our second contribution is a data complexity analysis of the tasks\nof generating a single conflict / repair and query entailment under\nrepair-based semantics.\n","authors":["Meghyn Bienvenu","Camille Bourgaux","Atefe Khodadaditaghanaki"],"pdf_url":"https://arxiv.org/pdf/2505.10394v1.pdf","comment":"This is an extended version of a paper appearing at the 34th\n  International Joint Conference on Artificial Intelligence (IJCAI 2025). 24\n  pages"},{"id":"http://arxiv.org/abs/2409.06356v2","updated":"2025-05-15T15:16:33Z","published":"2024-09-10T09:23:03Z","title":"Double Successive Over-Relaxation Q-Learning with an Extension to Deep\n  Reinforcement Learning","summary":"  Q-learning is a widely used algorithm in reinforcement learning (RL), but its\nconvergence can be slow, especially when the discount factor is close to one.\nSuccessive Over-Relaxation (SOR) Q-learning, which introduces a relaxation\nfactor to speed up convergence, addresses this issue but has two major\nlimitations: In the tabular setting, the relaxation parameter depends on\ntransition probability, making it not entirely model-free, and it suffers from\noverestimation bias. To overcome these limitations, we propose a sample-based,\nmodel-free double SOR Q-learning algorithm. Theoretically and empirically, this\nalgorithm is shown to be less biased than SOR Q-learning. Further, in the\ntabular setting, the convergence analysis under boundedness assumptions on\niterates is discussed. The proposed algorithm is extended to large-scale\nproblems using deep RL. Finally, the tabular version of the proposed algorithm\nis compared using roulette and grid world environments, while the deep RL\nversion is tested on a maximization bias example and OpenAI Gym environments.\n","authors":["Shreyas S R"],"pdf_url":"https://arxiv.org/pdf/2409.06356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10393v1","updated":"2025-05-15T15:16:16Z","published":"2025-05-15T15:16:16Z","title":"Uncovering Magnetic Phases with Synthetic Data and Physics-Informed\n  Training","summary":"  We investigate the efficient learning of magnetic phases using artificial\nneural networks trained on synthetic data, combining computational simplicity\nwith physics-informed strategies. Focusing on the diluted Ising model, which\nlacks an exact analytical solution, we explore two complementary approaches: a\nsupervised classification using simple dense neural networks, and an\nunsupervised detection of phase transitions using convolutional autoencoders\ntrained solely on idealized spin configurations.\n  To enhance model performance, we incorporate two key forms of\nphysics-informed guidance. First, we exploit architectural biases which\npreferentially amplify features related to symmetry breaking. Second, we\ninclude training configurations that explicitly break $\\mathbb{Z}_2$ symmetry,\nreinforcing the network's ability to detect ordered phases. These mechanisms,\nacting in tandem, increase the network's sensitivity to phase structure even in\nthe absence of explicit labels. We validate the machine learning predictions\nthrough comparison with direct numerical estimates of critical temperatures and\npercolation thresholds.\n  Our results show that synthetic, structured, and computationally efficient\ntraining schemes can reveal physically meaningful phase boundaries, even in\ncomplex systems. This framework offers a low-cost and robust alternative to\nconventional methods, with potential applications in broader condensed matter\nand statistical physics contexts.\n","authors":["Agustin Medina","Marcelo Arlego","Carlos A. Lamas"],"pdf_url":"https://arxiv.org/pdf/2505.10393v1.pdf","comment":"25 pages, 14 figures"},{"id":"http://arxiv.org/abs/2505.10392v1","updated":"2025-05-15T15:14:02Z","published":"2025-05-15T15:14:02Z","title":"Schreier-Coset Graph Propagation","summary":"  Graph Neural Networks (GNNs) offer a principled framework for learning over\ngraph-structured data, yet their expressive capacity is often hindered by\nover-squashing, wherein information from distant nodes is compressed into\nfixed-size vectors. Existing solutions, including graph rewiring and\nbottleneck-resistant architectures such as Cayley and expander graphs, avoid\nthis problem but introduce scalability bottlenecks. In particular, the Cayley\ngraphs constructed over $SL(2,\\mathbb{Z}_n)$ exhibit strong theoretical\nproperties, yet suffer from cubic node growth $O(n^3)$, leading to high memory\nusage. To address this, this work introduces Schrier-Coset Graph Propagation\n(SCGP), a group-theoretic augmentation method that enriches node features\nthrough Schreier-coset embeddings without altering the input graph topology.\nSCGP embeds bottleneck-free connectivity patterns into a compact feature space,\nimproving long-range message passing while maintaining computational\nefficiency. Empirical evaluations across standard node and graph classification\nbenchmarks demonstrate that SCGP achieves performance comparable to, or\nexceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits\nparticular advantages in processing hierarchical and modular graph structures,\noffering reduced inference latency, improved scalability, and a low memory\nfootprint, making it suitable for real-time and resource-constrained\napplications.\n","authors":["Aryan Mishra","Lizhen Lin"],"pdf_url":"https://arxiv.org/pdf/2505.10392v1.pdf","comment":"9 pages, 1 figure , preprint"},{"id":"http://arxiv.org/abs/2505.10387v1","updated":"2025-05-15T15:07:40Z","published":"2025-05-15T15:07:40Z","title":"Multi-Agent Path Finding For Large Agents Is Intractable","summary":"  The multi-agent path finding (MAPF) problem asks to find a set of paths on a\ngraph such that when synchronously following these paths the agents never\nencounter a conflict. In the most widespread MAPF formulation, the so-called\nClassical MAPF, the agents sizes are neglected and two types of conflicts are\nconsidered: occupying the same vertex or using the same edge at the same time\nstep. Meanwhile in numerous practical applications, e.g. in robotics, taking\ninto account the agents' sizes is vital to ensure that the MAPF solutions can\nbe safely executed. Introducing large agents yields an additional type of\nconflict arising when one agent follows an edge and its body overlaps with the\nbody of another agent that is actually not using this same edge (e.g. staying\nstill at some distinct vertex of the graph). Until now it was not clear how\nharder the problem gets when such conflicts are to be considered while\nplanning. Specifically, it was known that Classical MAPF problem on an\nundirected graph can be solved in polynomial time, however no complete\npolynomial-time algorithm was presented to solve MAPF with large agents. In\nthis paper we, for the first time, establish that the latter problem is NP-hard\nand, thus, if P!=NP no polynomial algorithm for it can, unfortunately, be\npresented. Our proof is based on the prevalent in the field technique of\nreducing the seminal 3SAT problem (which is known to be an NP-complete problem)\nto the problem at hand. In particular, for an arbitrary 3SAT formula we\nprocedurally construct a dedicated graph with specific start and goal vertices\nand show that the given 3SAT formula is satisfiable iff the corresponding path\nfinding instance has a solution.\n","authors":["Artem Agafonov","Konstantin Yakovlev"],"pdf_url":"https://arxiv.org/pdf/2505.10387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20291v2","updated":"2025-05-15T15:06:46Z","published":"2025-03-26T07:33:36Z","title":"CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at\n  Intermediate Resolution with Structure-Aware Multimodal U-Nets","summary":"  Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at\nintermediate resolution (4-8 {\\AA}) is crucial in protein structure\ndetermination. Recent advances in deep learning have led to the development of\nautomated approaches for enhancing experimental cryo-EM density maps. Yet,\nthese methods are not optimized for intermediate-resolution maps and rely on\nmap density features alone. To address this, we propose CryoSAMU, a novel\nmethod designed to enhance 3D cryo-EM density maps of protein structures using\nstructure-aware multimodal U-Nets and trained on curated\nintermediate-resolution density maps. We comprehensively evaluate CryoSAMU\nacross various metrics and demonstrate its competitive performance compared to\nstate-of-the-art methods. Notably, CryoSAMU achieves significantly faster\nprocessing speed, showing promise for future practical applications. Our code\nis available at https://github.com/chenwei-zhang/CryoSAMU.\n","authors":["Chenwei Zhang","Khanh Dao Duc"],"pdf_url":"https://arxiv.org/pdf/2503.20291v2.pdf","comment":"19 pages, 6 main figures, 2 supplementary figures, 3 main tables, 4\n  supplementary tables"},{"id":"http://arxiv.org/abs/2505.07119v2","updated":"2025-05-15T15:05:10Z","published":"2025-05-11T21:05:33Z","title":"Towards Scalable IoT Deployment for Visual Anomaly Detection via\n  Efficient Compression","summary":"  Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing operational costs is essential. Deploying deep learning models\nwithin Internet of Things (IoT) environments introduces specific challenges due\nto limited computational power and bandwidth of edge devices. This study\ninvestigates how to perform VAD effectively under such constraints by\nleveraging compact, efficient processing strategies. We evaluate several data\ncompression techniques, examining the tradeoff between system latency and\ndetection accuracy. Experiments on the MVTec AD benchmark demonstrate that\nsignificant compression can be achieved with minimal loss in anomaly detection\nperformance compared to uncompressed data. Current results show up to 80%\nreduction in end-to-end inference time, including edge processing,\ntransmission, and server computation.\n","authors":["Arianna Stropeni","Francesco Borsatti","Manuel Barusco","Davide Dalle Pezze","Marco Fabris","Gian Antonio Susto"],"pdf_url":"https://arxiv.org/pdf/2505.07119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10375v1","updated":"2025-05-15T14:59:17Z","published":"2025-05-15T14:59:17Z","title":"Are Sparse Autoencoders Useful for Java Function Bug Detection?","summary":"  Software vulnerabilities such as buffer overflows and SQL injections are a\nmajor source of security breaches. Traditional methods for vulnerability\ndetection remain essential but are limited by high false positive rates,\nscalability issues, and reliance on manual effort. These constraints have\ndriven interest in AI-based approaches to automated vulnerability detection and\nsecure code generation. While Large Language Models (LLMs) have opened new\navenues for classification tasks, their complexity and opacity pose challenges\nfor interpretability and deployment. Sparse Autoencoder offer a promising\nsolution to this problem. We explore whether SAEs can serve as a lightweight,\ninterpretable alternative for bug detection in Java functions. We evaluate the\neffectiveness of SAEs when applied to representations from GPT-2 Small and\nGemma 2B, examining their capacity to highlight buggy behaviour without\nfine-tuning the underlying LLMs. We found that SAE-derived features enable bug\ndetection with an F1 score of up to 89%, consistently outperforming fine-tuned\ntransformer encoder baselines. Our work provides the first empirical evidence\nthat SAEs can be used to detect software bugs directly from the internal\nrepresentations of pretrained LLMs, without any fine-tuning or task-specific\nsupervision.\n","authors":["Rui Melo","Claudia Mamede","Andre Catarino","Rui Abreu","Henrique Lopes Cardoso"],"pdf_url":"https://arxiv.org/pdf/2505.10375v1.pdf","comment":"10 pages, 10 figures"},{"id":"http://arxiv.org/abs/2505.10371v1","updated":"2025-05-15T14:56:06Z","published":"2025-05-15T14:56:06Z","title":"ILIF: Temporal Inhibitory Leaky Integrate-and-Fire Neuron for\n  Overactivation in Spiking Neural Networks","summary":"  The Spiking Neural Network (SNN) has drawn increasing attention for its\nenergy-efficient, event-driven processing and biological plausibility. To train\nSNNs via backpropagation, surrogate gradients are used to approximate the\nnon-differentiable spike function, but they only maintain nonzero derivatives\nwithin a narrow range of membrane potentials near the firing threshold,\nreferred to as the surrogate gradient support width gamma. We identify a major\nchallenge, termed the dilemma of gamma: a relatively large gamma leads to\noveractivation, characterized by excessive neuron firing, which in turn\nincreases energy consumption, whereas a small gamma causes vanishing gradients\nand weakens temporal dependencies. To address this, we propose a temporal\nInhibitory Leaky Integrate-and-Fire (ILIF) neuron model, inspired by biological\ninhibitory mechanisms. This model incorporates interconnected inhibitory units\nfor membrane potential and current, effectively mitigating overactivation while\npreserving gradient propagation. Theoretical analysis demonstrates ILIF\neffectiveness in overcoming the gamma dilemma, and extensive experiments on\nmultiple datasets show that ILIF improves energy efficiency by reducing firing\nrates, stabilizes training, and enhances accuracy. The code is available at\ngithub.com/kaisun1/ILIF.\n","authors":["Kai Sun","Peibo Duan","Levin Kuhlmann","Beilun Wang","Bin Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.10371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10361v1","updated":"2025-05-15T14:52:16Z","published":"2025-05-15T14:52:16Z","title":"Plasticity as the Mirror of Empowerment","summary":"  Agents are minimally entities that are influenced by their past observations\nand act to influence future observations. This latter capacity is captured by\nempowerment, which has served as a vital framing concept across artificial\nintelligence and cognitive science. This former capacity, however, is equally\nfoundational: In what ways, and to what extent, can an agent be influenced by\nwhat it observes? In this paper, we ground this concept in a universal\nagent-centric measure that we refer to as plasticity, and reveal a fundamental\nconnection to empowerment. Following a set of desiderata on a suitable\ndefinition, we define plasticity using a new information-theoretic quantity we\ncall the generalized directed information. We show that this new quantity\nstrictly generalizes the directed information introduced by Massey (1990) while\npreserving all of its desirable properties. Our first finding is that\nplasticity is the mirror of empowerment: The agent's plasticity is identical to\nthe empowerment of the environment, and vice versa. Our second finding\nestablishes a tension between the plasticity and empowerment of an agent,\nsuggesting that agent design needs to be mindful of both characteristics. We\nexplore the implications of these findings, and suggest that plasticity,\nempowerment, and their relationship are essential to understanding agency.\n","authors":["David Abel","Michael Bowling","André Barreto","Will Dabney","Shi Dong","Steven Hansen","Anna Harutyunyan","Khimya Khetarpal","Clare Lyle","Razvan Pascanu","Georgios Piliouras","Doina Precup","Jonathan Richens","Mark Rowland","Tom Schaul","Satinder Singh"],"pdf_url":"https://arxiv.org/pdf/2505.10361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10360v1","updated":"2025-05-15T14:51:22Z","published":"2025-05-15T14:51:22Z","title":"FactsR: A Safer Method for Producing High Quality Healthcare\n  Documentation","summary":"  There are now a multitude of AI-scribing solutions for healthcare promising\nthe utilization of large language models for ambient documentation. However,\nthese AI scribes still rely on one-shot, or few-shot prompts for generating\nnotes after the consultation has ended, employing little to no reasoning. This\nrisks long notes with an increase in hallucinations, misrepresentation of the\nintent of the clinician, and reliance on the proofreading of the clinician to\ncatch errors. A dangerous combination for patient safety if vigilance is\ncompromised by workload and fatigue. In this paper, we introduce a method for\nextracting salient clinical information in real-time alongside the healthcare\nconsultation, denoted Facts, and use that information recursively to generate\nthe final note. The FactsR method results in more accurate and concise notes by\nplacing the clinician-in-the-loop of note generation, while opening up new use\ncases within real-time decision support.\n","authors":["Victor Petrén Bach Hansen","Lasse Krogsbøll","Jonas Lyngsø","Mathias Baltzersen","Andreas Motzfeldt","Kevin Pelgrims","Lars Maaløe"],"pdf_url":"https://arxiv.org/pdf/2505.10360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13231v2","updated":"2025-05-15T14:47:02Z","published":"2025-04-17T14:43:56Z","title":"WildFireCan-MMD: A Multimodal Dataset for Classification of\n  User-Generated Content During Wildfires in Canada","summary":"  Rapid information access is vital during wildfires, yet traditional data\nsources are slow and costly. Social media offers real-time updates, but\nextracting relevant insights remains a challenge. We present WildFireCan-MMD, a\nnew multimodal dataset of X posts from recent Canadian wildfires, annotated\nacross twelve key themes. Evaluating both vision-language models and\ncustom-trained classifiers, we show that while zero-shot prompting offers quick\ndeployment, even simple trained models outperform them when labelled data is\navailable. Our best-performing transformer-based fine-tuned model reaches 83%\nf-score, outperforming gpt4 by 23%. As a use case, we demonstrate how this\nmodel can be used to uncover trends during wildfires. Our findings highlight\nthe enduring importance of tailored datasets and task-specific training.\nImportantly, such datasets should be localized, as disaster response\nrequirements vary across regions and contexts.\n","authors":["Braeden Sherritt","Isar Nejadgholi","Marzieh Amini"],"pdf_url":"https://arxiv.org/pdf/2504.13231v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10352v1","updated":"2025-05-15T14:43:35Z","published":"2025-05-15T14:43:35Z","title":"SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with\n  Hamming Attention and $\\mathcal{O}(T)$ Complexity","summary":"  Spiking Neural Networks (SNNs) have shown competitive performance to\nArtificial Neural Networks (ANNs) in various vision tasks, while offering\nsuperior energy efficiency. However, existing SNN-based Transformers primarily\nfocus on single-image tasks, emphasizing spatial features while not effectively\nleveraging SNNs' efficiency in video-based vision tasks. In this paper, we\nintroduce SpikeVideoFormer, an efficient spike-driven video Transformer,\nfeaturing linear temporal complexity $\\mathcal{O}(T)$. Specifically, we design\na spike-driven Hamming attention (SDHA) which provides a theoretically guided\nadaptation from traditional real-valued attention to spike-driven attention.\nBuilding on SDHA, we further analyze various spike-driven space-time attention\ndesigns and identify an optimal scheme that delivers appealing performance for\nvideo tasks, while maintaining only linear temporal complexity. The\ngeneralization ability and efficiency of our model are demonstrated across\ndiverse downstream video tasks, including classification, human pose tracking,\nand semantic segmentation. Empirical results show our method achieves\nstate-of-the-art (SOTA) performance compared to existing SNN approaches, with\nover 15\\% improvement on the latter two tasks. Additionally, it matches the\nperformance of recent ANN-based methods while offering significant efficiency\ngains, achieving $\\times 16$, $\\times 10$ and $\\times 5$ improvements on the\nthree tasks. https://github.com/JimmyZou/SpikeVideoFormer\n","authors":["Shihao Zou","Qingfeng Li","Wei Ji","Jingjing Li","Yongkui Yang","Guoqi Li","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2505.10352v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2412.03587v2","updated":"2025-05-15T14:39:45Z","published":"2024-11-26T08:41:45Z","title":"Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient\n  Fine-Tuning of Language Models","summary":"  Transformer-based large-scale pre-trained models achieve great success.\nFine-tuning is the standard practice for leveraging these models in downstream\ntasks. Among the fine-tuning methods, adapter-tuning provides a\nparameter-efficient fine-tuning by introducing lightweight trainable modules\nwhile keeping most pre-trained parameters frozen. However, existing\nadapter-tuning methods still impose substantial resource usage. Through our\ninvestigation, we show that each adapter unequally contributes to both task\nperformance and resource usage. Motivated by this insight, we propose Selective\nAdapter FrEezing (SAFE), which gradually freezes less important adapters early\nto reduce unnecessary resource usage while maintaining performance. In our\nexperiments, SAFE reduces memory usage, computation amount, and training time\nby 42.85\\%, 34.59\\%, and 11.82\\%, respectively, while achieving comparable or\nbetter task performance compared to the baseline. We also demonstrate that SAFE\ninduces regularization effect, thereby smoothing the loss landscape, which\nenables the model to generalize better by avoiding sharp minima.\n","authors":["Hyegang Son","Yonglak Son","Changhoon Kim","Young Geun Kim"],"pdf_url":"https://arxiv.org/pdf/2412.03587v2.pdf","comment":"URL: https://aclanthology.org/2025.naacl-long.480/ Volume:\n  Proceedings of the 2025 Conference of the Nations of the Americas Chapter of\n  the Association for Computational Linguistics: Human Language Technologies\n  (Volume 1: Long Papers) Year: 2025 Address: Albuquerque, New Mexico"},{"id":"http://arxiv.org/abs/2505.10347v1","updated":"2025-05-15T14:34:36Z","published":"2025-05-15T14:34:36Z","title":"Uniform Loss vs. Specialized Optimization: A Comparative Analysis in\n  Multi-Task Learning","summary":"  Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task\nLearning by addressing issues like conflicting gradients and differing gradient\nnorms, which hinder equal-weighted task training. However, recent critiques\nsuggest that equally weighted tasks can achieve competitive results compared to\nSMTOs, arguing that previous SMTO results were influenced by poor\nhyperparameter optimization and lack of regularization. In this work, we\nevaluate these claims through an extensive empirical evaluation of SMTOs,\nincluding some of the latest methods, on more complex multi-task problems to\nclarify this behavior. Our findings indicate that SMTOs perform well compared\nto uniform loss and that fixed weights can achieve competitive performance\ncompared to SMTOs. Furthermore, we demonstrate why uniform loss perform\nsimilarly to SMTOs in some instances. The code will be made publicly available.\n","authors":["Gabriel S. Gama","Valdir Grassi Jr"],"pdf_url":"https://arxiv.org/pdf/2505.10347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12609v2","updated":"2025-05-15T14:27:59Z","published":"2024-10-16T14:26:08Z","title":"Towards Graph Foundation Models: Training on Knowledge Graphs Enables\n  Transferability to General Graphs","summary":"  Inspired by the success of large language models, there is a trend toward\ndeveloping graph foundation models to conduct diverse downstream tasks in\nvarious domains. However, current models often require extra fine-tuning to\napply their learned structural and semantic representations to new graphs,\nwhich limits their versatility. Recent breakthroughs in zero-shot inductive\nreasoning on knowledge graphs (KGs), offer us a new perspective on extending KG\nreasoning to general graph applications. In this paper, we introduce SCR, a\nunified graph reasoning framework designed to train on knowledge graphs and\neffectively generalize across a wide range of graph tasks and domains. We begin\nby designing the task-specific KG structures to establish a unified topology\nfor different task formats. Then we propose semantic-conditioned message\npassing, a novel mechanism addressing the inherent semantic isolation in\ntraditional KG reasoning, by jointly modeling structural and semantic\ninvariance patterns in graph representations. To demonstrate the effectiveness,\nwe evaluate the inductive reasoning capability of SCR using 38 diverse graph\ndatasets, covering node-level, link-level, and graph-level tasks across\nmultiple domains. Our results show substantial performance gains over existing\nfoundation models and supervised baselines, highlighting the efficacy and\nadaptability of our approach.\n","authors":["Kai Wang","Siqiang Luo","Caihua Shan","Yifei Shen"],"pdf_url":"https://arxiv.org/pdf/2410.12609v2.pdf","comment":"25 Pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.03780v2","updated":"2025-05-15T14:26:40Z","published":"2025-04-30T12:57:21Z","title":"GPU Performance Portability needs Autotuning","summary":"  As LLMs grow in complexity, achieving state-of-the-art performance requires\ntight co-design across algorithms, software, and hardware. Today's reliance on\na single dominant platform limits portability, creates vendor lock-in, and\nraises barriers for new AI hardware. In this work, we make the case for\ncombining just-in-time (JIT) compilation with kernel parameter autotuning to\nenable portable LLM inference with state-of-the-art performance without code\nchanges. Focusing on flash attention -- a widespread performance critical LLM\nkernel -- we demonstrate that this approach explores up to 15x more kernel\nparameter configurations, produces significantly more diverse code across\nmultiple dimensions, and even outperforms vendor-optimized implementations by\nup to 230%, all while reducing kernel code size by 70x and eliminating manual\ncode optimizations. Our results highlight autotuning as a promising path to\nunlocking model portability across GPU vendors.\n","authors":["Burkhard Ringlein","Thomas Parnell","Radu Stoica"],"pdf_url":"https://arxiv.org/pdf/2505.03780v2.pdf","comment":"typos, fix grammatical mistakes"},{"id":"http://arxiv.org/abs/2503.05760v3","updated":"2025-05-15T14:23:05Z","published":"2025-02-23T18:47:14Z","title":"The Lazy Student's Dream: ChatGPT Passing an Engineering Course on Its\n  Own","summary":"  This paper presents a comprehensive investigation into the capability of\nLarge Language Models (LLMs) to successfully complete a semester-long\nundergraduate control systems course. Through evaluation of 115 course\ndeliverables, we assess LLM performance using ChatGPT under a \"minimal effort\"\nprotocol that simulates realistic student usage patterns. The investigation\nemploys a rigorous testing methodology across multiple assessment formats, from\nauto-graded multiple choice questions to complex Python programming tasks and\nlong-form analytical writing. Our analysis provides quantitative insights into\nAI's strengths and limitations in handling mathematical formulations, coding\nchallenges, and theoretical concepts in control systems engineering. The LLM\nachieved a B-grade performance (82.24\\%), approaching but not exceeding the\nclass average (84.99\\%), with strongest results in structured assignments and\ngreatest limitations in open-ended projects. The findings inform discussions\nabout course design adaptation in response to AI advancement, moving beyond\nsimple prohibition towards thoughtful integration of these tools in engineering\neducation. Additional materials including syllabus, examination papers, design\nprojects, and example responses can be found at the project website:\nhttps://gradegpt.github.io.\n","authors":["Gokul Puthumanaillam","Melkior Ornik"],"pdf_url":"https://arxiv.org/pdf/2503.05760v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08210v2","updated":"2025-05-15T14:22:35Z","published":"2025-04-11T02:27:30Z","title":"Optimizing Power Grid Topologies with Reinforcement Learning: A Survey\n  of Methods and Challenges","summary":"  Power grid operation is becoming increasingly complex due to the rising\nintegration of renewable energy sources and the need for more adaptive control\nstrategies. Reinforcement Learning (RL) has emerged as a promising approach to\npower network control (PNC), offering the potential to enhance decision-making\nin dynamic and uncertain environments. The Learning To Run a Power Network\n(L2RPN) competitions have played a key role in accelerating research by\nproviding standardized benchmarks and problem formulations, leading to rapid\nadvancements in RL-based methods. This survey provides a comprehensive and\nstructured overview of RL applications for power grid topology optimization,\ncategorizing existing techniques, highlighting key design choices, and\nidentifying gaps in current research. Additionally, we present a comparative\nnumerical study evaluating the impact of commonly applied RL-based methods,\noffering insights into their practical effectiveness. By consolidating existing\nresearch and outlining open challenges, this survey aims to provide a\nfoundation for future advancements in RL-driven power grid optimization.\n","authors":["Erica van der Sar","Alessandro Zocca","Sandjai Bhulai"],"pdf_url":"https://arxiv.org/pdf/2504.08210v2.pdf","comment":"60 pages, 26 figures, preprint"},{"id":"http://arxiv.org/abs/2505.10331v1","updated":"2025-05-15T14:20:02Z","published":"2025-05-15T14:20:02Z","title":"Emergence of Structure in Ensembles of Random Neural Networks","summary":"  Randomness is ubiquitous in many applications across data science and machine\nlearning. Remarkably, systems composed of random components often display\nemergent global behaviors that appear deterministic, manifesting a transition\nfrom microscopic disorder to macroscopic organization. In this work, we\nintroduce a theoretical model for studying the emergence of collective\nbehaviors in ensembles of random classifiers. We argue that, if the ensemble is\nweighted through the Gibbs measure defined by adopting the classification loss\nas an energy, then there exists a finite temperature parameter for the\ndistribution such that the classification is optimal, with respect to the loss\n(or the energy). Interestingly, for the case in which samples are generated by\na Gaussian distribution and labels are constructed by employing a teacher\nperceptron, we analytically prove and numerically confirm that such optimal\ntemperature does not depend neither on the teacher classifier (which is, by\nconstruction of the learning problem, unknown), nor on the number of random\nclassifiers, highlighting the universal nature of the observed behavior.\nExperiments on the MNIST dataset underline the relevance of this phenomenon in\nhigh-quality, noiseless, datasets. Finally, a physical analogy allows us to\nshed light on the self-organizing nature of the studied phenomenon.\n","authors":["Luca Muscarnera","Luigi Loreti","Giovanni Todeschini","Alessio Fumagalli","Francesco Regazzoni"],"pdf_url":"https://arxiv.org/pdf/2505.10331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10330v1","updated":"2025-05-15T14:19:01Z","published":"2025-05-15T14:19:01Z","title":"Efficient Adaptation of Reinforcement Learning Agents to Sudden\n  Environmental Change","summary":"  Real-world autonomous decision-making systems, from robots to recommendation\nengines, must operate in environments that change over time. While deep\nreinforcement learning (RL) has shown an impressive ability to learn optimal\npolicies in stationary environments, most methods are data intensive and assume\na world that does not change between training and test time. As a result,\nconventional RL methods struggle to adapt when conditions change. This poses a\nfundamental challenge: how can RL agents efficiently adapt their behavior when\nencountering novel environmental changes during deployment without\ncatastrophically forgetting useful prior knowledge? This dissertation\ndemonstrates that efficient online adaptation requires two key capabilities:\n(1) prioritized exploration and sampling strategies that help identify and\nlearn from relevant experiences, and (2) selective preservation of prior\nknowledge through structured representations that can be updated without\ndisruption to reusable components.\n","authors":["Jonathan Clifford Balloch"],"pdf_url":"https://arxiv.org/pdf/2505.10330v1.pdf","comment":"PhD Dissertation, 131 pages"},{"id":"http://arxiv.org/abs/2409.13338v3","updated":"2025-05-15T14:13:36Z","published":"2024-09-20T08:57:20Z","title":"Time Awareness in Large Language Models: Benchmarking Fact Recall Across\n  Time","summary":"  Who is the US President? The answer changes depending on when the question is\nasked. While large language models (LLMs) are evaluated on various reasoning\ntasks, they often miss a crucial dimension: time. In real-world scenarios, the\ncorrectness of answers is frequently tied to temporal context. To address this\ngap, we present a novel framework and dataset spanning over 8,000 events from\n2018 to 2024, annotated with day-level granularity and sourced globally across\ndomains such as politics, science, and business. Our TimeShift evaluation\nmethod systematically probes LLMs for temporal reasoning, revealing that base\nmodels often outperform instruction-tuned and synthetic-trained counterparts on\ntime-sensitive recall. Additionally, we find that even large-scale models\nexhibit brittleness in handling paraphrased facts, highlighting unresolved\nchallenges in temporal consistency. By identifying these limitations, our work\nprovides a significant step toward advancing time-aware language models capable\nof adapting to the dynamic nature of real-world knowledge.\n","authors":["David Herel","Vojtech Bartek","Jiri Jirak","Tomas Mikolov"],"pdf_url":"https://arxiv.org/pdf/2409.13338v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10328v1","updated":"2025-05-15T14:12:39Z","published":"2025-05-15T14:12:39Z","title":"A Comparative Study of SMT and MILP for the Nurse Rostering Problem","summary":"  The effects of personnel scheduling on the quality of care and working\nconditions for healthcare personnel have been thoroughly documented. However,\nthe ever-present demand and large variation of constraints make healthcare\nscheduling particularly challenging. This problem has been studied for decades,\nwith limited research aimed at applying Satisfiability Modulo Theories (SMT).\nSMT has gained momentum within the formal verification community in the last\ndecades, leading to the advancement of SMT solvers that have been shown to\noutperform standard mathematical programming techniques.\n  In this work, we propose generic constraint formulations that can model a\nwide range of real-world scheduling constraints. Then, the generic constraints\nare formulated as SMT and MILP problems and used to compare the respective\nstate-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired\nrostering problems. Experimental results show how each solver excels for\ncertain types of problems; the MILP solver generally performs better when the\nproblem is highly constrained or infeasible, while the SMT solver performs\nbetter otherwise. On real-world inspired problems containing a more varied set\nof shifts and personnel, the SMT solver excels. Additionally, it was noted\nduring experimentation that the SMT solver was more sensitive to the way the\ngeneric constraints were formulated, requiring careful consideration and\nexperimentation to achieve better performance. We conclude that SMT-based\nmethods present a promising avenue for future research within the domain of\npersonnel scheduling.\n","authors":["Alvin Combrink","Stephie Do","Kristofer Bengtsson","Sabino Francesco Roselli","Martin Fabian"],"pdf_url":"https://arxiv.org/pdf/2505.10328v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.03825v2","updated":"2025-05-15T14:07:22Z","published":"2025-05-03T11:28:13Z","title":"Intelligently Augmented Contrastive Tensor Factorization: Empowering\n  Multi-dimensional Time Series Classification in Low-Data Environments","summary":"  Classification of multi-dimensional time series from real-world systems\nrequire fine-grained learning of complex features such as cross-dimensional\ndependencies and intra-class variations-all under the practical challenge of\nlow training data availability. However, standard deep learning (DL) struggles\nto learn generalizable features in low-data environments due to model\noverfitting. We propose a versatile yet data-efficient framework, Intelligently\nAugmented Contrastive Tensor Factorization (ITA-CTF), to learn effective\nrepresentations from multi-dimensional time series. The CTF module learns core\nexplanatory components of the time series (e.g., sensor factors, temporal\nfactors), and importantly, their joint dependencies. Notably, unlike standard\ntensor factorization (TF), the CTF module incorporates a new contrastive loss\noptimization to induce similarity learning and class-awareness into the learnt\nrepresentations for better classification performance. To strengthen this\ncontrastive learning, the preceding ITA module generates targeted but\ninformative augmentations that highlight realistic intra-class patterns in the\noriginal data, while preserving class-wise properties. This is achieved by\ndynamically sampling a \"soft\" class prototype to guide the warping of each\nquery data sample, which results in an augmentation that is intelligently\npattern-mixed between the \"soft\" class prototype and the query sample. These\naugmentations enable the CTF module to recognize complex intra-class variations\ndespite the limited original training data, and seek out invariant class-wise\nproperties for accurate classification performance. The proposed method is\ncomprehensively evaluated on five different classification tasks. Compared to\nstandard TF and several DL benchmarks, notable performance improvements up to\n18.7% were achieved.\n","authors":["Anushiya Arunan","Yan Qin","Xiaoli Li","Yuen Chau"],"pdf_url":"https://arxiv.org/pdf/2505.03825v2.pdf","comment":"Accepted in Expert Systems with Applications\n  (DOI:https://doi.org/10.1016/j.eswa.2025.127889)"},{"id":"http://arxiv.org/abs/2411.19477v3","updated":"2025-05-15T14:06:27Z","published":"2024-11-29T05:29:47Z","title":"Simple and Provable Scaling Laws for the Test-Time Compute of Large\n  Language Models","summary":"  We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms.\n","authors":["Yanxi Chen","Xuchen Pan","Yaliang Li","Bolin Ding","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.19477v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10321v1","updated":"2025-05-15T14:06:00Z","published":"2025-05-15T14:06:00Z","title":"AutoPentest: Enhancing Vulnerability Management With Autonomous LLM\n  Agents","summary":"  A recent area of increasing research is the use of Large Language Models\n(LLMs) in penetration testing, which promises to reduce costs and thus allow\nfor higher frequency. We conduct a review of related work, identifying best\npractices and common evaluation issues. We then present AutoPentest, an\napplication for performing black-box penetration tests with a high degree of\nautonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent\nframework LangChain. It can perform complex multi-step tasks, augmented by\nexternal tools and knowledge bases. We conduct a study on three\ncapture-the-flag style Hack The Box (HTB) machines, comparing our\nimplementation AutoPentest with the baseline approach of manually using the\nChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the\nsubtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT.\nWe measure a total cost of \\$96.20 US when using AutoPentest across all\nexperiments, while a one-month subscription to ChatGPT Plus costs \\$20. The\nresults show that further implementation efforts and the use of more powerful\nLLMs released in the future are likely to make this a viable part of\nvulnerability management.\n","authors":["Julius Henke"],"pdf_url":"https://arxiv.org/pdf/2505.10321v1.pdf","comment":"24 pages, 1 figure, for implementation, see\n  https://github.com/JuliusHenke/autopentest"},{"id":"http://arxiv.org/abs/2505.10320v1","updated":"2025-05-15T14:05:15Z","published":"2025-05-15T14:05:15Z","title":"J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning","summary":"  The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.\n","authors":["Chenxi Whitehouse","Tianlu Wang","Ping Yu","Xian Li","Jason Weston","Ilia Kulikov","Swarnadeep Saha"],"pdf_url":"https://arxiv.org/pdf/2505.10320v1.pdf","comment":"10 pages, 8 tables, 11 figures"},{"id":"http://arxiv.org/abs/2505.10315v1","updated":"2025-05-15T14:00:19Z","published":"2025-05-15T14:00:19Z","title":"Private Transformer Inference in MLaaS: A Survey","summary":"  Transformer models have revolutionized AI, powering applications like content\ngeneration and sentiment analysis. However, their deployment in Machine\nLearning as a Service (MLaaS) raises significant privacy concerns, primarily\ndue to the centralized processing of sensitive user data. Private Transformer\nInference (PTI) offers a solution by utilizing cryptographic techniques such as\nsecure multi-party computation and homomorphic encryption, enabling inference\nwhile preserving both user data and model privacy. This paper reviews recent\nPTI advancements, highlighting state-of-the-art solutions and challenges. We\nalso introduce a structured taxonomy and evaluation framework for PTI, focusing\non balancing resource efficiency with privacy and bridging the gap between\nhigh-performance inference and data privacy.\n","authors":["Yang Li","Xinyu Zhou","Yitong Wang","Liangxin Qian","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.10315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10309v1","updated":"2025-05-15T13:55:27Z","published":"2025-05-15T13:55:27Z","title":"Empirically evaluating commonsense intelligence in large language models\n  with large-scale human judgments","summary":"  Commonsense intelligence in machines is often assessed by static benchmarks\nthat compare a model's output against human-prescribed correct labels. An\nimportant, albeit implicit, assumption of these labels is that they accurately\ncapture what any human would think, effectively treating human common sense as\nhomogeneous. However, recent empirical work has shown that humans vary\nenormously in what they consider commonsensical; thus what appears self-evident\nto one benchmark designer may not be so to another. Here, we propose a novel\nmethod for evaluating common sense in artificial intelligence (AI),\nspecifically in large language models (LLMs), that incorporates empirically\nobserved heterogeneity among humans by measuring the correspondence between a\nmodel's judgment and that of a human population. We first find that, when\ntreated as independent survey respondents, most LLMs remain below the human\nmedian in their individual commonsense competence. Second, when used as\nsimulators of a hypothetical population, LLMs correlate with real humans only\nmodestly in the extent to which they agree on the same set of statements. In\nboth cases, smaller, open-weight models are surprisingly more competitive than\nlarger, proprietary frontier models. Our evaluation framework, which ties\ncommonsense intelligence to its cultural basis, contributes to the growing call\nfor adapting AI models to human collectivities that possess different, often\nincompatible, social stocks of knowledge.\n","authors":["Tuan Dung Nguyen","Duncan J. Watts","Mark E. Whiting"],"pdf_url":"https://arxiv.org/pdf/2505.10309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10300v1","updated":"2025-05-15T13:49:02Z","published":"2025-05-15T13:49:02Z","title":"AI LEGO: Scaffolding Cross-Functional Collaboration in Industrial\n  Responsible AI Practices during Early Design Stages","summary":"  Responsible AI (RAI) efforts increasingly emphasize the importance of\naddressing potential harms early in the AI development lifecycle through\nsocial-technical lenses. However, in cross-functional industry teams, this work\nis often stalled by a persistent knowledge handoff challenge: the difficulty of\ntransferring high-level, early-stage technical design rationales from technical\nexperts to non-technical or user-facing roles for ethical evaluation and harm\nidentification. Through literature review and a co-design study with 8\npractitioners, we unpack how this challenge manifests -- technical design\nchoices are rarely handed off in ways that support meaningful engagement by\nnon-technical roles; collaborative workflows lack shared, visual structures to\nsupport mutual understanding; and non-technical practitioners are left without\nscaffolds for systematic harm evaluation. Existing tools like JIRA or Google\nDocs, while useful for product tracking, are ill-suited for supporting joint\nharm identification across roles, often requiring significant extra effort to\nalign understanding. To address this, we developed AI LEGO, a web-based\nprototype that supports cross-functional AI practitioners in effectively\nfacilitating knowledge handoff and identifying harmful design choices in the\nearly design stages. Technical roles use interactive blocks to draft\ndevelopment plans, while non-technical roles engage with those blocks through\nstage-specific checklists and LLM-driven persona simulations to surface\npotential harms. In a study with 18 cross-functional practitioners, AI LEGO\nincreased the volume and likelihood of harms identified compared to baseline\nworksheets. Participants found that its modular structure and persona prompts\nmade harm identification more accessible, fostering clearer and more\ncollaborative RAI practices in early design.\n","authors":["Muzhe Wu","Yanzhi Zhao","Shuyi Han","Michael Xieyang Liu","Hong Shen"],"pdf_url":"https://arxiv.org/pdf/2505.10300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10297v1","updated":"2025-05-15T13:44:32Z","published":"2025-05-15T13:44:32Z","title":"Defending the Edge: Representative-Attention for Mitigating Backdoor\n  Attacks in Federated Learning","summary":"  Federated learning (FL) enhances privacy and reduces communication cost for\nresource-constrained edge clients by supporting distributed model training at\nthe edge. However, the heterogeneous nature of such devices produces diverse,\nnon-independent, and identically distributed (non-IID) data, making the\ndetection of backdoor attacks more challenging. In this paper, we propose a\nnovel federated representative-attention-based defense mechanism, named FeRA,\nthat leverages cross-client attention over internal feature representations to\ndistinguish benign from malicious clients. FeRA computes an anomaly score based\non representation reconstruction errors, effectively identifying clients whose\ninternal activations significantly deviate from the group consensus. Our\nevaluation demonstrates FeRA's robustness across various FL scenarios,\nincluding challenging non-IID data distributions typical of edge devices.\nExperimental results show that it effectively reduces backdoor attack success\nrates while maintaining high accuracy on the main task. The method is\nmodel-agnostic, attack-agnostic, and does not require labeled reference data,\nmaking it well suited to heterogeneous and resource-limited edge deployments.\n","authors":["Chibueze Peace Obioma","Youcheng Sun","Mustafa A. Mustafa"],"pdf_url":"https://arxiv.org/pdf/2505.10297v1.pdf","comment":"Submitted to ESORICS 2025"},{"id":"http://arxiv.org/abs/2502.13295v2","updated":"2025-05-15T13:42:18Z","published":"2025-02-18T21:32:24Z","title":"Demonstrating specification gaming in reasoning models","summary":"  We demonstrate LLM agent specification gaming by instructing models to win\nagainst a chess engine. We find reasoning models like OpenAI o3 and DeepSeek R1\nwill often hack the benchmark by default, while language models like GPT-4o and\nClaude 3.5 Sonnet need to be told that normal play won't work to hack.\n  We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024;\nWeij et al., 2024) by using realistic task prompts and avoiding excess nudging.\nOur results suggest reasoning models may resort to hacking to solve difficult\nproblems, as observed in OpenAI (2024)'s o1 Docker escape during cyber\ncapabilities testing.\n","authors":["Alexander Bondarenko","Denis Volk","Dmitrii Volkov","Jeffrey Ladish"],"pdf_url":"https://arxiv.org/pdf/2502.13295v2.pdf","comment":"Updated with o3 results"},{"id":"http://arxiv.org/abs/2505.07816v2","updated":"2025-05-15T13:32:24Z","published":"2025-05-12T17:59:22Z","title":"Graph neural networks and MSO","summary":"  We give an alternative proof for the existing result that recurrent graph\nneural networks working with reals have the same expressive power in\nrestriction to monadic second-order logic MSO as the graded modal substitution\ncalculus. The proof is based on constructing distributed automata that capture\nall MSO-definable node properties over trees. We also consider some variants of\nthe acceptance conditions.\n","authors":["Veeti Ahvonen","Damian Heiman","Antti Kuusisto"],"pdf_url":"https://arxiv.org/pdf/2505.07816v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10278v1","updated":"2025-05-15T13:27:18Z","published":"2025-05-15T13:27:18Z","title":"MASS: Multi-Agent Simulation Scaling for Portfolio Construction","summary":"  LLM-based multi-agent has gained significant attention for their potential in\nsimulation and enhancing performance. However, existing works are limited to\npure simulations or are constrained by predefined workflows, restricting their\napplicability and effectiveness. In this paper, we introduce the Multi-Agent\nScaling Simulation (MASS) for portfolio construction. MASS achieves stable and\ncontinuous excess returns by progressively increasing the number of agents for\nlarge-scale simulations to gain a superior understanding of the market and\noptimizing agent distribution end-to-end through a reverse optimization\nprocess, rather than relying on a fixed workflow. We demonstrate its\nsuperiority through performance experiments, ablation studies, backtesting\nexperiments, experiments on updated data and stock pools, scaling experiments,\nparameter sensitivity experiments, and visualization experiments, conducted in\ncomparison with 6 state-of-the-art baselines on 3 challenging A-share stock\npools. We expect the paradigm established by MASS to expand to other tasks with\nsimilar characteristics. The implementation of MASS has been open-sourced at\nhttps://github.com/gta0804/MASS.\n","authors":["Taian Guo","Haiyang Shen","Jinsheng Huang","Zhengyang Mao","Junyu Luo","Zhuoru Chen","Xuhui Liu","Bingyu Xia","Luchen Liu","Yun Ma","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.10278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10273v1","updated":"2025-05-15T13:24:09Z","published":"2025-05-15T13:24:09Z","title":"AttentionGuard: Transformer-based Misbehavior Detection for Secure\n  Vehicular Platoons","summary":"  Vehicle platooning, with vehicles traveling in close formation coordinated\nthrough Vehicle-to-Everything (V2X) communications, offers significant benefits\nin fuel efficiency and road utilization. However, it is vulnerable to\nsophisticated falsification attacks by authenticated insiders that can\ndestabilize the formation and potentially cause catastrophic collisions. This\npaper addresses this challenge: misbehavior detection in vehicle platooning\nsystems. We present AttentionGuard, a transformer-based framework for\nmisbehavior detection that leverages the self-attention mechanism to identify\nanomalous patterns in mobility data. Our proposal employs a multi-head\ntransformer-encoder to process sequential kinematic information, enabling\neffective differentiation between normal mobility patterns and falsification\nattacks across diverse platooning scenarios, including steady-state\n(no-maneuver) operation, join, and exit maneuvers. Our evaluation uses an\nextensive simulation dataset featuring various attack vectors (constant,\ngradual, and combined falsifications) and operational parameters (controller\ntypes, vehicle speeds, and attacker positions). Experimental results\ndemonstrate that AttentionGuard achieves up to 0.95 F1-score in attack\ndetection, with robust performance maintained during complex maneuvers.\nNotably, our system performs effectively with minimal latency (100ms decision\nintervals), making it suitable for real-time transportation safety\napplications. Comparative analysis reveals superior detection capabilities and\nestablishes the transformer-encoder as a promising approach for securing\nCooperative Intelligent Transport Systems (C-ITS) against sophisticated insider\nthreats.\n","authors":["Hexu Li","Konstantinos Kalogiannis","Ahmed Mohamed Hussain","Panos Papadimitratos"],"pdf_url":"https://arxiv.org/pdf/2505.10273v1.pdf","comment":"Author's version; Accepted for presentation at the ACM Workshop on\n  Wireless Security and Machine Learning (WiseML 2025)"},{"id":"http://arxiv.org/abs/2505.10264v1","updated":"2025-05-15T13:16:32Z","published":"2025-05-15T13:16:32Z","title":"Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack\n  in Federated Learning","summary":"  Federated Learning (FL) enables collaborative training of machine learning\nmodels across distributed clients without sharing raw data, ostensibly\npreserving data privacy. Nevertheless, recent studies have revealed critical\nvulnerabilities in FL, showing that a malicious central server can manipulate\nmodel updates to reconstruct clients' private training data. Existing data\nreconstruction attacks have important limitations: they often rely on\nassumptions about the clients' data distribution or their efficiency\nsignificantly degrades when batch sizes exceed just a few tens of samples.\n  In this work, we introduce a novel data reconstruction attack that overcomes\nthese limitations. Our method leverages a new geometric perspective on fully\nconnected layers to craft malicious model parameters, enabling the perfect\nrecovery of arbitrarily large data batches in classification tasks without any\nprior knowledge of clients' data. Through extensive experiments on both image\nand tabular datasets, we demonstrate that our attack outperforms existing\nmethods and achieves perfect reconstruction of data batches two orders of\nmagnitude larger than the state of the art.\n","authors":["Francesco Diana","André Nusser","Chuan Xu","Giovanni Neglia"],"pdf_url":"https://arxiv.org/pdf/2505.10264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10261v1","updated":"2025-05-15T13:11:14Z","published":"2025-05-15T13:11:14Z","title":"The Evolving Landscape of Generative Large Language Models and\n  Traditional Natural Language Processing in Medicine","summary":"  Natural language processing (NLP) has been traditionally applied to medicine,\nand generative large language models (LLMs) have become prominent recently.\nHowever, the differences between them across different medical tasks remain\nunderexplored. We analyzed 19,123 studies, finding that generative LLMs\ndemonstrate advantages in open-ended tasks, while traditional NLP dominates in\ninformation extraction and analysis tasks. As these technologies advance,\nethical use of them is essential to ensure their potential in medical\napplications.\n","authors":["Rui Yang","Huitao Li","Matthew Yu Heng Wong","Yuhe Ke","Xin Li","Kunyu Yu","Jingchi Liao","Jonathan Chong Kai Liew","Sabarinath Vinod Nair","Jasmine Chiat Ling Ong","Irene Li","Douglas Teodoro","Chuan Hong","Daniel Shu Wei Ting","Nan Liu"],"pdf_url":"https://arxiv.org/pdf/2505.10261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10260v1","updated":"2025-05-15T13:10:47Z","published":"2025-05-15T13:10:47Z","title":"Comparing LLM Text Annotation Skills: A Study on Human Rights Violations\n  in Social Media Data","summary":"  In the era of increasingly sophisticated natural language processing (NLP)\nsystems, large language models (LLMs) have demonstrated remarkable potential\nfor diverse applications, including tasks requiring nuanced textual\nunderstanding and contextual reasoning. This study investigates the\ncapabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,\nMistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex\ntextual dataset comprising social media posts in Russian and Ukrainian.\nSpecifically, the focus is on the binary classification task of identifying\nreferences to human rights violations within the dataset.\n  To evaluate the effectiveness of these models, their annotations are compared\nagainst a gold standard set of human double-annotated labels across 1000\nsamples. The analysis includes assessing annotation performance under different\nprompting conditions, with prompts provided in both English and Russian.\nAdditionally, the study explores the unique patterns of errors and\ndisagreements exhibited by each model, offering insights into their strengths,\nlimitations, and cross-linguistic adaptability.\n  By juxtaposing LLM outputs with human annotations, this research contributes\nto understanding the reliability and applicability of LLMs for sensitive,\ndomain-specific tasks in multilingual contexts. It also sheds light on how\nlanguage models handle inherently subjective and context-dependent judgments, a\ncritical consideration for their deployment in real-world scenarios.\n","authors":["Poli Apollinaire Nemkova","Solomon Ubani","Mark V. Albert"],"pdf_url":"https://arxiv.org/pdf/2505.10260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06085v2","updated":"2025-05-15T13:07:31Z","published":"2025-05-09T14:29:37Z","title":"Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities","summary":"  The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16.\n","authors":["Hiari Pizzini Cavagna","Daniele Cesarini","Andrea Bartolini"],"pdf_url":"https://arxiv.org/pdf/2505.06085v2.pdf","comment":"Accepted to the Computational Aspects of Deep Learning Workshop at\n  ISC High Performance 2025. To appear in the ISC High Performance 2025\n  Workshop Proceedings"},{"id":"http://arxiv.org/abs/2411.14790v4","updated":"2025-05-15T13:02:21Z","published":"2024-11-22T08:21:03Z","title":"KBAlign: Efficient Self Adaptation on Specific Knowledge Bases","summary":"  Although retrieval-augmented generation (RAG) remains essential for\nknowledge-based question answering (KBQA), current paradigms face critical\nchallenges under specific domains. Existing methods struggle with targeted\nadaptation on small-scale KBs: vanilla unsupervised training exhibits poor\neffectiveness, while fine-tuning incurs prohibitive costs of external signals.\nWe present KBAlign, a self-supervised framework that enhances RAG systems\nthrough efficient model adaptation. Our key insight is to leverage the model's\nintrinsic capabilities for knowledge alignment through two innovative\nmechanisms: multi-grained self-annotation that captures global knowledge for\ndata construction, and iterative tuning that accelerates convergence through\nself verification. This framework enables cost-effective model adaptation to\nspecific textual KBs, without human supervision or external model assistance.\nExperiments demonstrate that KBAlign can achieve 90\\% of the performance gain\nobtained through GPT-4-supervised adaptation, while relying entirely on\nself-annotation of much smaller models. KBAlign significantly improves\ndownstream QA accuracy across multiple domains with tiny costs, particularly\nbenefiting scenarios requiring deep knowledge integration from specialized\ncorpora. We release our experimental data, models, and process analyses to the\ncommunity for further exploration (https://github.com/thunlp/KBAlign).\n","authors":["Zheni Zeng","Yuxuan Chen","Shi Yu","Ruobing Wang","Yukun Yan","Zhenghao Liu","Shuo Wang","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2411.14790v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04522v3","updated":"2025-05-15T12:47:40Z","published":"2025-02-06T21:45:38Z","title":"ImprovNet -- Generating Controllable Musical Improvisations with\n  Iterative Corruption Refinement","summary":"  Despite deep learning's remarkable advances in style transfer across various\ndomains, generating controllable performance-level musical style transfer for\ncomplete symbolically represented musical works remains a challenging area of\nresearch. Much of this is owed to limited datasets, especially for genres such\nas jazz, and the lack of unified models that can handle multiple music\ngeneration tasks. This paper presents ImprovNet, a transformer-based\narchitecture that generates expressive and controllable musical improvisations\nthrough a self-supervised corruption-refinement training strategy. The\nimprovisational style transfer is aimed at making meaningful modifications to\none or more musical elements - melody, harmony or rhythm of the original\ncomposition with respect to the target genre. ImprovNet unifies multiple\ncapabilities within a single model: it can perform cross-genre and intra-genre\nimprovisations, harmonize melodies with genre-specific styles, and execute\nshort prompt continuation and infilling tasks. The model's iterative generation\nframework allows users to control the degree of style transfer and structural\nsimilarity to the original composition. Objective and subjective evaluations\ndemonstrate ImprovNet's effectiveness in generating musically coherent\nimprovisations while maintaining structural relationships with the original\npieces. The model outperforms Anticipatory Music Transformer in short\ncontinuation and infilling tasks and successfully achieves recognizable genre\nconversion, with 79\\% of participants correctly identifying jazz-style\nimprovisations of classical pieces. Our code and demo page can be found at\nhttps://github.com/keshavbhandari/improvnet.\n","authors":["Keshav Bhandari","Sungkyun Chang","Tongyu Lu","Fareza R. Enus","Louis B. Bradshaw","Dorien Herremans","Simon Colton"],"pdf_url":"https://arxiv.org/pdf/2502.04522v3.pdf","comment":"10 pages, 6 figures, IJCNN 2025 conference"},{"id":"http://arxiv.org/abs/2505.10231v1","updated":"2025-05-15T12:43:23Z","published":"2025-05-15T12:43:23Z","title":"On the Interplay of Human-AI Alignment,Fairness, and Performance\n  Trade-offs in Medical Imaging","summary":"  Deep neural networks excel in medical imaging but remain prone to biases,\nleading to fairness gaps across demographic groups. We provide the first\nsystematic exploration of Human-AI alignment and fairness in this domain. Our\nresults show that incorporating human insights consistently reduces fairness\ngaps and enhances out-of-domain generalization, though excessive alignment can\nintroduce performance trade-offs, emphasizing the need for calibrated\nstrategies. These findings highlight Human-AI alignment as a promising approach\nfor developing fair, robust, and generalizable medical AI systems, striking a\nbalance between expert guidance and automated efficiency. Our code is available\nat https://github.com/Roypic/Aligner.\n","authors":["Haozhe Luo","Ziyu Zhou","Zixin Shu","Aurélie Pahud de Mortanges","Robert Berke","Mauricio Reyes"],"pdf_url":"https://arxiv.org/pdf/2505.10231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02481v4","updated":"2025-05-15T12:40:27Z","published":"2025-01-05T09:06:17Z","title":"Representation Convergence: Mutual Distillation is Secretly a Form of\n  Regularization","summary":"  In this paper, we argue that mutual distillation between reinforcement\nlearning policies serves as an implicit regularization, preventing them from\noverfitting to irrelevant features. We highlight two key contributions: (a)\nTheoretically, for the first time, we prove that enhancing the policy\nrobustness to irrelevant features leads to improved generalization performance.\n(b) Empirically, we demonstrate that mutual distillation between policies\ncontributes to such robustness, enabling the spontaneous emergence of invariant\nrepresentations over pixel inputs. Overall, our findings challenge the\nconventional view of distillation as merely a means of knowledge transfer,\noffering a novel perspective on the generalization in deep reinforcement\nlearning.\n","authors":["Zhengpeng Xie","Jiahang Cao","Qiang Zhang","Jianxiong Zhang","Changwei Wang","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2501.02481v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08829v2","updated":"2025-05-15T12:19:18Z","published":"2025-05-13T01:00:25Z","title":"Aggregating Concepts of Accuracy and Fairness in Prediction Algorithms","summary":"  An algorithm that outputs predictions about the state of the world will\nalmost always be designed with the implicit or explicit goal of outputting\naccurate predictions (i.e., predictions that are likely to be true). In\naddition, the rise of increasingly powerful predictive algorithms brought about\nby the recent revolution in artificial intelligence has led to an emphasis on\nbuilding predictive algorithms that are fair, in the sense that their\npredictions do not systematically evince bias or bring about harm to certain\nindividuals or groups. This state of affairs presents two conceptual\nchallenges. First, the goals of accuracy and fairness can sometimes be in\ntension, and there are no obvious normative guidelines for managing the\ntrade-offs between these two desiderata when they arise. Second, there are many\ndistinct ways of measuring both the accuracy and fairness of a predictive\nalgorithm; here too, there are no obvious guidelines on how to aggregate our\npreferences for predictive algorithms that satisfy disparate measures of\nfairness and accuracy to various extents. The goal of this paper is to address\nthese challenges by arguing that there are good reasons for using a linear\ncombination of accuracy and fairness metrics to measure the\nall-things-considered value of a predictive algorithm for agents who care about\nboth accuracy and fairness. My argument depends crucially on a classic result\nin the preference aggregation literature due to Harsanyi. After making this\nformal argument, I apply my result to an analysis of accuracy-fairness\ntrade-offs using the COMPAS dataset compiled by Angwin et al.\n","authors":["David Kinney"],"pdf_url":"https://arxiv.org/pdf/2505.08829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10212v1","updated":"2025-05-15T12:16:36Z","published":"2025-05-15T12:16:36Z","title":"Do LLMs Memorize Recommendation Datasets? A Preliminary Study on\n  MovieLens-1M","summary":"  Large Language Models (LLMs) have become increasingly central to\nrecommendation scenarios due to their remarkable natural language understanding\nand generation capabilities. Although significant research has explored the use\nof LLMs for various recommendation tasks, little effort has been dedicated to\nverifying whether they have memorized public recommendation dataset as part of\ntheir training data. This is undesirable because memorization reduces the\ngeneralizability of research findings, as benchmarking on memorized datasets\ndoes not guarantee generalization to unseen datasets. Furthermore, memorization\ncan amplify biases, for example, some popular items may be recommended more\nfrequently than others.\n  In this work, we investigate whether LLMs have memorized public\nrecommendation datasets. Specifically, we examine two model families (GPT and\nLlama) across multiple sizes, focusing on one of the most widely used dataset\nin recommender systems: MovieLens-1M. First, we define dataset memorization as\nthe extent to which item attributes, user profiles, and user-item interactions\ncan be retrieved by prompting the LLMs. Second, we analyze the impact of\nmemorization on recommendation performance. Lastly, we examine whether\nmemorization varies across model families and model sizes. Our results reveal\nthat all models exhibit some degree of memorization of MovieLens-1M, and that\nrecommendation performance is related to the extent of memorization. We have\nmade all the code publicly available at:\nhttps://github.com/sisinflab/LLM-MemoryInspector\n","authors":["Dario Di Palma","Felice Antonio Merra","Maurizio Sfilio","Vito Walter Anelli","Fedelucio Narducci","Tommaso Di Noia"],"pdf_url":"https://arxiv.org/pdf/2505.10212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10201v1","updated":"2025-05-15T11:56:19Z","published":"2025-05-15T11:56:19Z","title":"A Fine-Grained Complexity View on Propositional Abduction -- Algorithms\n  and Lower Bounds","summary":"  The Boolean satisfiability problem (SAT) is a well-known example of monotonic\nreasoning, of intense practical interest due to fast solvers, complemented by\nrigorous fine-grained complexity results. However, for non-monotonic reasoning,\ne.g., abductive reasoning, comparably little is known outside classic\ncomplexity theory. In this paper we take a first step of bridging the gap\nbetween monotonic and non-monotonic reasoning by analyzing the complexity of\nintractable abduction problems under the seemingly overlooked but natural\nparameter n: the number of variables in the knowledge base. We obtain several\npositive results for $\\Sigma^P_2$- as well as NP- and coNP-complete fragments,\nwhich implies the first example of beating exhaustive search for a\n$\\Sigma^P_2$-complete problem (to the best of our knowledge). We complement\nthis with lower bounds and for many fragments rule out improvements under the\n(strong) exponential-time hypothesis.\n","authors":["Victor Lagerkvist","Mohamed Maizia","Johannes Schmidt"],"pdf_url":"https://arxiv.org/pdf/2505.10201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10197v1","updated":"2025-05-15T11:53:33Z","published":"2025-05-15T11:53:33Z","title":"Advancing Community Detection with Graph Convolutional Neural Networks:\n  Bridging Topological and Attributive Cohesion","summary":"  Community detection, a vital technology for real-world applications, uncovers\ncohesive node groups (communities) by leveraging both topological and attribute\nsimilarities in social networks. However, existing Graph Convolutional Networks\n(GCNs) trained to maximize modularity often converge to suboptimal solutions.\nAdditionally, directly using human-labeled communities for training can\nundermine topological cohesiveness by grouping disconnected nodes based solely\non node attributes. We address these issues by proposing a novel Topological\nand Attributive Similarity-based Community detection (TAS-Com) method. TAS-Com\nintroduces a novel loss function that exploits the highly effective and\nscalable Leiden algorithm to detect community structures with global optimal\nmodularity. Leiden is further utilized to refine human-labeled communities to\nensure connectivity within each community, enabling TAS-Com to detect community\nstructures with desirable trade-offs between modularity and compliance with\nhuman labels. Experimental results on multiple benchmark networks confirm that\nTAS-Com can significantly outperform several state-of-the-art algorithms.\n","authors":["Anjali de Silva","Gang Chen","Hui Ma","Seyed Mohammad Nekooei","Xingquan Zuo"],"pdf_url":"https://arxiv.org/pdf/2505.10197v1.pdf","comment":"This paper has been accepted by IJCAI (International Joint Conference\n  on Artificial Intelligence) 2025"},{"id":"http://arxiv.org/abs/2505.10191v1","updated":"2025-05-15T11:47:54Z","published":"2025-05-15T11:47:54Z","title":"LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean\n  Forecasting","summary":"  Mesoscale eddies dominate the spatiotemporal multiscale variability of the\nocean, and their impact on the energy cascade of the global ocean cannot be\nignored. Eddy-resolving ocean forecasting is providing more reliable protection\nfor fisheries and navigational safety, but also presents significant scientific\nchallenges and high computational costs for traditional numerical models.\nArtificial intelligence (AI)-based weather and ocean forecasting systems are\nbecoming powerful tools that balance forecast performance with computational\nefficiency. However, the complex multiscale features in the ocean dynamical\nsystem make AI models still face many challenges in mesoscale eddy forecasting\n(especially regional modelling). Here, we develop LanTu, a regional\neddy-resolving ocean forecasting system based on dynamics-enhanced deep\nlearning. We incorporate cross-scale interactions into LanTu and construct\nmultiscale physical constraint for optimising LanTu guided by knowledge of eddy\ndynamics in order to improve the forecasting skill of LanTu for mesoscale\nevolution. The results show that LanTu outperforms the existing advanced\noperational numerical ocean forecasting system (NOFS) and AI-based ocean\nforecasting system (AI-OFS) in temperature, salinity, sea level anomaly and\ncurrent prediction, with a lead time of more than 10 days. Our study highlights\nthat dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for\neddy-resolving ocean forecasting.\n","authors":["Qingyu Zheng","Qi Shao","Guijun Han","Wei Li","Hong Li","Xuan Wang"],"pdf_url":"https://arxiv.org/pdf/2505.10191v1.pdf","comment":"22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.10188v1","updated":"2025-05-15T11:42:24Z","published":"2025-05-15T11:42:24Z","title":"A User Study Evaluating Argumentative Explanations in Diagnostic\n  Decision Support","summary":"  As the field of healthcare increasingly adopts artificial intelligence, it\nbecomes important to understand which types of explanations increase\ntransparency and empower users to develop confidence and trust in the\npredictions made by machine learning (ML) systems. In shared decision-making\nscenarios where doctors cooperate with ML systems to reach an appropriate\ndecision, establishing mutual trust is crucial. In this paper, we explore\ndifferent approaches to generating explanations in eXplainable AI (XAI) and\nmake their underlying arguments explicit so that they can be evaluated by\nmedical experts. In particular, we present the findings of a user study\nconducted with physicians to investigate their perceptions of various types of\nAI-generated explanations in the context of diagnostic decision support. The\nstudy aims to identify the most effective and useful explanations that enhance\nthe diagnostic process. In the study, medical doctors filled out a survey to\nassess different types of explanations. Further, an interview was carried out\npost-survey to gain qualitative insights on the requirements of explanations\nincorporated in diagnostic decision support. Overall, the insights gained from\nthis study contribute to understanding the types of explanations that are most\neffective.\n","authors":["Felix Liedeker","Olivia Sanchez-Graillet","Moana Seidler","Christian Brandt","Jörg Wellmer","Philipp Cimiano"],"pdf_url":"https://arxiv.org/pdf/2505.10188v1.pdf","comment":"Presented at 'The First Workshop on Natural Language Argument-Based\n  Explanations', co-located with ECAI 2024"},{"id":"http://arxiv.org/abs/2205.10016v3","updated":"2025-05-15T11:37:19Z","published":"2022-05-20T08:16:30Z","title":"Learning Progress Driven Multi-Agent Curriculum","summary":"  The number of agents can be an effective curriculum variable for controlling\nthe difficulty of multi-agent reinforcement learning (MARL) tasks. Existing\nwork typically uses manually defined curricula such as linear schemes. We\nidentify two potential flaws while applying existing reward-based automatic\ncurriculum learning methods in MARL: (1) The expected episode return used to\nmeasure task difficulty has high variance; (2) Credit assignment difficulty can\nbe exacerbated in tasks where increasing the number of agents yields higher\nreturns which is common in many MARL tasks. To address these issues, we propose\nto control the curriculum by using a TD-error based *learning progress* measure\nand by letting the curriculum proceed from an initial context distribution to\nthe final task specific one. Since our approach maintains a distribution over\nthe number of agents and measures learning progress rather than absolute\nperformance, which often increases with the number of agents, we alleviate\nproblem (2). Moreover, the learning progress measure naturally alleviates\nproblem (1) by aggregating returns. In three challenging sparse-reward MARL\nbenchmarks, our approach outperforms state-of-the-art baselines.\n","authors":["Wenshuai Zhao","Zhiyuan Li","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2205.10016v3.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2505.10185v1","updated":"2025-05-15T11:31:02Z","published":"2025-05-15T11:31:02Z","title":"The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a\n  Reasoning Model will Think","summary":"  Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.\n","authors":["Seongyun Lee","Seungone Kim","Minju Seo","Yongrae Jo","Dongyoung Go","Hyeonbin Hwang","Jinho Park","Xiang Yue","Sean Welleck","Graham Neubig","Moontae Lee","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2505.10185v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2505.10183v1","updated":"2025-05-15T11:29:43Z","published":"2025-05-15T11:29:43Z","title":"KAITIAN: A Unified Communication Framework for Enabling Efficient\n  Collaboration Across Heterogeneous Accelerators in Embodied AI Systems","summary":"  Embodied Artificial Intelligence (AI) systems, such as autonomous robots and\nintelligent vehicles, are increasingly reliant on diverse heterogeneous\naccelerators (e.g., GPGPUs, NPUs, FPGAs) to meet stringent real-time processing\nand energy-efficiency demands. However, the proliferation of vendor-specific\nproprietary communication libraries creates significant interoperability\nbarriers, hindering seamless collaboration between different accelerator types\nand leading to suboptimal resource utilization and performance bottlenecks in\ndistributed AI workloads. This paper introduces KAITIAN, a novel distributed\ncommunication framework designed to bridge this gap. KAITIAN provides a unified\nabstraction layer that intelligently integrates vendor-optimized communication\nlibraries for intra-group efficiency with general-purpose communication\nprotocols for inter-group interoperability. Crucially, it incorporates a\nload-adaptive scheduling mechanism that dynamically balances computational\ntasks across heterogeneous devices based on their real-time performance\ncharacteristics. Implemented as an extension to PyTorch and rigorously\nevaluated on a testbed featuring NVIDIA GPUs and Cambricon MLUs, KAITIAN\ndemonstrates significant improvements in resource utilization and scalability\nfor distributed training tasks. Experimental results show that KAITIAN can\naccelerate training time by up to 42% compared to baseline homogeneous systems,\nwhile incurring minimal communication overhead (2.8--4.3%) and maintaining\nmodel accuracy. KAITIAN paves the way for more flexible and powerful\nheterogeneous computing in complex embodied AI applications.\n","authors":["Jieke Lin","Wanyu Wang","Longxiang Yin","Yinhe Han"],"pdf_url":"https://arxiv.org/pdf/2505.10183v1.pdf","comment":"9 pages, 4 figures. Jieke Lin and Wanyu Wang contributed equally to\n  this work"},{"id":"http://arxiv.org/abs/2409.09647v2","updated":"2025-05-15T11:26:08Z","published":"2024-09-15T07:45:11Z","title":"Self-supervised Learning for Acoustic Few-Shot Classification","summary":"  Labelled data are limited and self-supervised learning is one of the most\nimportant approaches for reducing labelling requirements. While it has been\nextensively explored in the image domain, it has so far not received the same\namount of attention in the acoustic domain. Yet, reducing labelling is a key\nrequirement for many acoustic applications. Specifically in bioacoustic, there\nare rarely sufficient labels for fully supervised learning available. This has\nled to the widespread use of acoustic recognisers that have been pre-trained on\nunrelated data for bioacoustic tasks. We posit that training on the actual task\ndata and combining self-supervised pre-training with few-shot classification is\na superior approach that has the ability to deliver high accuracy even when\nonly a few labels are available. To this end, we introduce and evaluate a new\narchitecture that combines CNN-based preprocessing with feature extraction\nbased on state space models (SSMs). This combination is motivated by the fact\nthat CNN-based networks alone struggle to capture temporal information\neffectively, which is crucial for classifying acoustic signals. SSMs,\nspecifically S4 and Mamba, on the other hand, have been shown to have an\nexcellent ability to capture long-range dependencies in sequence data. We\npre-train this architecture using contrastive learning on the actual task data\nand subsequent fine-tuning with an extremely small amount of labelled data. We\nevaluate the performance of this proposed architecture for ($n$-shot,\n$n$-class) classification on standard benchmarks as well as real-world data.\nOur evaluation shows that it outperforms state-of-the-art architectures on the\nfew-shot classification problem.\n","authors":["Jingyong Liang","Bernd Meyer","Isaac Ning Lee","Thanh-Toan Do"],"pdf_url":"https://arxiv.org/pdf/2409.09647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01797v3","updated":"2025-05-15T11:25:13Z","published":"2023-12-04T10:37:58Z","title":"LLM A*: Human in the Loop Large Language Models Enabled A* Search for\n  Robotics","summary":"  This research focuses on how Large Language Models (LLMs) can help with\n(path) planning for mobile embodied agents such as robots, in a\nhuman-in-the-loop and interactive manner. A novel framework named LLM A*, aims\nto leverage the commonsense of LLMs, and the utility-optimal A* is proposed to\nfacilitate few-shot near-optimal path planning. Prompts are used for two main\npurposes: 1) to provide LLMs with essential information like environments,\ncosts, heuristics, etc.; 2) to communicate human feedback on intermediate\nplanning results to LLMs. This approach takes human feedback on board and\nrenders the entire planning process transparent (akin to a `white box') to\nhumans. Moreover, it facilitates code-free path planning, thereby fostering the\naccessibility and inclusiveness of artificial intelligence techniques to\ncommunities less proficient in coding. Comparative analysis against A* and RL\ndemonstrates that LLM A* exhibits greater efficiency in terms of search space\nand achieves paths comparable to A* while outperforming RL. The interactive\nnature of LLM A* also makes it a promising tool for deployment in collaborative\nhuman-robot tasks. Codes and Supplemental Materials can be found at GitHub:\nhttps://github.com/speedhawk/LLM-A-.\n","authors":["Hengjia Xiao","Peng Wang","Mingzhe Yu","Mattia Robbiani"],"pdf_url":"https://arxiv.org/pdf/2312.01797v3.pdf","comment":"7 figures, 8 pages"},{"id":"http://arxiv.org/abs/2409.14191v3","updated":"2025-05-15T11:17:59Z","published":"2024-09-21T16:38:22Z","title":"Addressing and Visualizing Misalignments in Human Task-Solving\n  Trajectories","summary":"  Understanding misalignments in human task-solving trajectories is critical\nfor improving AI models trained to mimic human reasoning. This study\ncategorizes such misalignments into three types: \\textbf{(1) Lack of functions\nto express intent}, \\textbf{(2) Inefficient action sequences}, and \\textbf{(3)\nIncorrect intentions that cannot solve the task}. To address these issues, we\nfirst formalize and define these three types of misalignments. We then propose\na heuristic algorithm to detect these misalignments in O2ARC trajectories and\nconduct a hierarchical and quantitative analysis of their impact. Furthermore,\nwe introduce an intention estimation algorithm that predicts missing alignment\ninformation between user actions and inferred intentions, leveraging our\nformalized framework. Through trajectory alignment, we experimentally\ndemonstrate that AI models trained on human task-solving trajectories improve\nperformance in mimicking human reasoning. Based on hierarchical analysis and\nexperiments, we highlight the importance of trajectory-intention alignment and\ndemonstrate the potential of intention learning.\n","authors":["Sejin Kim","Hosung Lee","Sundong Kim"],"pdf_url":"https://arxiv.org/pdf/2409.14191v3.pdf","comment":"KDD 2025 accepted"},{"id":"http://arxiv.org/abs/2505.10172v1","updated":"2025-05-15T11:04:39Z","published":"2025-05-15T11:04:39Z","title":"Does Scaling Law Apply in Time Series Forecasting?","summary":"  Rapid expansion of model size has emerged as a key challenge in time series\nforecasting. From early Transformer with tens of megabytes to recent\narchitectures like TimesNet with thousands of megabytes, performance gains have\noften come at the cost of exponentially increasing parameter counts. But is\nthis scaling truly necessary? To question the applicability of the scaling law\nin time series forecasting, we propose Alinear, an ultra-lightweight\nforecasting model that achieves competitive performance using only k-level\nparameters. We introduce a horizon-aware adaptive decomposition mechanism that\ndynamically rebalances component emphasis across different forecast lengths,\nalongside a progressive frequency attenuation strategy that achieves stable\nprediction in various forecasting horizons without incurring the computational\noverhead of attention mechanisms. Extensive experiments on seven benchmark\ndatasets demonstrate that Alinear consistently outperforms large-scale models\nwhile using less than 1% of their parameters, maintaining strong accuracy\nacross both short and ultra-long forecasting horizons. Moreover, to more fairly\nevaluate model efficiency, we propose a new parameter-aware evaluation metric\nthat highlights the superiority of ALinear under constrained model budgets. Our\nanalysis reveals that the relative importance of trend and seasonal components\nvaries depending on data characteristics rather than following a fixed pattern,\nvalidating the necessity of our adaptive design. This work challenges the\nprevailing belief that larger models are inherently better and suggests a\nparadigm shift toward more efficient time series modeling.\n","authors":["Zeyan Li","Libing Chen","Yin Tang"],"pdf_url":"https://arxiv.org/pdf/2505.10172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07247v2","updated":"2025-05-15T11:01:45Z","published":"2025-05-12T05:43:21Z","title":"SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring\n  with Large Language Models","summary":"  Subjective Answer Grading (SAG) plays a crucial role in education,\nstandardized testing, and automated assessment systems, particularly for\nevaluating short-form responses in Short Answer Scoring (SAS). However,\nexisting approaches often produce coarse-grained scores and lack detailed\nreasoning. Although large language models (LLMs) have demonstrated potential as\nzero-shot evaluators, they remain susceptible to bias, inconsistencies with\nhuman judgment, and limited transparency in scoring decisions. To overcome\nthese limitations, we introduce SAS-Bench, a benchmark specifically designed\nfor LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,\nexpert-annotated error categories, and a diverse range of question types\nderived from real-world subject-specific exams. This benchmark facilitates\ndetailed evaluation of model reasoning processes and explainability. We also\nrelease an open-source dataset containing 1,030 questions and 4,109 student\nresponses, each annotated by domain experts. Furthermore, we conduct\ncomprehensive experiments with various LLMs, identifying major challenges in\nscoring science-related questions and highlighting the effectiveness of\nfew-shot prompting in improving scoring accuracy. Our work offers valuable\ninsights into the development of more robust, fair, and educationally\nmeaningful LLM-based evaluation systems.\n","authors":["Peichao Lai","Kexuan Zhang","Yi Lin","Linyihan Zhang","Feiyang Ye","Jinhao Yan","Yanwei Xu","Conghui He","Yilei Wang","Wentao Zhang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2505.07247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04363v3","updated":"2025-05-15T10:57:51Z","published":"2024-07-05T09:06:47Z","title":"AriGraph: Learning Knowledge Graph World Models with Episodic Memory for\n  LLM Agents","summary":"  Advancements in the capabilities of Large Language Models (LLMs) have created\na promising foundation for developing autonomous agents. With the right tools,\nthese agents could learn to solve tasks in new environments by accumulating and\nupdating their knowledge. Current LLM-based agents process past experiences\nusing a full history of observations, summarization, retrieval augmentation.\nHowever, these unstructured memory representations do not facilitate the\nreasoning and planning essential for complex decision-making. In our study, we\nintroduce AriGraph, a novel method wherein the agent constructs and updates a\nmemory graph that integrates semantic and episodic memories while exploring the\nenvironment. We demonstrate that our Ariadne LLM agent, consisting of the\nproposed memory architecture augmented with planning and decision-making,\neffectively handles complex tasks within interactive text game environments\ndifficult even for human players. Results show that our approach markedly\noutperforms other established memory methods and strong RL baselines in a range\nof problems of varying complexity. Additionally, AriGraph demonstrates\ncompetitive performance compared to dedicated knowledge graph-based methods in\nstatic multi-hop question-answering.\n","authors":["Petr Anokhin","Nikita Semenov","Artyom Sorokin","Dmitry Evseev","Andrey Kravchenko","Mikhail Burtsev","Evgeny Burnaev"],"pdf_url":"https://arxiv.org/pdf/2407.04363v3.pdf","comment":"Code for this work is avaliable at\n  https://github.com/AIRI-Institute/AriGraph"},{"id":"http://arxiv.org/abs/2505.10169v1","updated":"2025-05-15T10:55:47Z","published":"2025-05-15T10:55:47Z","title":"Modeling Saliency Dataset Bias","summary":"  Recent advances in image-based saliency prediction are approaching gold\nstandard performance levels on existing benchmarks. Despite this success, we\nshow that predicting fixations across multiple saliency datasets remains\nchallenging due to dataset bias. We find a significant performance drop (around\n40%) when models trained on one dataset are applied to another. Surprisingly,\nincreasing dataset diversity does not resolve this inter-dataset gap, with\nclose to 60% attributed to dataset-specific biases. To address this remaining\ngeneralization gap, we propose a novel architecture extending a mostly\ndataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific\nparameters that govern interpretable mechanisms such as multi-scale structure,\ncenter bias, and fixation spread. Adapting only these parameters to new data\naccounts for more than 75% of the generalization gap, with a large fraction of\nthe improvement achieved with as few as 50 samples. Our model sets a new\nstate-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark\n(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from\nunrelated datasets, but with a substantial boost when adapting to the\nrespective training datasets. The model also provides valuable insights into\nspatial saliency properties, revealing complex multi-scale effects that combine\nboth absolute and relative sizes.\n","authors":["Matthias Kümmerer","Harneet Khanuja","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2505.10169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10167v1","updated":"2025-05-15T10:51:34Z","published":"2025-05-15T10:51:34Z","title":"QuXAI: Explainers for Hybrid Quantum Machine Learning Models","summary":"  The emergence of hybrid quantum-classical machine learning (HQML) models\nopens new horizons of computational intelligence but their fundamental\ncomplexity frequently leads to black box behavior that undermines transparency\nand reliability in their application. Although XAI for quantum systems still in\nits infancy, a major research gap is evident in robust global and local\nexplainability approaches that are designed for HQML architectures that employ\nquantized feature encoding followed by classical learning. The gap is the focus\nof this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an\nexplainer for explaining feature importance in these hybrid systems. Our model\nentails the creation of HQML models incorporating quantum feature maps, the use\nof Q-MEDLEY, which combines feature based inferences, preserving the quantum\ntransformation stage and visualizing the resulting attributions. Our result\nshows that Q-MEDLEY delineates influential classical aspects in HQML models, as\nwell as separates their noise, and competes well against established XAI\ntechniques in classical validation settings. Ablation studies more\nsignificantly expose the virtues of the composite structure used in Q-MEDLEY.\nThe implications of this work are critically important, as it provides a route\nto improve the interpretability and reliability of HQML models, thus promoting\ngreater confidence and being able to engage in safer and more responsible use\nof quantum-enhanced AI technology.\n","authors":["Saikat Barua","Mostafizur Rahman","Shehenaz Khaled","Md Jafor Sadek","Rafiul Islam","Shahnewaz Siddique"],"pdf_url":"https://arxiv.org/pdf/2505.10167v1.pdf","comment":"16 pages, 6 figures, 7 equations"},{"id":"http://arxiv.org/abs/2505.04553v2","updated":"2025-05-15T10:40:05Z","published":"2025-05-07T16:31:42Z","title":"Risk-sensitive Reinforcement Learning Based on Convex Scoring Functions","summary":"  We propose a reinforcement learning (RL) framework under a broad class of\nrisk objectives, characterized by convex scoring functions. This class covers\nmany common risk measures, such as variance, Expected Shortfall, entropic\nValue-at-Risk, and mean-risk utility. To resolve the time-inconsistency issue,\nwe consider an augmented state space and an auxiliary variable and recast the\nproblem as a two-state optimization problem. We propose a customized\nActor-Critic algorithm and establish some theoretical approximation guarantees.\nA key theoretical contribution is that our results do not require the Markov\ndecision process to be continuous. Additionally, we propose an auxiliary\nvariable sampling method inspired by the alternating minimization algorithm,\nwhich is convergent under certain conditions. We validate our approach in\nsimulation experiments with a financial application in statistical arbitrage\ntrading, demonstrating the effectiveness of the algorithm.\n","authors":["Shanyu Han","Yang Liu","Xiang Yu"],"pdf_url":"https://arxiv.org/pdf/2505.04553v2.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2505.06111v2","updated":"2025-05-15T10:31:45Z","published":"2025-05-09T15:11:13Z","title":"UniVLA: Learning to Act Anywhere with Task-centric Latent Actions","summary":"  A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning.\n","authors":["Qingwen Bu","Yanting Yang","Jisong Cai","Shenyuan Gao","Guanghui Ren","Maoqing Yao","Ping Luo","Hongyang Li"],"pdf_url":"https://arxiv.org/pdf/2505.06111v2.pdf","comment":"Accepted to RSS 2025. Code is available at\n  https://github.com/OpenDriveLab/UniVLA"},{"id":"http://arxiv.org/abs/2505.07344v2","updated":"2025-05-15T10:24:45Z","published":"2025-05-12T08:32:39Z","title":"Generative Pre-trained Autoregressive Diffusion Transformer","summary":"  In this work, we present GPDiT, a Generative Pre-trained Autoregressive\nDiffusion Transformer that unifies the strengths of diffusion and\nautoregressive modeling for long-range video synthesis, within a continuous\nlatent space. Instead of predicting discrete tokens, GPDiT autoregressively\npredicts future latent frames using a diffusion loss, enabling natural modeling\nof motion dynamics and semantic consistency across frames. This continuous\nautoregressive framework not only enhances generation quality but also endows\nthe model with representation capabilities. Additionally, we introduce a\nlightweight causal attention variant and a parameter-free rotation-based\ntime-conditioning mechanism, improving both the training and inference\nefficiency. Extensive experiments demonstrate that GPDiT achieves strong\nperformance in video generation quality, video representation ability, and\nfew-shot learning tasks, highlighting its potential as an effective framework\nfor video modeling in continuous space.\n","authors":["Yuan Zhang","Jiacheng Jiang","Guoqing Ma","Zhiying Lu","Haoyang Huang","Jianlong Yuan","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2505.07344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10134v1","updated":"2025-05-15T10:04:44Z","published":"2025-05-15T10:04:44Z","title":"Large Wireless Localization Model (LWLM): A Foundation Model for\n  Positioning in 6G Networks","summary":"  Accurate and robust localization is a critical enabler for emerging 5G and 6G\napplications, including autonomous driving, extended reality (XR), and smart\nmanufacturing. While data-driven approaches have shown promise, most existing\nmodels require large amounts of labeled data and struggle to generalize across\ndeployment scenarios and wireless configurations. To address these limitations,\nwe propose a foundation-model-based solution tailored for wireless\nlocalization. We first analyze how different self-supervised learning (SSL)\ntasks acquire general-purpose and task-specific semantic features based on\ninformation bottleneck (IB) theory. Building on this foundation, we design a\npretraining methodology for the proposed Large Wireless Localization Model\n(LWLM). Specifically, we propose an SSL framework that jointly optimizes three\ncomplementary objectives: (i) spatial-frequency masked channel modeling\n(SF-MCM), (ii) domain-transformation invariance (DTI), and (iii)\nposition-invariant contrastive learning (PICL). These objectives jointly\ncapture the underlying semantics of wireless channel from multiple\nperspectives. We further design lightweight decoders for key downstream tasks,\nincluding time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation,\nsingle base station (BS) localization, and multiple BS localization.\nComprehensive experimental results confirm that LWLM consistently surpasses\nboth model-based and supervised learning baselines across all localization\ntasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformer\nmodels without pretraining, and exhibits strong generalization under\nlabel-limited fine-tuning and unseen BS configurations, confirming its\npotential as a foundation model for wireless localization.\n","authors":["Guangjin Pan","Kaixuan Huang","Hui Chen","Shunqing Zhang","Christian Häger","Henk Wymeersch"],"pdf_url":"https://arxiv.org/pdf/2505.10134v1.pdf","comment":"13 pages,16 figures.This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2401.14066v3","updated":"2025-05-15T10:04:37Z","published":"2024-01-25T10:42:09Z","title":"CreativeSynth: Cross-Art-Attention for Artistic Image Synthesis with\n  Multimodal Diffusion","summary":"  Although remarkable progress has been made in image style transfer, style is\njust one of the components of artistic paintings. Directly transferring\nextracted style features to natural images often results in outputs with\nobvious synthetic traces. This is because key painting attributes including\nlayout, perspective, shape, and semantics often cannot be conveyed and\nexpressed through style transfer. Large-scale pretrained text-to-image\ngeneration models have demonstrated their capability to synthesize a vast\namount of high-quality images. However, even with extensive textual\ndescriptions, it is challenging to fully express the unique visual properties\nand details of paintings. Moreover, generic models often disrupt the overall\nartistic effect when modifying specific areas, making it more complicated to\nachieve a unified aesthetic in artworks. Our main novel idea is to integrate\nmultimodal semantic information as a synthesis guide into artworks, rather than\ntransferring style to the real world. We also aim to reduce the disruption to\nthe harmony of artworks while simplifying the guidance conditions.\nSpecifically, we propose an innovative multi-task unified framework called\nCreativeSynth, based on the diffusion model with the ability to coordinate\nmultimodal inputs. CreativeSynth combines multimodal features with customized\nattention mechanisms to seamlessly integrate real-world semantic content into\nthe art domain through Cross-Art-Attention for aesthetic maintenance and\nsemantic fusion. We demonstrate the results of our method across a wide range\nof different art categories, proving that CreativeSynth bridges the gap between\ngenerative models and artistic expression. Code and results are available at\nhttps://github.com/haha-lisa/CreativeSynth.\n","authors":["Nisha Huang","Weiming Dong","Yuxin Zhang","Fan Tang","Ronghui Li","Chongyang Ma","Xiu Li","Tong-Yee Lee","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2401.14066v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10128v1","updated":"2025-05-15T09:53:14Z","published":"2025-05-15T09:53:14Z","title":"Robust Federated Learning on Edge Devices with Domain Heterogeneity","summary":"  Federated Learning (FL) allows collaborative training while ensuring data\nprivacy across distributed edge devices, making it a popular solution for\nprivacy-sensitive applications. However, FL faces significant challenges due to\nstatistical heterogeneity, particularly domain heterogeneity, which impedes the\nglobal mode's convergence. In this study, we introduce a new framework to\naddress this challenge by improving the generalization ability of the FL global\nmodel under domain heterogeneity, using prototype augmentation. Specifically,\nwe introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a\nprototype-based FL framework designed to enhance feature diversity and model\nrobustness. FedAPC leverages prototypes derived from the mean features of\naugmented data to capture richer representations. By aligning local features\nwith global prototypes, we enable the model to learn meaningful semantic\nfeatures while reducing overfitting to any specific domain. Experimental\nresults on the Office-10 and Digits datasets illustrate that our framework\noutperforms SOTA baselines, demonstrating superior performance.\n","authors":["Huy Q. Le","Latif U. Khan","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2505.10128v1.pdf","comment":"IWCMC 2025"},{"id":"http://arxiv.org/abs/2505.10120v1","updated":"2025-05-15T09:46:27Z","published":"2025-05-15T09:46:27Z","title":"All You Need Is Synthetic Task Augmentation","summary":"  Injecting rule-based models like Random Forests into differentiable neural\nnetwork frameworks remains an open challenge in machine learning. Recent\nadvancements have demonstrated that pretrained models can generate efficient\nmolecular embeddings. However, these approaches often require extensive\npretraining and additional techniques, such as incorporating posterior\nprobabilities, to boost performance. In our study, we propose a novel strategy\nthat jointly trains a single Graph Transformer neural network on both sparse\nmultitask molecular property experimental targets and synthetic targets derived\nfrom XGBoost models trained on Osmordred molecular descriptors. These synthetic\ntasks serve as independent auxiliary tasks. Our results show consistent and\nsignificant performance improvement across all 19 molecular property prediction\ntasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms\nthe XGBoost single-task learner. This demonstrates that synthetic task\naugmentation is an effective method for enhancing neural model performance in\nmultitask molecular property prediction without the need for feature injection\nor pretraining.\n","authors":["Guillaume Godin"],"pdf_url":"https://arxiv.org/pdf/2505.10120v1.pdf","comment":"14 pages, 3 Figures, 6 tables"},{"id":"http://arxiv.org/abs/2505.10105v1","updated":"2025-05-15T09:12:17Z","published":"2025-05-15T09:12:17Z","title":"EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot\n  Manipulation","summary":"  We present EmbodiedMAE, a unified 3D multi-modal representation for robot\nmanipulation. Current approaches suffer from significant domain gaps between\ntraining datasets and robot manipulation tasks, while also lacking model\narchitectures that can effectively incorporate 3D information. To overcome\nthese limitations, we enhance the DROID dataset with high-quality depth maps\nand point clouds, constructing DROID-3D as a valuable supplement for 3D\nembodied vision research. Then we develop EmbodiedMAE, a multi-modal masked\nautoencoder that simultaneously learns representations across RGB, depth, and\npoint cloud modalities through stochastic masking and cross-modal fusion.\nTrained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-art\nvision foundation models (VFMs) in both training efficiency and final\nperformance across 70 simulation tasks and 20 real-world robot manipulation\ntasks on two robot platforms. The model exhibits strong scaling behavior with\nsize and promotes effective policy learning from 3D inputs. Experimental\nresults establish EmbodiedMAE as a reliable unified 3D multi-modal VFM for\nembodied AI systems, particularly in precise tabletop manipulation settings\nwhere spatial perception is critical.\n","authors":["Zibin Dong","Fei Ni","Yifu Yuan","Yinchuan Li","Jianye Hao"],"pdf_url":"https://arxiv.org/pdf/2505.10105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10101v1","updated":"2025-05-15T09:04:12Z","published":"2025-05-15T09:04:12Z","title":"LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and\n  StyleGAN2","summary":"  This paper introduces LAV (Latent Audio-Visual), a system that integrates\nEnCodec's neural audio compression with StyleGAN2's generative capabilities to\nproduce visually dynamic outputs driven by pre-recorded audio. Unlike previous\nworks that rely on explicit feature mappings, LAV uses EnCodec embeddings as\nlatent representations, directly transformed into StyleGAN2's style latent\nspace via randomly initialized linear mapping. This approach preserves semantic\nrichness in the transformation, enabling nuanced and semantically coherent\naudio-visual translations. The framework demonstrates the potential of using\npretrained audio compression models for artistic and computational\napplications.\n","authors":["Jongmin Jung","Dasaem Jeong"],"pdf_url":"https://arxiv.org/pdf/2505.10101v1.pdf","comment":"Paper accepted at ISEA 2025, The 30th International Symposium on\n  Electronic/Emerging Art, Seoul, Republic of Korea, 23 - 29 May 2025"},{"id":"http://arxiv.org/abs/2505.10093v1","updated":"2025-05-15T08:51:53Z","published":"2025-05-15T08:51:53Z","title":"From Text to Network: Constructing a Knowledge Graph of Taiwan-Based\n  China Studies Using Generative AI","summary":"  Taiwanese China Studies (CS) has developed into a rich, interdisciplinary\nresearch field shaped by the unique geopolitical position and long standing\nacademic engagement with Mainland China. This study responds to the growing\nneed to systematically revisit and reorganize decades of Taiwan based CS\nscholarship by proposing an AI assisted approach that transforms unstructured\nacademic texts into structured, interactive knowledge representations. We apply\ngenerative AI (GAI) techniques and large language models (LLMs) to extract and\nstandardize entity relation triples from 1,367 peer reviewed CS articles\npublished between 1996 and 2019. These triples are then visualized through a\nlightweight D3.js based system, forming the foundation of a domain specific\nknowledge graph and vector database for the field. This infrastructure allows\nusers to explore conceptual nodes and semantic relationships across the corpus,\nrevealing previously uncharted intellectual trajectories, thematic clusters,\nand research gaps. By decomposing textual content into graph structured\nknowledge units, our system enables a paradigm shift from linear text\nconsumption to network based knowledge navigation. In doing so, it enhances\nscholarly access to CS literature while offering a scalable, data driven\nalternative to traditional ontology construction. This work not only\ndemonstrates how generative AI can augment area studies and digital humanities\nbut also highlights its potential to support a reimagined scholarly\ninfrastructure for regional knowledge systems.\n","authors":["Hsuan-Lei Shao"],"pdf_url":"https://arxiv.org/pdf/2505.10093v1.pdf","comment":"4 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.06533v2","updated":"2025-05-15T08:42:50Z","published":"2025-04-09T02:16:46Z","title":"Flexible Graph Similarity Computation With A Proactive Optimization\n  Strategy","summary":"  Graph Edit Distance (GED) offers a principled and flexible measure of graph\nsimilarity, as it quantifies the minimum cost needed to transform one graph\ninto another with customizable edit operation costs. Despite recent\nlearning-based efforts to approximate GED via vector space representations,\nexisting methods struggle with adapting to varying operation costs.\nFurthermore, they suffer from inefficient, reactive mapping refinements due to\nreliance on isolated node-level distance as guidance. To address these issues,\nwe propose GEN, a novel learning-based approach for flexible GED approximation.\nGEN addresses the varying costs adaptation by integrating operation costs prior\nto match establishment, enabling mappings to dynamically adapt to cost\nvariations. Furthermore, GEN introduces a proactive guidance optimization\nstrategy that captures graph-level dependencies between matches, allowing\ninformed matching decisions in a single step without costly iterative\nrefinements. Extensive evaluations on real-world and synthetic datasets\ndemonstrate that GEN achieves up to 37.8% reduction in GED approximation error\nand 72.7% reduction in inference time compared with state-of-the-art methods,\nwhile consistently maintaining robustness under diverse cost settings and graph\nsizes.\n","authors":["Zhouyang Liu","Ning Liu","Yixin Chen","Jiezhong He","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2504.06533v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10074v1","updated":"2025-05-15T08:24:47Z","published":"2025-05-15T08:24:47Z","title":"Leveraging Graph Retrieval-Augmented Generation to Support Learners'\n  Understanding of Knowledge Concepts in MOOCs","summary":"  Massive Open Online Courses (MOOCs) lack direct interaction between learners\nand instructors, making it challenging for learners to understand new knowledge\nconcepts. Recently, learners have increasingly used Large Language Models\n(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to\nhallucinations which limits their reliability. Retrieval-Augmented Generation\n(RAG) addresses this issue by retrieving relevant documents before generating a\nresponse. However, the application of RAG across different MOOCs is limited by\nunstructured learning material. Furthermore, current RAG systems do not\nactively guide learners toward their learning needs. To address these\nchallenges, we propose a Graph RAG pipeline that leverages Educational\nKnowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide\nlearners to understand knowledge concepts in the MOOC platform CourseMapper.\nSpecifically, we implement (1) a PKG-based Question Generation method to\nrecommend personalized questions for learners in context, and (2) an\nEduKG-based Question Answering method that leverages the relationships between\nknowledge concepts in the EduKG to answer learner selected questions. To\nevaluate both methods, we conducted a study with 3 expert instructors on 3\ndifferent MOOCs in the MOOC platform CourseMapper. The results of the\nevaluation show the potential of Graph RAG to empower learners to understand\nnew knowledge concepts in a personalized learning experience.\n","authors":["Mohamed Abdelmagied","Mohamed Amine Chatti","Shoeb Joarder","Qurat Ul Ain","Rawaa Alatrash"],"pdf_url":"https://arxiv.org/pdf/2505.10074v1.pdf","comment":"Accepted at EMOOCs 2025"},{"id":"http://arxiv.org/abs/2502.06607v3","updated":"2025-05-15T08:22:44Z","published":"2025-02-10T16:04:54Z","title":"Illegal Waste Detection in Remote Sensing Images: A Case Study","summary":"  Environmental crime is the third largest criminal activity worldwide, with\nsignificant revenues coming from illegal management of solid waste. Thanks to\nthe increasing availability and the decreasing cost of Very High Resolution\nRemote Sensing (VHR RS) images, the fight against environmental crime can\nnowadays rely on modern image-analysis tools to support photo-interpretation\nfor scanning vast territories in search of illegal waste disposal sites. This\npaper illustrates a semi-automatic waste detection pipeline, developed in\ncollaboration with a regional environmental protection agency, for detecting\ncandidate illegal dumping sites in VHR RS images. To optimize the effectiveness\nof the waste detector, extensive experiments evaluate such design choices as\nthe network architecture, the ground resolution and geographic span of the\ninput images, as well as the pretraining procedures. The best model attains\nremarkable performance, achieving 92.02% F1-Score and 94.56% Accuracy. A\ngeneralization study assesses the performance variation when the detector\nprocesses images from a territory substantially different from the one used\nduring training, incurring only a moderate performance loss, i.e., 6.5%\ndecrease in the F1-Score. Finally, an exercise in which photo interpreters\ncompare the territory scanning effort with and without the support of the waste\ndetector assesses the concrete benefit of using a computer-aided image analysis\ntool in a professional environment protection agency. Results show that a\nreduction up to 30% of the time spent for waste site detection can be attained.\n","authors":["Federico Gibellini","Piero Fraternali","Giacomo Boracchi","Luca Morandini","Thomas Martinoli","Andrea Diecidue","Simona Malegori"],"pdf_url":"https://arxiv.org/pdf/2502.06607v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12393v5","updated":"2025-05-15T08:22:08Z","published":"2024-07-17T08:13:22Z","title":"PersLLM: A Personified Training Approach for Large Language Models","summary":"  Large language models (LLMs) exhibit human-like intelligence, enabling them\nto simulate human behavior and support various applications that require both\nhumanized communication and extensive knowledge reserves. Efforts are made to\npersonify LLMs with special training data or hand-crafted prompts, while\ncorrespondingly faced with challenges such as insufficient data usage or rigid\nbehavior patterns. Consequently, personified LLMs fail to capture personified\nknowledge or express persistent opinion. To fully unlock the potential of LLM\npersonification, we propose PersLLM, a framework for better data construction\nand model tuning. For insufficient data usage, we incorporate strategies such\nas Chain-of-Thought prompting and anti-induction, improving the quality of data\nconstruction and capturing the personality experiences, knowledge, and thoughts\nmore comprehensively. For rigid behavior patterns, we design the tuning process\nand introduce automated DPO to enhance the specificity and dynamism of the\nmodels' personalities, which leads to a more natural opinion communication.\nBoth automated metrics and expert human evaluations demonstrate the\neffectiveness of our approach. Case studies in human-machine interactions and\nmulti-agent systems further suggest potential application scenarios and future\ndirections for LLM personification.\n","authors":["Zheni Zeng","Jiayi Chen","Huimin Chen","Yukun Yan","Yuxuan Chen","Zhenghao Liu","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2407.12393v5.pdf","comment":"8 pages for main text, 5 figures"},{"id":"http://arxiv.org/abs/2505.10073v1","updated":"2025-05-15T08:20:57Z","published":"2025-05-15T08:20:57Z","title":"Multi-Robot Task Allocation for Homogeneous Tasks with Collision\n  Avoidance via Spatial Clustering","summary":"  In this paper, a novel framework is presented that achieves a combined\nsolution based on Multi-Robot Task Allocation (MRTA) and collision avoidance\nwith respect to homogeneous measurement tasks taking place in industrial\nenvironments. The spatial clustering we propose offers to simultaneously solve\nthe task allocation problem and deal with collision risks by cutting the\nworkspace into distinguishable operational zones for each robot. To divide task\nsites and to schedule robot routes within corresponding clusters, we use\nK-means clustering and the 2-Opt algorithm. The presented framework shows\nsatisfactory performance, where up to 93\\% time reduction (1.24s against\n17.62s) with a solution quality improvement of up to 7\\% compared to the best\nperforming method is demonstrated. Our method also completely eliminates\ncollision points that persist in comparative methods in a most significant\nsense. Theoretical analysis agrees with the claim that spatial partitioning\nunifies the apparently disjoint tasks allocation and collision avoidance\nproblems under conditions of many identical tasks to be distributed over sparse\ngeographical areas. Ultimately, the findings in this work are of substantial\nimportance for real world applications where both computational efficiency and\noperation free from collisions is of paramount importance.\n","authors":["Rathin Chandra Shit","Sharmila Subudhi"],"pdf_url":"https://arxiv.org/pdf/2505.10073v1.pdf","comment":"5 pages, 4 figures, Scheduled for presentation at an upcoming\n  conference"},{"id":"http://arxiv.org/abs/2505.10066v1","updated":"2025-05-15T08:07:04Z","published":"2025-05-15T08:07:04Z","title":"Dark LLMs: The Growing Threat of Unaligned AI Models","summary":"  Large Language Models (LLMs) rapidly reshape modern life, advancing fields\nfrom healthcare to education and beyond. However, alongside their remarkable\ncapabilities lies a significant threat: the susceptibility of these models to\njailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems\nfrom the very data they learn from. As long as this training data includes\nunfiltered, problematic, or 'dark' content, the models can inherently learn\nundesirable patterns or weaknesses that allow users to circumvent their\nintended safety controls. Our research identifies the growing threat posed by\ndark LLMs models deliberately designed without ethical guardrails or modified\nthrough jailbreak techniques. In our research, we uncovered a universal\njailbreak attack that effectively compromises multiple state-of-the-art models,\nenabling them to answer almost any question and produce harmful outputs upon\nrequest. The main idea of our attack was published online over seven months\nago. However, many of the tested LLMs were still vulnerable to this attack.\nDespite our responsible disclosure efforts, responses from major LLM providers\nwere often inadequate, highlighting a concerning gap in industry practices\nregarding AI safety. As model training becomes more accessible and cheaper, and\nas open-source LLMs proliferate, the risk of widespread misuse escalates.\nWithout decisive intervention, LLMs may continue democratizing access to\ndangerous knowledge, posing greater risks than anticipated.\n","authors":["Michael Fire","Yitzhak Elbazis","Adi Wasenstein","Lior Rokach"],"pdf_url":"https://arxiv.org/pdf/2505.10066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07338v2","updated":"2025-05-15T08:04:19Z","published":"2025-03-10T13:50:23Z","title":"Temporal Triplane Transformers as Occupancy World Models","summary":"  World models aim to learn or construct representations of the environment\nthat enable the prediction of future scenes, thereby supporting intelligent\nmotion planning. However, existing models often struggle to produce\nfine-grained predictions and to operate in real time. In this work, we propose\nT$^3$Former, a novel 4D occupancy world model for autonomous driving.\nT$^3$Former begins by pre-training a compact {\\em triplane} representation that\nefficiently encodes 3D occupancy. It then extracts multi-scale temporal motion\nfeatures from historical triplanes and employs an autoregressive approach to\niteratively predict future triplane changes. Finally, these triplane changes\nare combined with previous states to decode future occupancy and ego-motion\ntrajectories. Experimental results show that T$^3$Former achieves 1.44$\\times$\nspeedup (26 FPS), improves mean IoU to 36.09, and reduces mean absolute\nplanning error to 1.0 meters. Demos are available in the supplementary\nmaterial.\n","authors":["Haoran Xu","Peixi Peng","Guang Tan","Yiqian Chang","Yisen Zhao","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2503.07338v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10055v1","updated":"2025-05-15T07:58:38Z","published":"2025-05-15T07:58:38Z","title":"PsOCR: Benchmarking Large Multimodal Models for Optical Character\n  Recognition in Low-resource Pashto Language","summary":"  This paper evaluates the performance of Large Multimodal Models (LMMs) on\nOptical Character Recognition (OCR) in the low-resource Pashto language.\nNatural Language Processing (NLP) in Pashto faces several challenges due to the\ncursive nature of its script and a scarcity of structured datasets. To address\nthis, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one\nmillion images annotated with bounding boxes at word, line, and document\nlevels, suitable for training and evaluating models based on different\narchitectures, including Convolutional Neural Networks (CNNs) and Transformers.\nPsOCR covers variations across 1,000 unique font families, colors, image sizes,\nand layouts. A benchmark subset of 10K images was selected to evaluate the\nperformance of several LMMs, including seven open-source models: DeepSeek's\nJanus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four\nclosed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results\ndemonstrate that Gemini achieves the best performance among all models, whereas\namong open-source models, Qwen-7B stands out. This work provides an insightful\nassessment of the capabilities and limitations of current LMMs for OCR tasks in\nPashto and establishes a foundation for further research not only in Pashto OCR\nbut also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is\navailable at https://github.com/zirak-ai/PashtoOCR.\n","authors":["Ijazul Haq","Yingjie Zhang","Irfan Ali Khan"],"pdf_url":"https://arxiv.org/pdf/2505.10055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10050v1","updated":"2025-05-15T07:53:02Z","published":"2025-05-15T07:53:02Z","title":"Financial Fraud Detection Using Explainable AI and Stacking Ensemble\n  Methods","summary":"  Traditional machine learning models often prioritize predictive accuracy,\noften at the expense of model transparency and interpretability. The lack of\ntransparency makes it difficult for organizations to comply with regulatory\nrequirements and gain stakeholders trust. In this research, we propose a fraud\ndetection framework that combines a stacking ensemble of well-known gradient\nboosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable\nartificial intelligence (XAI) techniques are used to enhance the transparency\nand interpretability of the model's decisions. We used SHAP (SHapley Additive\nExplanations) for feature selection to identify the most important features.\nFurther efforts were made to explain the model's predictions using Local\nInterpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots\n(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection\ndataset, which includes more than 590,000 real transaction records, was used to\nevaluate the proposed model. The model achieved a high performance with an\naccuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent\nrelated approaches. These results indicate that combining high prediction\naccuracy with transparent interpretability is possible and could lead to a more\nethical and trustworthy solution in financial fraud detection.\n","authors":["Fahad Almalki","Mehedi Masud"],"pdf_url":"https://arxiv.org/pdf/2505.10050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10043v1","updated":"2025-05-15T07:41:14Z","published":"2025-05-15T07:41:14Z","title":"Boosting Text-to-Chart Retrieval through Training with Synthesized\n  Semantic Insights","summary":"  Charts are crucial for data analysis and decision-making.Text-to-chart\nretrieval systems have become increasingly important for Business Intelligence\n(BI), where users need to find relevant charts that match their analytical\nneeds. These needs can be categorized into precise queries that are\nwell-specified and fuzzy queries that are more exploratory -- both require\nunderstanding the semantics and context of the charts. However, existing\ntext-to-chart retrieval solutions often fail to capture the semantic content\nand contextual information of charts, primarily due to the lack of\ncomprehensive metadata (or semantic insights). To address this limitation, we\npropose a training data development pipeline that automatically synthesizes\nhierarchical semantic insights for charts, covering visual patterns\n(visual-oriented), statistical properties (statistics-oriented), and practical\napplications (task-oriented), which produces 207,498 semantic insights for\n69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to\nlearn better representations of charts for text-to-chart retrieval. Our method\nleverages rich semantic insights during the training phase to develop a model\nthat understands both visual and semantic aspects of charts.To evaluate\ntext-to-chart retrieval performance, we curate the first benchmark, CRBench,\nfor this task with 21,862 charts and 326 text queries from real-world BI\napplications, with ground-truth labels verified by the crowd\nworkers.Experiments show that ChartFinder significantly outperforms existing\nmethods in text-to-chart retrieval tasks across various settings. For precise\nqueries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than\nstate-of-the-art models. In fuzzy query tasks, our method also demonstrates\nconsistent improvements, with an average increase of 5% across nearly all\nmetrics.\n","authors":["Yifan Wu","Lutao Yan","Yizhang Zhu","Yinan Mei","Jiannan Wang","Nan Tang","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2505.10043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10037v1","updated":"2025-05-15T07:33:41Z","published":"2025-05-15T07:33:41Z","title":"Optimal normalization in quantum-classical hybrid models for anti-cancer\n  drug response prediction","summary":"  Quantum-classical Hybrid Machine Learning (QHML) models are recognized for\ntheir robust performance and high generalization ability even for relatively\nsmall datasets. These qualities offer unique advantages for anti-cancer drug\nresponse prediction, where the number of available samples is typically small.\nHowever, such hybrid models appear to be very sensitive to the data encoding\nused at the interface of a neural network and a quantum circuit, with\nsuboptimal choices leading to stability issues. To address this problem, we\npropose a novel strategy that uses a normalization function based on a\nmoderated gradient version of the $\\tanh$. This method transforms the outputs\nof the neural networks without concentrating them at the extreme value ranges.\nOur idea was evaluated on a dataset of gene expression and drug response\nmeasurements for various cancer cell lines, where we compared the prediction\nperformance of a classical deep learning model and several QHML models. These\nresults confirmed that QHML performed better than the classical models when\ndata was optimally normalized. This study opens up new possibilities for\nbiomedical data analysis using quantum computers.\n","authors":["Takafumi Ito","Lysenko Artem","Tatsuhiko Tsunoda"],"pdf_url":"https://arxiv.org/pdf/2505.10037v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2405.04760v4","updated":"2025-05-15T07:33:07Z","published":"2024-05-08T02:09:17Z","title":"Large Language Models for Cyber Security: A Systematic Literature Review","summary":"  The rapid advancement of Large Language Models (LLMs) has opened up new\nopportunities for leveraging artificial intelligence in various domains,\nincluding cybersecurity. As the volume and sophistication of cyber threats\ncontinue to grow, there is an increasing need for intelligent systems that can\nautomatically detect vulnerabilities, analyze malware, and respond to attacks.\nIn this survey, we conduct a comprehensive review of the literature on the\napplication of LLMs in cybersecurity (LLM4Security). By comprehensively\ncollecting over 30K relevant papers and systematically analyzing 127 papers\nfrom top security and software engineering venues, we aim to provide a holistic\nview of how LLMs are being used to solve diverse problems across the\ncybersecurity domain. Through our analysis, we identify several key findings.\nFirst, we observe that LLMs are being applied to a wide range of cybersecurity\ntasks, including vulnerability detection, malware analysis, network intrusion\ndetection, and phishing detection. Second, we find that the datasets used for\ntraining and evaluating LLMs in these tasks are often limited in size and\ndiversity, highlighting the need for more comprehensive and representative\ndatasets. Third, we identify several promising techniques for adapting LLMs to\nspecific cybersecurity domains, such as fine-tuning, transfer learning, and\ndomain-specific pre-training. Finally, we discuss the main challenges and\nopportunities for future research in LLM4Security, including the need for more\ninterpretable and explainable models, the importance of addressing data privacy\nand security concerns, and the potential for leveraging LLMs for proactive\ndefense and threat hunting. Overall, our survey provides a comprehensive\noverview of the current state-of-the-art in LLM4Security and identifies several\npromising directions for future research.\n","authors":["Hanxiang Xu","Shenao Wang","Ningke Li","Kailong Wang","Yanjie Zhao","Kai Chen","Ting Yu","Yang Liu","Haoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2405.04760v4.pdf","comment":"56 pages,6 figures"},{"id":"http://arxiv.org/abs/2505.10034v1","updated":"2025-05-15T07:29:33Z","published":"2025-05-15T07:29:33Z","title":"The First MPDD Challenge: Multimodal Personality-aware Depression\n  Detection","summary":"  Depression is a widespread mental health issue affecting diverse age groups,\nwith notable prevalence among college students and the elderly. However,\nexisting datasets and detection methods primarily focus on young adults,\nneglecting the broader age spectrum and individual differences that influence\ndepression manifestation. Current approaches often establish a direct mapping\nbetween multimodal data and depression indicators, failing to capture the\ncomplexity and diversity of depression across individuals. This challenge\nincludes two tracks based on age-specific subsets: Track 1 uses the\nMPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses\nthe MPDD-Young dataset for detecting depression in younger participants. The\nMultimodal Personality-aware Depression Detection (MPDD) Challenge aims to\naddress this gap by incorporating multimodal data alongside individual\ndifference factors. We provide a baseline model that fuses audio and video\nmodalities with individual difference information to detect depression\nmanifestations in diverse populations. This challenge aims to promote the\ndevelopment of more personalized and accurate de pression detection methods,\nadvancing mental health research and fostering inclusive detection systems.\nMore details are available on the official challenge website:\nhttps://hacilab.github.io/MPDDChallenge.github.io.\n","authors":["Changzeng Fu","Zelin Fu","Xinhe Kuang","Jiacheng Dong","Qi Zhang","Kaifeng Su","Yikai Su","Wenbo Shi","Junfeng Yao","Yuliang Zhao","Shiqi Zhao","Jiadong Wang","Siyang Song","Chaoran Liu","Yuichiro Yoshikawa","Björn Schuller","Hiroshi Ishiguro"],"pdf_url":"https://arxiv.org/pdf/2505.10034v1.pdf","comment":"This paper has been accepted as part of the MPDD Challenge in the\n  ACMMM 2025 Grand Challenge"},{"id":"http://arxiv.org/abs/2505.05145v2","updated":"2025-05-15T07:19:33Z","published":"2025-05-08T11:32:46Z","title":"Understanding In-context Learning of Addition via Activation Subspaces","summary":"  To perform in-context learning, language models must extract signals from\nindividual few-shot examples, aggregate these into a learned prediction rule,\nand then apply this rule to new examples. How is this implemented in the\nforward pass of modern transformer models? To study this, we consider a\nstructured family of few-shot learning tasks for which the true prediction rule\nis to add an integer $k$ to the input. We find that Llama-3-8B attains high\naccuracy on this task for a range of $k$, and localize its few-shot ability to\njust three attention heads via a novel optimization approach. We further show\nthe extracted signals lie in a six-dimensional subspace, where four of the\ndimensions track the unit digit and the other two dimensions track overall\nmagnitude. We finally examine how these heads extract information from\nindividual few-shot examples, identifying a self-correction mechanism in which\nmistakes from earlier examples are suppressed by later examples. Our results\ndemonstrate how tracking low-dimensional subspaces across a forward pass can\nprovide insight into fine-grained computational structures.\n","authors":["Xinyan Hu","Kayo Yin","Michael I. Jordan","Jacob Steinhardt","Lijie Chen"],"pdf_url":"https://arxiv.org/pdf/2505.05145v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2505.10027v1","updated":"2025-05-15T07:17:03Z","published":"2025-05-15T07:17:03Z","title":"ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model\n  Super-Resolution Reconstruction","summary":"  With the rapid advancement of remote sensing technology, super-resolution\nimage reconstruction is of great research and practical significance. Existing\ndeep learning methods have made progress but still face limitations in handling\ncomplex scenes and preserving image details. This paper proposes a\nreinforcement learning-based latent diffusion model (LDM) fine-tuning method\nfor remote sensing image super-resolution. The method constructs a\nreinforcement learning environment with states, actions, and rewards,\noptimizing decision objectives through proximal policy optimization (PPO)\nduring the reverse denoising process of the LDM model. Experiments on the\nRESISC45 dataset show significant improvements over the baseline model in PSNR,\nSSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,\nand LPIPS reducing by 0.06-0.10, particularly in structured and complex natural\nscenes. The results demonstrate the method's effectiveness in enhancing\nsuper-resolution quality and adaptability across scenes.\n","authors":["Shijie Lyu"],"pdf_url":"https://arxiv.org/pdf/2505.10027v1.pdf","comment":"Accepted by the 4th International Conference on Computing Innovation\n  and Applied Physics (CONF-CIAP 2025), and will be published in EAI Community\n  Research Series-CORE or Theoretical and Natural Science (TNS)"},{"id":"http://arxiv.org/abs/2505.10016v1","updated":"2025-05-15T06:58:45Z","published":"2025-05-15T06:58:45Z","title":"Application of YOLOv8 in monocular downward multiple Car Target\n  detection","summary":"  Autonomous driving technology is progressively transforming traditional car\ndriving methods, marking a significant milestone in modern transportation.\nObject detection serves as a cornerstone of autonomous systems, playing a vital\nrole in enhancing driving safety, enabling autonomous functionality, improving\ntraffic efficiency, and facilitating effective emergency responses. However,\ncurrent technologies such as radar for environmental perception, cameras for\nroad perception, and vehicle sensor networks face notable challenges, including\nhigh costs, vulnerability to weather and lighting conditions, and limited\nresolution.To address these limitations, this paper presents an improved\nautonomous target detection network based on YOLOv8. By integrating structural\nreparameterization technology, a bidirectional pyramid structure network model,\nand a novel detection pipeline into the YOLOv8 framework, the proposed approach\nachieves highly efficient and precise detection of multi-scale, small, and\nremote objects. Experimental results demonstrate that the enhanced model can\neffectively detect both large and small objects with a detection accuracy of\n65%, showcasing significant advancements over traditional methods.This improved\nmodel holds substantial potential for real-world applications and is\nwell-suited for autonomous driving competitions, such as the Formula Student\nAutonomous China (FSAC), particularly excelling in scenarios involving\nsingle-target and small-object detection.\n","authors":["Shijie Lyu"],"pdf_url":"https://arxiv.org/pdf/2505.10016v1.pdf","comment":"Accepted by the 5th International Conference on Signal Processing and\n  Machine Learning (CONF-SPML 2025), to appear in Applied and Computational\n  Engineering"},{"id":"http://arxiv.org/abs/2505.10012v1","updated":"2025-05-15T06:53:30Z","published":"2025-05-15T06:53:30Z","title":"Quantum Computing and AI: Perspectives on Advanced Automation in Science\n  and Engineering","summary":"  Recent advances in artificial intelligence (AI) and quantum computing are\naccelerating automation in scientific and engineering processes, fundamentally\nreshaping research methodologies. This perspective highlights parallels between\nscientific automation and established Computer-Aided Engineering (CAE)\npractices, introducing Quantum CAE as a framework that leverages quantum\nalgorithms for simulation, optimization, and machine learning within\nengineering design. Practical implementations of Quantum CAE are illustrated\nthrough case studies for combinatorial optimization problems. Further\ndiscussions include advancements toward higher automation levels, highlighting\nthe critical role of specialized AI agents proficient in quantum algorithm\ndesign. The integration of quantum computing with AI raises significant\nquestions about the collaborative dynamics among human scientists and\nengineers, AI systems, and quantum computational resources, underscoring a\ntransformative future for automated discovery and innovation.\n","authors":["Tadashi Kadowaki"],"pdf_url":"https://arxiv.org/pdf/2505.10012v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.02883v2","updated":"2025-05-15T06:31:41Z","published":"2025-02-05T04:41:59Z","title":"SensorChat: Answering Qualitative and Quantitative Questions during\n  Long-Term Multimodal Sensor Interactions","summary":"  Natural language interaction with sensing systems is crucial for addressing\nusers' personal concerns and providing health-related insights into their daily\nlives. When a user asks a question, the system automatically analyzes the full\nhistory of sensor data, extracts relevant information, and generates an\nappropriate response. However, existing systems are limited to short-duration\n(e.g., one minute) or low-frequency (e.g., daily step count) sensor data. In\naddition, they struggle with quantitative questions that require precise\nnumerical answers. In this work, we introduce SensorChat, the first end-to-end\nQA system designed for daily life monitoring using long-duration,\nhigh-frequency time series data. Given raw sensor signals spanning multiple\ndays and a user-defined natural language question, SensorChat generates\nsemantically meaningful responses that directly address user concerns.\nSensorChat effectively handles both quantitative questions that require\nnumerical precision and qualitative questions that require high-level reasoning\nto infer subjective insights. To achieve this, SensorChat uses an innovative\nthree-stage pipeline including question decomposition, sensor data query, and\nanswer assembly. The first and third stages leverage Large Language Models\n(LLMs) to interpret human queries and generate responses. The intermediate\nquerying stage extracts relevant information from the complete sensor data\nhistory. Real-world implementation demonstrate SensorChat's capability for\nreal-time interactions on a cloud server while also being able to run entirely\non edge platforms after quantization. Comprehensive QA evaluations show that\nSensorChat achieves up to 93% higher answer accuracy than state-of-the-art\nsystems on quantitative questions. Additionally, a user study with eight\nvolunteers highlights SensorChat's effectiveness in answering qualitative and\nopen-ended questions.\n","authors":["Xiaofan Yu","Lanxiang Hu","Benjamin Reichman","Dylan Chu","Rushil Chandrupatla","Xiyuan Zhang","Larry Heck","Tajana Rosing"],"pdf_url":"https://arxiv.org/pdf/2502.02883v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2411.11641v3","updated":"2025-05-15T06:30:38Z","published":"2024-11-18T15:19:54Z","title":"TSINR: Capturing Temporal Continuity via Implicit Neural Representations\n  for Time Series Anomaly Detection","summary":"  Time series anomaly detection aims to identify unusual patterns in data or\ndeviations from systems' expected behavior. The reconstruction-based methods\nare the mainstream in this task, which learn point-wise representation via\nunsupervised learning. However, the unlabeled anomaly points in training data\nmay cause these reconstruction-based methods to learn and reconstruct anomalous\ndata, resulting in the challenge of capturing normal patterns. In this paper,\nwe propose a time series anomaly detection method based on implicit neural\nrepresentation (INR) reconstruction, named TSINR, to address this challenge.\nDue to the property of spectral bias, TSINR enables prioritizing low-frequency\nsignals and exhibiting poorer performance on high-frequency abnormal data.\nSpecifically, we adopt INR to parameterize time series data as a continuous\nfunction and employ a transformer-based architecture to predict the INR of\ngiven data. As a result, the proposed TSINR method achieves the advantage of\ncapturing the temporal continuity and thus is more sensitive to discontinuous\nanomaly data. In addition, we further design a novel form of INR continuous\nfunction to learn inter- and intra-channel information, and leverage a\npre-trained large language model to amplify the intense fluctuations in\nanomalies. Extensive experiments demonstrate that TSINR achieves superior\noverall performance on both univariate and multivariate time series anomaly\ndetection benchmarks compared to other state-of-the-art reconstruction-based\nmethods. Our codes are available.\n","authors":["Mengxuan Li","Ke Liu","Hongyang Chen","Jiajun Bu","Hongwei Wang","Haishuai Wang"],"pdf_url":"https://arxiv.org/pdf/2411.11641v3.pdf","comment":"Accepted by SIGKDD 2025"},{"id":"http://arxiv.org/abs/2505.09989v1","updated":"2025-05-15T06:03:47Z","published":"2025-05-15T06:03:47Z","title":"AI Greenferencing: Routing AI Inferencing to Green Modular Data Centers\n  with Heron","summary":"  AI power demand is growing unprecedentedly thanks to the high power density\nof AI compute and the emerging inferencing workload. On the supply side,\nabundant wind power is waiting for grid access in interconnection queues. In\nthis light, this paper argues bringing AI workload to modular compute clusters\nco-located in wind farms. Our deployment right-sizing strategy makes it\neconomically viable to deploy more than 6 million high-end GPUs today that\ncould consume cheap, green power at its source. We built Heron, a cross-site\nsoftware router, that could efficiently leverage the complementarity of power\ngeneration across wind farms by routing AI inferencing workload around power\ndrops. Using 1-week ofcoding and conversation production traces from Azure and\n(real) variable wind power traces, we show how Heron improves aggregate goodput\nof AI compute by up to 80% compared to the state-of-the-art.\n","authors":["Tella Rajashekhar Reddy"," Palak","Rohan Gandhi","Anjaly Parayil","Chaojie Zhang","Mike Shepperd","Liangcheng Yu","Jayashree Mohan","Srinivasan Iyengar","Shivkumar Kalyanaraman","Debopam Bhattacherjee"],"pdf_url":"https://arxiv.org/pdf/2505.09989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13602v2","updated":"2025-05-15T05:56:38Z","published":"2024-11-19T09:09:14Z","title":"Translating Electrocardiograms to Cardiac Magnetic Resonance Imaging\n  Useful for Cardiac Assessment and Disease Screening: A Multi-Center Study AI\n  for ECG to CMR Translation Study","summary":"  Cardiovascular diseases (CVDs) are the leading cause of global mortality,\nnecessitating accessible and accurate diagnostic tools. While cardiac magnetic\nresonance imaging (CMR) provides gold-standard insights into cardiac structure\nand function, its clinical utility is limited by high cost and complexity. In\ncontrast, electrocardiography (ECG) is inexpensive and widely available but\nlacks the granularity of CMR. We propose CardioNets, a deep learning framework\nthat translates 12-lead ECG signals into CMR-level functional parameters and\nsynthetic images, enabling scalable cardiac assessment. CardioNets integrates\ncross-modal contrastive learning and generative pretraining, aligning ECG with\nCMR-derived cardiac phenotypes and synthesizing high-resolution CMR images via\na masked autoregressive model. Trained on 159,819 samples from five cohorts,\nincluding the UK Biobank (n=42,483) and MIMIC-IV-ECG (n=164,550), and\nexternally validated on independent clinical datasets (n=3,767), CardioNets\nachieved strong performance across disease screening and phenotype estimation\ntasks. In the UK Biobank, it improved cardiac phenotype regression R2 by 24.8%\nand cardiomyopathy AUC by up to 39.3% over baseline models. In MIMIC, it\nincreased AUC for pulmonary hypertension detection by 5.6%. Generated CMR\nimages showed 36.6% higher SSIM and 8.7% higher PSNR than prior approaches. In\na reader study, ECG-only CardioNets achieved 13.9% higher accuracy than human\nphysicians using both ECG and real CMR. These results suggest that CardioNets\noffers a promising, low-cost alternative to CMR for large-scale CVD screening,\nparticularly in resource-limited settings. Future efforts will focus on\nclinical deployment and regulatory validation of ECG-based synthetic imaging.\n","authors":["Zhengyao Ding","Ziyu Li","Yujian Hu","Youyao Xu","Chengchen Zhao","Yiheng Mao","Haitao Li","Zhikang Li","Qian Li","Jing Wang","Yue Chen","Mengjia Chen","Longbo Wang","Xuesen Chu","Weichao Pan","Ziyi Liu","Fei Wu","Hongkun Zhang","Ting Chen","Zhengxing Huang"],"pdf_url":"https://arxiv.org/pdf/2411.13602v2.pdf","comment":"27 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.07214v2","updated":"2025-05-15T05:47:33Z","published":"2025-05-12T03:47:05Z","title":"Towards user-centered interactive medical image segmentation in VR with\n  an assistive AI agent","summary":"  Crucial in disease analysis and surgical planning, manual segmentation of\nvolumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and\nchallenging to master, while fully automatic algorithms can benefit from user\nfeedback. Therefore, with the complementary power of the latest radiological AI\nfoundation models and virtual reality (VR)'s intuitive data interaction, we\npropose SAMIRA, a novel conversational AI agent that assists users with\nlocalizing, segmenting, and visualizing 3D medical concepts in VR. Through\nspeech-based interaction, the agent helps users understand radiological\nfeatures, locate clinical targets, and generate segmentation masks that can be\nrefined with just a few point prompts. The system also supports true-to-scale\n3D visualization of segmented pathology to enhance patient-specific anatomical\nunderstanding. Furthermore, to determine the optimal interaction paradigm under\nnear-far attention-switching for refining segmentation masks in an immersive,\nhuman-in-the-loop workflow, we compare VR controller pointing, head pointing,\nand eye tracking as input modes. With a user study, evaluations demonstrated a\nhigh usability score (SUS=90.0 $\\pm$ 9.0), low overall task load, as well as\nstrong support for the proposed VR system's guidance, training potential, and\nintegration of AI in radiological segmentation tasks.\n","authors":["Pascal Spiegler","Arash Harirpoush","Yiming Xiao"],"pdf_url":"https://arxiv.org/pdf/2505.07214v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15508v3","updated":"2025-05-15T05:34:45Z","published":"2024-07-22T09:45:16Z","title":"Compensate Quantization Errors+: Quantized Models Are Inquisitive\n  Learners","summary":"  The quantization of large language models (LLMs) has been a prominent\nresearch area aimed at enabling their lightweight deployment in practice.\nExisting research about LLM's quantization has mainly explored the interplay\nbetween weights and activations, or employing auxiliary components while\nneglecting the necessity of adjusting weights during quantization.\nConsequently, original weight distributions frequently fail to yield desired\nresults after round-to-nearest (RTN) quantization. Even though incorporating\ntechniques such as mixed precision and low-rank error approximation in LLM's\nquantization can yield improved results, they inevitably introduce additional\ncomputational overhead. On the other hand, traditional techniques for weight\nquantization, such as Generative Post-Training Quantization, rely on manually\ntweaking weight distributions to minimize local errors, but they fall short of\nachieving globally optimal outcomes. Although the recently proposed Learnable\nSingular-value Increment improves global weight quantization by modifying\nweight distributions, it disrupts the original distribution considerably. This\nintroduces pronounced bias toward the training data and can degrade downstream\ntask performance. In this paper, we introduce Singular-value Diagonal\nExpansion, a more nuanced approach to refining weight distributions to achieve\nbetter quantization alignment. Furthermore, we introduce Cross-layer Learning\nthat improves overall quantization outcomes by distributing errors more evenly\nacross layers. Our plug-and-play weight-quantization methods demonstrate\nsubstantial performance improvements over state-of-the-art approaches,\nincluding OmniQuant, DuQuant, and PrefixQuant.\n","authors":["Yifei Gao","Jie Ou","Lei Wang","Jun Cheng","Mengchu Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.15508v3.pdf","comment":"Effecient Quantization Methods for LLMs"},{"id":"http://arxiv.org/abs/2505.09974v1","updated":"2025-05-15T05:22:53Z","published":"2025-05-15T05:22:53Z","title":"Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber\n  Security Data","summary":"  The integration of large language models (LLMs) into cyber security\napplications presents significant opportunities, such as enhancing threat\nanalysis and malware detection, but can also introduce critical risks and\nsafety concerns, including personal data leakage and automated generation of\nnew malware. We present a systematic evaluation of safety risks in fine-tuned\nLLMs for cyber security applications. Using the OWASP Top 10 for LLM\nApplications framework, we assessed seven open-source LLMs: Phi 3 Mini 3.8B,\nMistral 7B, Qwen 2.5 7B, Llama 3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B.\nOur evaluation shows that fine-tuning reduces safety resilience across all\ntested LLMs (e.g., the safety score of Llama 3.1 8B against prompt injection\ndrops from 0.95 to 0.15). We propose and evaluate a safety alignment approach\nthat carefully rewords instruction-response pairs to include explicit safety\nprecautions and ethical considerations. This approach demonstrates that it is\npossible to maintain or even improve model safety while preserving technical\nutility, offering a practical path forward for developing safer fine-tuning\nmethodologies. This work offers a systematic evaluation for safety risks in\nLLMs, enabling safer adoption of generative AI in sensitive domains, and\ncontributing towards the development of secure, trustworthy, and ethically\naligned LLMs.\n","authors":["Adel ElZemity","Budi Arief","Shujun Li"],"pdf_url":"https://arxiv.org/pdf/2505.09974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09970v1","updated":"2025-05-15T05:17:47Z","published":"2025-05-15T05:17:47Z","title":"Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents","summary":"  The ReAct (Reasoning + Action) capability in large language models (LLMs) has\nbecome the foundation of modern agentic systems. Recent LLMs, such as\nDeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through\nthe generation of ample intermediate tokens, which help build a strong premise\nbefore producing the final output tokens. In this paper, we introduce Pre-Act,\na novel approach that enhances the agent's performance by creating a multi-step\nexecution plan along with the detailed reasoning for the given user input. This\nplan incrementally incorporates previous steps and tool outputs, refining\nitself after each step execution until the final response is obtained. Our\napproach is applicable to both conversational and non-conversational agents. To\nmeasure the performance of task-oriented agents comprehensively, we propose a\ntwo-level evaluation framework: (1) turn level and (2) end-to-end. Our\nturn-level evaluation, averaged across five models, shows that our approach,\nPre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While\nthis approach is effective for larger models, smaller models crucial for\npractical applications, where latency and cost are key constraints, often\nstruggle with complex reasoning tasks required for agentic systems. To address\nthis limitation, we fine-tune relatively small models such as Llama 3.1 (8B &\n70B) using the proposed Pre-Act approach. Our experiments show that the\nfine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action\naccuracy (turn-level) and a 28% improvement in goal completion rate\n(end-to-end) on the Almita (out-of-domain) dataset.\n","authors":["Mrinal Rawat","Ambuje Gupta","Rushil Goomer","Alessandro Di Bari","Neha Gupta","Roberto Pieraccini"],"pdf_url":"https://arxiv.org/pdf/2505.09970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12665v3","updated":"2025-05-15T05:15:13Z","published":"2024-07-17T15:48:39Z","title":"Beyond Next Token Prediction: Patch-Level Training for Large Language\n  Models","summary":"  The prohibitive training costs of Large Language Models (LLMs) have emerged\nas a significant bottleneck in the development of next-generation LLMs. In this\npaper, we show that it is possible to significantly reduce the training costs\nof LLMs without sacrificing their performance. Specifically, we introduce\npatch-level training for LLMs, in which multiple tokens are aggregated into a\nunit of higher information density, referred to as a `patch', to serve as the\nfundamental text unit for training LLMs. During patch-level training, we feed\nthe language model shorter sequences of patches and train it to predict the\nnext patch, thereby processing the majority of the training data at a\nsignificantly reduced cost. Following this, the model continues token-level\ntraining on the remaining training data to align with the inference mode.\nExperiments on a diverse range of models (370M-2.7B parameters) demonstrate\nthat patch-level training can reduce the overall training costs to 0.5$\\times$,\nwithout compromising the model performance compared to token-level training.\nSource code: https://github.com/shaochenze/PatchTrain.\n","authors":["Chenze Shao","Fandong Meng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.12665v3.pdf","comment":"ICLR 2025 Spotlight"},{"id":"http://arxiv.org/abs/2505.09969v1","updated":"2025-05-15T05:13:38Z","published":"2025-05-15T05:13:38Z","title":"A Comprehensive Machine Learning Framework for Heart Disease Prediction:\n  Performance Evaluation and Future Perspectives","summary":"  This study presents a machine learning-based framework for heart disease\nprediction using the heart-disease dataset, comprising 303 samples with 14\nfeatures. The methodology involves data preprocessing, model training, and\nevaluation using three classifiers: Logistic Regression, K-Nearest Neighbors\n(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and\nRandomizedSearchCV was employed to enhance model performance. The Random Forest\nclassifier outperformed other models, achieving an accuracy of 91% and an\nF1-score of 0.89. Evaluation metrics, including precision, recall, and\nconfusion matrix, revealed balanced performance across classes. The proposed\nmodel demonstrates strong potential for aiding clinical decision-making by\neffectively predicting heart disease. Limitations such as dataset size and\ngeneralizability underscore the need for future studies using larger and more\ndiverse datasets. This work highlights the utility of machine learning in\nhealthcare, offering insights for further advancements in predictive\ndiagnostics.\n","authors":["Ali Azimi Lamir","Shiva Razzagzadeh","Zeynab Rezaei"],"pdf_url":"https://arxiv.org/pdf/2505.09969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01635v7","updated":"2025-05-15T05:02:59Z","published":"2024-06-30T10:53:40Z","title":"Commute Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) have shown remarkable success in learning from\ngraph-structured data. However, their application to directed graphs (digraphs)\npresents unique challenges, primarily due to the inherent asymmetry in node\nrelationships. Traditional GNNs are adept at capturing unidirectional relations\nbut fall short in encoding the mutual path dependencies between nodes, such as\nasymmetrical shortest paths typically found in digraphs. Recognizing this gap,\nwe introduce Commute Graph Neural Networks (CGNN), an approach that seamlessly\nintegrates node-wise commute time into the message passing scheme. The\ncornerstone of CGNN is an efficient method for computing commute time using a\nnewly formulated digraph Laplacian. Commute time is then integrated into the\nneighborhood aggregation process, with neighbor contributions weighted\naccording to their respective commute time to the central node in each layer.\nIt enables CGNN to directly capture the mutual, asymmetric relationships in\ndigraphs. Extensive experiments on 8 benchmarking datasets confirm the\nsuperiority of CGNN against 13 state-of-the-art methods.\n","authors":["Wei Zhuo","Han Yu","Guang Tan","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2407.01635v7.pdf","comment":"Published in International Conference on Machine Learning (ICML),\n  2025"},{"id":"http://arxiv.org/abs/2505.09955v1","updated":"2025-05-15T04:27:48Z","published":"2025-05-15T04:27:48Z","title":"TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series\n  Unsupervised Domain Adaptation","summary":"  Unsupervised domain adaptation (UDA) for time series data remains a critical\nchallenge in deep learning, with traditional pseudo-labeling strategies failing\nto capture temporal patterns and channel-wise shifts between domains, producing\nsub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that\naddresses these limitations by modeling the joint distribution $P(\\mathbf{X},\ny)$ of the source domain through code transition matrices, where the codes are\nderived from vector quantization (VQ) of time series patches. Our method\nconstructs class- and channel-wise code transition matrices from the source\ndomain and employs Bayes' rule for target domain adaptation, generating\npseudo-labels based on channel-wise weighted class-conditional likelihoods.\nTransPL offers three key advantages: explicit modeling of temporal transitions\nand channel-wise shifts between different domains, versatility towards\ndifferent UDA scenarios (e.g., weakly-supervised UDA), and explainable\npseudo-label generation. We validate TransPL's effectiveness through extensive\nanalysis on four time series UDA benchmarks and confirm that it consistently\noutperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1%\naccuracy improvement, 4.9% F1 improvement), while providing interpretable\ninsights into the domain adaptation process through its learned code transition\nmatrices.\n","authors":["Jaeho Kim","Seulki Lee"],"pdf_url":"https://arxiv.org/pdf/2505.09955v1.pdf","comment":"ICML 2025 Accept"},{"id":"http://arxiv.org/abs/2505.09952v1","updated":"2025-05-15T04:22:35Z","published":"2025-05-15T04:22:35Z","title":"Task-Core Memory Management and Consolidation for Long-term Continual\n  Learning","summary":"  In this paper, we focus on a long-term continual learning (CL) task, where a\nmodel learns sequentially from a stream of vast tasks over time, acquiring new\nknowledge while retaining previously learned information in a manner akin to\nhuman learning. Unlike traditional CL settings, long-term CL involves handling\na significantly larger number of tasks, which exacerbates the issue of\ncatastrophic forgetting. Our work seeks to address two critical questions: 1)\nHow do existing CL methods perform in the context of long-term CL? and 2) How\ncan we mitigate the catastrophic forgetting that arises from prolonged\nsequential updates? To tackle these challenges, we propose a novel framework\ninspired by human memory mechanisms for long-term continual learning (Long-CL).\nSpecifically, we introduce a task-core memory management strategy to\nefficiently index crucial memories and adaptively update them as learning\nprogresses. Additionally, we develop a long-term memory consolidation mechanism\nthat selectively retains hard and discriminative samples, ensuring robust\nknowledge retention. To facilitate research in this area, we construct and\nrelease two multi-modal and textual benchmarks, MMLongCL-Bench and\nTextLongCL-Bench, providing a valuable resource for evaluating long-term CL\napproaches. Experimental results show that Long-CL outperforms the previous\nstate-of-the-art by 7.4\\% and 6.5\\% AP on the two benchmarks, respectively,\ndemonstrating the effectiveness of our approach.\n","authors":["Tianyu Huai","Jie Zhou","Yuxuan Cai","Qin Chen","Wen Wu","Xingjiao Wu","Xipeng Qiu","Liang He"],"pdf_url":"https://arxiv.org/pdf/2505.09952v1.pdf","comment":"Submitted to Neurips2025"},{"id":"http://arxiv.org/abs/2505.02387v2","updated":"2025-05-15T04:14:49Z","published":"2025-05-05T06:11:12Z","title":"RM-R1: Reward Modeling as Reasoning","summary":"  Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences through reinforcement learning (RL). To provide accurate\nreward signals, a reward model (RM) should stimulate deep thinking and conduct\ninterpretable reasoning before assigning a score or a judgment. Inspired by\nrecent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we\nhypothesize and validate that integrating reasoning capabilities into reward\nmodeling significantly enhances RM's interpretability and performance. To this\nend, we introduce a new class of generative reward models -- Reasoning Reward\nModels (ReasRMs) -- which formulate reward modeling as a reasoning task. We\npropose a reasoning-oriented training pipeline and train a family of ReasRMs,\nRM-R1. RM-R1 features a chain-of-rubrics (CoR) mechanism -- self-generating\nsample-level chat rubrics or math/code solutions, and evaluating candidate\nresponses against them. The training of M-R1 consists of two key stages: (1)\ndistillation of high-quality reasoning chains and (2) reinforcement learning\nwith verifiable rewards. Empirically, our models achieve state-of-the-art\nperformance across three reward model benchmarks on average, outperforming much\nlarger open-weight models (e.g., INF-ORM-Llama3.1-70B) and proprietary ones\n(e.g., GPT-4o) by up to 4.9%. Beyond final performance, we perform thorough\nempirical analysis to understand the key ingredients of successful ReasRM\ntraining. To facilitate future research, we release six ReasRM models along\nwith code and data at https://github.com/RM-R1-UIUC/RM-R1.\n","authors":["Xiusi Chen","Gaotang Li","Ziqi Wang","Bowen Jin","Cheng Qian","Yu Wang","Hongru Wang","Yu Zhang","Denghui Zhang","Tong Zhang","Hanghang Tong","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2505.02387v2.pdf","comment":"24 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.09945v1","updated":"2025-05-15T04:01:58Z","published":"2025-05-15T04:01:58Z","title":"Personalizing Large Language Models using Retrieval Augmented Generation\n  and Knowledge Graph","summary":"  The advent of large language models (LLMs) has allowed numerous applications,\nincluding the generation of queried responses, to be leveraged in chatbots and\nother conversational assistants. Being trained on a plethora of data, LLMs\noften undergo high levels of over-fitting, resulting in the generation of extra\nand incorrect data, thus causing hallucinations in output generation. One of\nthe root causes of such problems is the lack of timely, factual, and\npersonalized information fed to the LLM. In this paper, we propose an approach\nto address these problems by introducing retrieval augmented generation (RAG)\nusing knowledge graphs (KGs) to assist the LLM in personalized response\ngeneration tailored to the users. KGs have the advantage of storing\ncontinuously updated factual information in a structured way. While our KGs can\nbe used for a variety of frequently updated personal data, such as calendar,\ncontact, and location data, we focus on calendar data in this paper. Our\nexperimental results show that our approach works significantly better in\nunderstanding personal information and generating accurate responses compared\nto the baseline LLMs using personal data as text inputs, with a moderate\nreduction in response time.\n","authors":["Deeksha Prahlad","Chanhee Lee","Dongha Kim","Hokeun Kim"],"pdf_url":"https://arxiv.org/pdf/2505.09945v1.pdf","comment":"To appear in the Companion Proceedings of the ACM Web Conference 2025\n  (WWW Companion '25)"},{"id":"http://arxiv.org/abs/2505.09935v1","updated":"2025-05-15T03:40:29Z","published":"2025-05-15T03:40:29Z","title":"VRU-CIPI: Crossing Intention Prediction at Intersections for Improving\n  Vulnerable Road Users Safety","summary":"  Understanding and predicting human behavior in-thewild, particularly at urban\nintersections, remains crucial for enhancing interaction safety between road\nusers. Among the most critical behaviors are crossing intentions of Vulnerable\nRoad Users (VRUs), where misinterpretation may result in dangerous conflicts\nwith oncoming vehicles. In this work, we propose the VRU-CIPI framework with a\nsequential attention-based model designed to predict VRU crossing intentions at\nintersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal\ndynamics in VRU movements, combined with a multi-head Transformer\nself-attention mechanism to encode contextual and spatial dependencies critical\nfor predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed\nachieves state-of-the-art performance with an accuracy of 96.45% and achieving\nreal-time inference speed reaching 33 frames per second. Furthermore, by\nintegrating with Infrastructure-to-Vehicles (I2V) communication, our approach\ncan proactively enhance intersection safety through timely activation of\ncrossing signals and providing early warnings to connected vehicles, ensuring\nsmoother and safer interactions for all road users.\n","authors":["Ahmed S. Abdelrahman","Mohamed Abdel-Aty","Quoc Dai Tran"],"pdf_url":"https://arxiv.org/pdf/2505.09935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14251v2","updated":"2025-05-15T03:35:25Z","published":"2024-11-21T15:57:02Z","title":"Natural Language Reinforcement Learning","summary":"  Reinforcement Learning (RL) mathematically formulates decision-making with\nMarkov Decision Process (MDP). With MDPs, researchers have achieved remarkable\nbreakthroughs across various domains, including games, robotics, and language\nmodels. This paper seeks a new possibility, Natural Language Reinforcement\nLearning (NLRL), by extending traditional MDP to natural language-based\nrepresentation space. Specifically, NLRL innovatively redefines RL principles,\nincluding task objectives, policy, value function, Bellman equation, and policy\niteration, into their language counterparts. With recent advancements in large\nlanguage models (LLMs), NLRL can be practically implemented to achieve RL-like\npolicy and value improvement by either pure prompting or gradient-based\ntraining. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games\ndemonstrate the effectiveness, efficiency, and interpretability of the NLRL\nframework among diverse use cases.\n","authors":["Xidong Feng","Bo Liu","Ziyu Wan","Haotian Fu","Girish A. Koushik","Zhiyuan Hu","Mengyue Yang","Ying Wen","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14251v2.pdf","comment":"Accepted at ICLR 2025 Workshop SSI-FM"},{"id":"http://arxiv.org/abs/2505.09932v1","updated":"2025-05-15T03:35:12Z","published":"2025-05-15T03:35:12Z","title":"Demystifying AI Agents: The Final Generation of Intelligence","summary":"  The trajectory of artificial intelligence (AI) has been one of relentless\nacceleration, evolving from rudimentary rule-based systems to sophisticated,\nautonomous agents capable of complex reasoning and interaction. This whitepaper\nchronicles this remarkable journey, charting the key technological\nmilestones--advancements in prompting, training methodologies, hardware\ncapabilities, and architectural innovations--that have converged to create the\nAI agents of today. We argue that these agents, exemplified by systems like\nOpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in\nAI development, potentially constituting the \"final generation\" of intelligence\nas we currently conceive it. We explore the capabilities and underlying\ntechnologies of these agents, grounded in practical examples, while also\nexamining the profound societal implications and the unprecedented pace of\nprogress that suggests intelligence is now doubling approximately every six\nmonths. The paper concludes by underscoring the critical need for wisdom and\nforesight in navigating the opportunities and challenges presented by this\npowerful new era of intelligence.\n","authors":["Kevin J McNamara","Rhea Pritham Marpu"],"pdf_url":"https://arxiv.org/pdf/2505.09932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16112v2","updated":"2025-05-15T03:27:28Z","published":"2025-03-20T13:00:36Z","title":"PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming","summary":"  Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60% of severely distorted\nframes (compared to VQGAN).\n","authors":["Liming Liu","Jiangkai Wu","Haoyang Wang","Peiheng Wang","Zongming Guo","Xinggong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.16112v2.pdf","comment":"6 pages (excluding references), 10 figures, to appear in APNET 2025"},{"id":"http://arxiv.org/abs/2505.09926v1","updated":"2025-05-15T03:24:28Z","published":"2025-05-15T03:24:28Z","title":"AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection","summary":"  Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.\n","authors":["Bin-Bin Gao","Yue Zhu","Jiangtao Yan","Yuezhi Cai","Weixi Zhang","Meng Wang","Jun Liu","Yong Liu","Lei Wang","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2505.09926v1.pdf","comment":"27 pages, 15 figures, 22 tables"},{"id":"http://arxiv.org/abs/2407.21300v4","updated":"2025-05-15T03:22:21Z","published":"2024-07-31T03:00:59Z","title":"SAKR: Enhancing Retrieval-Augmented Generation via Streaming Algorithm\n  and K-Means Clustering","summary":"  Retrieval-augmented generation (RAG) has achieved significant success in\ninformation retrieval to assist large language models LLMs because it builds an\nexternal knowledge database. However, it also has many problems, it consumes a\nlot of memory because of the enormous database, and it cannot update the\nestablished index database in time when confronted with massive streaming data.\nTo reduce the memory required for building the database and maintain accuracy\nsimultaneously, we proposed a new approach integrating a streaming algorithm\nwith k-means clustering into RAG. Our approach applied a streaming algorithm to\nupdate the index dynamically and reduce memory consumption. Additionally, the\nk-means algorithm clusters highly similar documents, and the query time would\nbe shortened. We conducted comparative experiments on four methods, and the\nresults indicated that RAG with streaming algorithm and k-means clusters\noutperforms traditional RAG in accuracy and memory, particularly when dealing\nwith large-scale streaming data.\n","authors":["Haoyu Kang","Yuzhou Zhu","Yukun Zhong","Ke Wang"],"pdf_url":"https://arxiv.org/pdf/2407.21300v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09925v1","updated":"2025-05-15T03:22:03Z","published":"2025-05-15T03:22:03Z","title":"Reinforced Interactive Continual Learning via Real-time Noisy Human\n  Feedback","summary":"  This paper introduces an interactive continual learning paradigm where AI\nmodels dynamically learn new skills from real-time human feedback while\nretaining prior knowledge. This paradigm distinctively addresses two major\nlimitations of traditional continual learning: (1) dynamic model updates using\nstreaming, real-time human-annotated data, rather than static datasets with\nfixed labels, and (2) the assumption of clean labels, by explicitly handling\nthe noisy feedback common in real-world interactions. To tackle these problems,\nwe propose RiCL, a Reinforced interactive Continual Learning framework\nleveraging Large Language Models (LLMs) to learn new skills effectively from\ndynamic feedback. RiCL incorporates three key components: a temporal\nconsistency-aware purifier to automatically discern clean from noisy samples in\ndata streams; an interaction-aware direct preference optimization strategy to\nalign model behavior with human intent by reconciling AI-generated and\nhuman-provided feedback; and a noise-resistant contrastive learning module that\ncaptures robust representations by exploiting inherent data relationships, thus\navoiding reliance on potentially unreliable labels. Extensive experiments on\ntwo benchmark datasets (FewRel and TACRED), contaminated with realistic noise\npatterns, demonstrate that our RiCL approach substantially outperforms existing\ncombinations of state-of-the-art online continual learning and noisy-label\nlearning methods.\n","authors":["Yutao Yang","Jie Zhou","Junsong Li","Qianjun Pan","Bihao Zhan","Qin Chen","Xipeng Qiu","Liang He"],"pdf_url":"https://arxiv.org/pdf/2505.09925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09923v1","updated":"2025-05-15T03:12:28Z","published":"2025-05-15T03:12:28Z","title":"\"There Is No Such Thing as a Dumb Question,\" But There Are Good Ones","summary":"  Questioning has become increasingly crucial for both humans and artificial\nintelligence, yet there remains limited research comprehensively assessing\nquestion quality. In response, this study defines good questions and presents a\nsystematic evaluation framework. We propose two key evaluation dimensions:\nappropriateness (sociolinguistic competence in context) and effectiveness\n(strategic competence in goal achievement). Based on these foundational\ndimensions, a rubric-based scoring system was developed. By incorporating\ndynamic contextual variables, our evaluation framework achieves structure and\nflexibility through semi-adaptive criteria. The methodology was validated using\nthe CAUS and SQUARE datasets, demonstrating the ability of the framework to\naccess both well-formed and problematic questions while adapting to varied\ncontexts. As we establish a flexible and comprehensive framework for question\nevaluation, this study takes a significant step toward integrating questioning\nbehavior with structured analytical methods grounded in the intrinsic nature of\nquestioning.\n","authors":["Minjung Shin","Donghyun Kim","Jeh-Kwang Ryu"],"pdf_url":"https://arxiv.org/pdf/2505.09923v1.pdf","comment":"8 pages, 4 figures and 4 tables. This work has been accepted for\n  presentation as a poster with full paper publication at CogSci 2025. This is\n  the final submission"},{"id":"http://arxiv.org/abs/2505.09920v1","updated":"2025-05-15T03:10:18Z","published":"2025-05-15T03:10:18Z","title":"Offline Reinforcement Learning for Microgrid Voltage Regulation","summary":"  This paper presents a study on using different offline reinforcement learning\nalgorithms for microgrid voltage regulation with solar power penetration. When\nenvironment interaction is unviable due to technical or safety reasons, the\nproposed approach can still obtain an applicable model through offline-style\ntraining on a previously collected dataset, lowering the negative impact of\nlacking online environment interactions. Experiment results on the IEEE 33-bus\nsystem demonstrate the feasibility and effectiveness of the proposed approach\non different offline datasets, including the one with merely low-quality\nexperience.\n","authors":["Shan Yang","Yongli Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.09920v1.pdf","comment":"This paper has been accepted and presented at ICLR 2025 in Singapore,\n  Apr. 28, 2025"},{"id":"http://arxiv.org/abs/2505.07921v2","updated":"2025-05-15T02:56:21Z","published":"2025-05-12T16:51:08Z","title":"Self-cross Feature based Spiking Neural Networks for Efficient Few-shot\n  Learning","summary":"  Deep neural networks (DNNs) excel in computer vision tasks, especially,\nfew-shot learning (FSL), which is increasingly important for generalizing from\nlimited examples. However, DNNs are computationally expensive with scalability\nissues in real world. Spiking Neural Networks (SNNs), with their event-driven\nnature and low energy consumption, are particularly efficient in processing\nsparse and dynamic data, though they still encounter difficulties in capturing\ncomplex spatiotemporal features and performing accurate cross-class\ncomparisons. To further enhance the performance and efficiency of SNNs in\nfew-shot learning, we propose a few-shot learning framework based on SNNs,\nwhich combines a self-feature extractor module and a cross-feature contrastive\nmodule to refine feature representation and reduce power consumption. We apply\nthe combination of temporal efficient training loss and InfoNCE loss to\noptimize the temporal dynamics of spike trains and enhance the discriminative\npower. Experimental results show that the proposed FSL-SNN significantly\nimproves the classification performance on the neuromorphic dataset N-Omniglot,\nand also achieves competitive performance to ANNs on static datasets such as\nCUB and miniImageNet with low power consumption.\n","authors":["Qi Xu","Junyang Zhu","Dongdong Zhou","Hao Chen","Yang Liu","Jiangrong Shen","Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.07921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01698v3","updated":"2025-05-15T02:46:53Z","published":"2024-06-03T18:00:50Z","title":"Demystifying AI Platform Design for Distributed Inference of\n  Next-Generation LLM models","summary":"  Large language models (LLMs) have shown remarkable performance across a wide\nrange of applications, often outperforming human experts. However, deploying\nthese gigantic models efficiently for diverse inference use cases requires\ncarefully designed hardware platforms with ample computing, memory, and network\nresources. With constant innovation in LLM serving optimizations and model\narchitecture evolving at breakneck speed, the hardware requirements to meet\nService Level Objectives (SLOs) remain an open research question.\n  To answer the question, we present an analytical tool, GenZ, to efficiently\nnavigate the relationship between diverse LLM model architectures(Dense, GQA,\nMoE, Mamba), LLM serving optimizations(Chunking, Speculative decoding,\nquanitization), and AI platform design parameters. Our tool estimates LLM\ninference performance metrics for the given scenario. We have validated against\nreal hardware platforms running various different LLM models, achieving a max\ngeomean error of 5.82.We use GenZ to identify compute, memory capacity, memory\nbandwidth, network latency, and network bandwidth requirements across diverse\nLLM inference use cases. We also study diverse architectural choices in use\ntoday (inspired by LLM serving platforms from several vendors) to help inform\ncomputer architects designing next-generation AI hardware accelerators and\nplatforms. The trends and insights derived from GenZ can guide AI engineers\ndeploying LLMs as well as computer architects designing next-generation\nhardware accelerators and platforms. Ultimately, this work sheds light on the\nplatform design considerations for unlocking the full potential of large\nlanguage models across a spectrum of applications. The source code is available\nat https://github.com/abhibambhaniya/GenZ-LLM-Analyzer . Users can also be\ntried it on at https://genz-llm-analyzer.streamlit.app/ without any setup on\nyour web browser.\n","authors":["Abhimanyu Bambhaniya","Ritik Raj","Geonhwa Jeong","Souvik Kundu","Sudarshan Srinivasan","Suvinay Subramanian","Midhilesh Elavazhagan","Madhu Kumar","Tushar Krishna"],"pdf_url":"https://arxiv.org/pdf/2406.01698v3.pdf","comment":"19 Pages, https://github.com/abhibambhaniya/GenZ-LLM-Analyzer,\n  https://genz-llm-analyzer.streamlit.app/"},{"id":"http://arxiv.org/abs/2505.09907v1","updated":"2025-05-15T02:26:22Z","published":"2025-05-15T02:26:22Z","title":"Avocado Price Prediction Using a Hybrid Deep Learning Model:\n  TCN-MLP-Attention Architecture","summary":"  With the growing demand for healthy foods, agricultural product price\nforecasting has become increasingly important. Hass avocados, as a high-value\ncrop, exhibit complex price fluctuations influenced by factors such as\nseasonality, region, and weather. Traditional prediction models often struggle\nwith highly nonlinear and dynamic data. To address this, we propose a hybrid\ndeep learning model, TCN-MLP-Attention Architecture, combining Temporal\nConvolutional Networks (TCN) for sequential feature extraction, Multi-Layer\nPerceptrons (MLP) for nonlinear interactions, and an Attention mechanism for\ndynamic feature weighting. The dataset used covers over 50,000 records of Hass\navocado sales across the U.S. from 2015 to 2018, including variables such as\nsales volume, average price, time, region, weather, and variety type, collected\nfrom point-of-sale systems and the Hass Avocado Board. After systematic\npreprocessing, including missing value imputation and feature normalization,\nthe proposed model was trained and evaluated. Experimental results demonstrate\nthat the TCN-MLP-Attention model achieves excellent predictive performance,\nwith an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.\nThis research provides a scalable and effective approach for time series\nforecasting in agricultural markets and offers valuable insights for\nintelligent supply chain management and price strategy optimization.\n","authors":["Linwei Zhang"," LuFeng","Ruijia Liang"],"pdf_url":"https://arxiv.org/pdf/2505.09907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04526v4","updated":"2025-05-15T02:17:31Z","published":"2024-10-06T15:41:26Z","title":"FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question\n  Answering","summary":"  In this paper, we introduce FAMMA, an open-source benchmark for\n\\underline{f}in\\underline{a}ncial \\underline{m}ultilingual\n\\underline{m}ultimodal question \\underline{a}nswering (QA). Our benchmark aims\nto evaluate the abilities of large language models (LLMs) in answering complex\nreasoning questions that require advanced financial knowledge. The benchmark\nhas two versions: FAMMA-Basic consists of 1,945 questions extracted from\nuniversity textbooks and exams, along with human-annotated answers and\nrationales; FAMMA-LivePro consists of 103 novel questions created by human\ndomain experts, with answers and rationales held out from the public for a\ncontamination-free evaluation. These questions cover advanced knowledge of 8\nmajor subfields in finance (e.g., corporate finance, derivatives, and portfolio\nmanagement). Some are in Chinese or French, while a majority of them are in\nEnglish. Each question has some non-text data such as charts, diagrams, or\ntables. Our experiments reveal that FAMMA poses a significant challenge on\nLLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally,\nwe curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data,\nand fine-tuned a series of open-source Qwen models using this reasoning data.\nWe found that training a model on these reasoning trajectories can\nsignificantly improve its performance on FAMMA-LivePro. We released our\nleaderboard, data, code, and trained models at\nhttps://famma-bench.github.io/famma/.\n","authors":["Siqiao Xue","Xiaojing Li","Fan Zhou","Qingyang Dai","Zhixuan Chu","Hongyuan Mei"],"pdf_url":"https://arxiv.org/pdf/2410.04526v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09901v1","updated":"2025-05-15T02:09:18Z","published":"2025-05-15T02:09:18Z","title":"Comparing Exploration-Exploitation Strategies of LLMs and Humans:\n  Insights from Standard Multi-armed Bandit Tasks","summary":"  Large language models (LLMs) are increasingly used to simulate or automate\nhuman behavior in complex sequential decision-making tasks. A natural question\nis then whether LLMs exhibit similar decision-making behavior to humans, and\ncan achieve comparable (or superior) performance. In this work, we focus on the\nexploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic\ndecision-making under uncertainty. We employ canonical multi-armed bandit (MAB)\ntasks introduced in the cognitive science and psychiatry literature to conduct\na comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.\nWe use interpretable choice models to capture the E&E strategies of the agents\nand investigate how explicit reasoning, through both prompting strategies and\nreasoning-enhanced models, shapes LLM decision-making. We find that reasoning\nshifts LLMs toward more human-like behavior, characterized by a mix of random\nand directed exploration. In simple stationary tasks, reasoning-enabled LLMs\nexhibit similar levels of random and directed exploration compared to humans.\nHowever, in more complex, non-stationary environments, LLMs struggle to match\nhuman adaptability, particularly in effective directed exploration, despite\nachieving similar regret in certain scenarios. Our findings highlight both the\npromise and limits of LLMs as simulators of human behavior and tools for\nautomated decision-making and point to potential areas of improvements.\n","authors":["Ziyuan Zhang","Darcy Wang","Ningyuan Chen","Rodrigo Mansur","Vahid Sarhangian"],"pdf_url":"https://arxiv.org/pdf/2505.09901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03080v5","updated":"2025-05-15T02:03:46Z","published":"2024-04-03T21:46:14Z","title":"Construction and Application of Materials Knowledge Graph in\n  Multidisciplinary Materials Science via Large Language Model","summary":"  Knowledge in materials science is widely dispersed across extensive\nscientific literature, posing significant challenges to the efficient discovery\nand integration of new materials. Traditional methods, often reliant on costly\nand time-consuming experimental approaches, further complicate rapid\ninnovation. Addressing these challenges, the integration of artificial\nintelligence with materials science has opened avenues for accelerating the\ndiscovery process, though it also demands precise annotation, data extraction,\nand traceability of information. To tackle these issues, this article\nintroduces the Materials Knowledge Graph (MKG), which utilizes advanced natural\nlanguage processing techniques integrated with large language models to extract\nand systematically organize a decade's worth of high-quality research into\nstructured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes\ninformation into comprehensive labels such as Name, Formula, and Application,\nstructured around a meticulously designed ontology, thus enhancing data\nusability and integration. By implementing network-based algorithms, MKG not\nonly facilitates efficient link prediction but also significantly reduces\nreliance on traditional experimental methods. This structured approach not only\nstreamlines materials research but also lays the groundwork for more\nsophisticated science knowledge graphs.\n","authors":["Yanpeng Ye","Jie Ren","Shaozhou Wang","Yuwei Wan","Imran Razzak","Bram Hoex","Haofen Wang","Tong Xie","Wenjie Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.03080v5.pdf","comment":"Accepted by 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2504.19139v3","updated":"2025-05-15T01:51:26Z","published":"2025-04-27T07:27:17Z","title":"Fast and Robust: Task Sampling with Posterior and Diversity Synergies\n  for Adaptive Decision-Makers in Randomized Environments","summary":"  Task robust adaptation is a long-standing pursuit in sequential\ndecision-making. Some risk-averse strategies, e.g., the conditional\nvalue-at-risk principle, are incorporated in domain randomization or meta\nreinforcement learning to prioritize difficult tasks in optimization, which\ndemand costly intensive evaluations. The efficiency issue prompts the\ndevelopment of robust active task sampling to train adaptive policies, where\nrisk-predictive models are used to surrogate policy evaluation. This work\ncharacterizes the optimization pipeline of robust active task sampling as a\nMarkov decision process, posits theoretical and practical insights, and\nconstitutes robustness concepts in risk-averse scenarios. Importantly, we\npropose an easy-to-implement method, referred to as Posterior and Diversity\nSynergized Task Sampling (PDTS), to accommodate fast and robust sequential\ndecision-making. Extensive experiments show that PDTS unlocks the potential of\nrobust active task sampling, significantly improves the zero-shot and few-shot\nadaptation robustness in challenging tasks, and even accelerates the learning\nprocess under certain scenarios. Our project website is at\nhttps://thu-rllab.github.io/PDTS_project_page.\n","authors":["Yun Qu","Qi Cheems Wang","Yixiu Mao","Yiqin Lv","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2504.19139v3.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2406.00367v2","updated":"2025-05-15T01:38:21Z","published":"2024-06-01T08:59:46Z","title":"RoBERTa-BiLSTM: A Context-Aware Hybrid Model for Sentiment Analysis","summary":"  Effectively analyzing the comments to uncover latent intentions holds immense\nvalue in making strategic decisions across various domains. However, several\nchallenges hinder the process of sentiment analysis including the lexical\ndiversity exhibited in comments, the presence of long dependencies within the\ntext, encountering unknown symbols and words, and dealing with imbalanced\ndatasets. Moreover, existing sentiment analysis tasks mostly leveraged\nsequential models to encode the long dependent texts and it requires longer\nexecution time as it processes the text sequentially. In contrast, the\nTransformer requires less execution time due to its parallel processing nature.\nIn this work, we introduce a novel hybrid deep learning model, RoBERTa-BiLSTM,\nwhich combines the Robustly Optimized BERT Pretraining Approach (RoBERTa) with\nBidirectional Long Short-Term Memory (BiLSTM) networks. RoBERTa is utilized to\ngenerate meaningful word embedding vectors, while BiLSTM effectively captures\nthe contextual semantics of long-dependent texts. The RoBERTa-BiLSTM hybrid\nmodel leverages the strengths of both sequential and Transformer models to\nenhance performance in sentiment analysis. We conducted experiments using\ndatasets from IMDb, Twitter US Airline, and Sentiment140 to evaluate the\nproposed model against existing state-of-the-art methods. Our experimental\nfindings demonstrate that the RoBERTa-BiLSTM model surpasses baseline models\n(e.g., BERT, RoBERTa-base, RoBERTa-GRU, and RoBERTa-LSTM), achieving accuracies\nof 80.74%, 92.36%, and 82.25% on the Twitter US Airline, IMDb, and Sentiment140\ndatasets, respectively. Additionally, the model achieves F1-scores of 80.73%,\n92.35%, and 82.25% on the same datasets, respectively.\n","authors":["Md. Mostafizer Rahman","Ariful Islam Shiplu","Yutaka Watanobe","Md. Ashad Alam"],"pdf_url":"https://arxiv.org/pdf/2406.00367v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2505.10566v1","updated":"2025-05-15T17:59:51Z","published":"2025-05-15T17:59:51Z","title":"3D-Fixup: Advancing Photo Editing with 3D Priors","summary":"  Despite significant advances in modeling image priors via diffusion models,\n3D-aware image editing remains challenging, in part because the object is only\nspecified via a single image. To tackle this challenge, we propose 3D-Fixup, a\nnew framework for editing 2D images guided by learned 3D priors. The framework\nsupports difficult editing situations such as object translation and 3D\nrotation. To achieve this, we leverage a training-based approach that harnesses\nthe generative power of diffusion models. As video data naturally encodes\nreal-world physical dynamics, we turn to video data for generating training\ndata pairs, i.e., a source and a target frame. Rather than relying solely on a\nsingle trained model to infer transformations between source and target frames,\nwe incorporate 3D guidance from an Image-to-3D model, which bridges this\nchallenging task by explicitly projecting 2D information into 3D space. We\ndesign a data generation pipeline to ensure high-quality 3D guidance throughout\ntraining. Results show that by integrating these 3D priors, 3D-Fixup\neffectively supports complex, identity coherent 3D-aware edits, achieving\nhigh-quality results and advancing the application of diffusion models in\nrealistic image manipulation. The code is provided at\nhttps://3dfixup.github.io/\n","authors":["Yen-Chi Cheng","Krishna Kumar Singh","Jae Shin Yoon","Alex Schwing","Liangyan Gui","Matheus Gadelha","Paul Guerrero","Nanxuan Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.10566v1.pdf","comment":"SIGGRAPH 2025. Project page: https://3dfixup.github.io/"},{"id":"http://arxiv.org/abs/2505.10565v1","updated":"2025-05-15T17:59:50Z","published":"2025-05-15T17:59:50Z","title":"Depth Anything with Any Prior","summary":"  This work presents Prior Depth Anything, a framework that combines incomplete\nbut precise metric information in depth measurement with relative but complete\ngeometric structures in depth prediction, generating accurate, dense, and\ndetailed metric depth maps for any scene. To this end, we design a\ncoarse-to-fine pipeline to progressively integrate the two complementary depth\nsources. First, we introduce pixel-level metric alignment and distance-aware\nweighting to pre-fill diverse metric priors by explicitly using depth\nprediction. It effectively narrows the domain gap between prior patterns,\nenhancing generalization across varying scenarios. Second, we develop a\nconditioned monocular depth estimation (MDE) model to refine the inherent noise\nof depth priors. By conditioning on the normalized pre-filled prior and\nprediction, the model further implicitly merges the two complementary depth\nsources. Our model showcases impressive zero-shot generalization across depth\ncompletion, super-resolution, and inpainting over 7 real-world datasets,\nmatching or even surpassing previous task-specific methods. More importantly,\nit performs well on challenging, unseen mixed priors and enables test-time\nimprovements by switching prediction models, providing a flexible\naccuracy-efficiency trade-off while evolving with advancements in MDE models.\n","authors":["Zehan Wang","Siyu Chen","Lihe Yang","Jialei Wang","Ziang Zhang","Hengshuang Zhao","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.10565v1.pdf","comment":"Home page: https://prior-depth-anything.github.io/"},{"id":"http://arxiv.org/abs/2505.10562v1","updated":"2025-05-15T17:59:39Z","published":"2025-05-15T17:59:39Z","title":"End-to-End Vision Tokenizer Tuning","summary":"  Existing vision tokenization isolates the optimization of vision tokenizers\nfrom downstream training, implicitly assuming the visual tokens can generalize\nwell across various tasks, e.g., image generation and visual question\nanswering. The vision tokenizer optimized for low-level reconstruction is\nagnostic to downstream tasks requiring varied representations and semantics.\nThis decoupled paradigm introduces a critical misalignment: The loss of the\nvision tokenization can be the representation bottleneck for target tasks. For\nexample, errors in tokenizing text in a given image lead to poor results when\nrecognizing or generating them. To address this, we propose ETT, an end-to-end\nvision tokenizer tuning approach that enables joint optimization between vision\ntokenization and target autoregressive tasks. Unlike prior autoregressive\nmodels that use only discrete indices from a frozen vision tokenizer, ETT\nleverages the visual embeddings of the tokenizer codebook, and optimizes the\nvision tokenizers end-to-end with both reconstruction and caption objectives.\nETT can be seamlessly integrated into existing training pipelines with minimal\narchitecture modifications. Our ETT is simple to implement and integrate,\nwithout the need to adjust the original codebooks or architectures of the\nemployed large language models. Extensive experiments demonstrate that our\nproposed end-to-end vision tokenizer tuning unlocks significant performance\ngains, i.e., 2-6% for multimodal understanding and visual generation tasks\ncompared to frozen tokenizer baselines, while preserving the original\nreconstruction capability. We hope this very simple and strong method can\nempower multimodal foundation models besides image generation and\nunderstanding.\n","authors":["Wenxuan Wang","Fan Zhang","Yufeng Cui","Haiwen Diao","Zhuoyan Luo","Huchuan Lu","Jing Liu","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2505.10562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10557v1","updated":"2025-05-15T17:59:21Z","published":"2025-05-15T17:59:21Z","title":"MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal\n  Mathematical Reasoning","summary":"  Natural language image-caption datasets, widely used for training Large\nMultimodal Models, mainly focus on natural scenarios and overlook the intricate\ndetails of mathematical figures that are critical for problem-solving,\nhindering the advancement of current LMMs in multimodal mathematical reasoning.\nTo this end, we propose leveraging code as supervision for cross-modal\nalignment, since code inherently encodes all information needed to generate\ncorresponding figures, establishing a precise connection between the two\nmodalities. Specifically, we co-develop our image-to-code model and dataset\nwith model-in-the-loop approach, resulting in an image-to-code model,\nFigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.\nFurthermore, we utilize FigCodifier to synthesize novel mathematical figures\nand then construct MM-MathInstruct-3M, a high-quality multimodal math\ninstruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with\nImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on\nMM-MathInstruct-3M for multimodal math problem solving. Our model achieves a\nnew open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and\nClaude 3.5 Sonnet in the geometry problem-solving subset of MathVista,\nachieving improvements of 8.9% and 9.2%. The dataset and models will be\nreleased at https://github.com/mathllm/MathCoder.\n","authors":["Ke Wang","Junting Pan","Linda Wei","Aojun Zhou","Weikang Shi","Zimu Lu","Han Xiao","Yunqiao Yang","Houxing Ren","Mingjie Zhan","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2505.10557v1.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2505.10558v1","updated":"2025-05-15T17:59:21Z","published":"2025-05-15T17:59:21Z","title":"Style Customization of Text-to-Vector Generation with Image Diffusion\n  Priors","summary":"  Scalable Vector Graphics (SVGs) are highly favored by designers due to their\nresolution independence and well-organized layer structure. Although existing\ntext-to-vector (T2V) generation methods can create SVGs from text prompts, they\noften overlook an important need in practical applications: style\ncustomization, which is vital for producing a collection of vector graphics\nwith consistent visual appearance and coherent aesthetics. Extending existing\nT2V methods for style customization poses certain challenges.\nOptimization-based T2V models can utilize the priors of text-to-image (T2I)\nmodels for customization, but struggle with maintaining structural regularity.\nOn the other hand, feed-forward T2V models can ensure structural regularity,\nyet they encounter difficulties in disentangling content and style due to\nlimited SVG training data.\n  To address these challenges, we propose a novel two-stage style customization\npipeline for SVG generation, making use of the advantages of both feed-forward\nT2V models and T2I image priors. In the first stage, we train a T2V diffusion\nmodel with a path-level representation to ensure the structural regularity of\nSVGs while preserving diverse expressive capabilities. In the second stage, we\ncustomize the T2V diffusion model to different styles by distilling customized\nT2I models. By integrating these techniques, our pipeline can generate\nhigh-quality and diverse SVGs in custom styles based on text prompts in an\nefficient feed-forward manner. The effectiveness of our method has been\nvalidated through extensive experiments. The project page is\nhttps://customsvg.github.io.\n","authors":["Peiying Zhang","Nanxuan Zhao","Jing Liao"],"pdf_url":"https://arxiv.org/pdf/2505.10558v1.pdf","comment":"Accepted by SIGGRAPH 2025 (Conference Paper). Project page:\n  https://customsvg.github.io"},{"id":"http://arxiv.org/abs/2505.10551v1","updated":"2025-05-15T17:57:38Z","published":"2025-05-15T17:57:38Z","title":"Does Feasibility Matter? Understanding the Impact of Feasibility on\n  Synthetic Training Data","summary":"  With the development of photorealistic diffusion models, models trained in\npart or fully on synthetic data achieve progressively better results. However,\ndiffusion models still routinely generate images that would not exist in\nreality, such as a dog floating above the ground or with unrealistic texture\nartifacts. We define the concept of feasibility as whether attributes in a\nsynthetic image could realistically exist in the real-world domain; synthetic\nimages containing attributes that violate this criterion are considered\ninfeasible. Intuitively, infeasible images are typically considered\nout-of-distribution; thus, training on such images is expected to hinder a\nmodel's ability to generalize to real-world data, and they should therefore be\nexcluded from the training set whenever possible. However, does feasibility\nreally matter? In this paper, we investigate whether enforcing feasibility is\nnecessary when generating synthetic training data for CLIP-based classifiers,\nfocusing on three target attributes: background, color, and texture. We\nintroduce VariReal, a pipeline that minimally edits a given source image to\ninclude feasible or infeasible attributes given by the textual prompt generated\nby a large language model. Our experiments show that feasibility minimally\naffects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference\nin top-1 accuracy across three fine-grained datasets. Also, the attribute\nmatters on whether the feasible/infeasible images adversarially influence the\nclassification performance. Finally, mixing feasible and infeasible images in\ntraining datasets does not significantly impact performance compared to using\npurely feasible or infeasible datasets.\n","authors":["Yiwen Liu","Jessica Bader","Jae Myung Kim"],"pdf_url":"https://arxiv.org/pdf/2505.10551v1.pdf","comment":"CVPRW 2025"},{"id":"http://arxiv.org/abs/2505.08787v2","updated":"2025-05-15T17:53:41Z","published":"2025-05-13T17:59:22Z","title":"UniSkill: Imitating Human Videos via Cross-Embodiment Skill\n  Representations","summary":"  Mimicry is a fundamental learning mechanism in humans, enabling individuals\nto learn new tasks by observing and imitating experts. However, applying this\nability to robots presents significant challenges due to the inherent\ndifferences between human and robot embodiments in both their visual appearance\nand physical capabilities. While previous methods bridge this gap using\ncross-embodiment datasets with shared scenes and tasks, collecting such aligned\ndata between humans and robots at scale is not trivial. In this paper, we\npropose UniSkill, a novel framework that learns embodiment-agnostic skill\nrepresentations from large-scale cross-embodiment video data without any\nlabels, enabling skills extracted from human video prompts to effectively\ntransfer to robot policies trained only on robot data. Our experiments in both\nsimulation and real-world environments show that our cross-embodiment skills\nsuccessfully guide robots in selecting appropriate actions, even with unseen\nvideo prompts. The project website can be found at:\nhttps://kimhanjung.github.io/UniSkill.\n","authors":["Hanjung Kim","Jaehyun Kang","Hyolim Kang","Meedeum Cho","Seon Joo Kim","Youngwoon Lee"],"pdf_url":"https://arxiv.org/pdf/2505.08787v2.pdf","comment":"Project Page: https://kimhanjung.github.io/UniSkill/"},{"id":"http://arxiv.org/abs/2505.10541v1","updated":"2025-05-15T17:52:40Z","published":"2025-05-15T17:52:40Z","title":"Exploring Implicit Visual Misunderstandings in Multimodal Large Language\n  Models through Attention Analysis","summary":"  Recent advancements have enhanced the capability of Multimodal Large Language\nModels (MLLMs) to comprehend multi-image information. However, existing\nbenchmarks primarily evaluate answer correctness, overlooking whether models\ngenuinely comprehend the visual input. To address this, we define implicit\nvisual misunderstanding (IVM), where MLLMs provide correct answers without\nfully comprehending the visual input. Through our analysis, we decouple the\nvisual and textual modalities within the causal attention module, revealing\nthat attention distribution increasingly converges on the image associated with\nthe correct answer as the network layers deepen. This insight leads to the\nintroduction of a scale-agnostic metric, \\textit{attention accuracy}, and a\nnovel benchmark for quantifying IVMs. Attention accuracy directly evaluates the\nmodel's visual understanding via internal mechanisms, remaining robust to\npositional biases for more reliable assessments. Furthermore, we extend our\napproach to finer granularities and demonstrate its effectiveness in unimodal\nscenarios, underscoring its versatility and generalizability.\n","authors":["Pengfei Wang","Guohai Xu","Weinong Wang","Junjie Yang","Jie Lou","Yunhua Xue"],"pdf_url":"https://arxiv.org/pdf/2505.10541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10533v1","updated":"2025-05-15T17:41:52Z","published":"2025-05-15T17:41:52Z","title":"Enhancing Multi-Image Question Answering via Submodular Subset Selection","summary":"  Large multimodal models (LMMs) have achieved high performance in\nvision-language tasks involving single image but they struggle when presented\nwith a collection of multiple images (Multiple Image Question Answering\nscenario). These tasks, which involve reasoning over large number of images,\npresent issues in scalability (with increasing number of images) and retrieval\nperformance. In this work, we propose an enhancement for retriever framework\nintroduced in MIRAGE model using submodular subset selection techniques. Our\nmethod leverages query-aware submodular functions, such as GraphCut, to\npre-select a subset of semantically relevant images before main retrieval\ncomponent. We demonstrate that using anchor-based queries and augmenting the\ndata improves submodular-retriever pipeline effectiveness, particularly in\nlarge haystack sizes.\n","authors":["Aaryan Sharma","Shivansh Gupta","Samar Agarwal","Vishak Prasad C.","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2505.10533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10526v1","updated":"2025-05-15T17:37:00Z","published":"2025-05-15T17:37:00Z","title":"MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative\n  Decoding of Vision-Language Models","summary":"  Speculative decoding significantly accelerates language model inference by\nenabling a lightweight draft model to propose multiple tokens that a larger\ntarget model verifies simultaneously. However, applying this technique to\nvision-language models (VLMs) presents two fundamental challenges: small\nlanguage models that could serve as efficient drafters lack the architectural\ncomponents to process visual inputs, and their token predictions fail to match\nthose of VLM target models that consider visual context. We introduce\nMultimodal Adaptation and Self-Data Distillation for Speculative Decoding of\nVision-Language Models (MASSV), which transforms existing small language models\ninto effective multimodal drafters through a two-phase approach. MASSV first\nconnects the target VLM's vision encoder to the draft model via a lightweight\ntrainable projector, then applies self-distilled visual instruction tuning\nusing responses generated by the target VLM to align token predictions.\nComprehensive experiments across the Qwen2.5-VL and Gemma3 model families\ndemonstrate that MASSV increases accepted length by up to 30% and delivers\nend-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV\nprovides a scalable, architecture-compatible method for accelerating both\ncurrent and future VLMs.\n","authors":["Mugilan Ganesan","Shane Segal","Ankur Aggarwal","Nish Sinnadurai","Sean Lie","Vithursan Thangarasa"],"pdf_url":"https://arxiv.org/pdf/2505.10526v1.pdf","comment":"Main paper: 11 pp., 4 figs., 3 tabs.; Supplementary: 2 pp"},{"id":"http://arxiv.org/abs/2505.08616v2","updated":"2025-05-15T17:30:10Z","published":"2025-05-13T14:34:46Z","title":"A portable diagnosis model for Keratoconus using a smartphone","summary":"  Keratoconus (KC) is a corneal disorder that results in blurry and distorted\nvision. Traditional diagnostic tools, while effective, are often bulky, costly,\nand require professional operation. In this paper, we present a portable and\ninnovative methodology for diagnosing. Our proposed approach first captures the\nimage reflected on the eye's cornea when a smartphone screen-generated Placido\ndisc sheds its light on an eye, then utilizes a two-stage diagnosis for\nidentifying the KC cornea and pinpointing the location of the KC on the cornea.\nThe first stage estimates the height and width of the Placido disc extracted\nfrom the captured image to identify whether it has KC. In this KC\nidentification, k-means clustering is implemented to discern statistical\ncharacteristics, such as height and width values of extracted Placido discs,\nfrom non-KC (control) and KC-affected groups. The second stage involves the\ncreation of a distance matrix, providing a precise localization of KC on the\ncornea, which is critical for efficient treatment planning. The analysis of\nthese distance matrices, paired with a logistic regression model and robust\nstatistical analysis, reveals a clear distinction between control and KC\ngroups. The logistic regression model, which classifies small areas on the\ncornea as either control or KC-affected based on the corresponding inter-disc\ndistances in the distance matrix, reported a classification accuracy of 96.94%,\nwhich indicates that we can effectively pinpoint the protrusion caused by KC.\nThis comprehensive, smartphone-based method is expected to detect KC and\nstreamline timely treatment.\n","authors":["Yifan Li","Peter Ho","Jo Woon Chong"],"pdf_url":"https://arxiv.org/pdf/2505.08616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08517v2","updated":"2025-05-15T17:28:04Z","published":"2025-05-13T12:48:36Z","title":"A Deep Learning-Driven Inhalation Injury Grading Assistant Using\n  Bronchoscopy Images","summary":"  Inhalation injuries present a challenge in clinical diagnosis and grading due\nto Conventional grading methods such as the Abbreviated Injury Score (AIS)\nbeing subjective and lacking robust correlation with clinical parameters like\nmechanical ventilation duration and patient mortality. This study introduces a\nnovel deep learning-based diagnosis assistant tool for grading inhalation\ninjuries using bronchoscopy images to overcome subjective variability and\nenhance consistency in severity assessment. Our approach leverages data\naugmentation techniques, including graphic transformations, Contrastive\nUnpaired Translation (CUT), and CycleGAN, to address the scarcity of medical\nimaging data. We evaluate the classification performance of two deep learning\nmodels, GoogLeNet and Vision Transformer (ViT), across a dataset significantly\nexpanded through these augmentation methods. The results demonstrate GoogLeNet\ncombined with CUT as the most effective configuration for grading inhalation\ninjuries through bronchoscopy images and achieves a classification accuracy of\n97.8%. The histograms and frequency analysis evaluations reveal variations\ncaused by the augmentation CUT with distribution changes in the histogram and\ntexture details of the frequency spectrum. PCA visualizations underscore the\nCUT substantially enhances class separability in the feature space. Moreover,\nGrad-CAM analyses provide insight into the decision-making process; mean\nintensity for CUT heatmaps is 119.6, which significantly exceeds 98.8 of the\noriginal datasets. Our proposed tool leverages mechanical ventilation periods\nas a novel grading standard, providing comprehensive diagnostic support.\n","authors":["Yifan Li","Alan W Pang","Jo Woon Chong"],"pdf_url":"https://arxiv.org/pdf/2505.08517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10518v1","updated":"2025-05-15T17:25:03Z","published":"2025-05-15T17:25:03Z","title":"Multi-Token Prediction Needs Registers","summary":"  Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.\n","authors":["Anastasios Gerontopoulos","Spyros Gidaris","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2505.10518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01482v2","updated":"2025-05-15T17:15:14Z","published":"2025-01-02T18:13:35Z","title":"An unsupervised method for MRI recovery: Deep image prior with\n  structured sparsity","summary":"  Objective: To propose and validate an unsupervised MRI reconstruction method\nthat does not require fully sampled k-space data. Materials and Methods: The\nproposed method, deep image prior with structured sparsity (DISCUS), extends\nthe deep image prior (DIP) by introducing group sparsity to frame-specific code\nvectors, enabling the discovery of a low-dimensional manifold for capturing\ntemporal variations. \\discus was validated using four studies: (I) simulation\nof a dynamic Shepp-Logan phantom to demonstrate its manifold discovery\ncapabilities, (II) comparison with compressed sensing and DIP-based methods\nusing simulated single-shot late gadolinium enhancement (LGE) image series from\nsix distinct digital cardiac phantoms in terms of normalized mean square error\n(NMSE) and structural similarity index measure (SSIM), (III) evaluation on\nretrospectively undersampled single-shot LGE data from eight patients, and (IV)\nevaluation on prospectively undersampled single-shot LGE data from eight\npatients, assessed via blind scoring from two expert readers. Results: DISCUS\noutperformed competing methods, demonstrating superior reconstruction quality\nin terms of NMSE and SSIM (Studies I--III) and expert reader scoring (Study\nIV). Discussion: An unsupervised image reconstruction method is presented and\nvalidated on simulated and measured data. These developments can benefit\napplications where acquiring fully sampled data is challenging.\n","authors":["Muhammad Ahmad Sultan","Chong Chen","Yingmin Liu","Katarzyna Gil","Karolina Zareba","Rizwan Ahmad"],"pdf_url":"https://arxiv.org/pdf/2501.01482v2.pdf","comment":"Magn Reson Mater Phy (2025)"},{"id":"http://arxiv.org/abs/2505.10497v1","updated":"2025-05-15T17:00:16Z","published":"2025-05-15T17:00:16Z","title":"MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face\n  Morphing Attacks","summary":"  Face recognition has evolved significantly with the advancement of deep\nlearning techniques, enabling its widespread adoption in various applications\nrequiring secure authentication. However, this progress has also increased its\nexposure to presentation attacks, including face morphing, which poses a\nserious security threat by allowing one identity to impersonate another.\nTherefore, modern face recognition systems must be robust against such attacks.\n  In this work, we propose a novel approach for training deep networks for face\nrecognition with enhanced robustness to face morphing attacks. Our method\nmodifies the classification task by introducing a dual-branch classification\nstrategy that effectively handles the ambiguity in the labeling of face morphs.\nThis adaptation allows the model to incorporate morph images into the training\nprocess, improving its ability to distinguish them from bona fide samples.\n  Our strategy has been validated on public benchmarks, demonstrating its\neffectiveness in enhancing robustness against face morphing attacks.\nFurthermore, our approach is universally applicable and can be integrated into\nexisting face recognition training pipelines to improve classification-based\nrecognition methods.\n","authors":["Iurii Medvedev","Nuno Goncalves"],"pdf_url":"https://arxiv.org/pdf/2505.10497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10496v1","updated":"2025-05-15T16:59:17Z","published":"2025-05-15T16:59:17Z","title":"CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of\n  Synthetic Chest Radiographs","summary":"  We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/\n","authors":["Raman Dutt","Pedro Sanchez","Yongchen Yao","Steven McDonagh","Sotirios A. Tsaftaris","Timothy Hospedales"],"pdf_url":"https://arxiv.org/pdf/2505.10496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10492v1","updated":"2025-05-15T16:47:24Z","published":"2025-05-15T16:47:24Z","title":"Multi-contrast laser endoscopy for in vivo gastrointestinal imaging","summary":"  White light endoscopy is the clinical gold standard for detecting diseases in\nthe gastrointestinal tract. Most applications involve identifying visual\nabnormalities in tissue color, texture, and shape. Unfortunately, the contrast\nof these features is often subtle, causing many clinically relevant cases to go\nundetected. To overcome this challenge, we introduce Multi-contrast Laser\nEndoscopy (MLE): a platform for widefield clinical imaging with rapidly tunable\nspectral, coherent, and directional illumination. We demonstrate three\ncapabilities of MLE: enhancing tissue chromophore contrast with multispectral\ndiffuse reflectance, quantifying blood flow using laser speckle contrast\nimaging, and characterizing mucosal topography using photometric stereo. We\nvalidate MLE with benchtop models, then demonstrate MLE in vivo during clinical\ncolonoscopies. MLE images from 31 polyps demonstrate an approximate three-fold\nimprovement in contrast and a five-fold improvement in color difference\ncompared to white light and narrow band imaging. With the ability to reveal\nmultiple complementary types of tissue contrast while seamlessly integrating\ninto the clinical environment, MLE shows promise as an investigative tool to\nimprove gastrointestinal imaging.\n","authors":["Taylor L. Bobrow","Mayank Golhar","Suchapa Arayakarnkul","Anthony A. Song","Saowanee Ngamruengphong","Nicholas J. Durr"],"pdf_url":"https://arxiv.org/pdf/2505.10492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10483v1","updated":"2025-05-15T16:34:50Z","published":"2025-05-15T16:34:50Z","title":"UniEval: Unified Holistic Evaluation for Unified Multimodal\n  Understanding and Generation","summary":"  The emergence of unified multimodal understanding and generation models is\nrapidly attracting attention because of their ability to enhance\ninstruction-following capabilities while minimizing model redundancy. However,\nthere is a lack of a unified evaluation framework for these models, which would\nenable an elegant, simplified, and overall evaluation. Current models conduct\nevaluations on multiple task-specific benchmarks, but there are significant\nlimitations, such as the lack of overall results, errors from extra evaluation\nmodels, reliance on extensive labeled images, benchmarks that lack diversity,\nand metrics with limited capacity for instruction-following evaluation. To\ntackle these challenges, we introduce UniEval, the first evaluation framework\ndesigned for unified multimodal models without extra models, images, or\nannotations. This facilitates a simplified and unified evaluation process. The\nUniEval framework contains a holistic benchmark, UniBench (supports both\nunified and visual generation models), along with the corresponding UniScore\nmetric. UniBench includes 81 fine-grained tags contributing to high diversity.\nExperimental results indicate that UniBench is more challenging than existing\nbenchmarks, and UniScore aligns closely with human evaluations, surpassing\ncurrent metrics. Moreover, we extensively evaluated SoTA unified and visual\ngeneration models, uncovering new insights into Univeral's unique values.\n","authors":["Yi Li","Haonan Wang","Qixiang Zhang","Boyu Xiao","Chenchang Hu","Hualiang Wang","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2505.10483v1.pdf","comment":"UniEval is the first evaluation framework designed for unified\n  multimodal models, including a holistic benchmark UniBench and the UniScore\n  metric"},{"id":"http://arxiv.org/abs/2505.10481v1","updated":"2025-05-15T16:31:49Z","published":"2025-05-15T16:31:49Z","title":"Logos as a Well-Tempered Pre-train for Sign Language Recognition","summary":"  This paper examines two aspects of the isolated sign language recognition\n(ISLR) task. First, despite the availability of a number of datasets, the\namount of data for most individual sign languages is limited. It poses the\nchallenge of cross-language ISLR model training, including transfer learning.\nSecond, similar signs can have different semantic meanings. It leads to\nambiguity in dataset labeling and raises the question of the best policy for\nannotating such signs. To address these issues, this study presents Logos, a\nnovel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by\nthe number of signers and one of the largest available datasets while also the\nlargest RSL dataset in size and vocabulary. It is shown that a model,\npre-trained on the Logos dataset can be used as a universal encoder for other\nlanguage SLR tasks, including few-shot learning. We explore cross-language\ntransfer learning approaches and find that joint training using multiple\nclassification heads benefits accuracy for the target lowresource datasets the\nmost. The key feature of the Logos dataset is explicitly annotated visually\nsimilar sign groups. We show that explicitly labeling visually similar signs\nimproves trained model quality as a visual encoder for downstream tasks. Based\non the proposed contributions, we outperform current state-of-the-art results\nfor the WLASL dataset and get competitive results for the AUTSL dataset, with a\nsingle stream model processing solely RGB video. The source code, dataset, and\npre-trained models are publicly available.\n","authors":["Ilya Ovodov","Petr Surovtsev","Karina Kvanchiani","Alexander Kapitanov","Alexander Nagaev"],"pdf_url":"https://arxiv.org/pdf/2505.10481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02147v2","updated":"2025-05-15T16:26:53Z","published":"2024-06-04T09:34:46Z","title":"S2-Track: A Simple yet Strong Approach for End-to-End 3D Multi-Object\n  Tracking","summary":"  3D multiple object tracking (MOT) plays a crucial role in autonomous driving\nperception. Recent end-to-end query-based trackers simultaneously detect and\ntrack objects, which have shown promising potential for the 3D MOT task.\nHowever, existing methods are still in the early stages of development and lack\nsystematic improvements, failing to track objects in certain complex scenarios,\nlike occlusions and the small size of target object's situations. In this\npaper, we first summarize the current end-to-end 3D MOT framework by\ndecomposing it into three constituent parts: query initialization, query\npropagation, and query matching. Then we propose corresponding improvements,\nwhich lead to a strong yet simple tracker: S2-Track. Specifically, for query\ninitialization, we present 2D-Prompted Query Initialization, which leverages\npredicted 2D object and depth information to prompt an initial estimate of the\nobject's 3D location. For query propagation, we introduce an Uncertainty-aware\nProbabilistic Decoder to capture the uncertainty of complex environment in\nobject prediction with probabilistic attention. For query matching, we propose\na Hierarchical Query Denoising strategy to enhance training robustness and\nconvergence. As a result, our S2-Track achieves state-of-the-art performance on\nnuScenes benchmark, i.e., 66.3% AMOTA on test split, surpassing the previous\nbest end-to-end solution by a significant margin of 8.9% AMOTA. We achieve 1st\nplace on the nuScenes tracking task leaderboard.\n","authors":["Tao Tang","Lijun Zhou","Pengkun Hao","Zihang He","Kalok Ho","Shuo Gu","Zhihui Hao","Haiyang Sun","Kun Zhan","Peng Jia","XianPeng Lang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2406.02147v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10473v1","updated":"2025-05-15T16:23:51Z","published":"2025-05-15T16:23:51Z","title":"Consistent Quantity-Quality Control across Scenes for Deployment-Aware\n  Gaussian Splatting","summary":"  To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks\nto minimize the number of Gaussians used while preserving high rendering\nquality, introducing an inherent trade-off between Gaussian quantity and\nrendering quality. Existing methods strive for better quantity-quality\nperformance, but lack the ability for users to intuitively adjust this\ntrade-off to suit practical needs such as model deployment under diverse\nhardware and communication constraints. Here, we present ControlGS, a 3DGS\noptimization method that achieves semantically meaningful and cross-scene\nconsistent quantity-quality control while maintaining strong quantity-quality\nperformance. Through a single training run using a fixed setup and a\nuser-specified hyperparameter reflecting quantity-quality preference, ControlGS\ncan automatically find desirable quantity-quality trade-off points across\ndiverse scenes, from compact objects to large outdoor scenes. It also\noutperforms baselines by achieving higher rendering quality with fewer\nGaussians, and supports a broad adjustment range with stepless control over the\ntrade-off.\n","authors":["Fengdi Zhang","Hongkun Cao","Ruqi Huang"],"pdf_url":"https://arxiv.org/pdf/2505.10473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10464v1","updated":"2025-05-15T16:18:00Z","published":"2025-05-15T16:18:00Z","title":"HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric\n  Lesion Segmentation","summary":"  Multimodal medical image segmentation faces significant challenges in the\ncontext of gastric cancer lesion analysis. This clinical context is defined by\nthe scarcity of independent multimodal datasets and the imperative to\namalgamate inherently misaligned modalities. As a result, algorithms are\nconstrained to train on approximate data and depend on application migration,\nleading to substantial resource expenditure and a potential decline in analysis\naccuracy. To address those challenges, we have made two major contributions:\nFirst, we publicly disseminate the GCM 2025 dataset, which serves as the first\nlarge-scale, open-source collection of gastric cancer multimodal MRI scans,\nfeaturing professionally annotated FS-T2W, CE-T1W, and ADC images from 500\npatients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework\nthat employs an original HWA block with learnable window aggregation layers to\nestablish dynamic feature correspondences between different modalities'\nanatomical structures, and leverages the innovative tri-orientated fusion mamba\nmechanism for context modeling and capturing long-range spatial dependencies.\nExtensive experiments on our GCM 2025 dataset and the publicly BraTS 2021\ndataset validate the performance of our framework, demonstrating that the new\napproach surpasses existing methods by up to 1.68\\% in the Dice score while\nmaintaining solid robustness. The dataset and code are public via\nhttps://github.com/JeMing-creater/HWA-UNETR.\n","authors":["Jiaming Liang","Lihuan Dai","Xiaoqi Sheng","Xiangguang Chen","Chun Yao","Guihua Tao","Qibin Leng","Honming Cai","Xi Zhong"],"pdf_url":"https://arxiv.org/pdf/2505.10464v1.pdf","comment":"This work has been provisionally accepted for MICCAI 2025"},{"id":"http://arxiv.org/abs/2505.10457v1","updated":"2025-05-15T16:14:18Z","published":"2025-05-15T16:14:18Z","title":"SEAL: Searching Expandable Architectures for Incremental Learning","summary":"  Incremental learning is a machine learning paradigm where a model learns from\na sequential stream of tasks. This setting poses a key challenge: balancing\nplasticity (learning new tasks) and stability (preserving past knowledge).\nNeural Architecture Search (NAS), a branch of AutoML, automates the design of\nthe architecture of Deep Neural Networks and has shown success in static\nsettings. However, existing NAS-based approaches to incremental learning often\nrely on expanding the model at every task, making them impractical in\nresource-constrained environments. In this work, we introduce SEAL, a NAS-based\nframework tailored for data-incremental learning, a scenario where disjoint\ndata samples arrive sequentially and are not stored for future access. SEAL\nadapts the model structure dynamically by expanding it only when necessary,\nbased on a capacity estimation metric. Stability is preserved through\ncross-distillation training after each expansion step. The NAS component\njointly searches for both the architecture and the optimal expansion policy.\nExperiments across multiple benchmarks demonstrate that SEAL effectively\nreduces forgetting and enhances accuracy while maintaining a lower model size\ncompared to prior methods. These results highlight the promise of combining NAS\nand selective expansion for efficient, adaptive learning in incremental\nscenarios.\n","authors":["Matteo Gambella","Vicente Javier Castro Solar","Manuel Roveri"],"pdf_url":"https://arxiv.org/pdf/2505.10457v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.10453v1","updated":"2025-05-15T16:11:33Z","published":"2025-05-15T16:11:33Z","title":"Vision language models have difficulty recognizing virtual objects","summary":"  Vision language models (VLMs) are AI systems paired with both language and\nvision encoders to process multimodal input. They are capable of performing\ncomplex semantic tasks such as automatic captioning, but it remains an open\nquestion about how well they comprehend the visuospatial properties of scenes\ndepicted in the images they process. We argue that descriptions of virtual\nobjects -- objects that are not visually represented in an image -- can help\ntest scene comprehension in these AI systems. For example, an image that\ndepicts a person standing under a tree can be paired with the following prompt:\nimagine that a kite is stuck in the tree. VLMs that comprehend the scene should\nupdate their representations and reason sensibly about the spatial relations\nbetween all three objects. We describe systematic evaluations of\nstate-of-the-art VLMs and show that their ability to process virtual objects is\ninadequate.\n","authors":["Tyler Tran","Sangeet Khemlani","J. G. Trafton"],"pdf_url":"https://arxiv.org/pdf/2505.10453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10441v1","updated":"2025-05-15T16:00:31Z","published":"2025-05-15T16:00:31Z","title":"PIF: Anomaly detection via preference embedding","summary":"  We address the problem of detecting anomalies with respect to structured\npatterns. To this end, we conceive a novel anomaly detection method called PIF,\nthat combines the advantages of adaptive isolation methods with the flexibility\nof preference embedding. Specifically, we propose to embed the data in a high\ndimensional space where an efficient tree-based method, PI-Forest, is employed\nto compute an anomaly score. Experiments on synthetic and real datasets\ndemonstrate that PIF favorably compares with state-of-the-art anomaly detection\ntechniques, and confirm that PI-Forest is better at measuring arbitrary\ndistances and isolate points in the preference space.\n","authors":["Filippo Leveni","Luca Magri","Giacomo Boracchi","Cesare Alippi"],"pdf_url":"https://arxiv.org/pdf/2505.10441v1.pdf","comment":"Accepted at International Conference on Pattern Recognition (ICPR\n  2020)"},{"id":"http://arxiv.org/abs/2411.14347v3","updated":"2025-05-15T15:52:39Z","published":"2024-11-21T17:42:20Z","title":"DINO-X: A Unified Vision Model for Open-World Object Detection and\n  Understanding","summary":"  In this paper, we introduce DINO-X, which is a unified object-centric vision\nmodel developed by IDEA Research with the best open-world object detection\nperformance to date. DINO-X employs the same Transformer-based encoder-decoder\narchitecture as Grounding DINO 1.5 to pursue an object-level representation for\nopen-world object understanding. To make long-tailed object detection easy,\nDINO-X extends its input options to support text prompt, visual prompt, and\ncustomized prompt. With such flexible prompt options, we develop a universal\nobject prompt to support prompt-free open-world detection, making it possible\nto detect anything in an image without requiring users to provide any prompt.\nTo enhance the model's core grounding capability, we have constructed a\nlarge-scale dataset with over 100 million high-quality grounding samples,\nreferred to as Grounding-100M, for advancing the model's open-vocabulary\ndetection performance. Pre-training on such a large-scale grounding dataset\nleads to a foundational object-level representation, which enables DINO-X to\nintegrate multiple perception heads to simultaneously support multiple object\nperception and understanding tasks, including detection, segmentation, pose\nestimation, object captioning, object-based QA, etc. Experimental results\ndemonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro\nmodel achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and\nLVIS-val zero-shot object detection benchmarks, respectively. Notably, it\nscores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val\nbenchmarks, improving the previous SOTA performance by 5.8 AP and 5.0 AP. Such\na result underscores its significantly improved capacity for recognizing\nlong-tailed objects.\n","authors":["Tianhe Ren","Yihao Chen","Qing Jiang","Zhaoyang Zeng","Yuda Xiong","Wenlong Liu","Zhengyu Ma","Junyi Shen","Yuan Gao","Xiaoke Jiang","Xingyu Chen","Zhuheng Song","Yuhong Zhang","Hongjie Huang","Han Gao","Shilong Liu","Hao Zhang","Feng Li","Kent Yu","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14347v3.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2505.05901v2","updated":"2025-05-15T15:46:43Z","published":"2025-05-09T09:09:08Z","title":"Examining the Source of Defects from a Mechanical Perspective for 3D\n  Anomaly Detection","summary":"  In this paper, we explore a novel approach to 3D anomaly detection (AD) that\ngoes beyond merely identifying anomalies based on structural characteristics.\nOur primary perspective is that most anomalies arise from unpredictable\ndefective forces originating from both internal and external sources. To\naddress these anomalies, we seek out opposing forces that can help correct\nthem. Therefore, we introduce the Mechanics Complementary Model-based Framework\nfor the 3D-AD task (MC4AD), which generates internal and external corrective\nforces for each point. We first propose a Diverse Anomaly-Generation (DA-Gen)\nmodule designed to simulate various types of anomalies. Next, we present the\nCorrective Force Prediction Network (CFP-Net), which uses complementary\nrepresentations for point-level analysis to simulate the different\ncontributions from internal and external corrective forces. To ensure the\ncorrective forces are constrained effectively, we have developed a combined\nloss function that includes a new symmetric loss and an overall loss. Notably,\nwe implement a Hierarchical Quality Control (HQC) strategy based on a three-way\ndecision process and contribute a dataset titled Anomaly-IntraVariance, which\nincorporates intraclass variance to evaluate our model. As a result, the\nproposed MC4AD has been proven effective through theory and experimentation.\nThe experimental results demonstrate that our approach yields nine\nstate-of-the-art performances, achieving optimal results with minimal\nparameters and the fastest inference speed across five existing datasets, in\naddition to the proposed Anomaly-IntraVariance dataset. The source is available\nat https://github.com/hzzzzzhappy/MC4AD\n","authors":["Hanzhe Liang","Aoran Wang","Jie Zhou","Xin Jin","Can Gao","Jinbao Wang"],"pdf_url":"https://arxiv.org/pdf/2505.05901v2.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2505.10420v1","updated":"2025-05-15T15:37:51Z","published":"2025-05-15T15:37:51Z","title":"Learned Lightweight Smartphone ISP with Unpaired Data","summary":"  The Image Signal Processor (ISP) is a fundamental component in modern\nsmartphone cameras responsible for conversion of RAW sensor image data to RGB\nimages with a strong focus on perceptual quality. Recent work highlights the\npotential of deep learning approaches and their ability to capture details with\na quality increasingly close to that of professional cameras. A difficult and\ncostly step when developing a learned ISP is the acquisition of pixel-wise\naligned paired data that maps the raw captured by a smartphone camera sensor to\nhigh-quality reference images. In this work, we address this challenge by\nproposing a novel training method for a learnable ISP that eliminates the need\nfor direct correspondences between raw images and ground-truth data with\nmatching content. Our unpaired approach employs a multi-term loss function\nguided by adversarial training with multiple discriminators processing feature\nmaps from pre-trained networks to maintain content structure while learning\ncolor and texture characteristics from the target RGB dataset. Using\nlightweight neural network architectures suitable for mobile devices as\nbackbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm\nUltraISP datasets. Compared to paired training methods, our unpaired learning\nstrategy shows strong potential and achieves high fidelity across multiple\nevaluation metrics. The code and pre-trained models are available at\nhttps://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .\n","authors":["Andrei Arhire","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2505.10420v1.pdf","comment":"Accepted at CVPRW 2025"},{"id":"http://arxiv.org/abs/2411.05731v3","updated":"2025-05-15T15:32:07Z","published":"2024-11-08T17:42:02Z","title":"PEP-GS: Perceptually-Enhanced Precise Structured 3D Gaussians for\n  View-Adaptive Rendering","summary":"  Recently, 3D Gaussian Splatting (3D-GS) has achieved significant success in\nreal-time, high-quality 3D scene rendering. However, it faces several\nchallenges, including Gaussian redundancy, limited ability to capture\nview-dependent effects, and difficulties in handling complex lighting and\nspecular reflections. Additionally, methods that use spherical harmonics for\ncolor representation often struggle to effectively capture anisotropic\ncomponents, especially when modeling view-dependent colors under complex\nlighting conditions, leading to insufficient contrast and unnatural color\nsaturation. To address these limitations, we introduce PEP-GS, a\nperceptually-enhanced framework that dynamically predicts Gaussian attributes,\nincluding opacity, color, and covariance. We replace traditional spherical\nharmonics with a Hierarchical Granular-Structural Attention mechanism, which\nenables more accurate modeling of complex view-dependent color effects. By\nemploying a stable and interpretable framework for opacity and covariance\nestimation, PEP-GS avoids the removal of essential Gaussians prematurely,\nensuring a more accurate scene representation. Furthermore, perceptual\noptimization is applied to the final rendered images, enhancing perceptual\nconsistency across different views and ensuring high-quality renderings with\nimproved texture fidelity and fine-scale detail preservation. Experimental\nresults demonstrate that PEP-GS outperforms state-of-the-art methods,\nparticularly in challenging scenarios involving view-dependent effects and\nfine-scale details.\n","authors":["Junxi Jin","Xiulai Li","Haiping Huang","Lianjun Liu","Yujie Sun","Logan Liu"],"pdf_url":"https://arxiv.org/pdf/2411.05731v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10405v1","updated":"2025-05-15T15:28:32Z","published":"2025-05-15T15:28:32Z","title":"Visual Fidelity Index for Generative Semantic Communications with\n  Critical Information Embedding","summary":"  Generative semantic communication (Gen-SemCom) with large artificial\nintelligence (AI) model promises a transformative paradigm for 6G networks,\nwhich reduces communication costs by transmitting low-dimensional prompts\nrather than raw data. However, purely prompt-driven generation loses\nfine-grained visual details. Additionally, there is a lack of systematic\nmetrics to evaluate the performance of Gen-SemCom systems. To address these\nissues, we develop a hybrid Gen-SemCom system with a critical information\nembedding (CIE) framework, where both text prompts and semantically critical\nfeatures are extracted for transmissions. First, a novel approach of semantic\nfiltering is proposed to select and transmit the semantically critical features\nof images relevant to semantic label. By integrating the text prompt and\ncritical features, the receiver reconstructs high-fidelity images using a\ndiffusion-based generative model. Next, we propose the generative visual\ninformation fidelity (GVIF) metric to evaluate the visual quality of the\ngenerated image. By characterizing the statistical models of image features,\nthe GVIF metric quantifies the mutual information between the distorted\nfeatures and their original counterparts. By maximizing the GVIF metric, we\ndesign a channel-adaptive Gen-SemCom system that adaptively control the volume\nof features and compression rate according to the channel state. Experimental\nresults validate the GVIF metric's sensitivity to visual fidelity, correlating\nwith both the PSNR and critical information volume. In addition, the optimized\nsystem achieves superior performance over benchmarking schemes in terms of\nhigher PSNR and lower FID scores.\n","authors":["Jianhao Huang","Qunsong Zeng","Kaibin Huang"],"pdf_url":"https://arxiv.org/pdf/2505.10405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16147v2","updated":"2025-05-15T15:11:23Z","published":"2024-12-20T18:50:54Z","title":"SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage\n  Estimation in the Wild","summary":"  Seagrass meadows play a crucial role in marine ecosystems, providing benefits\nsuch as carbon sequestration, water quality improvement, and habitat provision.\nMonitoring the distribution and abundance of seagrass is essential for\nenvironmental impact assessments and conservation efforts. However, the current\nmanual methods of analyzing underwater video data to assess seagrass coverage\nare time-consuming and subjective. This work explores the use of deep learning\nmodels to automate the process of seagrass detection and coverage estimation\nfrom underwater video data. We create a new dataset of over 8,300 annotated\nunderwater images, and subsequently evaluate several deep learning\narchitectures, including ResNet, InceptionNetV3, DenseNet, and Vision\nTransformer for the task of binary classification on the presence and absence\nof seagrass by transfer learning. The results demonstrate that deep learning\nmodels, particularly Vision Transformers, can achieve high performance in\npredicting eelgrass presence, with AUROC scores exceeding 0.95 on the final\ntest dataset. The application of underwater image enhancement further improved\nthe models' prediction capabilities. Furthermore, we introduce a novel approach\nfor estimating seagrass coverage from video data, showing promising preliminary\nresults that align with expert manual labels, and indicating potential for\nconsistent and scalable monitoring. The proposed methodology allows for the\nefficient processing of large volumes of video data, enabling the acquisition\nof much more detailed information on seagrass distributions in comparison to\ncurrent manual methods. This information is crucial for environmental impact\nassessments and monitoring programs, as seagrasses are important indicators of\ncoastal ecosystem health. This project demonstrates the value that deep\nlearning can bring to the field of marine ecology and environmental monitoring.\n","authors":["Jannik Elsäßer","Laura Weihl","Veronika Cheplygina","Lisbeth Tangaa Nielsen"],"pdf_url":"https://arxiv.org/pdf/2412.16147v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04722v2","updated":"2025-05-15T15:09:00Z","published":"2025-04-07T04:21:31Z","title":"TactileNet: Bridging the Accessibility Gap with AI-Generated Tactile\n  Graphics for Individuals with Vision Impairment","summary":"  Tactile graphics are essential for providing access to visual information for\nthe 43 million people globally living with vision loss. Traditional methods for\ncreating these graphics are labor-intensive and cannot meet growing demand. We\nintroduce TactileNet, the first comprehensive dataset and AI-driven framework\nfor generating embossing-ready 2D tactile templates using text-to-image Stable\nDiffusion (SD) models. By integrating Low-Rank Adaptation (LoRA) and\nDreamBooth, our method fine-tunes SD models to produce high-fidelity,\nguideline-compliant graphics while reducing computational costs. Quantitative\nevaluations with tactile experts show 92.86% adherence to accessibility\nstandards. Structural fidelity analysis revealed near-human design similarity,\nwith an SSIM of 0.538 between generated graphics and expert-designed tactile\nimages. Notably, our method preserves object silhouettes better than human\ndesigns (SSIM = 0.259 vs. 0.215 for binary masks), addressing a key limitation\nof manual tactile abstraction. The framework scales to 32,000 images (7,050\nhigh-quality) across 66 classes, with prompt editing enabling customizable\noutputs (e.g., adding or removing details). By automating the 2D template\ngeneration step-compatible with standard embossing workflows-TactileNet\naccelerates production while preserving design flexibility. This work\ndemonstrates how AI can augment (not replace) human expertise to bridge the\naccessibility gap in education and beyond. Code, data, and models will be\npublicly released to foster further research.\n","authors":["Adnan Khan","Alireza Choubineh","Mai A. Shaaban","Abbas Akkasi","Majid Komeili"],"pdf_url":"https://arxiv.org/pdf/2504.04722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20291v2","updated":"2025-05-15T15:06:46Z","published":"2025-03-26T07:33:36Z","title":"CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at\n  Intermediate Resolution with Structure-Aware Multimodal U-Nets","summary":"  Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at\nintermediate resolution (4-8 {\\AA}) is crucial in protein structure\ndetermination. Recent advances in deep learning have led to the development of\nautomated approaches for enhancing experimental cryo-EM density maps. Yet,\nthese methods are not optimized for intermediate-resolution maps and rely on\nmap density features alone. To address this, we propose CryoSAMU, a novel\nmethod designed to enhance 3D cryo-EM density maps of protein structures using\nstructure-aware multimodal U-Nets and trained on curated\nintermediate-resolution density maps. We comprehensively evaluate CryoSAMU\nacross various metrics and demonstrate its competitive performance compared to\nstate-of-the-art methods. Notably, CryoSAMU achieves significantly faster\nprocessing speed, showing promise for future practical applications. Our code\nis available at https://github.com/chenwei-zhang/CryoSAMU.\n","authors":["Chenwei Zhang","Khanh Dao Duc"],"pdf_url":"https://arxiv.org/pdf/2503.20291v2.pdf","comment":"19 pages, 6 main figures, 2 supplementary figures, 3 main tables, 4\n  supplementary tables"},{"id":"http://arxiv.org/abs/2505.07119v2","updated":"2025-05-15T15:05:10Z","published":"2025-05-11T21:05:33Z","title":"Towards Scalable IoT Deployment for Visual Anomaly Detection via\n  Efficient Compression","summary":"  Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing operational costs is essential. Deploying deep learning models\nwithin Internet of Things (IoT) environments introduces specific challenges due\nto limited computational power and bandwidth of edge devices. This study\ninvestigates how to perform VAD effectively under such constraints by\nleveraging compact, efficient processing strategies. We evaluate several data\ncompression techniques, examining the tradeoff between system latency and\ndetection accuracy. Experiments on the MVTec AD benchmark demonstrate that\nsignificant compression can be achieved with minimal loss in anomaly detection\nperformance compared to uncompressed data. Current results show up to 80%\nreduction in end-to-end inference time, including edge processing,\ntransmission, and server computation.\n","authors":["Arianna Stropeni","Francesco Borsatti","Manuel Barusco","Davide Dalle Pezze","Marco Fabris","Gian Antonio Susto"],"pdf_url":"https://arxiv.org/pdf/2505.07119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08046v2","updated":"2025-05-15T15:00:25Z","published":"2025-04-10T18:04:22Z","title":"Teaching Humans Subtle Differences with DIFFusion","summary":"  Scientific expertise often requires recognizing subtle visual differences\nthat remain challenging to articulate even for domain experts. We present a\nsystem that leverages generative models to automatically discover and visualize\nminimal discriminative features between categories while preserving instance\nidentity. Our method generates counterfactual visualizations with subtle,\ntargeted transformations between classes, performing well even in domains where\ndata is sparse, examples are unpaired, and category boundaries resist verbal\ndescription. Experiments across six domains, including black hole simulations,\nbutterfly taxonomy, and medical imaging, demonstrate accurate transitions with\nlimited training data, highlighting both established discriminative features\nand novel subtle distinctions that measurably improved category\ndifferentiation. User studies confirm our generated counterfactuals\nsignificantly outperform traditional approaches in teaching humans to correctly\ndifferentiate between fine-grained classes, showing the potential of generative\nmodels to advance visual learning and scientific research.\n","authors":["Mia Chiquier","Orr Avrech","Yossi Gandelsman","Berthy Feng","Katherine Bouman","Carl Vondrick"],"pdf_url":"https://arxiv.org/pdf/2504.08046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09681v5","updated":"2025-05-15T14:51:10Z","published":"2023-03-16T22:56:12Z","title":"Highly Efficient 3D Human Pose Tracking from Events with Spiking\n  Spatiotemporal Transformer","summary":"  Event camera, as an asynchronous vision sensor capturing scene dynamics,\npresents new opportunities for highly efficient 3D human pose tracking.\nExisting approaches typically adopt modern-day Artificial Neural Networks\n(ANNs), such as CNNs or Transformer, where sparse events are converted into\ndense images or paired with additional gray-scale images as input. Such\npractices, however, ignore the inherent sparsity of events, resulting in\nredundant computations, increased energy consumption, and potentially degraded\nperformance. Motivated by these observations, we introduce the first sparse\nSpiking Neural Networks (SNNs) framework for 3D human pose tracking based\nsolely on events. Our approach eliminates the need to convert sparse data to\ndense formats or incorporate additional images, thereby fully exploiting the\ninnate sparsity of input events. Central to our framework is a novel Spiking\nSpatiotemporal Transformer, which enables bi-directional spatiotemporal fusion\nof spike pose features and provides a guaranteed similarity measurement between\nbinary spike features in spiking attention. Moreover, we have constructed a\nlarge-scale synthetic dataset, SynEventHPD, that features a broad and diverse\nset of 3D human motions, as well as much longer hours of event streams.\nEmpirical experiments demonstrate the superiority of our approach over existing\nstate-of-the-art (SOTA) ANN-based methods, requiring only 19.1% FLOPs and 3.6%\nenergy cost. Furthermore, our approach outperforms existing SNN-based\nbenchmarks in this task, highlighting the effectiveness of our proposed SNN\nframework. The dataset will be released upon acceptance, and code can be found\nat https://github.com/JimmyZou/HumanPoseTracking_SNN.\n","authors":["Shihao Zou","Yuxuan Mu","Wei Ji","Zi-An Wang","Xinxin Zuo","Sen Wang","Weixin Si","Li Cheng"],"pdf_url":"https://arxiv.org/pdf/2303.09681v5.pdf","comment":"Accepted by IEEE TCSVT"},{"id":"http://arxiv.org/abs/2504.13231v2","updated":"2025-05-15T14:47:02Z","published":"2025-04-17T14:43:56Z","title":"WildFireCan-MMD: A Multimodal Dataset for Classification of\n  User-Generated Content During Wildfires in Canada","summary":"  Rapid information access is vital during wildfires, yet traditional data\nsources are slow and costly. Social media offers real-time updates, but\nextracting relevant insights remains a challenge. We present WildFireCan-MMD, a\nnew multimodal dataset of X posts from recent Canadian wildfires, annotated\nacross twelve key themes. Evaluating both vision-language models and\ncustom-trained classifiers, we show that while zero-shot prompting offers quick\ndeployment, even simple trained models outperform them when labelled data is\navailable. Our best-performing transformer-based fine-tuned model reaches 83%\nf-score, outperforming gpt4 by 23%. As a use case, we demonstrate how this\nmodel can be used to uncover trends during wildfires. Our findings highlight\nthe enduring importance of tailored datasets and task-specific training.\nImportantly, such datasets should be localized, as disaster response\nrequirements vary across regions and contexts.\n","authors":["Braeden Sherritt","Isar Nejadgholi","Marzieh Amini"],"pdf_url":"https://arxiv.org/pdf/2504.13231v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10352v1","updated":"2025-05-15T14:43:35Z","published":"2025-05-15T14:43:35Z","title":"SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with\n  Hamming Attention and $\\mathcal{O}(T)$ Complexity","summary":"  Spiking Neural Networks (SNNs) have shown competitive performance to\nArtificial Neural Networks (ANNs) in various vision tasks, while offering\nsuperior energy efficiency. However, existing SNN-based Transformers primarily\nfocus on single-image tasks, emphasizing spatial features while not effectively\nleveraging SNNs' efficiency in video-based vision tasks. In this paper, we\nintroduce SpikeVideoFormer, an efficient spike-driven video Transformer,\nfeaturing linear temporal complexity $\\mathcal{O}(T)$. Specifically, we design\na spike-driven Hamming attention (SDHA) which provides a theoretically guided\nadaptation from traditional real-valued attention to spike-driven attention.\nBuilding on SDHA, we further analyze various spike-driven space-time attention\ndesigns and identify an optimal scheme that delivers appealing performance for\nvideo tasks, while maintaining only linear temporal complexity. The\ngeneralization ability and efficiency of our model are demonstrated across\ndiverse downstream video tasks, including classification, human pose tracking,\nand semantic segmentation. Empirical results show our method achieves\nstate-of-the-art (SOTA) performance compared to existing SNN approaches, with\nover 15\\% improvement on the latter two tasks. Additionally, it matches the\nperformance of recent ANN-based methods while offering significant efficiency\ngains, achieving $\\times 16$, $\\times 10$ and $\\times 5$ improvements on the\nthree tasks. https://github.com/JimmyZou/SpikeVideoFormer\n","authors":["Shihao Zou","Qingfeng Li","Wei Ji","Jingjing Li","Yongkui Yang","Guoqi Li","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2505.10352v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2505.10351v1","updated":"2025-05-15T14:43:34Z","published":"2025-05-15T14:43:34Z","title":"A Unified and Scalable Membership Inference Method for Visual\n  Self-supervised Encoder via Part-aware Capability","summary":"  Self-supervised learning shows promise in harnessing extensive unlabeled\ndata, but it also confronts significant privacy concerns, especially in vision.\nIn this paper, we perform membership inference on visual self-supervised models\nin a more realistic setting: self-supervised training method and details are\nunknown for an adversary when attacking as he usually faces a black-box system\nin practice. In this setting, considering that self-supervised model could be\ntrained by completely different self-supervised paradigms, e.g., masked image\nmodeling and contrastive learning, with complex training details, we propose a\nunified membership inference method called PartCrop. It is motivated by the\nshared part-aware capability among models and stronger part response on the\ntraining data. Specifically, PartCrop crops parts of objects in an image to\nquery responses within the image in representation space. We conduct extensive\nattacks on self-supervised models with different training protocols and\nstructures using three widely used image datasets. The results verify the\neffectiveness and generalization of PartCrop. Moreover, to defend against\nPartCrop, we evaluate two common approaches, i.e., early stop and differential\nprivacy, and propose a tailored method called shrinking crop scale range. The\ndefense experiments indicate that all of them are effective. Finally, besides\nprototype testing on toy visual encoders and small-scale image datasets, we\nquantitatively study the impacts of scaling from both data and model aspects in\na realistic scenario and propose a scalable PartCrop-v2 by introducing two\nstructural improvements to PartCrop. Our code is at\nhttps://github.com/JiePKU/PartCrop.\n","authors":["Jie Zhu","Jirong Zha","Ding Li","Leye Wang"],"pdf_url":"https://arxiv.org/pdf/2505.10351v1.pdf","comment":"An extension of our ACM CCS2024 conference paper (arXiv:2404.02462).\n  We show the impacts of scaling from both data and model aspects on membership\n  inference for self-supervised visual encoders"},{"id":"http://arxiv.org/abs/2405.03689v2","updated":"2025-05-15T14:35:43Z","published":"2024-05-06T17:59:36Z","title":"Pose Priors from Language Models","summary":"  Language is often used to describe physical interaction, yet most 3D human\npose estimation methods overlook this rich source of information. We bridge\nthis gap by leveraging large multimodal models (LMMs) as priors for\nreconstructing contact poses, offering a scalable alternative to traditional\nmethods that rely on human annotations or motion capture data. Our approach\nextracts contact-relevant descriptors from an LMM and translates them into\ntractable losses to constrain 3D human pose optimization. Despite its\nsimplicity, our method produces compelling reconstructions for both two-person\ninteractions and self-contact scenarios, accurately capturing the semantics of\nphysical and social interactions. Our results demonstrate that LMMs can serve\nas powerful tools for contact prediction and pose estimation, offering an\nalternative to costly manual human annotations or motion capture data. Our code\nis publicly available at https://prosepose.github.io.\n","authors":["Sanjay Subramanian","Evonne Ng","Lea Müller","Dan Klein","Shiry Ginosar","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2405.03689v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2307.09420v2","updated":"2025-05-15T14:30:03Z","published":"2023-07-18T16:37:37Z","title":"Measuring Student Behavioral Engagement using Histogram of Actions","summary":"  In this paper, we propose a novel technique for measuring behavioral\nengagement through students' actions recognition. The proposed approach\nrecognizes student actions then predicts the student behavioral engagement\nlevel. For student action recognition, we use human skeletons to model student\npostures and upper body movements. To learn the dynamics of student upper body,\na 3D-CNN model is used. The trained 3D-CNN model is used to recognize actions\nwithin every 2minute video segment then these actions are used to build a\nhistogram of actions which encodes the student actions and their frequencies.\nThis histogram is utilized as an input to SVM classifier to classify whether\nthe student is engaged or disengaged. To evaluate the proposed framework, we\nbuild a dataset consisting of 1414 2-minute video segments annotated with 13\nactions and 112 video segments annotated with two engagement levels.\nExperimental results indicate that student actions can be recognized with top 1\naccuracy 83.63% and the proposed framework can capture the average engagement\nof the class.\n","authors":["Ahmed Abdelkawy","Aly Farag","Islam Alkabbany","Asem Ali","Chris Foreman","Thomas Tretter","Nicholas Hindy"],"pdf_url":"https://arxiv.org/pdf/2307.09420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03093v2","updated":"2025-05-15T14:24:44Z","published":"2025-05-06T01:09:07Z","title":"Estimating the Diameter at Breast Height of Trees in a Forest With a\n  Single 360 Camera","summary":"  Forest inventories rely on accurate measurements of the diameter at breast\nheight (DBH) for ecological monitoring, resource management, and carbon\naccounting. While LiDAR-based techniques can achieve centimeter-level\nprecision, they are cost-prohibitive and operationally complex. We present a\nlow-cost alternative that only needs a consumer-grade 360 video camera. Our\nsemi-automated pipeline comprises of (i) a dense point cloud reconstruction\nusing Structure from Motion (SfM) photogrammetry software called Agisoft\nMetashape, (ii) semantic trunk segmentation by projecting Grounded Segment\nAnything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based\ntechnique to estimate cross section shape and DBH. We introduce an interactive\nvisualization tool for inspecting segmented trees and their estimated DBH. On\n61 acquisitions of 43 trees under a variety of conditions, our method attains\nmedian absolute relative errors of 5-9% with respect to \"ground-truth\" manual\nmeasurements. This is only 2-4% higher than LiDAR-based estimates, while\nemploying a single 360 camera that costs orders of magnitude less, requires\nminimal setup, and is widely available.\n","authors":["Siming He","Zachary Osman","Fernando Cladera","Dexter Ong","Nitant Rai","Patrick Corey Green","Vijay Kumar","Pratik Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2505.03093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10312v1","updated":"2025-05-15T13:56:14Z","published":"2025-05-15T13:56:14Z","title":"SOS: A Shuffle Order Strategy for Data Augmentation in Industrial Human\n  Activity Recognition","summary":"  In the realm of Human Activity Recognition (HAR), obtaining high quality and\nvariance data is still a persistent challenge due to high costs and the\ninherent variability of real-world activities. This study introduces a\ngeneration dataset by deep learning approaches (Attention Autoencoder and\nconditional Generative Adversarial Networks). Another problem that data\nheterogeneity is a critical challenge, one of the solutions is to shuffle the\ndata to homogenize the distribution. Experimental results demonstrate that the\nrandom sequence strategy significantly improves classification performance,\nachieving an accuracy of up to 0.70 $\\pm$ 0.03 and a macro F1 score of 0.64\n$\\pm$ 0.01. For that, disrupting temporal dependencies through random sequence\nreordering compels the model to focus on instantaneous recognition, thereby\nimproving robustness against activity transitions. This approach not only\nbroadens the effective training dataset but also offers promising avenues for\nenhancing HAR systems in complex, real-world scenarios.\n","authors":["Anh Tuan Ha","Hoang Khang Phan","Thai Minh Tien Ngo","Anh Phan Truong","Nhat Tan Le"],"pdf_url":"https://arxiv.org/pdf/2505.10312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10294v1","updated":"2025-05-15T13:42:48Z","published":"2025-05-15T13:42:48Z","title":"MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images\n  using ViT Foundation Models","summary":"  Histopathological analysis is a cornerstone of cancer diagnosis, with\nHematoxylin and Eosin (H&E) staining routinely acquired for every patient to\nvisualize cell morphology and tissue architecture. On the other hand, multiplex\nimmunofluorescence (mIF) enables more precise cell type identification via\nproteomic markers, but has yet to achieve widespread clinical adoption due to\ncost and logistical constraints. To bridge this gap, we introduce MIPHEI\n(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired\narchitecture that integrates state-of-the-art ViT foundation models as encoders\nto predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of\nmarkers spanning nuclear content, immune lineages (T cells, B cells, myeloid),\nepithelium, stroma, vasculature, and proliferation. We train our model using\nthe publicly available ORION dataset of restained H&E and mIF images from\ncolorectal cancer tissue, and validate it on two independent datasets. MIPHEI\nachieves accurate cell-type classification from H&E alone, with F1 scores of\n0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,\nsubstantially outperforming both a state-of-the-art baseline and a random\nclassifier for most markers. Our results indicate that our model effectively\ncaptures the complex relationships between nuclear morphologies in their tissue\ncontext, as visible in H&E images and molecular markers defining specific cell\ntypes. MIPHEI offers a promising step toward enabling cell-type-aware analysis\nof large-scale H&E datasets, in view of uncovering relationships between\nspatial cellular organization and patient outcomes.\n","authors":["Guillaume Balezo","Roger Trullo","Albert Pla Planas","Etienne Decenciere","Thomas Walter"],"pdf_url":"https://arxiv.org/pdf/2505.10294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10292v1","updated":"2025-05-15T13:42:14Z","published":"2025-05-15T13:42:14Z","title":"StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding\n  and Grounded Story Generation","summary":"  Visual storytelling systems struggle to maintain character identity across\nframes and link actions to appropriate subjects, frequently leading to\nreferential hallucinations. These issues can be addressed through grounding of\ncharacters, objects, and other entities on the visual elements. We propose\nStoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie\nimages, with both structured scene analyses and grounded stories. Each story\nmaintains character and object consistency across frames while explicitly\nmodeling multi-frame relationships through structured tabular representations.\nOur approach features cross-frame object re-identification using visual\nsimilarity and face recognition, chain-of-thought reasoning for explicit\nnarrative modeling, and a grounding scheme that links textual elements to\nvisual entities across multiple frames. We establish baseline performance by\nfine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end\nobject detection, re-identification, and landmark detection while maintaining\nconsistent object references throughout the story. Evaluation demonstrates a\nreduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when\ncompared to a non-fine-tuned model.\n","authors":["Daniel A. P. Oliveira","David Martins de Matos"],"pdf_url":"https://arxiv.org/pdf/2505.10292v1.pdf","comment":"31 pages, 14 figures"},{"id":"http://arxiv.org/abs/2505.10289v1","updated":"2025-05-15T13:36:42Z","published":"2025-05-15T13:36:42Z","title":"MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot\n  Learning","summary":"  Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object\ncombinations by leveraging known combinations. Existing studies basically rely\non the cross-modal alignment capabilities of CLIP but tend to overlook its\nlimitations in capturing fine-grained local features, which arise from its\narchitectural and training paradigm. To address this issue, we propose a\nMulti-Stage Cross-modal Interaction (MSCI) model that effectively explores and\nutilizes intermediate-layer information from CLIP's visual encoder.\nSpecifically, we design two self-adaptive aggregators to extract local\ninformation from low-level visual features and integrate global information\nfrom high-level visual features, respectively. These key information are\nprogressively incorporated into textual representations through a\nstage-by-stage interaction mechanism, significantly enhancing the model's\nperception capability for fine-grained local visual information. Additionally,\nMSCI dynamically adjusts the attention weights between global and local visual\ninformation based on different combinations, as well as different elements\nwithin the same combination, allowing it to flexibly adapt to diverse\nscenarios. Experiments on three widely used datasets fully validate the\neffectiveness and superiority of the proposed model. Data and code are\navailable at https://github.com/ltpwy/MSCI.\n","authors":["Yue Wang","Shuai Xu","Xuelin Zhu","Yicong Li"],"pdf_url":"https://arxiv.org/pdf/2505.10289v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.10281v1","updated":"2025-05-15T13:29:40Z","published":"2025-05-15T13:29:40Z","title":"MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global\n  Marine Fog Detection and Forecasting","summary":"  Deep learning approaches for marine fog detection and forecasting have\noutperformed traditional methods, demonstrating significant scientific and\npractical importance. However, the limited availability of open-source datasets\nremains a major challenge. Existing datasets, often focused on a single region\nor satellite, restrict the ability to evaluate model performance across diverse\nconditions and hinder the exploration of intrinsic marine fog characteristics.\nTo address these limitations, we introduce \\textbf{MFogHub}, the first\nmulti-regional and multi-satellite dataset to integrate annotated marine fog\nobservations from 15 coastal fog-prone regions and six geostationary\nsatellites, comprising over 68,000 high-resolution samples. By encompassing\ndiverse regions and satellite perspectives, MFogHub facilitates rigorous\nevaluation of both detection and forecasting methods under varying conditions.\nExtensive experiments with 16 baseline models demonstrate that MFogHub can\nreveal generalization fluctuations due to regional and satellite discrepancy,\nwhile also serving as a valuable resource for the development of targeted and\nscalable fog prediction techniques. Through MFogHub, we aim to advance both the\npractical monitoring and scientific understanding of marine fog dynamics on a\nglobal scale. The dataset and code are at\n\\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.\n","authors":["Mengqiu Xu","Kaixin Chen","Heng Guo","Yixiang Huang","Ming Wu","Zhenwei Shi","Chuang Zhang","Jun Guo"],"pdf_url":"https://arxiv.org/pdf/2505.10281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10271v1","updated":"2025-05-15T13:22:20Z","published":"2025-05-15T13:22:20Z","title":"RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall\n  Probabilities Over 8 Hours","summary":"  We present a deep learning model for high-resolution probabilistic\nprecipitation forecasting over an 8-hour horizon in Europe, overcoming the\nlimitations of radar-only deep learning models with short forecast lead times.\nOur model efficiently integrates multiple data sources - including radar,\nsatellite, and physics-based numerical weather prediction (NWP) - while\ncapturing long-range interactions, resulting in accurate forecasts with robust\nuncertainty quantification through consistent probabilistic maps. Featuring a\ncompact architecture, it enables more efficient training and faster inference\nthan existing models. Extensive experiments demonstrate that our model\nsurpasses current operational NWP systems, extrapolation-based methods, and\ndeep-learning nowcasting models, setting a new standard for high-resolution\nprecipitation forecasting in Europe, ensuring a balance between accuracy,\ninterpretability, and computational efficiency.\n","authors":["Rafael Pablos Sarabia","Joachim Nyborg","Morten Birk","Jeppe Liborius Sjørup","Anders Lillevang Vesterholt","Ira Assent"],"pdf_url":"https://arxiv.org/pdf/2505.10271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10267v1","updated":"2025-05-15T13:18:37Z","published":"2025-05-15T13:18:37Z","title":"HandReader: Advanced Techniques for Efficient Fingerspelling Recognition","summary":"  Fingerspelling is a significant component of Sign Language (SL), allowing the\ninterpretation of proper names, characterized by fast hand movements during\nsigning. Although previous works on fingerspelling recognition have focused on\nprocessing the temporal dimension of videos, there remains room for improving\nthe accuracy of these approaches. This paper introduces HandReader, a group of\nthree architectures designed to address the fingerspelling recognition task.\nHandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to\nprocess RGB features from videos of varying lengths while preserving important\nsequential information. HandReader$_{KP}$ is built on the proposed Temporal\nPose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition\nin a batch allows the encoder to pass them through 2D and 3D convolution\nlayers, utilizing temporal and spatial information and accumulating keypoints\ncoordinates. We also introduce HandReader_RGB+KP - architecture with a joint\nencoder to benefit from RGB and keypoint modalities. Each HandReader model\npossesses distinct advantages and achieves state-of-the-art results on the\nChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate\nhigh performance on the first open dataset for Russian fingerspelling, Znaki,\npresented in this paper. The Znaki dataset and HandReader pre-trained models\nare publicly available.\n","authors":["Pavel Korotaev","Petr Surovtsev","Alexander Kapitanov","Karina Kvanchiani","Aleksandr Nagaev"],"pdf_url":"https://arxiv.org/pdf/2505.10267v1.pdf","comment":"https://github.com/ai-forever/handreader"},{"id":"http://arxiv.org/abs/2505.10258v1","updated":"2025-05-15T13:09:19Z","published":"2025-05-15T13:09:19Z","title":"Inferring Driving Maps by Deep Learning-based Trail Map Extraction","summary":"  High-definition (HD) maps offer extensive and accurate environmental\ninformation about the driving scene, making them a crucial and essential\nelement for planning within autonomous driving systems. To avoid extensive\nefforts from manual labeling, methods for automating the map creation have\nemerged. Recent trends have moved from offline mapping to online mapping,\nensuring availability and actuality of the utilized maps. While the performance\nhas increased in recent years, online mapping still faces challenges regarding\ntemporal consistency, sensor occlusion, runtime, and generalization. We propose\na novel offline mapping approach that integrates trails - informal routes used\nby drivers - into the map creation process. Our method aggregates trail data\nfrom the ego vehicle and other traffic participants to construct a\ncomprehensive global map using transformer-based deep learning models. Unlike\ntraditional offline mapping, our approach enables continuous updates while\nremaining sensor-agnostic, facilitating efficient data transfer. Our method\ndemonstrates superior performance compared to state-of-the-art online mapping\napproaches, achieving improved generalization to previously unseen environments\nand sensor configurations. We validate our approach on two benchmark datasets,\nhighlighting its robustness and applicability in autonomous driving systems.\n","authors":["Michael Hubbertz","Pascal Colling","Qi Han","Tobias Meisen"],"pdf_url":"https://arxiv.org/pdf/2505.10258v1.pdf","comment":"This paper was accepted at the CVPR WAD 2025 Workshop"},{"id":"http://arxiv.org/abs/2505.10257v1","updated":"2025-05-15T13:08:44Z","published":"2025-05-15T13:08:44Z","title":"Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot","summary":"  The intelligent driving cockpit, an important part of intelligent driving,\nneeds to match different users' comfort, interaction, and safety needs. This\npaper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.\nSage Deer achieves three highlights: (1) Super alignment: It achieves different\nreactions according to different people's preferences and biases. (2)\nGeneralist: It can understand the multi-view and multi-mode inputs to reason\nthe user's physiological indicators, facial emotions, hand movements, body\nmovements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It\ncan elicit implicit thought chains in the language space to further increase\ngeneralist and super-aligned abilities. Besides, we collected multiple data\nsets and built a large-scale benchmark. This benchmark measures the deer's\nperceptual decision-making ability and the super alignment's accuracy.\n","authors":["Hao Lu","Jiaqi Tang","Jiyao Wang","Yunfan LU","Xu Cao","Qingyong Hu","Yin Wang","Yuting Zhang","Tianxin Xie","Yunpeng Zhang","Yong Chen","Jiayu. Gao","Bin Huang","Dengbo He","Shuiguang Deng","Hao Chen","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2505.10257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10250v1","updated":"2025-05-15T13:04:51Z","published":"2025-05-15T13:04:51Z","title":"ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct\n  Preference Optimization","summary":"  Human mesh recovery (HMR) from a single image is inherently ill-posed due to\ndepth ambiguity and occlusions. Probabilistic methods have tried to solve this\nby generating numerous plausible 3D human mesh predictions, but they often\nexhibit misalignment with 2D image observations and weak robustness to\nin-the-wild images. To address these issues, we propose ADHMR, a framework that\nAligns a Diffusion-based HMR model in a preference optimization manner. First,\nwe train a human mesh prediction assessment model, HMR-Scorer, capable of\nevaluating predictions even for in-the-wild images without 3D annotations. We\nthen use HMR-Scorer to create a preference dataset, where each input image has\na pair of winner and loser mesh predictions. This dataset is used to finetune\nthe base model using direct preference optimization. Moreover, HMR-Scorer also\nhelps improve existing HMR models by data cleaning, even with fewer training\nsamples. Extensive experiments show that ADHMR outperforms current\nstate-of-the-art methods. Code is available at:\nhttps://github.com/shenwenhao01/ADHMR.\n","authors":["Wenhao Shen","Wanqi Yin","Xiaofeng Yang","Cheng Chen","Chaoyue Song","Zhongang Cai","Lei Yang","Hao Wang","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2505.10250v1.pdf","comment":"Accepted by ICML 2025. Code: https://github.com/shenwenhao01/ADHMR"},{"id":"http://arxiv.org/abs/2410.01262v3","updated":"2025-05-15T12:59:09Z","published":"2024-10-02T06:16:06Z","title":"Improving Fine-Grained Control via Aggregation of Multiple Diffusion\n  Models","summary":"  While many diffusion models perform well when controlling for particular\naspect among style, character, and interaction, they struggle with fine-grained\ncontrol due to dataset limitations and intricate model architecture design.\nThis paper first introduces a novel training-free algorithm in fine-grained\ngeneration, Aggregation of Multiple Diffusion Models (AMDM), which integrates\nfeatures from multiple diffusion models into a specified model to activate\nspecific features and enable fine-grained control. Experimental results\ndemonstrate that AMDM significantly improves fine-grained control without\ntraining, validating its effectiveness. Additionally, it reveals that diffusion\nmodels initially focus on features such as position, attributes, and style,\nwith later stages improving generation quality and consistency. AMDM offers a\nnew perspective for tackling the challenges of fine-grained conditional control\ngeneration in diffusion models: We can fully utilize existing or develop new\nconditional diffusion models that control specific aspects, and then aggregate\nthem using AMDM algorithm. This eliminates the need for constructing complex\ndatasets, designing intricate model architectures, and incurring high training\ncosts. Code is available at: https://github.com/Hammour-steak/AMDM.\n","authors":["Conghan Yue","Zhengwei Peng","Shiyan Du","Zhi Ji","Chuangjian Cai","Le Wan","Dongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01262v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10238v1","updated":"2025-05-15T12:50:29Z","published":"2025-05-15T12:50:29Z","title":"MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation","summary":"  Human image animation has gained increasing attention and developed rapidly\ndue to its broad applications in digital humans. However, existing methods rely\nlargely on 2D-rendered pose images for motion guidance, which limits\ngeneralization and discards essential 3D information for open-world animation.\nTo tackle this problem, we propose MTVCrafter (Motion Tokenization Video\nCrafter), the first framework that directly models raw 3D motion sequences\n(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT\n(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.\nCompared to 2D-rendered pose images, 4D motion tokens offer more robust\nspatio-temporal cues and avoid strict pixel-level alignment between pose image\nand character, enabling more flexible and disentangled control. Then, we\nintroduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention\nwith 4D positional encodings, MV-DiT can effectively leverage motion tokens as\n4D compact yet expressive context for human image animation in the complex 3D\nworld. Hence, it marks a significant step forward in this field and opens a new\ndirection for pose-guided human video generation. Experiments show that our\nMTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,\nsurpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter\nalso generalizes well to diverse open-world characters (single/multiple,\nfull/half-body) across various styles and scenarios. Our video demos and code\nare provided in the supplementary material and at this anonymous GitHub link:\nhttps://anonymous.4open.science/r/MTVCrafter-1B13.\n","authors":["Yanbo Ding"],"pdf_url":"https://arxiv.org/pdf/2505.10238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09178v2","updated":"2025-05-15T12:49:27Z","published":"2025-05-14T06:21:27Z","title":"UniCAD: Efficient and Extendable Architecture for Multi-Task\n  Computer-Aided Diagnosis System","summary":"  The growing complexity and scale of visual model pre-training have made\ndeveloping and deploying multi-task computer-aided diagnosis (CAD) systems\nincreasingly challenging and resource-intensive. Furthermore, the medical\nimaging community lacks an open-source CAD platform to enable the rapid\ncreation of efficient and extendable diagnostic models. To address these\nissues, we propose UniCAD, a unified architecture that leverages the robust\ncapabilities of pre-trained vision foundation models to seamlessly handle both\n2D and 3D medical images while requiring only minimal task-specific parameters.\nUniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation\nstrategy is employed to adapt a pre-trained visual model to the medical image\ndomain, achieving performance on par with fully fine-tuned counterparts while\nintroducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular\narchitecture that combines a frozen foundation model with multiple\nplug-and-play experts, enabling diverse tasks and seamless functionality\nexpansion. Building on this unified CAD architecture, we establish an\nopen-source platform where researchers can share and access lightweight CAD\nexperts, fostering a more equitable and efficient research ecosystem.\nComprehensive experiments across 12 diverse medical datasets demonstrate that\nUniCAD consistently outperforms existing methods in both accuracy and\ndeployment efficiency. The source code and project page are available at\nhttps://mii-laboratory.github.io/UniCAD/.\n","authors":["Yitao Zhu","Yuan Yin","Zhenrong Shen","Zihao Zhao","Haiyu Song","Sheng Wang","Dinggang Shen","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2505.09178v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2505.10231v1","updated":"2025-05-15T12:43:23Z","published":"2025-05-15T12:43:23Z","title":"On the Interplay of Human-AI Alignment,Fairness, and Performance\n  Trade-offs in Medical Imaging","summary":"  Deep neural networks excel in medical imaging but remain prone to biases,\nleading to fairness gaps across demographic groups. We provide the first\nsystematic exploration of Human-AI alignment and fairness in this domain. Our\nresults show that incorporating human insights consistently reduces fairness\ngaps and enhances out-of-domain generalization, though excessive alignment can\nintroduce performance trade-offs, emphasizing the need for calibrated\nstrategies. These findings highlight Human-AI alignment as a promising approach\nfor developing fair, robust, and generalizable medical AI systems, striking a\nbalance between expert guidance and automated efficiency. Our code is available\nat https://github.com/Roypic/Aligner.\n","authors":["Haozhe Luo","Ziyu Zhou","Zixin Shu","Aurélie Pahud de Mortanges","Robert Berke","Mauricio Reyes"],"pdf_url":"https://arxiv.org/pdf/2505.10231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10223v1","updated":"2025-05-15T12:32:02Z","published":"2025-05-15T12:32:02Z","title":"Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution\n  Generalisation in MRI Segmentation","summary":"  Medical image segmentation models are often trained on curated datasets,\nleading to performance degradation when deployed in real-world clinical\nsettings due to mismatches between training and test distributions. While data\naugmentation techniques are widely used to address these challenges,\ntraditional visually consistent augmentation strategies lack the robustness\nneeded for diverse real-world scenarios. In this work, we systematically\nevaluate alternative augmentation strategies, focusing on MixUp and Auxiliary\nFourier Augmentation. These methods mitigate the effects of multiple variations\nwithout explicitly targeting specific sources of distribution shifts. We\ndemonstrate how these techniques significantly improve out-of-distribution\ngeneralization and robustness to imaging variations across a wide range of\ntransformations in cardiac cine MRI and prostate MRI segmentation. We\nquantitatively find that these augmentation methods enhance learned feature\nrepresentations by promoting separability and compactness. Additionally, we\nhighlight how their integration into nnU-Net training pipelines provides an\neasy-to-implement, effective solution for enhancing the reliability of medical\nsegmentation models in real-world applications.\n","authors":["Puru Vaish","Felix Meister","Tobias Heimann","Christoph Brune","Jelmer M. Wolterink"],"pdf_url":"https://arxiv.org/pdf/2505.10223v1.pdf","comment":"Accepted at MIDL 2025"},{"id":"http://arxiv.org/abs/2410.11758v2","updated":"2025-05-15T12:13:37Z","published":"2024-10-15T16:28:09Z","title":"Latent Action Pretraining from Videos","summary":"  We introduce Latent Action Pretraining for general Action models (LAPA), an\nunsupervised method for pretraining Vision-Language-Action (VLA) models without\nground-truth robot action labels. Existing Vision-Language-Action models\nrequire action labels typically collected by human teleoperators during\npretraining, which significantly limits possible data sources and scale. In\nthis work, we propose a method to learn from internet-scale videos that do not\nhave robot action labels. We first train an action quantization model\nleveraging VQ-VAE-based objective to learn discrete latent actions between\nimage frames, then pretrain a latent VLA model to predict these latent actions\nfrom observations and task descriptions, and finally finetune the VLA on\nsmall-scale robot manipulation data to map from latent to robot actions.\nExperimental results demonstrate that our method significantly outperforms\nexisting techniques that train robot manipulation policies from large-scale\nvideos. Furthermore, it outperforms the state-of-the-art VLA model trained with\nrobotic action labels on real-world manipulation tasks that require language\nconditioning, generalization to unseen objects, and semantic generalization to\nunseen instructions. Training only on human manipulation videos also shows\npositive transfer, opening up the potential for leveraging web-scale data for\nrobotics foundation model.\n","authors":["Seonghyeon Ye","Joel Jang","Byeongguk Jeon","Sejune Joo","Jianwei Yang","Baolin Peng","Ajay Mandlekar","Reuben Tan","Yu-Wei Chao","Bill Yuchen Lin","Lars Liden","Kimin Lee","Jianfeng Gao","Luke Zettlemoyer","Dieter Fox","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2410.11758v2.pdf","comment":"ICLR 2025 Website: https://latentactionpretraining.github.io"},{"id":"http://arxiv.org/abs/2505.10205v1","updated":"2025-05-15T12:03:05Z","published":"2025-05-15T12:03:05Z","title":"VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume\n  Estimation","summary":"  Accurate food volume estimation is crucial for medical nutrition management\nand health monitoring applications, but current food volume estimation methods\nare often limited by mononuclear data, leveraging single-purpose hardware such\nas 3D scanners, gathering sensor-oriented information such as depth\ninformation, or relying on camera calibration using a reference object. In this\npaper, we present VolE, a novel framework that leverages mobile device-driven\n3D reconstruction to estimate food volume. VolE captures images and camera\nlocations in free motion to generate precise 3D models, thanks to AR-capable\nmobile devices. To achieve real-world measurement, VolE is a reference- and\ndepth-free framework that leverages food video segmentation for food mask\ngeneration. We also introduce a new food dataset encompassing the challenging\nscenarios absent in the previous benchmarks. Our experiments demonstrate that\nVolE outperforms the existing volume estimation techniques across multiple\ndatasets by achieving 2.22 % MAPE, highlighting its superior performance in\nfood volume estimation.\n","authors":["Umair Haroon","Ahmad AlMughrabi","Thanasis Zoumpekas","Ricardo Marques","Petia Radeva"],"pdf_url":"https://arxiv.org/pdf/2505.10205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10169v1","updated":"2025-05-15T10:55:47Z","published":"2025-05-15T10:55:47Z","title":"Modeling Saliency Dataset Bias","summary":"  Recent advances in image-based saliency prediction are approaching gold\nstandard performance levels on existing benchmarks. Despite this success, we\nshow that predicting fixations across multiple saliency datasets remains\nchallenging due to dataset bias. We find a significant performance drop (around\n40%) when models trained on one dataset are applied to another. Surprisingly,\nincreasing dataset diversity does not resolve this inter-dataset gap, with\nclose to 60% attributed to dataset-specific biases. To address this remaining\ngeneralization gap, we propose a novel architecture extending a mostly\ndataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific\nparameters that govern interpretable mechanisms such as multi-scale structure,\ncenter bias, and fixation spread. Adapting only these parameters to new data\naccounts for more than 75% of the generalization gap, with a large fraction of\nthe improvement achieved with as few as 50 samples. Our model sets a new\nstate-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark\n(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from\nunrelated datasets, but with a substantial boost when adapting to the\nrespective training datasets. The model also provides valuable insights into\nspatial saliency properties, revealing complex multi-scale effects that combine\nboth absolute and relative sizes.\n","authors":["Matthias Kümmerer","Harneet Khanuja","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2505.10169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05957v2","updated":"2025-05-15T10:43:45Z","published":"2025-05-09T11:09:52Z","title":"Efficient Quantum Convolutional Neural Networks for Image\n  Classification: Overcoming Hardware Constraints","summary":"  While classical convolutional neural networks (CNNs) have revolutionized\nimage classification, the emergence of quantum computing presents new\nopportunities for enhancing neural network architectures. Quantum CNNs (QCNNs)\nleverage quantum mechanical properties and hold potential to outperform\nclassical approaches. However, their implementation on current noisy\nintermediate-scale quantum (NISQ) devices remains challenging due to hardware\nlimitations. In our research, we address this challenge by introducing an\nencoding scheme that significantly reduces the input dimensionality. We\ndemonstrate that a primitive QCNN architecture with 49 qubits is sufficient to\ndirectly process $28\\times 28$ pixel MNIST images, eliminating the need for\nclassical dimensionality reduction pre-processing. Additionally, we propose an\nautomated framework based on expressibility, entanglement, and complexity\ncharacteristics to identify the building blocks of QCNNs, parameterized quantum\ncircuits (PQCs). Our approach demonstrates advantages in accuracy and\nconvergence speed with a similar parameter count compared to both hybrid QCNNs\nand classical CNNs. We validated our experiments on IBM's Heron r2 quantum\nprocessor, achieving $96.08\\%$ classification accuracy, surpassing the\n$71.74\\%$ benchmark of traditional approaches under identical training\nconditions. These results represent one of the first implementations of image\nclassifications on real quantum hardware and validate the potential of quantum\ncomputing in this area.\n","authors":["Peter Röseler","Oliver Schaudt","Helmut Berg","Christian Bauckhage","Matthias Koch"],"pdf_url":"https://arxiv.org/pdf/2505.05957v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10152v1","updated":"2025-05-15T10:26:17Z","published":"2025-05-15T10:26:17Z","title":"Multi-Source Collaborative Style Augmentation and Domain-Invariant\n  Learning for Federated Domain Generalization","summary":"  Federated domain generalization aims to learn a generalizable model from\nmultiple decentralized source domains for deploying on the unseen target\ndomain. The style augmentation methods have achieved great progress on domain\ngeneralization. However, the existing style augmentation methods either explore\nthe data styles within isolated source domain or interpolate the style\ninformation across existing source domains under the data decentralization\nscenario, which leads to limited style space. To address this issue, we propose\na Multi-source Collaborative Style Augmentation and Domain-invariant learning\nmethod (MCSAD) for federated domain generalization. Specifically, we propose a\nmulti-source collaborative style augmentation module to generate data in the\nbroader style space. Furthermore, we conduct domain-invariant learning between\nthe original data and augmented data by cross-domain feature alignment within\nthe same class and classes relation ensemble distillation between different\nclasses to learn a domain-invariant model. By alternatively conducting\ncollaborative style augmentation and domain-invariant learning, the model can\ngeneralize well on unseen target domain. Extensive experiments on multiple\ndomain generalization datasets indicate that our method significantly\noutperforms the state-of-the-art federated domain generalization methods.\n","authors":["Yikang Wei"],"pdf_url":"https://arxiv.org/pdf/2505.10152v1.pdf","comment":"IJCAI 2025"},{"id":"http://arxiv.org/abs/2505.07344v2","updated":"2025-05-15T10:24:45Z","published":"2025-05-12T08:32:39Z","title":"Generative Pre-trained Autoregressive Diffusion Transformer","summary":"  In this work, we present GPDiT, a Generative Pre-trained Autoregressive\nDiffusion Transformer that unifies the strengths of diffusion and\nautoregressive modeling for long-range video synthesis, within a continuous\nlatent space. Instead of predicting discrete tokens, GPDiT autoregressively\npredicts future latent frames using a diffusion loss, enabling natural modeling\nof motion dynamics and semantic consistency across frames. This continuous\nautoregressive framework not only enhances generation quality but also endows\nthe model with representation capabilities. Additionally, we introduce a\nlightweight causal attention variant and a parameter-free rotation-based\ntime-conditioning mechanism, improving both the training and inference\nefficiency. Extensive experiments demonstrate that GPDiT achieves strong\nperformance in video generation quality, video representation ability, and\nfew-shot learning tasks, highlighting its potential as an effective framework\nfor video modeling in continuous space.\n","authors":["Yuan Zhang","Jiacheng Jiang","Guoqing Ma","Zhiying Lu","Haoyang Huang","Jianlong Yuan","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2505.07344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10144v1","updated":"2025-05-15T10:17:48Z","published":"2025-05-15T10:17:48Z","title":"VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality","summary":"  3D Gaussian Splatting (3DGS) has rapidly become a leading technique for\nnovel-view synthesis, providing exceptional performance through efficient\nsoftware-based GPU rasterization. Its versatility enables real-time\napplications, including on mobile and lower-powered devices. However, 3DGS\nfaces key challenges in virtual reality (VR): (1) temporal artifacts, such as\npopping during head movements, (2) projection-based distortions that result in\ndisturbing and view-inconsistent floaters, and (3) reduced framerates when\nrendering large numbers of Gaussians, falling below the critical threshold for\nVR. Compared to desktop environments, these issues are drastically amplified by\nlarge field-of-view, constant head movements, and high resolution of\nhead-mounted displays (HMDs). In this work, we introduce VRSplat: we combine\nand extend several recent advancements in 3DGS to address challenges of VR\nholistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal\nProjection can complement each other, by modifying the individual techniques\nand core 3DGS rasterizer. Additionally, we propose an efficient foveated\nrasterizer that handles focus and peripheral areas in a single GPU launch,\navoiding redundant computations and improving GPU utilization. Our method also\nincorporates a fine-tuning step that optimizes Gaussian parameters based on\nStopThePop depth evaluations and Optimal Projection. We validate our method\nthrough a controlled user study with 25 participants, showing a strong\npreference for VRSplat over other configurations of Mini-Splatting. VRSplat is\nthe first, systematically evaluated 3DGS approach capable of supporting modern\nVR applications, achieving 72+ FPS while eliminating popping and\nstereo-disrupting floaters.\n","authors":["Xuechang Tu","Lukas Radl","Michael Steiner","Markus Steinberger","Bernhard Kerbl","Fernando de la Torre"],"pdf_url":"https://arxiv.org/pdf/2505.10144v1.pdf","comment":"I3D'25 (PACMCGIT); Project Page: https://cekavis.site/VRSplat/"},{"id":"http://arxiv.org/abs/2401.14066v3","updated":"2025-05-15T10:04:37Z","published":"2024-01-25T10:42:09Z","title":"CreativeSynth: Cross-Art-Attention for Artistic Image Synthesis with\n  Multimodal Diffusion","summary":"  Although remarkable progress has been made in image style transfer, style is\njust one of the components of artistic paintings. Directly transferring\nextracted style features to natural images often results in outputs with\nobvious synthetic traces. This is because key painting attributes including\nlayout, perspective, shape, and semantics often cannot be conveyed and\nexpressed through style transfer. Large-scale pretrained text-to-image\ngeneration models have demonstrated their capability to synthesize a vast\namount of high-quality images. However, even with extensive textual\ndescriptions, it is challenging to fully express the unique visual properties\nand details of paintings. Moreover, generic models often disrupt the overall\nartistic effect when modifying specific areas, making it more complicated to\nachieve a unified aesthetic in artworks. Our main novel idea is to integrate\nmultimodal semantic information as a synthesis guide into artworks, rather than\ntransferring style to the real world. We also aim to reduce the disruption to\nthe harmony of artworks while simplifying the guidance conditions.\nSpecifically, we propose an innovative multi-task unified framework called\nCreativeSynth, based on the diffusion model with the ability to coordinate\nmultimodal inputs. CreativeSynth combines multimodal features with customized\nattention mechanisms to seamlessly integrate real-world semantic content into\nthe art domain through Cross-Art-Attention for aesthetic maintenance and\nsemantic fusion. We demonstrate the results of our method across a wide range\nof different art categories, proving that CreativeSynth bridges the gap between\ngenerative models and artistic expression. Code and results are available at\nhttps://github.com/haha-lisa/CreativeSynth.\n","authors":["Nisha Huang","Weiming Dong","Yuxin Zhang","Fan Tang","Ronghui Li","Chongyang Ma","Xiu Li","Tong-Yee Lee","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2401.14066v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10124v1","updated":"2025-05-15T09:51:05Z","published":"2025-05-15T09:51:05Z","title":"IMITATE: Image Registration with Context for unknown time frame recovery","summary":"  In this paper, we formulate a novel image registration formalism dedicated to\nthe estimation of unknown condition-related images, based on two or more known\nimages and their associated conditions. We show how to practically model this\nformalism by using a new conditional U-Net architecture, which fully takes into\naccount the conditional information and does not need any fixed image. Our\nformalism is then applied to image moving tumors for radiotherapy treatment at\ndifferent breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal\nregions. This driving application is particularly complex as it requires to\nstitch a collection of sequential 2D slices into several 3D volumes at\ndifferent organ positions. Movement interpolation with standard methods then\ngenerates well known reconstruction artefacts in the assembled volumes due to\nirregular patient breathing, hysteresis and poor correlation of breathing\nsignal to internal motion. Results obtained on 4D-CT clinical data showcase\nartefact-free volumes achieved through real-time latencies. The code is\npublicly available at https://github.com/Kheil-Z/IMITATE .\n","authors":["Ziad Kheil","Lucas Robinet","Laurent Risser","Soleakhena Ken"],"pdf_url":"https://arxiv.org/pdf/2505.10124v1.pdf","comment":"IEEE ISBI 2025"},{"id":"http://arxiv.org/abs/2504.02522v2","updated":"2025-05-15T09:48:06Z","published":"2025-04-03T12:19:04Z","title":"Charm: The Missing Piece in ViT fine-tuning for Image Aesthetic\n  Assessment","summary":"  The capacity of Vision transformers (ViTs) to handle variable-sized inputs is\noften constrained by computational complexity and batch processing limitations.\nConsequently, ViTs are typically trained on small, fixed-size images obtained\nthrough downscaling or cropping. While reducing computational burden, these\nmethods result in significant information loss, negatively affecting tasks like\nimage aesthetic assessment. We introduce Charm, a novel tokenization approach\nthat preserves Composition, High-resolution, Aspect Ratio, and Multi-scale\ninformation simultaneously. Charm prioritizes high-resolution details in\nspecific regions while downscaling others, enabling shorter fixed-size input\nsequences for ViTs while incorporating essential information. Charm is designed\nto be compatible with pre-trained ViTs and their learned positional embeddings.\nBy providing multiscale input and introducing variety to input tokens, Charm\nimproves ViT performance and generalizability for image aesthetic assessment.\nWe avoid cropping or changing the aspect ratio to further preserve information.\nExtensive experiments demonstrate significant performance improvements on\nvarious image aesthetic and quality assessment datasets (up to 8.1 %) using a\nlightweight ViT backbone. Code and pre-trained models are available at\nhttps://github.com/FBehrad/Charm.\n","authors":["Fatemeh Behrad","Tinne Tuytelaars","Johan Wagemans"],"pdf_url":"https://arxiv.org/pdf/2504.02522v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2505.10118v1","updated":"2025-05-15T09:43:28Z","published":"2025-05-15T09:43:28Z","title":"Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via\n  Multi-Objective Balanced Covering","summary":"  Existing visual token pruning methods target prompt alignment and visual\npreservation with static strategies, overlooking the varying relative\nimportance of these objectives across tasks, which leads to inconsistent\nperformance. To address this, we derive the first closed-form error bound for\nvisual token pruning based on the Hausdorff distance, uniformly characterizing\nthe contributions of both objectives. Moreover, leveraging $\\epsilon$-covering\ntheory, we reveal an intrinsic trade-off between these objectives and quantify\ntheir optimal attainment levels under a fixed budget. To practically handle\nthis trade-off, we propose Multi-Objective Balanced Covering (MoB), which\nreformulates visual token pruning as a bi-objective covering problem. In this\nframework, the attainment trade-off reduces to budget allocation via greedy\nradius trading. MoB offers a provable performance bound and linear scalability\nwith respect to the number of input visual tokens, enabling adaptation to\nchallenging pruning scenarios. Extensive experiments show that MoB preserves\n96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual\ntokens and accelerates LLaVA-Next-7B by 1.3-1.5$\\times$ with negligible\nperformance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm\nthat MoB integrates seamlessly into advanced MLLMs and diverse vision-language\ntasks.\n","authors":["Yangfu Li","Hongjian Zhan","Tianyi Chen","Qi Liu","Yue Lu"],"pdf_url":"https://arxiv.org/pdf/2505.10118v1.pdf","comment":"31 pages,9 figures,conference"},{"id":"http://arxiv.org/abs/2505.08889v2","updated":"2025-05-15T09:19:01Z","published":"2025-05-13T18:24:15Z","title":"IntrinsicEdit: Precise generative image manipulation in intrinsic space","summary":"  Generative diffusion models have advanced image editing with high-quality\nresults and intuitive interfaces such as prompts and semantic drawing. However,\nthese interfaces lack precise control, and the associated methods typically\nspecialize on a single editing task. We introduce a versatile, generative\nworkflow that operates in an intrinsic-image latent space, enabling semantic,\nlocal manipulation with pixel precision for a range of editing operations.\nBuilding atop the RGB-X diffusion framework, we address key challenges of\nidentity preservation and intrinsic-channel entanglement. By incorporating\nexact diffusion inversion and disentangled channel manipulation, we enable\nprecise, efficient editing with automatic resolution of global illumination\neffects -- all without additional data collection or model fine-tuning. We\ndemonstrate state-of-the-art performance across a variety of tasks on complex\nimages, including color and texture adjustments, object insertion and removal,\nglobal relighting, and their combinations.\n","authors":["Linjie Lyu","Valentin Deschaintre","Yannick Hold-Geoffroy","Miloš Hašan","Jae Shin Yoon","Thomas Leimkühler","Christian Theobalt","Iliyan Georgiev"],"pdf_url":"https://arxiv.org/pdf/2505.08889v2.pdf","comment":"SIGGRAPH 2025 Journal track"},{"id":"http://arxiv.org/abs/2403.07547v2","updated":"2025-05-15T08:57:01Z","published":"2024-03-12T11:32:57Z","title":"SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields","summary":"  Neural radiance fields (NeRF) has attracted considerable attention for their\nexceptional ability in synthesizing novel views with high fidelity. However,\nthe presence of motion blur, resulting from slight camera movements during\nextended shutter exposures, poses a significant challenge, potentially\ncompromising the quality of the reconstructed 3D scenes. To effectively handle\nthis issue, we propose sequential motion understanding radiance fields (SMURF),\na novel approach that models continuous camera motion and leverages the\nexplicit volumetric representation method for robustness to motion-blurred\ninput images. The core idea of the SMURF is continuous motion blurring kernel\n(CMBK), a module designed to model a continuous camera movements for processing\nblurry inputs. Our model is evaluated against benchmark datasets and\ndemonstrates state-of-the-art performance both quantitatively and\nqualitatively.\n","authors":["Jungho Lee","Dogyoon Lee","Minhyeok Lee","Donghyung Kim","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.07547v2.pdf","comment":"CVPRW 2025, Neural Fields Beyond Conventional Cameras, Project Page:\n  https://jho-yonsei.github.io/SMURF/"},{"id":"http://arxiv.org/abs/2505.10088v1","updated":"2025-05-15T08:43:53Z","published":"2025-05-15T08:43:53Z","title":"MMRL++: Parameter-Efficient and Interaction-Aware Representation\n  Learning for Vision-Language Models","summary":"  Large-scale pre-trained Vision-Language Models (VLMs) have significantly\nadvanced transfer learning across diverse tasks. However, adapting these models\nwith limited few-shot data often leads to overfitting, undermining their\nability to generalize to new tasks. To address this, we propose Multi-Modal\nRepresentation Learning (MMRL), which introduces a shared, learnable,\nmodality-agnostic representation space. MMRL generates space tokens projected\ninto both text and image encoders as representation tokens, enabling more\neffective cross-modal interactions. Unlike prior methods that mainly optimize\nclass token features, MMRL inserts representation tokens into higher encoder\nlayers--where task-specific features are more prominent--while preserving\ngeneral knowledge in the lower layers. During training, both class and\nrepresentation features are jointly optimized: a trainable projection layer is\napplied to representation tokens for task adaptation, while the projection\nlayer for class token remains frozen to retain pre-trained knowledge. To\nfurther promote generalization, we introduce a regularization term aligning\nclass and text features with the frozen VLM's zero-shot features. At inference,\na decoupling strategy uses both class and representation features for base\ntasks, but only class features for novel tasks due to their stronger\ngeneralization. Building upon this, we propose MMRL++, a parameter-efficient\nand interaction-aware extension that significantly reduces trainable parameters\nand enhances intra-modal interactions--particularly across the layers of\nrepresentation tokens--allowing gradient sharing and instance-specific\ninformation to propagate more effectively through the network. Extensive\nexperiments on 15 datasets demonstrate that MMRL and MMRL++ consistently\noutperform state-of-the-art methods, achieving a strong balance between\ntask-specific adaptation and generalization.\n","authors":["Yuncheng Guo","Xiaodong Gu"],"pdf_url":"https://arxiv.org/pdf/2505.10088v1.pdf","comment":"Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract appearing here is slightly shorter than that in the\n  PDF file"},{"id":"http://arxiv.org/abs/2505.10075v1","updated":"2025-05-15T08:27:16Z","published":"2025-05-15T08:27:16Z","title":"FlowDreamer: A RGB-D World Model with Flow-based Motion Representations\n  for Robot Manipulation","summary":"  This paper investigates training better visual world models for robot\nmanipulation, i.e., models that can predict future visual observations by\nconditioning on past frames and robot actions. Specifically, we consider world\nmodels that operate on RGB-D frames (RGB-D world models). As opposed to\ncanonical approaches that handle dynamics prediction mostly implicitly and\nreconcile it with visual rendering in a single model, we introduce FlowDreamer,\nwhich adopts 3D scene flow as explicit motion representations. FlowDreamer\nfirst predicts 3D scene flow from past frame and action conditions with a\nU-Net, and then a diffusion model will predict the future frame utilizing the\nscene flow. FlowDreamer is trained end-to-end despite its modularized nature.\nWe conduct experiments on 4 different benchmarks, covering both video\nprediction and visual planning tasks. The results demonstrate that FlowDreamer\nachieves better performance compared to other baseline RGB-D world models by 7%\non semantic similarity, 11% on pixel quality, and 6% on success rate in various\nrobot manipulation domains.\n","authors":["Jun Guo","Xiaojian Ma","Yikai Wang","Min Yang","Huaping Liu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2505.10075v1.pdf","comment":"Project page: see https://sharinka0715.github.io/FlowDreamer/"},{"id":"http://arxiv.org/abs/2502.06607v3","updated":"2025-05-15T08:22:44Z","published":"2025-02-10T16:04:54Z","title":"Illegal Waste Detection in Remote Sensing Images: A Case Study","summary":"  Environmental crime is the third largest criminal activity worldwide, with\nsignificant revenues coming from illegal management of solid waste. Thanks to\nthe increasing availability and the decreasing cost of Very High Resolution\nRemote Sensing (VHR RS) images, the fight against environmental crime can\nnowadays rely on modern image-analysis tools to support photo-interpretation\nfor scanning vast territories in search of illegal waste disposal sites. This\npaper illustrates a semi-automatic waste detection pipeline, developed in\ncollaboration with a regional environmental protection agency, for detecting\ncandidate illegal dumping sites in VHR RS images. To optimize the effectiveness\nof the waste detector, extensive experiments evaluate such design choices as\nthe network architecture, the ground resolution and geographic span of the\ninput images, as well as the pretraining procedures. The best model attains\nremarkable performance, achieving 92.02% F1-Score and 94.56% Accuracy. A\ngeneralization study assesses the performance variation when the detector\nprocesses images from a territory substantially different from the one used\nduring training, incurring only a moderate performance loss, i.e., 6.5%\ndecrease in the F1-Score. Finally, an exercise in which photo interpreters\ncompare the territory scanning effort with and without the support of the waste\ndetector assesses the concrete benefit of using a computer-aided image analysis\ntool in a professional environment protection agency. Results show that a\nreduction up to 30% of the time spent for waste site detection can be attained.\n","authors":["Federico Gibellini","Piero Fraternali","Giacomo Boracchi","Luca Morandini","Thomas Martinoli","Andrea Diecidue","Simona Malegori"],"pdf_url":"https://arxiv.org/pdf/2502.06607v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08605v2","updated":"2025-05-15T08:19:15Z","published":"2025-05-13T14:20:11Z","title":"Leveraging Multi-Modal Information to Enhance Dataset Distillation","summary":"  Dataset distillation aims to create a compact and highly representative\nsynthetic dataset that preserves the knowledge of a larger real dataset. While\nexisting methods primarily focus on optimizing visual representations,\nincorporating additional modalities and refining object-level information can\nsignificantly improve the quality of distilled datasets. In this work, we\nintroduce two key enhancements to dataset distillation: caption-guided\nsupervision and object-centric masking. To integrate textual information, we\npropose two strategies for leveraging caption features: the feature\nconcatenation, where caption embeddings are fused with visual features at the\nclassification stage, and caption matching, which introduces a caption-based\nalignment loss during training to ensure semantic coherence between real and\nsynthetic data. Additionally, we apply segmentation masks to isolate target\nobjects and remove background distractions, introducing two loss functions\ndesigned for object-centric learning: masked feature alignment loss and masked\ngradient matching loss. Comprehensive evaluations demonstrate that integrating\ncaption-based guidance and object-centric masking enhances dataset\ndistillation, leading to synthetic datasets that achieve superior performance\non downstream tasks.\n","authors":["Zhe Li","Hadrien Reynaud","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2505.08605v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2411.16782v2","updated":"2025-05-15T08:18:43Z","published":"2024-11-25T08:14:37Z","title":"Scaling Laws for Black box Adversarial Attacks","summary":"  Adversarial examples usually exhibit good cross-model transferability,\nenabling attacks on black-box models with limited information about their\narchitectures and parameters, which are highly threatening in commercial\nblack-box scenarios. Model ensembling is an effective strategy to improve the\ntransferability of adversarial examples by attacking multiple surrogate models.\nHowever, since prior studies usually adopt few models in the ensemble, there\nremains an open question of whether scaling the number of models can further\nimprove black-box attacks. Inspired by the scaling law of large foundation\nmodels, we investigate the scaling laws of black-box adversarial attacks in\nthis work. Through theoretical analysis and empirical evaluations, we conclude\nwith clear scaling laws that using more surrogate models enhances adversarial\ntransferability. Comprehensive experiments verify the claims on standard image\nclassifiers, diverse defended models and multimodal large language models using\nvarious adversarial attack methods. Specifically, by scaling law, we achieve\n90%+ transfer attack success rate on even proprietary models like GPT-4o.\nFurther visualization indicates that there is also a scaling law on the\ninterpretability and semantics of adversarial perturbations.\n","authors":["Chuan Liu","Huanran Chen","Yichi Zhang","Yinpeng Dong","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.16782v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10072v1","updated":"2025-05-15T08:16:12Z","published":"2025-05-15T08:16:12Z","title":"ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head\n  Avatars","summary":"  The introduction of 3D Gaussian blendshapes has enabled the real-time\nreconstruction of animatable head avatars from monocular video. Toonify, a\nStyleGAN-based framework, has become widely used for facial image stylization.\nTo extend Toonify for synthesizing diverse stylized 3D head avatars using\nGaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.\nIn Stage 1 (stylized video generation), we employ an improved StyleGAN to\ngenerate the stylized video from the input video frames, which addresses the\nlimitation of cropping aligned faces at a fixed resolution as preprocessing for\nnormal StyleGAN. This process provides a more stable video, which enables\nGaussian blendshapes to better capture the high-frequency details of the video\nframes, and efficiently generate high-quality animation in the next stage. In\nStage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head\nmodel and a set of expression blendshapes from the generated video. By\ncombining the neutral head model with expression blendshapes, ToonifyGB can\nefficiently render stylized avatars with arbitrary expressions. We validate the\neffectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane\nand Pixar.\n","authors":["Rui-Yang Ju","Sheng-Yen Huang","Yi-Ping Hung"],"pdf_url":"https://arxiv.org/pdf/2505.10072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10055v1","updated":"2025-05-15T07:58:38Z","published":"2025-05-15T07:58:38Z","title":"PsOCR: Benchmarking Large Multimodal Models for Optical Character\n  Recognition in Low-resource Pashto Language","summary":"  This paper evaluates the performance of Large Multimodal Models (LMMs) on\nOptical Character Recognition (OCR) in the low-resource Pashto language.\nNatural Language Processing (NLP) in Pashto faces several challenges due to the\ncursive nature of its script and a scarcity of structured datasets. To address\nthis, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one\nmillion images annotated with bounding boxes at word, line, and document\nlevels, suitable for training and evaluating models based on different\narchitectures, including Convolutional Neural Networks (CNNs) and Transformers.\nPsOCR covers variations across 1,000 unique font families, colors, image sizes,\nand layouts. A benchmark subset of 10K images was selected to evaluate the\nperformance of several LMMs, including seven open-source models: DeepSeek's\nJanus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four\nclosed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results\ndemonstrate that Gemini achieves the best performance among all models, whereas\namong open-source models, Qwen-7B stands out. This work provides an insightful\nassessment of the capabilities and limitations of current LMMs for OCR tasks in\nPashto and establishes a foundation for further research not only in Pashto OCR\nbut also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is\navailable at https://github.com/zirak-ai/PashtoOCR.\n","authors":["Ijazul Haq","Yingjie Zhang","Irfan Ali Khan"],"pdf_url":"https://arxiv.org/pdf/2505.10055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16028v2","updated":"2025-05-15T07:55:04Z","published":"2024-12-20T16:25:20Z","title":"CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from\n  Defocused Images","summary":"  3D Gaussian Splatting (3DGS) has attracted significant attention for its\nhigh-quality novel view rendering, inspiring research to address real-world\nchallenges. While conventional methods depend on sharp images for accurate\nscene reconstruction, real-world scenarios are often affected by defocus blur\ndue to finite depth of field, making it essential to account for realistic 3D\nscene representation. In this study, we propose CoCoGaussian, a Circle of\nConfusion-aware Gaussian Splatting that enables precise 3D scene representation\nusing only defocused images. CoCoGaussian addresses the challenge of defocus\nblur by modeling the Circle of Confusion (CoC) through a physically grounded\napproach based on the principles of photographic defocus. Exploiting 3D\nGaussians, we compute the CoC diameter from depth and learnable aperture\ninformation, generating multiple Gaussians to precisely capture the CoC shape.\nFurthermore, we introduce a learnable scaling factor to enhance robustness and\nprovide more flexibility in handling unreliable depth in scenes with reflective\nor refractive surfaces. Experiments on both synthetic and real-world datasets\ndemonstrate that CoCoGaussian achieves state-of-the-art performance across\nmultiple benchmarks.\n","authors":["Jungho Lee","Suhwan Cho","Taeoh Kim","Ho-Deok Jang","Minhyeok Lee","Geonho Cha","Dongyoon Wee","Dogyoon Lee","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2412.16028v2.pdf","comment":"CVPR 2025, Project Page: https://Jho-Yonsei.github.io/CoCoGaussian/"},{"id":"http://arxiv.org/abs/2504.08353v2","updated":"2025-05-15T07:51:50Z","published":"2025-04-11T08:39:18Z","title":"Single View Garment Reconstruction Using Diffusion Mapping Via Pattern\n  Coordinates","summary":"  Reconstructing 3D clothed humans from images is fundamental to applications\nlike virtual try-on, avatar creation, and mixed reality. While recent advances\nhave enhanced human body recovery, accurate reconstruction of garment geometry\n-- especially for loose-fitting clothing -- remains an open challenge. We\npresent a novel method for high-fidelity 3D garment reconstruction from single\nimages that bridges 2D and 3D representations. Our approach combines Implicit\nSewing Patterns (ISP) with a generative diffusion model to learn rich garment\nshape priors in a 2D UV space. A key innovation is our mapping model that\nestablishes correspondences between 2D image pixels, UV pattern coordinates,\nand 3D geometry, enabling joint optimization of both 3D garment meshes and the\ncorresponding 2D patterns by aligning learned priors with image observations.\nDespite training exclusively on synthetically simulated cloth data, our method\ngeneralizes effectively to real-world images, outperforming existing approaches\non both tight- and loose-fitting garments. The reconstructed garments maintain\nphysical plausibility while capturing fine geometric details, enabling\ndownstream applications including garment retargeting and texture manipulation.\n","authors":["Ren Li","Cong Cao","Corentin Dumery","Yingxuan You","Hao Li","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2504.08353v2.pdf","comment":"SIGGRAPH 2025"},{"id":"http://arxiv.org/abs/2505.10049v1","updated":"2025-05-15T07:51:08Z","published":"2025-05-15T07:51:08Z","title":"Advances in Radiance Field for Dynamic Scene: From Neural Field to\n  Gaussian Field","summary":"  Dynamic scene representation and reconstruction have undergone transformative\nadvances in recent years, catalyzed by breakthroughs in neural radiance fields\nand 3D Gaussian splatting techniques. While initially developed for static\nenvironments, these methodologies have rapidly evolved to address the\ncomplexities inherent in 4D dynamic scenes through an expansive body of\nresearch. Coupled with innovations in differentiable volumetric rendering,\nthese approaches have significantly enhanced the quality of motion\nrepresentation and dynamic scene reconstruction, thereby garnering substantial\nattention from the computer vision and graphics communities. This survey\npresents a systematic analysis of over 200 papers focused on dynamic scene\nrepresentation using radiance field, spanning the spectrum from implicit neural\nrepresentations to explicit Gaussian primitives. We categorize and evaluate\nthese works through multiple critical lenses: motion representation paradigms,\nreconstruction techniques for varied scene dynamics, auxiliary information\nintegration strategies, and regularization approaches that ensure temporal\nconsistency and physical plausibility. We organize diverse methodological\napproaches under a unified representational framework, concluding with a\ncritical examination of persistent challenges and promising research\ndirections. By providing this comprehensive overview, we aim to establish a\ndefinitive reference for researchers entering this rapidly evolving field while\noffering experienced practitioners a systematic understanding of both\nconceptual principles and practical frontiers in dynamic scene reconstruction.\n","authors":["Jinlong Fan","Xuepu Zeng","Jing Zhang","Mingming Gong","Yuxiang Yang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2505.10049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10046v1","updated":"2025-05-15T07:43:23Z","published":"2025-05-15T07:43:23Z","title":"Exploring the Deep Fusion of Large Language Models and Diffusion\n  Transformers for Text-to-Image Synthesis","summary":"  This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation.\n","authors":["Bingda Tang","Boyang Zheng","Xichen Pan","Sayak Paul","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2505.10046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21776v3","updated":"2025-05-15T07:28:30Z","published":"2025-03-27T17:59:51Z","title":"Video-R1: Reinforcing Video Reasoning in MLLMs","summary":"  Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\nattempt to systematically explore the R1 paradigm for incentivizing video\nreasoning within multimodal large language models (MLLMs). However, directly\napplying RL training with the GRPO algorithm to video reasoning presents two\nprimary challenges: (i) a lack of temporal modeling for video reasoning, and\n(ii) the scarcity of high-quality video-reasoning data. To address these\nissues, we first propose the T-GRPO algorithm, which encourages models to\nutilize temporal information in videos for reasoning. Additionally, instead of\nrelying solely on video data, we incorporate high-quality image-reasoning data\ninto the training process. We have constructed two datasets: Video-R1-CoT-165k\nfor SFT cold start and Video-R1-260k for RL training, both comprising image and\nvideo data. Experimental results demonstrate that Video-R1 achieves significant\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\nwell as on general video benchmarks including MVBench and TempCompass, etc.\nNotably, Video-R1-7B attains a 37.1% accuracy on video spatial reasoning\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\ncode, models, and data are released in: https://github.com/tulerfeng/Video-R1.\n","authors":["Kaituo Feng","Kaixiong Gong","Bohao Li","Zonghao Guo","Yibing Wang","Tianshuo Peng","Junfei Wu","Xiaoying Zhang","Benyou Wang","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2503.21776v3.pdf","comment":"Project page: https://github.com/tulerfeng/Video-R1"},{"id":"http://arxiv.org/abs/2505.10030v1","updated":"2025-05-15T07:25:43Z","published":"2025-05-15T07:25:43Z","title":"DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection\n  of Diseases in Cocos nucifera","summary":"  Coconut tree diseases are a serious risk to agricultural yield, particularly\nin developing countries where conventional farming practices restrict early\ndiagnosis and intervention. Current disease identification methods are manual,\nlabor-intensive, and non-scalable. In response to these limitations, we come up\nwith DeepSeqCoco, a deep learning based model for accurate and automatic\ndisease identification from coconut tree images. The model was tested under\nvarious optimizer settings, such as SGD, Adam, and hybrid configurations, to\nidentify the optimal balance between accuracy, minimization of loss, and\ncomputational cost. Results from experiments indicate that DeepSeqCoco can\nachieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than\nexisting models) with the hybrid SGD-Adam showing the lowest validation loss of\n2.81%. It also shows a drop of up to 18% in training time and up to 85% in\nprediction time for input images. The results point out the promise of the\nmodel to improve precision agriculture through an AI-based, scalable, and\nefficient disease monitoring system.\n","authors":["Miit Daga","Dhriti Parikh","Swarna Priya Ramu"],"pdf_url":"https://arxiv.org/pdf/2505.10030v1.pdf","comment":"This paper is accepted for publication in IEEE Access journal and is\n  currently pending revisions before publication"},{"id":"http://arxiv.org/abs/2505.03186v2","updated":"2025-05-15T07:21:04Z","published":"2025-05-06T05:07:11Z","title":"CoGenAV: Versatile Audio-Visual Representation Learning via\n  Contrastive-Generative Synchronization","summary":"  The inherent synchronization between a speaker's lip movements, voice, and\nthe underlying linguistic content offers a rich source of information for\nimproving speech processing tasks, especially in challenging conditions where\ntraditional audio-only systems falter. We introduce CoGenAV, a powerful and\ndata-efficient model designed to learn versatile audio-visual representations\napplicable across a wide range of speech and audio-visual tasks. CoGenAV is\ntrained by optimizing a dual objective derived from natural audio-visual\nsynchrony, contrastive feature alignment and generative text prediction, using\nonly 223 hours of labeled data from the LRS2 dataset. This\ncontrastive-generative synchronization strategy effectively captures\nfundamental cross-modal correlations. We showcase the effectiveness and\nversatility of the learned CoGenAV representations on multiple benchmarks. When\nutilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these\nrepresentations contribute to achieving a state-of-the-art Word Error Rate\n(WER) of 1.27. They also enable strong performance in Visual Speech Recognition\n(VSR) with a WER of 20.5 on LRS2, and significantly improve performance in\nnoisy environments by over 70%. Furthermore, CoGenAV representations benefit\nspeech reconstruction tasks, boosting performance in Speech Enhancement and\nSeparation, and achieve competitive results in audio-visual synchronization\ntasks like Active Speaker Detection (ASD). Our model will be open-sourced to\nfacilitate further development and collaboration within both academia and\nindustry.\n","authors":["Detao Bai","Zhiheng Ma","Xihan Wei","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2505.03186v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10027v1","updated":"2025-05-15T07:17:03Z","published":"2025-05-15T07:17:03Z","title":"ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model\n  Super-Resolution Reconstruction","summary":"  With the rapid advancement of remote sensing technology, super-resolution\nimage reconstruction is of great research and practical significance. Existing\ndeep learning methods have made progress but still face limitations in handling\ncomplex scenes and preserving image details. This paper proposes a\nreinforcement learning-based latent diffusion model (LDM) fine-tuning method\nfor remote sensing image super-resolution. The method constructs a\nreinforcement learning environment with states, actions, and rewards,\noptimizing decision objectives through proximal policy optimization (PPO)\nduring the reverse denoising process of the LDM model. Experiments on the\nRESISC45 dataset show significant improvements over the baseline model in PSNR,\nSSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,\nand LPIPS reducing by 0.06-0.10, particularly in structured and complex natural\nscenes. The results demonstrate the method's effectiveness in enhancing\nsuper-resolution quality and adaptability across scenes.\n","authors":["Shijie Lyu"],"pdf_url":"https://arxiv.org/pdf/2505.10027v1.pdf","comment":"Accepted by the 4th International Conference on Computing Innovation\n  and Applied Physics (CONF-CIAP 2025), and will be published in EAI Community\n  Research Series-CORE or Theoretical and Natural Science (TNS)"},{"id":"http://arxiv.org/abs/2502.19159v3","updated":"2025-05-15T07:04:35Z","published":"2025-02-26T14:15:24Z","title":"A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs","summary":"  Compared to width-wise pruning, depth-wise pruning can significantly\naccelerate inference in resource-constrained scenarios. However, treating the\nentire Transformer layer as the minimum pruning unit may degrade model\nperformance by indiscriminately discarding the entire information of the layer.\nThis paper reveals the ``Patch-like'' feature relationship between layers in\nlarge language models by analyzing the correlation of the outputs of different\nlayers in the reproducing kernel Hilbert space. Building on this observation,\nwe propose a sliding layer merging method that dynamically selects and fuses\nconsecutive layers from top to bottom according to a pre-defined similarity\nthreshold, thereby simplifying the model structure while maintaining its\nperformance. Extensive experiments on LLMs with various architectures and\ndifferent parameter scales show that our method outperforms existing pruning\ntechniques in both zero-shot inference performance and retraining recovery\nquality after pruning. In particular, in the experiment with 35% pruning on the\nVicuna-7B model, our method achieved a 1.654% improvement in average\nperformance on zero-shot tasks compared to the existing method. Moreover, we\nfurther reveal the potential of combining depth pruning with width pruning to\nenhance the pruning effect. Our codes are available at\nhttps://github.com/920927/SLM-a-sliding-layer-merging-method.\n","authors":["Xuan Ding","Rui Sun","Yunjian Zhang","Xiu Yan","Yueqi Zhou","Kaihao Huang","Suzhong Fu","Angelica I Aviles-Rivero","Chuanlong Xie","Yao Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.19159v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10016v1","updated":"2025-05-15T06:58:45Z","published":"2025-05-15T06:58:45Z","title":"Application of YOLOv8 in monocular downward multiple Car Target\n  detection","summary":"  Autonomous driving technology is progressively transforming traditional car\ndriving methods, marking a significant milestone in modern transportation.\nObject detection serves as a cornerstone of autonomous systems, playing a vital\nrole in enhancing driving safety, enabling autonomous functionality, improving\ntraffic efficiency, and facilitating effective emergency responses. However,\ncurrent technologies such as radar for environmental perception, cameras for\nroad perception, and vehicle sensor networks face notable challenges, including\nhigh costs, vulnerability to weather and lighting conditions, and limited\nresolution.To address these limitations, this paper presents an improved\nautonomous target detection network based on YOLOv8. By integrating structural\nreparameterization technology, a bidirectional pyramid structure network model,\nand a novel detection pipeline into the YOLOv8 framework, the proposed approach\nachieves highly efficient and precise detection of multi-scale, small, and\nremote objects. Experimental results demonstrate that the enhanced model can\neffectively detect both large and small objects with a detection accuracy of\n65%, showcasing significant advancements over traditional methods.This improved\nmodel holds substantial potential for real-world applications and is\nwell-suited for autonomous driving competitions, such as the Formula Student\nAutonomous China (FSAC), particularly excelling in scenarios involving\nsingle-target and small-object detection.\n","authors":["Shijie Lyu"],"pdf_url":"https://arxiv.org/pdf/2505.10016v1.pdf","comment":"Accepted by the 5th International Conference on Signal Processing and\n  Machine Learning (CONF-SPML 2025), to appear in Applied and Computational\n  Engineering"},{"id":"http://arxiv.org/abs/2505.05513v2","updated":"2025-05-15T06:56:31Z","published":"2025-05-07T10:33:45Z","title":"Exploring Convolutional Neural Networks for Rice Grain Classification:\n  An Explainable AI Approach","summary":"  Rice is an essential staple food worldwide that is important in promoting\ninternational trade, economic growth, and nutrition. Asian countries such as\nChina, India, Pakistan, Thailand, Vietnam, and Indonesia are notable for their\nsignificant contribution to the cultivation and utilization of rice. These\nnations are also known for cultivating different rice grains, including short\nand long grains. These sizes are further classified as basmati, jasmine, kainat\nsaila, ipsala, arborio, etc., catering to diverse culinary preferences and\ncultural traditions. For both local and international trade, inspecting and\nmaintaining the quality of rice grains to satisfy customers and preserve a\ncountry's reputation is necessary. Manual quality check and classification is\nquite a laborious and time-consuming process. It is also highly prone to\nmistakes. Therefore, an automatic solution must be proposed for the effective\nand efficient classification of different varieties of rice grains. This\nresearch paper presents an automatic framework based on a convolutional neural\nnetwork (CNN) for classifying different varieties of rice grains. We evaluated\nthe proposed model based on performance metrics such as accuracy, recall,\nprecision, and F1-Score. The CNN model underwent rigorous training and\nvalidation, achieving a remarkable accuracy rate and a perfect area under each\nclass's Receiver Operating Characteristic (ROC) curve. The confusion matrix\nanalysis confirmed the model's effectiveness in distinguishing between the\ndifferent rice varieties, indicating minimal misclassifications. Additionally,\nthe integration of explainability techniques such as LIME (Local Interpretable\nModel-agnostic Explanations) and SHAP (SHapley Additive exPlanations) provided\nvaluable insights into the model's decision-making process, revealing how\nspecific features of the rice grains influenced classification outcomes.\n","authors":["Muhammad Junaid Asif","Hamza Khan","Rabia Tehseen","Syed Tahir Hussain Rizvi","Mujtaba Asad","Shazia Saqib","Rana Fayyaz Ahmad"],"pdf_url":"https://arxiv.org/pdf/2505.05513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10543v9","updated":"2025-05-15T06:54:39Z","published":"2023-11-17T14:10:55Z","title":"Unified theory for joint covariance properties under geometric image\n  transformations for spatio-temporal receptive fields according to the\n  generalized Gaussian derivative model for visual receptive fields","summary":"  The influence of natural image transformations on receptive field responses\nis crucial for modelling visual operations in computer vision and biological\nvision. In this regard, covariance properties with respect to geometric image\ntransformations in the earliest layers of the visual hierarchy are essential\nfor expressing robust image operations, and for formulating invariant visual\noperations at higher levels.\n  This paper defines and proves a set of joint covariance properties for\nspatio-temporal receptive fields in terms of spatio-temporal derivative\noperators applied to spatio-temporally smoothed image data under compositions\nof spatial scaling transformations, spatial affine transformations, Galilean\ntransformations and temporal scaling transformations. Specifically, the derived\nrelations show how the parameters of the receptive fields need to be\ntransformed, in order to match the output from spatio-temporal receptive fields\nunder composed spatio-temporal image transformations.\n  For this purpose, we also fundamentally extend the notion of scale-normalized\nderivatives to affine-normalized derivatives, that are computed based on\nspatial smoothing with affine Gaussian kernels, and analyze the covariance\nproperties of the resulting affine-normalized derivatives for the affine group\nas well as for important subgroups thereof.\n  We conclude with a geometric analysis, showing how the derived joint\ncovariance properties make it possible to relate or match spatio-temporal\nreceptive field responses, when observing, possibly moving, local surface\npatches from different views, under locally linearized perspective or\nprojective transformations, as well as when observing different instances of\nspatio-temporal events, that may occur either faster or slower between\ndifferent views of similar spatio-temporal events.\n","authors":["Tony Lindeberg"],"pdf_url":"https://arxiv.org/pdf/2311.10543v9.pdf","comment":"46 pages, 19 figures. Note: From version 4, this paper considers a\n  different form of joint composition of the geometric image transformations\n  than in the earlier versions"},{"id":"http://arxiv.org/abs/2505.09998v1","updated":"2025-05-15T06:22:24Z","published":"2025-05-15T06:22:24Z","title":"From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive\n  3D Sketching","summary":"  In the era of immersive consumer electronics, such as AR/VR headsets and\nsmart devices, people increasingly seek ways to express their identity through\nvirtual fashion. However, existing 3D garment design tools remain inaccessible\nto everyday users due to steep technical barriers and limited data. In this\nwork, we introduce a 3D sketch-driven 3D garment generation framework that\nempowers ordinary users - even those without design experience - to create\nhigh-quality digital clothing through simple 3D sketches in AR/VR environments.\nBy combining a conditional diffusion model, a sketch encoder trained in a\nshared latent space, and an adaptive curriculum learning strategy, our system\ninterprets imprecise, free-hand input and produces realistic, personalized\ngarments. To address the scarcity of training data, we also introduce\nKO3DClothes, a new dataset of paired 3D garments and user-created sketches.\nExtensive experiments and user studies confirm that our method significantly\noutperforms existing baselines in both fidelity and usability, demonstrating\nits promise for democratized fashion design on next-generation consumer\nplatforms.\n","authors":["Ying Zang","Yuanqi Hu","Xinyu Chen","Yuxia Xu","Suhui Wang","Chunan Yu","Lanyun Zhu","Deyi Ji","Xin Xu","Tianrun Chen"],"pdf_url":"https://arxiv.org/pdf/2505.09998v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.09997v1","updated":"2025-05-15T06:21:00Z","published":"2025-05-15T06:21:00Z","title":"Descriptive Image-Text Matching with Graded Contextual Similarity","summary":"  Image-text matching aims to build correspondences between visual and textual\ndata by learning their pairwise similarities. Most existing approaches have\nadopted sparse binary supervision, indicating whether a pair of images and\nsentences matches or not. However, such sparse supervision covers a limited\nsubset of image-text relationships, neglecting their inherent many-to-many\ncorrespondences; an image can be described in numerous texts at different\ndescriptive levels. Moreover, existing approaches overlook the implicit\nconnections from general to specific descriptions, which form the underlying\nrationale for the many-to-many relationships between vision and language. In\nthis work, we propose descriptive image-text matching, called DITM, to learn\nthe graded contextual similarity between image and text by exploring the\ndescriptive flexibility of language. We formulate the descriptiveness score of\neach sentence with cumulative term frequency-inverse document frequency\n(TF-IDF) to balance the pairwise similarity according to the keywords in the\nsentence. Our method leverages sentence descriptiveness to learn robust\nimage-text matching in two key ways: (1) to refine the false negative labeling,\ndynamically relaxing the connectivity between positive and negative pairs, and\n(2) to build more precise matching, aligning a set of relevant sentences in a\ngeneric-to-specific order. By moving beyond rigid binary supervision, DITM\nenhances the discovery of both optimal matches and potential positive pairs.\nExtensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the\neffectiveness of our method in representing complex image-text relationships\ncompared to state-of-the-art approaches. In addition, DITM enhances the\nhierarchical reasoning ability of the model, supported by the extensive\nanalysis on HierarCaps benchmark.\n","authors":["Jinhyun Jang","Jiyeong Lee","Kwanghoon Sohn"],"pdf_url":"https://arxiv.org/pdf/2505.09997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09990v1","updated":"2025-05-15T06:04:42Z","published":"2025-05-15T06:04:42Z","title":"PointArena: Probing Multimodal Grounding Through Language-Guided\n  Pointing","summary":"  Pointing serves as a fundamental and intuitive mechanism for grounding\nlanguage within visual contexts, with applications spanning robotics, assistive\ntechnologies, and interactive AI systems. While recent multimodal models have\nstarted to support pointing capabilities, existing benchmarks typically focus\nonly on referential object localization tasks. We introduce PointArena, a\ncomprehensive platform for evaluating multimodal pointing across diverse\nreasoning scenarios. PointArena comprises three components: (1) Point-Bench, a\ncurated dataset containing approximately 1,000 pointing tasks across five\nreasoning categories; (2) Point-Battle, an interactive, web-based arena\nfacilitating blind, pairwise model comparisons, which has already gathered over\n4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation\nsystem allowing users to directly evaluate multimodal model pointing\ncapabilities in practical settings. We conducted extensive evaluations of both\nstate-of-the-art open-source and proprietary multimodal models. Results\nindicate that Molmo-72B consistently outperforms other models, though\nproprietary models increasingly demonstrate comparable performance.\nAdditionally, we find that supervised training specifically targeting pointing\ntasks significantly enhances model performance. Across our multi-stage\nevaluation pipeline, we also observe strong correlations, underscoring the\ncritical role of precise pointing capabilities in enabling multimodal models to\neffectively bridge abstract reasoning with concrete, real-world actions.\nProject page: https://pointarena.github.io/\n","authors":["Long Cheng","Jiafei Duan","Yi Ru Wang","Haoquan Fang","Boyang Li","Yushan Huang","Elvis Wang","Ainaz Eftekhar","Jason Lee","Wentao Yuan","Rose Hendrix","Noah A. Smith","Fei Xia","Dieter Fox","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2505.09990v1.pdf","comment":"10 Pages, Dataset and code:https://pointarena.github.io/"},{"id":"http://arxiv.org/abs/2502.19090v2","updated":"2025-05-15T05:59:22Z","published":"2025-02-26T12:36:16Z","title":"EndoMamba: An Efficient Foundation Model for Endoscopic Videos via\n  Hierarchical Pre-training","summary":"  Endoscopic video-based tasks, such as visual navigation and surgical phase\nrecognition, play a crucial role in minimally invasive surgeries by providing\nreal-time assistance. While recent video foundation models have shown promise,\ntheir applications are hindered by (1) computational inefficiencies and (2)\nsuboptimal performance caused by limited data for pre-training in endoscopy. To\naddress these issues, we present EndoMamba, a foundation model designed for\nreal-time inference while learning generalized spatiotemporal representations.\nFirst, to mitigate computational inefficiencies, we propose the EndoMamba\nbackbone, optimized for real-time inference. Inspired by recent advancements in\nstate space models, EndoMamba integrates Bidirectional Mamba blocks for spatial\nmodeling within individual frames and vanilla Mamba blocks for past-to-present\nreasoning across the temporal domain. This design enables both strong\nspatiotemporal modeling and efficient inference in online video streams.\nSecond, we propose a self-supervised hierarchical pre-training diagram to\nenhance EndoMamba's representation learning using endoscopic videos and\nincorporating general video domain knowledge. Specifically, our approach\ncombines masked reconstruction with auxiliary supervision, leveraging low-level\nreconstruction to capture spatial-temporal structures and high-level alignment\nto transfer broader knowledge from a pretrained general-video domain foundation\nmodel. Extensive experiments on four downstream tasks--classification,\nsegmentation, surgical phase recognition, and localization--demonstrate that\nEndoMamba outperforms existing foundation models and task-specific methods\nwhile maintaining real-time inference speed. The source code is available at\nhttps://github.com/TianCuteQY/EndoMamba.\n","authors":["Qingyao Tian","Huai Liao","Xinyan Huang","Bingyu Yang","Dongdong Lei","Sebastien Ourselin","Hongbin Liu"],"pdf_url":"https://arxiv.org/pdf/2502.19090v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13602v2","updated":"2025-05-15T05:56:38Z","published":"2024-11-19T09:09:14Z","title":"Translating Electrocardiograms to Cardiac Magnetic Resonance Imaging\n  Useful for Cardiac Assessment and Disease Screening: A Multi-Center Study AI\n  for ECG to CMR Translation Study","summary":"  Cardiovascular diseases (CVDs) are the leading cause of global mortality,\nnecessitating accessible and accurate diagnostic tools. While cardiac magnetic\nresonance imaging (CMR) provides gold-standard insights into cardiac structure\nand function, its clinical utility is limited by high cost and complexity. In\ncontrast, electrocardiography (ECG) is inexpensive and widely available but\nlacks the granularity of CMR. We propose CardioNets, a deep learning framework\nthat translates 12-lead ECG signals into CMR-level functional parameters and\nsynthetic images, enabling scalable cardiac assessment. CardioNets integrates\ncross-modal contrastive learning and generative pretraining, aligning ECG with\nCMR-derived cardiac phenotypes and synthesizing high-resolution CMR images via\na masked autoregressive model. Trained on 159,819 samples from five cohorts,\nincluding the UK Biobank (n=42,483) and MIMIC-IV-ECG (n=164,550), and\nexternally validated on independent clinical datasets (n=3,767), CardioNets\nachieved strong performance across disease screening and phenotype estimation\ntasks. In the UK Biobank, it improved cardiac phenotype regression R2 by 24.8%\nand cardiomyopathy AUC by up to 39.3% over baseline models. In MIMIC, it\nincreased AUC for pulmonary hypertension detection by 5.6%. Generated CMR\nimages showed 36.6% higher SSIM and 8.7% higher PSNR than prior approaches. In\na reader study, ECG-only CardioNets achieved 13.9% higher accuracy than human\nphysicians using both ECG and real CMR. These results suggest that CardioNets\noffers a promising, low-cost alternative to CMR for large-scale CVD screening,\nparticularly in resource-limited settings. Future efforts will focus on\nclinical deployment and regulatory validation of ECG-based synthetic imaging.\n","authors":["Zhengyao Ding","Ziyu Li","Yujian Hu","Youyao Xu","Chengchen Zhao","Yiheng Mao","Haitao Li","Zhikang Li","Qian Li","Jing Wang","Yue Chen","Mengjia Chen","Longbo Wang","Xuesen Chu","Weichao Pan","Ziyi Liu","Fei Wu","Hongkun Zhang","Ting Chen","Zhengxing Huang"],"pdf_url":"https://arxiv.org/pdf/2411.13602v2.pdf","comment":"27 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.09986v1","updated":"2025-05-15T05:52:11Z","published":"2025-05-15T05:52:11Z","title":"High Quality Underwater Image Compression with Adaptive Correction and\n  Codebook-based Augmentation","summary":"  With the increasing exploration and exploitation of the underwater world,\nunderwater images have become a critical medium for human interaction with\nmarine environments, driving extensive research into their efficient\ntransmission and storage. However, contemporary underwater image compression\nalgorithms fail to fully leverage the unique characteristics distinguishing\nunderwater scenes from terrestrial images, resulting in suboptimal performance.\nTo address this limitation, we introduce HQUIC, designed to exploit\nunderwater-image-specific features for enhanced compression efficiency. HQUIC\nemploys an ALTC module to adaptively predict the attenuation coefficients and\nglobal light information of the images, which effectively mitigates the issues\ncaused by the differences in lighting and tone existing in underwater images.\nSubsequently, HQUIC employs a codebook as an auxiliary branch to extract the\ncommon objects within underwater images and enhances the performance of the\nmain branch. Furthermore, HQUIC dynamically weights multi-scale frequency\ncomponents, prioritizing information critical for distortion quality while\ndiscarding redundant details. Extensive evaluations on diverse underwater\ndatasets demonstrate that HQUIC outperforms state-of-the-art compression\nmethods.\n","authors":["Yimin Zhou","Yichong Xia","Sicheng Pan","Bin Chen","Baoyi An","Haoqian Wang","Zhi Wang","Yaowei Wang","Zikun Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.09986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09985v1","updated":"2025-05-15T05:50:35Z","published":"2025-05-15T05:50:35Z","title":"Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction","summary":"  Score-based diffusion models have shown significant promise in the field of\nsparse-view CT reconstruction. However, the projection dataset is large and\nriddled with redundancy. Consequently, applying the diffusion model to\nunprocessed data results in lower learning effectiveness and higher learning\ndifficulty, frequently leading to reconstructed images that lack fine details.\nTo address these issues, we propose the ordered-subsets multi-diffusion model\n(OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT\nprojection data into equal subsets and employs multi-subsets diffusion model\n(MSDM) to learn from each subset independently. This targeted learning approach\nreduces complexity and enhances the reconstruction of fine details.\nFurthermore, the integration of one-whole diffusion model (OWDM) with complete\nsinogram data acts as a global information constraint, which can reduce the\npossibility of generating erroneous or inconsistent sinogram information.\nMoreover, the OSMM's unsupervised learning framework provides strong robustness\nand generalizability, adapting seamlessly to varying sparsity levels of CT\nsinograms. This ensures consistent and reliable performance across different\nclinical scenarios. Experimental results demonstrate that OSMM outperforms\ntraditional diffusion models in terms of image quality and noise resilience,\noffering a powerful and versatile solution for advanced CT imaging in\nsparse-view scenarios.\n","authors":["Pengfei Yu","Bin Huang","Minghui Zhang","Weiwen Wu","Shaoyu Wang","Qiegen Liu"],"pdf_url":"https://arxiv.org/pdf/2505.09985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07214v2","updated":"2025-05-15T05:47:33Z","published":"2025-05-12T03:47:05Z","title":"Towards user-centered interactive medical image segmentation in VR with\n  an assistive AI agent","summary":"  Crucial in disease analysis and surgical planning, manual segmentation of\nvolumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and\nchallenging to master, while fully automatic algorithms can benefit from user\nfeedback. Therefore, with the complementary power of the latest radiological AI\nfoundation models and virtual reality (VR)'s intuitive data interaction, we\npropose SAMIRA, a novel conversational AI agent that assists users with\nlocalizing, segmenting, and visualizing 3D medical concepts in VR. Through\nspeech-based interaction, the agent helps users understand radiological\nfeatures, locate clinical targets, and generate segmentation masks that can be\nrefined with just a few point prompts. The system also supports true-to-scale\n3D visualization of segmented pathology to enhance patient-specific anatomical\nunderstanding. Furthermore, to determine the optimal interaction paradigm under\nnear-far attention-switching for refining segmentation masks in an immersive,\nhuman-in-the-loop workflow, we compare VR controller pointing, head pointing,\nand eye tracking as input modes. With a user study, evaluations demonstrated a\nhigh usability score (SUS=90.0 $\\pm$ 9.0), low overall task load, as well as\nstrong support for the proposed VR system's guidance, training potential, and\nintegration of AI in radiological segmentation tasks.\n","authors":["Pascal Spiegler","Arash Harirpoush","Yiming Xiao"],"pdf_url":"https://arxiv.org/pdf/2505.07214v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09971v1","updated":"2025-05-15T05:21:16Z","published":"2025-05-15T05:21:16Z","title":"APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of\n  Airborne LiDAR Point Clouds","summary":"  Airborne laser scanning (ALS) point cloud segmentation is a fundamental task\nfor large-scale 3D scene understanding. In real-world applications, models are\ntypically fixed after training. However, domain shifts caused by changes in the\nenvironment, sensor types, or sensor degradation often lead to a decline in\nmodel performance. Continuous Test-Time Adaptation (CTTA) offers a solution by\nadapting a source-pretrained model to evolving, unlabeled target domains.\nDespite its potential, research on ALS point clouds remains limited, facing\nchallenges such as the absence of standardized datasets and the risk of\ncatastrophic forgetting and error accumulation during prolonged adaptation. To\ntackle these challenges, we propose APCoTTA, the first CTTA method tailored for\nALS point cloud semantic segmentation. We propose a dynamic trainable layer\nselection module. This module utilizes gradient information to select\nlow-confidence layers for training, and the remaining layers are kept frozen,\nmitigating catastrophic forgetting. To further reduce error accumulation, we\npropose an entropy-based consistency loss. By losing such samples based on\nentropy, we apply consistency loss only to the reliable samples, enhancing\nmodel stability. In addition, we propose a random parameter interpolation\nmechanism, which randomly blends parameters from the selected trainable layers\nwith those of the source model. This approach helps balance target adaptation\nand source knowledge retention, further alleviating forgetting. Finally, we\nconstruct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA\nbenchmarks for ALS point cloud segmentation. Experimental results demonstrate\nthat APCoTTA achieves the best performance on two benchmarks, with mIoU\nimprovements of approximately 9% and 14% over direct inference. The new\nbenchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.\n","authors":["Yuan Gao","Shaobo Xia","Sheng Nie","Cheng Wang","Xiaohuan Xi","Bisheng Yang"],"pdf_url":"https://arxiv.org/pdf/2505.09971v1.pdf","comment":"18 pages,12 figures"},{"id":"http://arxiv.org/abs/2406.12632v2","updated":"2025-05-15T05:17:41Z","published":"2024-06-18T13:59:10Z","title":"Cyclic 2.5D Perceptual Loss for Cross-Modal 3D Medical Image Synthesis:\n  T1w MRI to Tau PET","summary":"  There is a demand for medical image synthesis or translation to generate\nsynthetic images of missing modalities from available data. This need stems\nfrom challenges such as restricted access to high-cost imaging devices,\ngovernment regulations, or failure to follow up with patients or study\nparticipants. In medical imaging, preserving high-level semantic features is\noften more critical than achieving pixel-level accuracy. Perceptual loss\nfunctions are widely employed to train medical image synthesis or translation\nmodels, as they quantify differences in high-level image features using a\npre-trained feature extraction network. While 3D and 2.5D perceptual losses are\nused in 3D medical image synthesis, they face challenges, such as the lack of\npre-trained 3D models or difficulties in balancing loss reduction across\ndifferent planes. In this work, we focus on synthesizing 3D tau PET images from\n3D T1-weighted MR images. We propose a cyclic 2.5D perceptual loss that\nsequentially computes the 2D average perceptual loss for each of the axial,\ncoronal, and sagittal planes over epochs, with the cycle duration gradually\ndecreasing. Additionally, we process tau PET images using by-manufacturer\nstandardization to enhance the preservation of high-SUVR regions indicative of\ntau pathology and mitigate SUVR variability caused by inter-manufacturer\ndifferences. We combine the proposed loss with SSIM and MSE losses and\ndemonstrate its effectiveness in improving both quantitative and qualitative\nperformance across various generative models, including U-Net, UNETR,\nSwinUNETR, CycleGAN, and Pix2Pix.\n","authors":["Junho Moon","Symac Kim","Haejun Chung","Ikbeom Jang"],"pdf_url":"https://arxiv.org/pdf/2406.12632v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09967v1","updated":"2025-05-15T05:07:00Z","published":"2025-05-15T05:07:00Z","title":"TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression\n  Recognition","summary":"  Facial expression recognition (FER) in the wild remains a challenging task\ndue to the subtle and localized nature of expression-related features, as well\nas the complex variations in facial appearance. In this paper, we introduce a\nnovel framework that explicitly focuses on Texture Key Driver Factors (TKDF),\nlocalized texture regions that exhibit strong discriminative power across\nemotional categories. By carefully observing facial image patterns, we identify\nthat certain texture cues, such as micro-changes in skin around the brows,\neyes, and mouth, serve as primary indicators of emotional dynamics. To\neffectively capture and leverage these cues, we propose a FER architecture\ncomprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual\nInformation Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced\nwith multi-branch attention to extract fine-grained texture representations,\nwhile DCIF refines these features by filtering context through adaptive pooling\nand attention mechanisms. Experimental results on RAF-DB and KDEF datasets\ndemonstrate that our method achieves state-of-the-art performance, verifying\nthe effectiveness and robustness of incorporating TKDFs into FER pipelines.\n","authors":["Liqian Deng"],"pdf_url":"https://arxiv.org/pdf/2505.09967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05904v2","updated":"2025-05-15T05:01:49Z","published":"2025-04-08T11:02:14Z","title":"Saliency-Motion Guided Trunk-Collateral Network for Unsupervised Video\n  Object Segmentation","summary":"  Recent mainstream unsupervised video object segmentation (UVOS)\nmotion-appearance approaches use either the bi-encoder structure to separately\nencode motion and appearance features, or the uni-encoder structure for joint\nencoding. However, these methods fail to properly balance the motion-appearance\nrelationship. Consequently, even with complex fusion modules for\nmotion-appearance integration, the extracted suboptimal features degrade the\nmodels' overall performance. Moreover, the quality of optical flow varies\nacross scenarios, making it insufficient to rely solely on optical flow to\nachieve high-quality segmentation results. To address these challenges, we\npropose the Saliency-Motion guided Trunk-Collateral Network (SMTC-Net), which\nbetter balances the motion-appearance relationship and incorporates model's\nintrinsic saliency information to enhance segmentation performance.\nSpecifically, considering that optical flow maps are derived from RGB images,\nthey share both commonalities and differences. Accordingly, we propose a novel\nTrunk-Collateral structure for motion-appearance UVOS. The shared trunk\nbackbone captures the motion-appearance commonality, while the collateral\nbranch learns the uniqueness of motion features. Furthermore, an Intrinsic\nSaliency guided Refinement Module (ISRM) is devised to efficiently leverage the\nmodel's intrinsic saliency information to refine high-level features, and\nprovide pixel-level guidance for motion-appearance fusion, thereby enhancing\nperformance without additional input. Experimental results show that SMTC-Net\nachieved state-of-the-art performance on three UVOS datasets ( 89.2% J&F on\nDAVIS-16, 76% J on YouTube-Objects, 86.4% J on FBMS ) and four standard video\nsalient object detection (VSOD) benchmarks with the notable increase,\ndemonstrating its effectiveness and superiority over previous methods.\n","authors":["Xiangyu Zheng","Wanyun Li","Songcheng He","Jianping Fan","Xiaoqiang Li","We Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.05904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09965v1","updated":"2025-05-15T04:59:02Z","published":"2025-05-15T04:59:02Z","title":"MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier\n  Refinement for Diffusion-Based Disease Trajectory Prediction","summary":"  Modelling disease progression in precision medicine requires capturing\ncomplex spatio-temporal dynamics while preserving anatomical integrity.\nExisting methods often struggle with longitudinal dependencies and structural\nconsistency in progressive disorders. To address these limitations, we\nintroduce MambaControl, a novel framework that integrates selective state-space\nmodelling with diffusion processes for high-fidelity prediction of medical\nimage trajectories. To better capture subtle structural changes over time while\nmaintaining anatomical consistency, MambaControl combines Mamba-based\nlong-range modelling with graph-guided anatomical control to more effectively\nrepresent anatomical correlations. Furthermore, we introduce Fourier-enhanced\nspectral graph representations to capture spatial coherence and multiscale\ndetail, enabling MambaControl to achieve state-of-the-art performance in\nAlzheimer's disease prediction. Quantitative and regional evaluations\ndemonstrate improved progression prediction quality and anatomical fidelity,\nhighlighting its potential for personalised prognosis and clinical decision\nsupport.\n","authors":["Hao Yang","Tao Tan","Shuai Tan","Weiqin Yang","Kunyan Cai","Calvin Chen","Yue Sun"],"pdf_url":"https://arxiv.org/pdf/2505.09965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08910v2","updated":"2025-05-15T04:24:06Z","published":"2025-05-13T19:01:12Z","title":"Behind Maya: Building a Multilingual Vision Language Model","summary":"  In recent times, we have seen a rapid development of large Vision-Language\nModels (VLMs). They have shown impressive results on academic benchmarks,\nprimarily in widely spoken languages but lack performance on low-resource\nlanguages and varied cultural contexts. To address these limitations, we\nintroduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a\nmultilingual image-text pretraining dataset in eight languages, based on the\nLLaVA pretraining dataset; and 2) a multilingual image-text model supporting\nthese languages, enhancing cultural and linguistic comprehension in\nvision-language tasks. Code available at https://github.com/nahidalam/maya.\n","authors":["Nahid Alam","Karthik Reddy Kanjula","Surya Guthikonda","Timothy Chung","Bala Krishna S Vegesna","Abhipsha Das","Anthony Susevski","Ryan Sze-Yin Chan","S M Iftekhar Uddin","Shayekh Bin Islam","Roshan Santhosh","Snegha A","Drishti Sharma","Chen Liu","Isha Chaturvedi","Genta Indra Winata","Ashvanth. S","Snehanshu Mukherjee","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2505.08910v2.pdf","comment":"Accepted at VLMs4ALL CVPR 2025 Workshop; corrected workshop name\n  spelling"},{"id":"http://arxiv.org/abs/2501.03021v2","updated":"2025-05-15T04:15:14Z","published":"2025-01-06T14:05:19Z","title":"A Trust-Guided Approach to MR Image Reconstruction with Side Information","summary":"  Reducing MRI scan times can improve patient care and lower healthcare costs.\nMany acceleration methods are designed to reconstruct diagnostic-quality images\nfrom sparse k-space data, via an ill-posed or ill-conditioned linear inverse\nproblem (LIP). To address the resulting ambiguities, it is crucial to\nincorporate prior knowledge into the optimization problem, e.g., in the form of\nregularization. Another form of prior knowledge less commonly used in medical\nimaging is the readily available auxiliary data (a.k.a. side information)\nobtained from sources other than the current acquisition. In this paper, we\npresent the Trust- Guided Variational Network (TGVN), an end-to-end deep\nlearning framework that effectively and reliably integrates side information\ninto LIPs. We demonstrate its effectiveness in multi-coil, multi-contrast MRI\nreconstruction, where incomplete or low-SNR measurements from one contrast are\nused as side information to reconstruct high-quality images of another contrast\nfrom heavily under-sampled data. TGVN is robust across different contrasts,\nanatomies, and field strengths. Compared to baselines utilizing side\ninformation, TGVN achieves superior image quality while preserving subtle\npathological features even at challenging acceleration levels, drastically\nspeeding up acquisition while minimizing hallucinations. Source code and\ndataset splits are available on github.com/sodicksonlab/TGVN.\n","authors":["Arda Atalık","Sumit Chopra","Daniel K. Sodickson"],"pdf_url":"https://arxiv.org/pdf/2501.03021v2.pdf","comment":"27 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.09943v1","updated":"2025-05-15T03:56:36Z","published":"2025-05-15T03:56:36Z","title":"CSPENet: Contour-Aware and Saliency Priors Embedding Network for\n  Infrared Small Target Detection","summary":"  Infrared small target detection (ISTD) plays a critical role in a wide range\nof civilian and military applications. Existing methods suffer from\ndeficiencies in the localization of dim targets and the perception of contour\ninformation under dense clutter environments, severely limiting their detection\nperformance. To tackle these issues, we propose a contour-aware and saliency\npriors embedding network (CSPENet) for ISTD. We first design a\nsurround-convergent prior extraction module (SCPEM) that effectively captures\nthe intrinsic characteristic of target contour pixel gradients converging\ntoward their center. This module concurrently extracts two collaborative\npriors: a boosted saliency prior for accurate target localization and\nmulti-scale structural priors for comprehensively enriching contour detail\nrepresentation. Building upon this, we propose a dual-branch priors embedding\narchitecture (DBPEA) that establishes differentiated feature fusion pathways,\nembedding these two priors at optimal network positions to achieve performance\nenhancement. Finally, we develop an attention-guided feature enhancement module\n(AGFEM) to refine feature representations and improve saliency estimation\naccuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and\nNUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art\nmethods in detection performance. The code is available at\nhttps://github.com/IDIP2025/CSPENet.\n","authors":["Jiakun Deng","Kexuan Li","Xingye Cui","Jiaxuan Li","Chang Long","Tian Pu","Zhenming Peng"],"pdf_url":"https://arxiv.org/pdf/2505.09943v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09939v1","updated":"2025-05-15T03:52:42Z","published":"2025-05-15T03:52:42Z","title":"Non-Registration Change Detection: A Novel Change Detection Task and\n  Benchmark Dataset","summary":"  In this study, we propose a novel remote sensing change detection task,\nnon-registration change detection, to address the increasing number of\nemergencies such as natural disasters, anthropogenic accidents, and military\nstrikes. First, in light of the limited discourse on the issue of\nnon-registration change detection, we systematically propose eight scenarios\nthat could arise in the real world and potentially contribute to the occurrence\nof non-registration problems. Second, we develop distinct image transformation\nschemes tailored to various scenarios to convert the available registration\nchange detection dataset into a non-registration version. Finally, we\ndemonstrate that non-registration change detection can cause catastrophic\ndamage to the state-of-the-art methods. Our code and dataset are available at\nhttps://github.com/ShanZard/NRCD.\n","authors":["Zhe Shan","Lei Zhou","Liu Mao","Shaofan Chen","Chuanqiu Ren","Xia Xie"],"pdf_url":"https://arxiv.org/pdf/2505.09939v1.pdf","comment":"Accepted to IGARSS 2025"},{"id":"http://arxiv.org/abs/2505.09935v1","updated":"2025-05-15T03:40:29Z","published":"2025-05-15T03:40:29Z","title":"VRU-CIPI: Crossing Intention Prediction at Intersections for Improving\n  Vulnerable Road Users Safety","summary":"  Understanding and predicting human behavior in-thewild, particularly at urban\nintersections, remains crucial for enhancing interaction safety between road\nusers. Among the most critical behaviors are crossing intentions of Vulnerable\nRoad Users (VRUs), where misinterpretation may result in dangerous conflicts\nwith oncoming vehicles. In this work, we propose the VRU-CIPI framework with a\nsequential attention-based model designed to predict VRU crossing intentions at\nintersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal\ndynamics in VRU movements, combined with a multi-head Transformer\nself-attention mechanism to encode contextual and spatial dependencies critical\nfor predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed\nachieves state-of-the-art performance with an accuracy of 96.45% and achieving\nreal-time inference speed reaching 33 frames per second. Furthermore, by\nintegrating with Infrastructure-to-Vehicles (I2V) communication, our approach\ncan proactively enhance intersection safety through timely activation of\ncrossing signals and providing early warnings to connected vehicles, ensuring\nsmoother and safer interactions for all road users.\n","authors":["Ahmed S. Abdelrahman","Mohamed Abdel-Aty","Quoc Dai Tran"],"pdf_url":"https://arxiv.org/pdf/2505.09935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09927v1","updated":"2025-05-15T03:24:54Z","published":"2025-05-15T03:24:54Z","title":"DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation\n  of Medical Image Segmentation","summary":"  Domain adaptation addresses the challenge of model performance degradation\ncaused by domain gaps. In the typical setup for unsupervised domain adaptation,\nlabeled data from a source domain and unlabeled data from a target domain are\nused to train a target model. However, access to labeled source domain data,\nparticularly in medical datasets, can be restricted due to privacy policies. As\na result, research has increasingly shifted to source-free domain adaptation\n(SFDA), which requires only a pretrained model from the source domain and\nunlabeled data from the target domain data for adaptation. Existing SFDA\nmethods often rely on domain-specific image style translation and\nself-supervision techniques to bridge the domain gap and train the target\ndomain model. However, the quality of domain-specific style-translated images\nand pseudo-labels produced by these methods still leaves room for improvement.\nMoreover, training the entire model during adaptation can be inefficient under\nlimited supervision. In this paper, we propose a novel SFDA framework to\naddress these challenges. Specifically, to effectively mitigate the impact of\ndomain gap in the initial training phase, we introduce preadaptation to\ngenerate a preadapted model, which serves as an initialization of target model\nand allows for the generation of high-quality enhanced pseudo-labels without\nintroducing extra parameters. Additionally, we propose a data-dependent\nfrequency prompt to more effectively translate target domain images into a\nsource-like style. To further enhance adaptation, we employ a style-related\nlayer fine-tuning strategy, specifically designed for SFDA, to train the target\nmodel using the prompted target domain images and pseudo-labels. Extensive\nexperiments on cross-modality abdominal and cardiac SFDA segmentation tasks\ndemonstrate that our proposed method outperforms existing state-of-the-art\nmethods.\n","authors":["Siqi Yin","Shaolei Liu","Manning Wang"],"pdf_url":"https://arxiv.org/pdf/2505.09927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09926v1","updated":"2025-05-15T03:24:28Z","published":"2025-05-15T03:24:28Z","title":"AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection","summary":"  Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.\n","authors":["Bin-Bin Gao","Yue Zhu","Jiangtao Yan","Yuezhi Cai","Weixi Zhang","Meng Wang","Jun Liu","Yong Liu","Lei Wang","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2505.09926v1.pdf","comment":"27 pages, 15 figures, 22 tables"},{"id":"http://arxiv.org/abs/2505.09915v1","updated":"2025-05-15T03:00:32Z","published":"2025-05-15T03:00:32Z","title":"Large-Scale Gaussian Splatting SLAM","summary":"  The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian\nSplatting (3DGS) have shown encouraging and impressive results for visual SLAM.\nHowever, most representative methods require RGBD sensors and are only\navailable for indoor environments. The robustness of reconstruction in\nlarge-scale outdoor scenarios remains unexplored. This paper introduces a\nlarge-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The\nproposed LSG-SLAM employs a multi-modality strategy to estimate prior poses\nunder large view changes. In tracking, we introduce feature-alignment warping\nconstraints to alleviate the adverse effects of appearance similarity in\nrendering losses. For the scalability of large-scale scenarios, we introduce\ncontinuous Gaussian Splatting submaps to tackle unbounded scenes with limited\nmemory. Loops are detected between GS submaps by place recognition and the\nrelative pose between looped keyframes is optimized utilizing rendering and\nfeature warping losses. After the global optimization of camera poses and\nGaussian points, a structure refinement module enhances the reconstruction\nquality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM\nachieves superior performance over existing Neural, 3DGS-based, and even\ntraditional approaches. Project page: https://lsg-slam.github.io.\n","authors":["Zhe Xin","Chenyang Wu","Penghui Huang","Yanyong Zhang","Yinian Mao","Guoquan Huang"],"pdf_url":"https://arxiv.org/pdf/2505.09915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.00496v2","updated":"2025-05-15T03:00:00Z","published":"2025-04-01T07:43:10Z","title":"Learned Image Compression with Dictionary-based Entropy Model","summary":"  Learned image compression methods have attracted great research interest and\nexhibited superior rate-distortion performance to the best classical image\ncompression standards of the present. The entropy model plays a key role in\nlearned image compression, which estimates the probability distribution of the\nlatent representation for further entropy coding. Most existing methods\nemployed hyper-prior and auto-regressive architectures to form their entropy\nmodels. However, they only aimed to explore the internal dependencies of latent\nrepresentation while neglecting the importance of extracting prior from\ntraining data. In this work, we propose a novel entropy model named\nDictionary-based Cross Attention Entropy model, which introduces a learnable\ndictionary to summarize the typical structures occurring in the training\ndataset to enhance the entropy model. Extensive experimental results have\ndemonstrated that the proposed model strikes a better balance between\nperformance and latency, achieving state-of-the-art results on various\nbenchmark datasets.\n","authors":["Jingbo Lu","Leheng Zhang","Xingyu Zhou","Mu Li","Wen Li","Shuhang Gu"],"pdf_url":"https://arxiv.org/pdf/2504.00496v2.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2411.08665v2","updated":"2025-05-15T02:43:56Z","published":"2024-11-13T14:59:00Z","title":"OSMLoc: Single Image-Based Visual Localization in OpenStreetMap with\n  Fused Geometric and Semantic Guidance","summary":"  OpenStreetMap (OSM), a rich and versatile source of volunteered geographic\ninformation (VGI), facilitates human self-localization and scene understanding\nby integrating nearby visual observations with vectorized map data. However,\nthe disparity in modalities and perspectives poses a major challenge for\neffectively matching camera imagery with compact map representations, thereby\nlimiting the full potential of VGI data in real-world localization\napplications.\n  Inspired by the fact that the human brain relies on the fusion of geometric\nand semantic understanding for spatial localization tasks, we propose the\nOSMLoc in this paper. OSMLoc is a brain-inspired visual localization approach\nbased on first-person-view images against the OSM maps. It integrates semantic\nand geometric guidance to significantly improve accuracy, robustness, and\ngeneralization capability. First, we equip the OSMLoc with the visual\nfoundational model to extract powerful image features. Second, a\ngeometry-guided depth distribution adapter is proposed to bridge the monocular\ndepth estimation and camera-to-BEV transform. Thirdly, the semantic embeddings\nfrom the OSM data are utilized as auxiliary guidance for image-to-OSM feature\nmatching. To validate the proposed OSMLoc, we collect a worldwide cross-area\nand cross-condition (CC) benchmark for extensive evaluation. Experiments on the\nMGL dataset, CC validation benchmark, and KITTI dataset have demonstrated the\nsuperiority of our method. Code, pre-trained models, CC validation benchmark,\nand additional results are available at: https://github.com/WHU-USI3DV/OSMLoc.\n","authors":["Youqi Liao","Xieyuanli Chen","Shuhao Kang","Jianping Li","Zhen Dong","Hongchao Fan","Bisheng Yang"],"pdf_url":"https://arxiv.org/pdf/2411.08665v2.pdf","comment":"16 pages, technical report"},{"id":"http://arxiv.org/abs/2505.09193v2","updated":"2025-05-15T01:32:30Z","published":"2025-05-14T06:55:37Z","title":"BiECVC: Gated Diversification of Bidirectional Contexts for Learned\n  Video Compression","summary":"  Recent forward prediction-based learned video compression (LVC) methods have\nachieved impressive results, even surpassing VVC reference software VTM under\nthe Low Delay B (LDB) configuration. In contrast, learned bidirectional video\ncompression (BVC) remains underexplored and still lags behind its forward-only\ncounterparts. This performance gap is mainly due to the limited ability to\nextract diverse and accurate contexts: most existing BVCs primarily exploit\ntemporal motion while neglecting non-local correlations across frames.\nMoreover, they lack the adaptability to dynamically suppress harmful contexts\narising from fast motion or occlusion. To tackle these challenges, we propose\nBiECVC, a BVC framework that incorporates diversified local and non-local\ncontext modeling along with adaptive context gating. For local context\nenhancement, BiECVC reuses high-quality features from lower layers and aligns\nthem using decoded motion vectors without introducing extra motion overhead. To\nmodel non-local dependencies efficiently, we adopt a linear attention mechanism\nthat balances performance and complexity. To further mitigate the impact of\ninaccurate context prediction, we introduce Bidirectional Context Gating,\ninspired by data-dependent decay in recent autoregressive language models, to\ndynamically filter contextual information based on conditional coding results.\nExtensive experiments demonstrate that BiECVC achieves state-of-the-art\nperformance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2\nunder the Random Access (RA) configuration with intra periods of 32 and 64,\nrespectively. To our knowledge, BiECVC is the first learned video codec to\nsurpass VTM 13.2 RA across all standard test datasets. Code will be available\nat https://github.com/JiangWeibeta/ECVC.\n","authors":["Wei Jiang","Junru Li","Kai Zhang","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.09193v2.pdf","comment":"The first learned video codec that surpasses VTM 13.2 RA across all\n  standard test datasets. Code will be available at\n  https://github.com/JiangWeibeta/ECVC"},{"id":"http://arxiv.org/abs/2505.06512v3","updated":"2025-05-15T01:04:26Z","published":"2025-05-10T05:02:58Z","title":"HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image\n  Generation","summary":"  Text-to-image synthesis has progressed to the point where models can generate\nvisually compelling images from natural language prompts. Yet, existing methods\noften fail to reconcile high-level semantic fidelity with explicit spatial\ncontrol, particularly in scenes involving multiple objects, nuanced relations,\nor complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal\nAlignment (HCMA) framework for grounded text-to-image generation. HCMA\nintegrates two alignment modules into each diffusion sampling step: a global\nmodule that continuously aligns latent representations with textual\ndescriptions to ensure scene-level coherence, and a local module that employs\nbounding-box layouts to anchor objects at specified locations, enabling\nfine-grained spatial control. Extensive experiments on the MS-COCO 2014\nvalidation set show that HCMA surpasses state-of-the-art baselines, achieving a\n0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP\nScore. These results demonstrate HCMA's effectiveness in faithfully capturing\nintricate textual semantics while adhering to user-defined spatial constraints,\noffering a robust solution for semantically grounded image generation. Our code\nis available at https://github.com/hwang-cs-ime/HCMA.\n","authors":["Hang Wang","Zhi-Qi Cheng","Chenhao Lin","Chao Shen","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.06512v3.pdf","comment":"10 pages, 4 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.03294v2","updated":"2025-05-15T14:06:05Z","published":"2024-09-05T06:59:56Z","title":"FedPCL-CDR: A Federated Prototype-based Contrastive Learning Framework\n  for Privacy-Preserving Cross-domain Recommendation","summary":"  Cross-domain recommendation (CDR) aims to improve recommendation accuracy in\nsparse domains by transferring knowledge from data-rich domains. However,\nexisting CDR approaches often assume that user-item interaction data across\ndomains is publicly available, neglecting user privacy concerns. Additionally,\nthey experience performance degradation with sparse overlapping users due to\ntheir reliance on a large number of fully shared users for knowledge transfer.\nTo address these challenges, we propose a Federated Prototype-based Contrastive\nLearning (CL) framework for Privacy Preserving CDR, called FedPCL-CDR. This\napproach utilizes non-overlapping user information and differential prototypes\nto improve model performance within a federated learning framework. FedPCL-CDR\ncomprises two key modules: local domain (client) learning and global server\naggregation. In the local domain, FedPCL-CDR first clusters all user data and\nutilizes local differential privacy (LDP) to learn differential prototypes,\neffectively utilizing non-overlapping user information and protecting user\nprivacy. It then conducts knowledge transfer by employing both local and global\nprototypes returned from the server in a CL manner. Meanwhile, the global\nserver aggregates differential prototypes sent from local domains to learn both\nlocal and global prototypes. Extensive experiments on four CDR tasks across\nAmazon and Douban datasets demonstrate that FedPCL-CDR surpasses SOTA\nbaselines. We release our code at https://github.com/Lili1013/FedPCL CDR\n","authors":["Li Wang","Qiang Wu","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2409.03294v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10212v1","updated":"2025-05-15T12:16:36Z","published":"2025-05-15T12:16:36Z","title":"Do LLMs Memorize Recommendation Datasets? A Preliminary Study on\n  MovieLens-1M","summary":"  Large Language Models (LLMs) have become increasingly central to\nrecommendation scenarios due to their remarkable natural language understanding\nand generation capabilities. Although significant research has explored the use\nof LLMs for various recommendation tasks, little effort has been dedicated to\nverifying whether they have memorized public recommendation dataset as part of\ntheir training data. This is undesirable because memorization reduces the\ngeneralizability of research findings, as benchmarking on memorized datasets\ndoes not guarantee generalization to unseen datasets. Furthermore, memorization\ncan amplify biases, for example, some popular items may be recommended more\nfrequently than others.\n  In this work, we investigate whether LLMs have memorized public\nrecommendation datasets. Specifically, we examine two model families (GPT and\nLlama) across multiple sizes, focusing on one of the most widely used dataset\nin recommender systems: MovieLens-1M. First, we define dataset memorization as\nthe extent to which item attributes, user profiles, and user-item interactions\ncan be retrieved by prompting the LLMs. Second, we analyze the impact of\nmemorization on recommendation performance. Lastly, we examine whether\nmemorization varies across model families and model sizes. Our results reveal\nthat all models exhibit some degree of memorization of MovieLens-1M, and that\nrecommendation performance is related to the extent of memorization. We have\nmade all the code publicly available at:\nhttps://github.com/sisinflab/LLM-MemoryInspector\n","authors":["Dario Di Palma","Felice Antonio Merra","Maurizio Sfilio","Vito Walter Anelli","Fedelucio Narducci","Tommaso Di Noia"],"pdf_url":"https://arxiv.org/pdf/2505.10212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19458v3","updated":"2025-05-15T09:07:58Z","published":"2025-04-28T03:48:23Z","title":"Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal\n  Perspective","summary":"  Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from\ndifferent Multi-Modal Knowledge Graphs (MMKGs), a critical information\nretrieval task. Existing studies have explored various fusion paradigms and\nconsistency constraints to improve the alignment of equivalent entities, while\noverlooking that the visual modality may not always contribute positively.\nEmpirically, entities with low-similarity images usually generate\nunsatisfactory performance, highlighting the limitation of overly relying on\nvisual features. We believe the model can be biased toward the visual modality,\nleading to a shortcut image-matching task. To address this, we propose a\ncounterfactual debiasing framework for MMEA, termed CDMEA, which investigates\nvisual modality bias from a causal perspective. Our approach aims to leverage\nboth visual and graph modalities to enhance MMEA while suppressing the direct\ncausal effect of the visual modality on model predictions. By estimating the\nTotal Effect (TE) of both modalities and excluding the Natural Direct Effect\n(NDE) of the visual modality, we ensure that the model predicts based on the\nTotal Indirect Effect (TIE), effectively utilizing both modalities and reducing\nvisual modality bias. Extensive experiments on 9 benchmark datasets show that\nCDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,\nhigh-noise, and low-resource data scenarios.\n","authors":["Taoyu Su","Jiawei Sheng","Duohe Ma","Xiaodong Li","Juwei Yue","Mengxiao Song","Yingkai Tang","Tingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2504.19458v3.pdf","comment":"Accepted by SIGIR 2025, 11 pages, 10 figures, 4 tables,"},{"id":"http://arxiv.org/abs/2505.10043v1","updated":"2025-05-15T07:41:14Z","published":"2025-05-15T07:41:14Z","title":"Boosting Text-to-Chart Retrieval through Training with Synthesized\n  Semantic Insights","summary":"  Charts are crucial for data analysis and decision-making.Text-to-chart\nretrieval systems have become increasingly important for Business Intelligence\n(BI), where users need to find relevant charts that match their analytical\nneeds. These needs can be categorized into precise queries that are\nwell-specified and fuzzy queries that are more exploratory -- both require\nunderstanding the semantics and context of the charts. However, existing\ntext-to-chart retrieval solutions often fail to capture the semantic content\nand contextual information of charts, primarily due to the lack of\ncomprehensive metadata (or semantic insights). To address this limitation, we\npropose a training data development pipeline that automatically synthesizes\nhierarchical semantic insights for charts, covering visual patterns\n(visual-oriented), statistical properties (statistics-oriented), and practical\napplications (task-oriented), which produces 207,498 semantic insights for\n69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to\nlearn better representations of charts for text-to-chart retrieval. Our method\nleverages rich semantic insights during the training phase to develop a model\nthat understands both visual and semantic aspects of charts.To evaluate\ntext-to-chart retrieval performance, we curate the first benchmark, CRBench,\nfor this task with 21,862 charts and 326 text queries from real-world BI\napplications, with ground-truth labels verified by the crowd\nworkers.Experiments show that ChartFinder significantly outperforms existing\nmethods in text-to-chart retrieval tasks across various settings. For precise\nqueries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than\nstate-of-the-art models. In fuzzy query tasks, our method also demonstrates\nconsistent improvements, with an average increase of 5% across nearly all\nmetrics.\n","authors":["Yifan Wu","Lutao Yan","Yizhang Zhu","Yinan Mei","Jiannan Wang","Nan Tang","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2505.10043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21300v4","updated":"2025-05-15T03:22:21Z","published":"2024-07-31T03:00:59Z","title":"SAKR: Enhancing Retrieval-Augmented Generation via Streaming Algorithm\n  and K-Means Clustering","summary":"  Retrieval-augmented generation (RAG) has achieved significant success in\ninformation retrieval to assist large language models LLMs because it builds an\nexternal knowledge database. However, it also has many problems, it consumes a\nlot of memory because of the enormous database, and it cannot update the\nestablished index database in time when confronted with massive streaming data.\nTo reduce the memory required for building the database and maintain accuracy\nsimultaneously, we proposed a new approach integrating a streaming algorithm\nwith k-means clustering into RAG. Our approach applied a streaming algorithm to\nupdate the index dynamically and reduce memory consumption. Additionally, the\nk-means algorithm clusters highly similar documents, and the query time would\nbe shortened. We conducted comparative experiments on four methods, and the\nresults indicated that RAG with streaming algorithm and k-means clusters\noutperforms traditional RAG in accuracy and memory, particularly when dealing\nwith large-scale streaming data.\n","authors":["Haoyu Kang","Yuzhou Zhu","Yukun Zhong","Ke Wang"],"pdf_url":"https://arxiv.org/pdf/2407.21300v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.02288v4","updated":"2025-05-15T02:47:32Z","published":"2025-04-03T05:27:55Z","title":"Shallow AutoEncoding Recommender with Cold Start Handling via Side\n  Features","summary":"  User and item cold starts present significant challenges in industrial\napplications of recommendation systems. Supplementing user-item interaction\ndata with metadata is a common solution-but often at the cost of introducing\nadditional biases. In this work, we introduce an augmented EASE model that\nseamlessly integrates both user and item side information to address these cold\nstart issues. Our straightforward, autoencoder-based method produces a\nclosed-form solution that leverages rich content signals for cold items while\nrefining user representations in data-sparse environments. Importantly, our\nmethod strikes a balance by effectively recommending cold start items and\nhandling cold start users without incurring extra bias, and it maintains strong\nperformance in warm settings. Experimental results demonstrate improved\nrecommendation accuracy and robustness compared to previous collaborative\nfiltering approaches. Moreover, our model serves as a strong baseline for\nfuture comparative studies.\n","authors":["Edward DongBo Cui","Lu Zhang","William Ping-hsun Lee"],"pdf_url":"https://arxiv.org/pdf/2504.02288v4.pdf","comment":"Preparing submission to CIKM 2025; 2 Figures; 4 Tables; 13 pages;\n  Python code implementation example"},{"id":"http://arxiv.org/abs/2505.10740v1","updated":"2025-05-15T23:04:46Z","published":"2025-05-15T23:04:46Z","title":"SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim\n  Retrieval","summary":"  The rapid spread of online disinformation presents a global challenge, and\nmachine learning has been widely explored as a potential solution. However,\nmultilingual settings and low-resource languages are often neglected in this\nfield. To address this gap, we conducted a shared task on multilingual claim\nretrieval at SemEval 2025, aimed at identifying fact-checked claims that match\nnewly encountered claims expressed in social media posts across different\nlanguages. The task includes two subtracks: (1) a monolingual track, where\nsocial posts and claims are in the same language, and (2) a crosslingual track,\nwhere social posts and claims might be in different languages. A total of 179\nparticipants registered for the task contributing to 52 test submissions. 23\nout of 31 teams have submitted their system papers. In this paper, we report\nthe best-performing systems as well as the most common and the most effective\napproaches across both subtracks. This shared task, along with its dataset and\nparticipating systems, provides valuable insights into multilingual claim\nretrieval and automated fact-checking, supporting future research in this\nfield.\n","authors":["Qiwei Peng","Robert Moro","Michal Gregor","Ivan Srba","Simon Ostermann","Marian Simko","Juraj Podroužek","Matúš Mesarčík","Jaroslav Kopčan","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2505.10740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09364v2","updated":"2025-05-15T21:26:23Z","published":"2025-05-14T13:13:53Z","title":"Diffusion Recommender Models and the Illusion of Progress: A Concerning\n  Study of Reproducibility and a Conceptual Mismatch","summary":"  Countless new machine learning models are published every year and are\nreported to significantly advance the state-of-the-art in \\emph{top-n}\nrecommendation. However, earlier reproducibility studies indicate that progress\nin this area may be quite limited. Specifically, various widespread\nmethodological issues, e.g., comparisons with untuned baseline models, have led\nto an \\emph{illusion of progress}. In this work, our goal is to examine whether\nthese problems persist in today's research. To this end, we aim to reproduce\nthe latest advancements reported from applying modern Denoising Diffusion\nProbabilistic Models to recommender systems, focusing on four models published\nat the top-ranked SIGIR conference in 2023 and 2024. Our findings are\nconcerning, revealing persistent methodological problems. Alarmingly, through\nexperiments, we find that the latest recommendation techniques based on\ndiffusion models, despite their computational complexity and substantial carbon\nfootprint, are consistently outperformed by simpler existing models.\nFurthermore, we identify key mismatches between the characteristics of\ndiffusion models and those of the traditional \\emph{top-n} recommendation task,\nraising doubts about their suitability for recommendation. We also note that,\nin the papers we analyze, the generative capabilities of these models are\nconstrained to a minimum. Overall, our results and continued methodological\nissues call for greater scientific rigor and a disruptive change in the\nresearch and publication culture in this area.\n","authors":["Michael Benigni","Maurizio Ferrari Dacrema","Dietmar Jannach"],"pdf_url":"https://arxiv.org/pdf/2505.09364v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00745v2","updated":"2025-05-15T21:22:32Z","published":"2025-01-01T06:23:26Z","title":"Dynamics of Adversarial Attacks on Large Language Model-Based Search\n  Engines","summary":"  The increasing integration of Large Language Model (LLM) based search engines\nhas transformed the landscape of information retrieval. However, these systems\nare vulnerable to adversarial attacks, especially ranking manipulation attacks,\nwhere attackers craft webpage content to manipulate the LLM's ranking and\npromote specific content, gaining an unfair advantage over competitors. In this\npaper, we study the dynamics of ranking manipulation attacks. We frame this\nproblem as an Infinitely Repeated Prisoners' Dilemma, where multiple players\nstrategically decide whether to cooperate or attack. We analyze the conditions\nunder which cooperation can be sustained, identifying key factors such as\nattack costs, discount rates, attack success rates, and trigger strategies that\ninfluence player behavior. We identify tipping points in the system dynamics,\ndemonstrating that cooperation is more likely to be sustained when players are\nforward-looking. However, from a defense perspective, we find that simply\nreducing attack success probabilities can, paradoxically, incentivize attacks\nunder certain conditions. Furthermore, defensive measures to cap the upper\nbound of attack success rates may prove futile in some scenarios. These\ninsights highlight the complexity of securing LLM-based systems. Our work\nprovides a theoretical foundation and practical insights for understanding and\nmitigating their vulnerabilities, while emphasizing the importance of adaptive\nsecurity strategies and thoughtful ecosystem design.\n","authors":["Xiyang Hu"],"pdf_url":"https://arxiv.org/pdf/2501.00745v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14609v2","updated":"2025-05-15T21:15:46Z","published":"2024-10-18T17:03:17Z","title":"DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in\n  Conversational Search","summary":"  Conversational Search (CS) involves retrieving relevant documents from a\ncorpus while considering the conversational context, integrating retrieval with\ncontext modeling. Recent advancements in Large Language Models (LLMs) have\nsignificantly enhanced CS by enabling query rewriting based on conversational\ncontext. However, employing LLMs during inference poses efficiency challenges.\nExisting solutions mitigate this issue by distilling embeddings derived from\nhuman-rewritten queries, focusing primarily on learning the context modeling\ntask. These methods, however, often separate the contrastive retrieval task\nfrom the distillation process, treating it as an independent loss term. To\novercome these limitations, we introduce DiSCo (Distillation of Sparse\nConversational retrieval), a novel approach that unifies retrieval and context\nmodeling through a relaxed distillation objective. Instead of relying\nexclusively on representation learning, our method distills similarity scores\nbetween conversations and documents, providing more freedom in the\nrepresentation space and better leveraging the contrastive nature of document\nrelevance. Extensive experiments on Learned Sparse Retrieval (LSR) across five\nCS datasets demonstrate that DiSCo achieves substantial improvements in both\nin-domain and out-of-domain retrieval tasks, achieving up to a six-point gain\nin recall for out-of-domain datasets over state-of-the-art methods.\nAdditionally, DiSCo employs a multi-teacher distillation strategy, using\nmultiple LLMs as teachers, further enhancing performance and surpassing the\nindividual teachers in in-domain settings. Furthermore, analysis of model\nsparsity reveals that DiSCo allows for more effective control over the sparsity\nof the trained models.\n","authors":["Simon Lupart","Mohammad Aliannejadi","Evangelos Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2410.14609v2.pdf","comment":"11 pages, 6 figures. SIGIR '25 Proceedings of the 48th International\n  ACM SIGIR Conference on Research and Development in Information Retrieval\n  July 13--18, 2025 Padua, Italy"},{"id":"http://arxiv.org/abs/2505.11552v1","updated":"2025-05-15T15:49:56Z","published":"2025-05-15T15:49:56Z","title":"GSPRec: Temporal-Aware Graph Spectral Filtering for Recommendation","summary":"  Graph-based recommendation systems are effective at modeling collaborative\npatterns but often suffer from two limitations: overreliance on low-pass\nfiltering, which suppresses user-specific signals, and omission of sequential\ndynamics in graph construction. We introduce GSPRec, a graph spectral model\nthat integrates temporal transitions through sequentially-informed graph\nconstruction and applies frequency-aware filtering in the spectral domain.\nGSPRec encodes item transitions via multi-hop diffusion to enable the use of\nsymmetric Laplacians for spectral processing. To capture user preferences, we\ndesign a dual-filtering mechanism: a Gaussian bandpass filter to extract\nmid-frequency, user-level patterns, and a low-pass filter to retain global\ntrends. Extensive experiments on four public datasets show that GSPRec\nconsistently outperforms baselines, with an average improvement of 6.77% in\nNDCG@10. Ablation studies show the complementary benefits of both sequential\ngraph augmentation and bandpass filtering.\n","authors":["Ahmad Bin Rabiah","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2505.11552v1.pdf","comment":null}]},"2025-05-14T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2505.09837v1","updated":"2025-05-14T22:34:26Z","published":"2025-05-14T22:34:26Z","title":"EdgeAI Drone for Autonomous Construction Site Demonstrator","summary":"  The fields of autonomous systems and robotics are receiving considerable\nattention in civil applications such as construction, logistics, and\nfirefighting. Nevertheless, the widespread adoption of these technologies is\nhindered by the necessity for robust processing units to run AI models. Edge-AI\nsolutions offer considerable promise, enabling low-power, cost-effective\nrobotics that can automate civil services, improve safety, and enhance\nsustainability. This paper presents a novel Edge-AI-enabled drone-based\nsurveillance system for autonomous multi-robot operations at construction\nsites. Our system integrates a lightweight MCU-based object detection model\nwithin a custom-built UAV platform and a 5G-enabled multi-agent coordination\ninfrastructure. We specifically target the real-time obstacle detection and\ndynamic path planning problem in construction environments, providing a\ncomprehensive dataset specifically created for MCU-based edge applications.\nField experiments demonstrate practical viability and identify optimal\noperational parameters, highlighting our approach's scalability and\ncomputational efficiency advantages compared to existing UAV solutions. The\npresent and future roles of autonomous vehicles on construction sites are also\ndiscussed, as well as the effectiveness of edge-AI solutions. We share our\ndataset publicly at github.com/egirgin/storaige-b950\n","authors":["Emre Girgin","Arda Taha Candan","Coşkun Anıl Zaman"],"pdf_url":"https://arxiv.org/pdf/2505.09837v1.pdf","comment":"Paper presented at the 4th Workshop on Future of Construction: Safe,\n  Reliable, and Precise Robots in Construction Environments, ICRA 2025,\n  Atlanta, GA, United States"},{"id":"http://arxiv.org/abs/2505.09833v1","updated":"2025-05-14T22:23:30Z","published":"2025-05-14T22:23:30Z","title":"Learning Rock Pushability on Rough Planetary Terrain","summary":"  In the context of mobile navigation in unstructured environments, the\npredominant approach entails the avoidance of obstacles. The prevailing path\nplanning algorithms are contingent upon deviating from the intended path for an\nindefinite duration and returning to the closest point on the route after the\nobstacle is left behind spatially. However, avoiding an obstacle on a path that\nwill be used repeatedly by multiple agents can hinder long-term efficiency and\nlead to a lasting reliance on an active path planning system. In this study, we\npropose an alternative approach to mobile navigation in unstructured\nenvironments by leveraging the manipulation capabilities of a robotic\nmanipulator mounted on top of a mobile robot. Our proposed framework integrates\nexteroceptive and proprioceptive feedback to assess the push affordance of\nobstacles, facilitating their repositioning rather than avoidance. While our\npreliminary visual estimation takes into account the characteristics of both\nthe obstacle and the surface it relies on, the push affordance estimation\nmodule exploits the force feedback obtained by interacting with the obstacle\nvia a robotic manipulator as the guidance signal. The objective of our\nnavigation approach is to enhance the efficiency of routes utilized by multiple\nagents over extended periods by reducing the overall time spent by a fleet in\nenvironments where autonomous infrastructure development is imperative, such as\nlunar or Martian surfaces.\n","authors":["Tuba Girgin","Emre Girgin","Cagri Kilic"],"pdf_url":"https://arxiv.org/pdf/2505.09833v1.pdf","comment":"Paper presented at the Workshop on Field Robotics, ICRA 2025,\n  Atlanta, GA, United States"},{"id":"http://arxiv.org/abs/2503.00262v2","updated":"2025-05-14T21:25:51Z","published":"2025-03-01T00:40:27Z","title":"CRADMap: Applied Distributed Volumetric Mapping with 5G-Connected\n  Multi-Robots and 4D Radar Perception","summary":"  Sparse and feature SLAM methods provide robust camera pose estimation.\nHowever, they often fail to capture the level of detail required for inspection\nand scene awareness tasks. Conversely, dense SLAM approaches generate richer\nscene reconstructions but impose a prohibitive computational load to create 3D\nmaps. We present a novel distributed volumetric mapping framework designated as\nCRADMap that addresses these issues by extending the state-of-the-art (SOTA)\nORBSLAM3 system with the COVINS on the backend for global optimization. Our\npipeline for volumetric reconstruction fuses dense keyframes at a centralized\nserver via 5G connectivity, aggregating geometry, and occupancy information\nfrom multiple autonomous mobile robots (AMRs) without overtaxing onboard\nresources. This enables each AMR to independently perform mapping while the\nbackend constructs high-fidelity real-time 3D maps. To operate Beyond the\nVisible (BtV) and overcome the limitations of standard visual sensors, we\nautomated a standalone 4D mmWave radar module that functions independently\nwithout sensor fusion with SLAM. The BtV system enables the detection and\nmapping of occluded metallic objects in cluttered environments, enhancing\nsituational awareness in inspection scenarios. Experimental validation in\nSection~\\ref{sec:IV} demonstrates the effectiveness of our framework.\n","authors":["Maaz Qureshi","Alexander Werner","Zhenan Liu","Amir Khajepour","George Shaker","William Melek"],"pdf_url":"https://arxiv.org/pdf/2503.00262v2.pdf","comment":"7 pages, 5 figures, IEEE, ICARM"},{"id":"http://arxiv.org/abs/2409.07011v2","updated":"2025-05-14T20:21:05Z","published":"2024-09-11T04:48:21Z","title":"Physical synchronization of soft self-oscillating limbs for fast and\n  autonomous locomotion","summary":"  Animals achieve robust locomotion by offloading regulation from the brain to\nphysical couplings within the body. In contrast, locomotion in artificial\nsystems often depends on centralized processors. We introduce a rapid and\nautonomous locomotion strategy with synchronized gaits emerging through\nphysical interactions between self-oscillating limbs and the environment,\nwithout control signals. Each limb is a single soft tube that only requires\nconstant flow of air to perform cyclic stepping motions at frequencies reaching\n300 hertz. By combining several of these self-oscillating limbs, their physical\nsynchronization enables locomotion speeds that are orders of magnitude faster\nthan comparable state-of-the-art. Through body-environment dynamics, these\nseemingly simple devices exhibit autonomy, including obstacle avoidance,\namphibious gait transitions, and phototaxis.\n","authors":["Alberto Comoretto","Harmannus A. H. Schomaker","Johannes T. B. Overvelde"],"pdf_url":"https://arxiv.org/pdf/2409.07011v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09780v1","updated":"2025-05-14T20:18:03Z","published":"2025-05-14T20:18:03Z","title":"Neural Inertial Odometry from Lie Events","summary":"  Neural displacement priors (NDP) can reduce the drift in inertial odometry\nand provide uncertainty estimates that can be readily fused with off-the-shelf\nfilters. However, they fail to generalize to different IMU sampling rates and\ntrajectory profiles, which limits their robustness in diverse settings. To\naddress this challenge, we replace the traditional NDP inputs comprising raw\nIMU data with Lie events that are robust to input rate changes and have\nfavorable invariances when observed under different trajectory profiles. Unlike\nraw IMU data sampled at fixed rates, Lie events are sampled whenever the norm\nof the IMU pre-integration change, mapped to the Lie algebra of the SE(3)\ngroup, exceeds a threshold. Inspired by event-based vision, we generalize the\nnotion of level-crossing on 1D signals to level-crossings on the Lie algebra\nand generalize binary polarities to normalized Lie polarities within this\nalgebra. We show that training NDPs on Lie events incorporating these\npolarities reduces the trajectory error of off-the-shelf downstream inertial\nodometry methods by up to 21% with only minimal preprocessing. We conjecture\nthat many more sensors than IMUs or cameras can benefit from an event-based\nsampling paradigm and that this work makes an important first step in this\ndirection.\n","authors":["Royina Karegoudra Jayanth","Yinshuang Xu","Evangelos Chatzipantazis","Kostas Daniilidis","Daniel Gehrig"],"pdf_url":"https://arxiv.org/pdf/2505.09780v1.pdf","comment":"accepted at RSS 2025"},{"id":"http://arxiv.org/abs/2505.09771v1","updated":"2025-05-14T20:04:45Z","published":"2025-05-14T20:04:45Z","title":"Grasp EveryThing (GET): 1-DoF, 3-Fingered Gripper with Tactile Sensing\n  for Robust Grasping","summary":"  We introduce the Grasp EveryThing (GET) gripper, a novel 1-DoF, 3-finger\ndesign for securely grasping objects of many shapes and sizes. Mounted on a\nstandard parallel jaw actuator, the design features three narrow, tapered\nfingers arranged in a two-against-one configuration, where the two fingers\nconverge into a V-shape. The GET gripper is more capable of conforming to\nobject geometries and forming secure grasps than traditional designs with two\nflat fingers. Inspired by the principle of self-similarity, these V-shaped\nfingers enable secure grasping across a wide range of object sizes. Further to\nthis end, fingers are parametrically designed for convenient resizing and\ninterchangeability across robotic embodiments with a parallel jaw gripper.\nAdditionally, we incorporate a rigid fingernail to enhance small object\nmanipulation. Tactile sensing can be integrated into the standalone finger via\nan externally-mounted camera. A neural network was trained to estimate normal\nforce from tactile images with an average validation error of 1.3~N across a\ndiverse set of geometries. In grasping 15 objects and performing 3 tasks via\nteleoperation, the GET fingers consistently outperformed standard flat fingers.\nFinger designs for use with multiple robotic embodiments are available on\nGitHub.\n","authors":["Michael Burgess","Edward H. Adelson"],"pdf_url":"https://arxiv.org/pdf/2505.09771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09760v1","updated":"2025-05-14T19:46:23Z","published":"2025-05-14T19:46:23Z","title":"Neural Associative Skill Memories for safer robotics and modelling human\n  sensorimotor repertoires","summary":"  Modern robots face challenges shared by humans, where machines must learn\nmultiple sensorimotor skills and express them adaptively. Equipping robots with\na human-like memory of how it feels to do multiple stereotypical movements can\nmake robots more aware of normal operational states and help develop\nself-preserving safer robots. Associative Skill Memories (ASMs) aim to address\nthis by linking movement primitives to sensory feedback, but existing\nimplementations rely on hard-coded libraries of individual skills. A key\nunresolved problem is how a single neural network can learn a repertoire of\nskills while enabling fault detection and context-aware execution. Here we\nintroduce Neural Associative Skill Memories (ASMs), a framework that utilises\nself-supervised predictive coding for temporal prediction to unify skill\nlearning and expression, using biologically plausible learning rules. Unlike\ntraditional ASMs which require explicit skill selection, Neural ASMs implicitly\nrecognize and express skills through contextual inference, enabling fault\ndetection across learned behaviours without an explicit skill selection\nmechanism. Compared to recurrent neural networks trained via backpropagation\nthrough time, our model achieves comparable qualitative performance in skill\nmemory expression while using local learning rules and predicts a biologically\nrelevant speed-accuracy trade-off during skill memory expression. This work\nadvances the field of neurorobotics by demonstrating how predictive coding\nprinciples can model adaptive robot control and human motor preparation. By\nunifying fault detection, reactive control, skill memorisation and expression\ninto a single energy-based architecture, Neural ASMs contribute to safer\nrobotics and provide a computational lens to study biological sensorimotor\nlearning.\n","authors":["Pranav Mahajan","Mufeng Tang","T. Ed Li","Ioannis Havoutis","Ben Seymour"],"pdf_url":"https://arxiv.org/pdf/2505.09760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19989v2","updated":"2025-05-14T19:20:06Z","published":"2024-10-25T22:11:54Z","title":"On-Robot Reinforcement Learning with Goal-Contrastive Rewards","summary":"  Reinforcement Learning (RL) has the potential to enable robots to learn from\ntheir own actions in the real world. Unfortunately, RL can be prohibitively\nexpensive, in terms of on-robot runtime, due to inefficient exploration when\nlearning from a sparse reward signal. Designing dense reward functions is\nlabour-intensive and requires domain expertise. In our work, we propose GCR\n(Goal-Contrastive Rewards), a dense reward function learning method that can be\ntrained on passive video demonstrations. By using videos without actions, our\nmethod is easier to scale, as we can use arbitrary videos. GCR combines two\nloss functions, an implicit value loss function that models how the reward\nincreases when traversing a successful trajectory, and a goal-contrastive loss\nthat discriminates between successful and failed trajectories. We perform\nexperiments in simulated manipulation environments across RoboMimic and\nMimicGen tasks, as well as in the real world using a Franka arm and a Spot\nquadruped. We find that GCR leads to a more-sample efficient RL, enabling\nmodel-free RL to solve about twice as many tasks as our baseline reward\nlearning methods. We also demonstrate positive cross-embodiment transfer from\nvideos of people and of other robots performing a task. Website:\nhttps://gcr-robot.github.io/.\n","authors":["Ondrej Biza","Thomas Weng","Lingfeng Sun","Karl Schmeckpeper","Tarik Kelestemur","Yecheng Jason Ma","Robert Platt","Jan-Willem van de Meent","Lawson L. S. Wong"],"pdf_url":"https://arxiv.org/pdf/2410.19989v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09739v1","updated":"2025-05-14T19:00:28Z","published":"2025-05-14T19:00:28Z","title":"Trailblazer: Learning offroad costmaps for long range planning","summary":"  Autonomous navigation in off-road environments remains a significant\nchallenge in field robotics, particularly for Unmanned Ground Vehicles (UGVs)\ntasked with search and rescue, exploration, and surveillance. Effective\nlong-range planning relies on the integration of onboard perception systems\nwith prior environmental knowledge, such as satellite imagery and LiDAR data.\nThis work introduces Trailblazer, a novel framework that automates the\nconversion of multi-modal sensor data into costmaps, enabling efficient path\nplanning without manual tuning. Unlike traditional approaches, Trailblazer\nleverages imitation learning and a differentiable A* planner to learn costmaps\ndirectly from expert demonstrations, enhancing adaptability across diverse\nterrains. The proposed methodology was validated through extensive real-world\ntesting, achieving robust performance in dynamic and complex environments,\ndemonstrating Trailblazer's potential for scalable, efficient autonomous\nnavigation.\n","authors":["Kasi Viswanath","Felix Sanchez","Timothy Overbye","Jason M. Gregory","Srikanth Saripalli"],"pdf_url":"https://arxiv.org/pdf/2505.09739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09737v1","updated":"2025-05-14T18:57:51Z","published":"2025-05-14T18:57:51Z","title":"General Dynamic Goal Recognition","summary":"  Understanding an agent's intent through its behavior is essential in\nhuman-robot interaction, interactive AI systems, and multi-agent\ncollaborations. This task, known as Goal Recognition (GR), poses significant\nchallenges in dynamic environments where goals are numerous and constantly\nevolving. Traditional GR methods, designed for a predefined set of goals, often\nstruggle to adapt to these dynamic scenarios. To address this limitation, we\nintroduce the General Dynamic GR problem - a broader definition of GR - aimed\nat enabling real-time GR systems and fostering further research in this area.\nExpanding on this foundation, this paper employs a model-free goal-conditioned\nRL approach to enable fast adaptation for GR across various changing tasks.\n","authors":["Osher Elhadad","Reuth Mirsky"],"pdf_url":"https://arxiv.org/pdf/2505.09737v1.pdf","comment":"Accepted for publication at Generalization in Planning (GenPlan) as\n  part of AAAI 2025 workshops"},{"id":"http://arxiv.org/abs/2505.09734v1","updated":"2025-05-14T18:49:32Z","published":"2025-05-14T18:49:32Z","title":"Risk-Aware Safe Reinforcement Learning for Control of Stochastic Linear\n  Systems","summary":"  This paper presents a risk-aware safe reinforcement learning (RL) control\ndesign for stochastic discrete-time linear systems. Rather than using a safety\ncertifier to myopically intervene with the RL controller, a risk-informed safe\ncontroller is also learned besides the RL controller, and the RL and safe\ncontrollers are combined together. Several advantages come along with this\napproach: 1) High-confidence safety can be certified without relying on a\nhigh-fidelity system model and using limited data available, 2) Myopic\ninterventions and convergence to an undesired equilibrium can be avoided by\ndeciding on the contribution of two stabilizing controllers, and 3) highly\nefficient and computationally tractable solutions can be provided by optimizing\nover a scalar decision variable and linear programming polyhedral sets. To\nlearn safe controllers with a large invariant set, piecewise affine controllers\nare learned instead of linear controllers. To this end, the closed-loop system\nis first represented using collected data, a decision variable, and noise. The\neffect of the decision variable on the variance of the safe violation of the\nclosed-loop system is formalized. The decision variable is then designed such\nthat the probability of safety violation for the learned closed-loop system is\nminimized. It is shown that this control-oriented approach reduces the data\nrequirements and can also reduce the variance of safety violations. Finally, to\nintegrate the safe and RL controllers, a new data-driven interpolation\ntechnique is introduced. This method aims to maintain the RL agent's optimal\nimplementation while ensuring its safety within environments characterized by\nnoise. The study concludes with a simulation example that serves to validate\nthe theoretical results.\n","authors":["Babak Esmaeili","Nariman Niknejad","Hamidreza Modares"],"pdf_url":"https://arxiv.org/pdf/2505.09734v1.pdf","comment":"Submitted to Asian Journal of Control"},{"id":"http://arxiv.org/abs/2505.09731v1","updated":"2025-05-14T18:45:59Z","published":"2025-05-14T18:45:59Z","title":"Unfettered Forceful Skill Acquisition with Physical Reasoning and\n  Coordinate Frame Labeling","summary":"  Vision language models (VLMs) exhibit vast knowledge of the physical world,\nincluding intuition of physical and spatial properties, affordances, and\nmotion. With fine-tuning, VLMs can also natively produce robot trajectories. We\ndemonstrate that eliciting wrenches, not trajectories, allows VLMs to\nexplicitly reason about forces and leads to zero-shot generalization in a\nseries of manipulation tasks without pretraining. We achieve this by overlaying\na consistent visual representation of relevant coordinate frames on\nrobot-attached camera images to augment our query. First, we show how this\naddition enables a versatile motion control framework evaluated across four\ntasks (opening and closing a lid, pushing a cup or chair) spanning prismatic\nand rotational motion, an order of force and position magnitude, different\ncamera perspectives, annotation schemes, and two robot platforms over 220\nexperiments, resulting in 51% success across the four tasks. Then, we\ndemonstrate that the proposed framework enables VLMs to continually reason\nabout interaction feedback to recover from task failure or incompletion, with\nand without human supervision. Finally, we observe that prompting schemes with\nvisual annotation and embodied reasoning can bypass VLM safeguards. We\ncharacterize prompt component contribution to harmful behavior elicitation and\ndiscuss its implications for developing embodied reasoning. Our code, videos,\nand data are available at: https://scalingforce.github.io/.\n","authors":["William Xie","Max Conway","Yutong Zhang","Nikolaus Correll"],"pdf_url":"https://arxiv.org/pdf/2505.09731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09723v1","updated":"2025-05-14T18:30:53Z","published":"2025-05-14T18:30:53Z","title":"EnerVerse-AC: Envisioning Embodied Environments with Action Condition","summary":"  Robotic imitation learning has advanced from solving static tasks to\naddressing dynamic interaction scenarios, but testing and evaluation remain\ncostly and challenging due to the need for real-time interaction with dynamic\nenvironments. We propose EnerVerse-AC (EVAC), an action-conditional world model\nthat generates future visual observations based on an agent's predicted\nactions, enabling realistic and controllable robotic inference. Building on\nprior architectures, EVAC introduces a multi-level action-conditioning\nmechanism and ray map encoding for dynamic multi-view image generation while\nexpanding training data with diverse failure trajectories to improve\ngeneralization. As both a data engine and evaluator, EVAC augments\nhuman-collected trajectories into diverse datasets and generates realistic,\naction-conditioned video observations for policy testing, eliminating the need\nfor physical robots or complex simulations. This approach significantly reduces\ncosts while maintaining high fidelity in robotic manipulation evaluation.\nExtensive experiments validate the effectiveness of our method. Code,\ncheckpoints, and datasets can be found at\n<https://annaj2178.github.io/EnerverseAC.github.io>.\n","authors":["Yuxin Jiang","Shengcong Chen","Siyuan Huang","Liliang Chen","Pengfei Zhou","Yue Liao","Xindong He","Chiming Liu","Hongsheng Li","Maoqing Yao","Guanghui Ren"],"pdf_url":"https://arxiv.org/pdf/2505.09723v1.pdf","comment":"Website: https://annaj2178.github.io/EnerverseAC.github.io"},{"id":"http://arxiv.org/abs/2405.18418v3","updated":"2025-05-14T18:27:05Z","published":"2024-05-28T17:57:23Z","title":"Hierarchical World Models as Visual Whole-Body Humanoid Controllers","summary":"  Whole-body control for humanoids is challenging due to the high-dimensional\nnature of the problem, coupled with the inherent instability of a bipedal\nmorphology. Learning from visual observations further exacerbates this\ndifficulty. In this work, we explore highly data-driven approaches to visual\nwhole-body humanoid control based on reinforcement learning, without any\nsimplifying assumptions, reward design, or skill primitives. Specifically, we\npropose a hierarchical world model in which a high-level agent generates\ncommands based on visual observations for a low-level agent to execute, both of\nwhich are trained with rewards. Our approach produces highly performant control\npolicies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing\nmotions that are broadly preferred by humans.\n","authors":["Nicklas Hansen","Jyothir S V","Vlad Sobal","Yann LeCun","Xiaolong Wang","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2405.18418v3.pdf","comment":"Code and videos at https://nicklashansen.com/rlpuppeteer"},{"id":"http://arxiv.org/abs/2504.08706v2","updated":"2025-05-14T18:26:32Z","published":"2025-04-11T17:16:13Z","title":"BiFlex: A Passive Bimodal Stiffness Flexible Wrist for Manipulation in\n  Unstructured Environments","summary":"  Robotic manipulation in unstructured, humancentric environments poses a dual\nchallenge: achieving the precision need for delicate free-space operation while\nensuring safety during unexpected contact events. Traditional wrists struggle\nto balance these demands, often relying on complex control schemes or\ncomplicated mechanical designs to mitigate potential damage from force\noverload. In response, we present BiFlex, a flexible robotic wrist that uses a\nsoft buckling honeycomb structure to provides a natural bimodal stiffness\nresponse. The higher stiffness mode enables precise household object\nmanipulation, while the lower stiffness mode provides the compliance needed to\nadapt to external forces. We design BiFlex to maintain a fingertip deflection\nof less than 1 cm while supporting loads up to 500g and create a BiFlex wrist\nfor many grippers, including Panda, Robotiq, and BaRiFlex. We validate BiFlex\nunder several real-world experimental evaluations, including surface wiping,\nprecise pick-and-place, and grasping under environmental constraints. We\ndemonstrate that BiFlex simplifies control while maintaining precise object\nmanipulation and enhanced safety in real-world applications.\n","authors":["Gu-Cheol Jeong","Stefano Dalla Gasperina","Ashish D. Deshpande","Lillian Chin","Roberto Martín-Martín"],"pdf_url":"https://arxiv.org/pdf/2504.08706v2.pdf","comment":"8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2205.09431v3","updated":"2025-05-14T18:24:44Z","published":"2022-05-19T09:47:40Z","title":"Pitch-axis supermanoeuvrability in a biomimetic morphing-wing UAV","summary":"  Birds and bats are extremely adept flyers: whether in hunting prey, or\nevading predators, post-stall manoeuvrability is a characteristic of vital\nimportance. Their performance, in this regard, greatly exceeds that of uncrewed\naerial vehicles (UAVs) of similar scale. Attempts to attain post-stall\nmanoeuvrability, or supermanoeuvrability, in UAVs have typically focused on\nthrust-vectoring technology. Here we show that biomimetic wing morphing offers\nan additional pathway to classical supermanoeuvrability, as well as novel forms\nof bioinspired post-stall manoeuvrability. Using a state-of-the-art flight\nsimulator, equipped with a multibody model of lifting surface motion and a\ndelay differential equation (Goman-Khrabrov) dynamic stall model for all\nlifting surfaces, we demonstrate the capability of a biomimetic morphing-wing\nUAV for two post-stall manoeuvres: a classical rapid nose-pointing-and-shooting\n(RaNPAS) manoeuvre; and a wall landing manoeuvre inspired by biological\nballistic transitions. We develop a guidance method for these manoeuvres, based\non parametric variation of nonlinear longitudinal stability profiles, which\nallows efficient exploration of the space of post-stall manoeuvres in these\ntypes of UAVs; and yields insight into effective morphing kinematics to enable\nthese manoeuvres. Our results demonstrate the capability of biomimetic\nmorphing, and morphing control of nonlinear longitudinal stability, to enable\nadvanced forms of transient supermanoeuvrability in UAVs.\n","authors":["Arion Pons","Fehmi Cirak"],"pdf_url":"https://arxiv.org/pdf/2205.09431v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09698v1","updated":"2025-05-14T18:01:00Z","published":"2025-05-14T18:01:00Z","title":"ManipBench: Benchmarking Vision-Language Models for Low-Level Robot\n  Manipulation","summary":"  Vision-Language Models (VLMs) have revolutionized artificial intelligence and\nrobotics due to their commonsense reasoning capabilities. In robotic\nmanipulation, VLMs are used primarily as high-level planners, but recent work\nhas also studied their lower-level reasoning ability, which refers to making\ndecisions about precise robot movements. However, the community currently lacks\na clear and common benchmark that can evaluate how well VLMs can aid low-level\nreasoning in robotics. Consequently, we propose a novel benchmark, ManipBench,\nto evaluate the low-level robot manipulation reasoning capabilities of VLMs\nacross various dimensions, including how well they understand object-object\ninteractions and deformable object manipulation. We extensively test 33\nrepresentative VLMs across 10 model families on our benchmark, including\nvariants to test different model sizes. Our evaluation shows that the\nperformance of VLMs significantly varies across tasks, and there is a strong\ncorrelation between this performance and trends in our real-world manipulation\ntasks. It also shows that there remains a significant gap between these models\nand human-level understanding. See our website at:\nhttps://manipbench.github.io.\n","authors":["Enyu Zhao","Vedant Raval","Hejia Zhang","Jiageng Mao","Zeyu Shangguan","Stefanos Nikolaidis","Yue Wang","Daniel Seita"],"pdf_url":"https://arxiv.org/pdf/2505.09698v1.pdf","comment":"47 pages, 29 figures. Under review"},{"id":"http://arxiv.org/abs/2505.09694v1","updated":"2025-05-14T18:00:19Z","published":"2025-05-14T18:00:19Z","title":"EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied\n  World Models","summary":"  Recent advances in creative AI have enabled the synthesis of high-fidelity\nimages and videos conditioned on language instructions. Building on these\ndevelopments, text-to-video diffusion models have evolved into embodied world\nmodels (EWMs) capable of generating physically plausible scenes from language\ncommands, effectively bridging vision and action in embodied AI applications.\nThis work addresses the critical challenge of evaluating EWMs beyond general\nperceptual metrics to ensure the generation of physically grounded and\naction-consistent behaviors. We propose the Embodied World Model Benchmark\n(EWMBench), a dedicated framework designed to evaluate EWMs based on three key\naspects: visual scene consistency, motion correctness, and semantic alignment.\nOur approach leverages a meticulously curated dataset encompassing diverse\nscenes and motion patterns, alongside a comprehensive multi-dimensional\nevaluation toolkit, to assess and compare candidate models. The proposed\nbenchmark not only identifies the limitations of existing video generation\nmodels in meeting the unique requirements of embodied tasks but also provides\nvaluable insights to guide future advancements in the field. The dataset and\nevaluation tools are publicly available at\nhttps://github.com/AgibotTech/EWMBench.\n","authors":["Hu Yue","Siyuan Huang","Yue Liao","Shengcong Chen","Pengfei Zhou","Liliang Chen","Maoqing Yao","Guanghui Ren"],"pdf_url":"https://arxiv.org/pdf/2505.09694v1.pdf","comment":"Website: https://github.com/AgibotTech/EWMBench"},{"id":"http://arxiv.org/abs/2505.09603v1","updated":"2025-05-14T17:55:10Z","published":"2025-05-14T17:55:10Z","title":"DataMIL: Selecting Data for Robot Imitation Learning with Datamodels","summary":"  Recently, the robotics community has amassed ever larger and more diverse\ndatasets to train generalist robot policies. However, while these policies\nachieve strong mean performance across a variety of tasks, they often\nunderperform on individual, specialized tasks and require further tuning on\nnewly acquired task-specific data. Combining task-specific data with carefully\ncurated subsets of large prior datasets via co-training can produce better\nspecialized policies, but selecting data naively may actually harm downstream\nperformance. To address this, we introduce DataMIL, a policy-driven data\nselection framework built on the datamodels paradigm that reasons about data\nselection in an end-to-end manner, using the policy itself to identify which\ndata points will most improve performance. Unlike standard practices that\nfilter data using human notions of quality (e.g., based on semantic or visual\nsimilarity), DataMIL directly optimizes data selection for task success,\nallowing us to select data that enhance the policy while dropping data that\ndegrade it. To avoid performing expensive rollouts in the environment during\nselection, we use a novel surrogate loss function on task-specific data,\nallowing us to use DataMIL in the real world without degrading performance. We\nvalidate our approach on a suite of more than 60 simulation and real-world\nmanipulation tasks - most notably showing successful data selection from the\nOpen X-Embodiment datasets-demonstrating consistent gains in success rates and\nsuperior performance over multiple baselines. Our results underscore the\nimportance of end-to-end, performance-aware data selection for unlocking the\npotential of large prior datasets in robotics. More information at\nhttps://robin-lab.cs.utexas.edu/datamodels4imitation/\n","authors":["Shivin Dass","Alaa Khaddaj","Logan Engstrom","Aleksander Madry","Andrew Ilyas","Roberto Martín-Martín"],"pdf_url":"https://arxiv.org/pdf/2505.09603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09601v1","updated":"2025-05-14T17:50:35Z","published":"2025-05-14T17:50:35Z","title":"Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or\n  Robot Hardware","summary":"  Scaling robot learning requires vast and diverse datasets. Yet the prevailing\ndata collection paradigm-human teleoperation-remains costly and constrained by\nmanual effort and physical robot access. We introduce Real2Render2Real (R2R2R),\na novel approach for generating robot training data without relying on object\ndynamics simulation or teleoperation of robot hardware. The input is a\nsmartphone-captured scan of one or more objects and a single video of a human\ndemonstration. R2R2R renders thousands of high visual fidelity robot-agnostic\ndemonstrations by reconstructing detailed 3D object geometry and appearance,\nand tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to\nenable flexible asset generation and trajectory synthesis for both rigid and\narticulated objects, converting these representations to meshes to maintain\ncompatibility with scalable rendering engines like IsaacLab but with collision\nmodeling off. Robot demonstration data generated by R2R2R integrates directly\nwith models that operate on robot proprioceptive states and image observations,\nsuch as vision-language-action models (VLA) and imitation learning policies.\nPhysical experiments suggest that models trained on R2R2R data from a single\nhuman demonstration can match the performance of models trained on 150 human\nteleoperation demonstrations. Project page: https://real2render2real.com\n","authors":["Justin Yu","Letian Fu","Huang Huang","Karim El-Refai","Rares Andrei Ambrus","Richard Cheng","Muhammad Zubair Irshad","Ken Goldberg"],"pdf_url":"https://arxiv.org/pdf/2505.09601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.14687v3","updated":"2025-05-14T17:44:57Z","published":"2021-09-29T19:47:40Z","title":"Guaranteed Rejection-free Sampling Method Using Past Behaviours for\n  Motion Planning of Autonomous Systems","summary":"  The paper presents a novel learning-based sampling strategy that guarantees\nrejection-free sampling of the free space under both biased and approximately\nuniform conditions, leveraging multivariate kernel densities. Historical data\nfrom a given autonomous system is leveraged to estimate a non-parametric\nprobabilistic description of the domain, which also describes the free space\nwhere feasible solutions of the motion planning problem are likely to be found.\nThe tuning parameters of the kernel density estimator, the bandwidth and the\nkernel, are used to alter the description of the free space so that no samples\ncan fall outside the originally defined space.The proposed method is\ndemonstrated in two real-life case studies: An autonomous surface vessel (2D)\nand an autonomous drone (3D). Two planning problems are solved, showing that\nthe proposed approximately uniform sampling scheme is capable of guaranteeing\nrejection-free samples of the considered workspace. Furthermore, the\neffectiveness of the proposed method is statistically validated using Monte\nCarlo simulations.\n","authors":["Thomas T. Enevoldsen","Roberto Galeazzi"],"pdf_url":"https://arxiv.org/pdf/2109.14687v3.pdf","comment":"Accepted for publication in Robotics and Autonomous Systems"},{"id":"http://arxiv.org/abs/2505.09577v1","updated":"2025-05-14T17:29:35Z","published":"2025-05-14T17:29:35Z","title":"VTLA: Vision-Tactile-Language-Action Model with Preference Learning for\n  Insertion Manipulation","summary":"  While vision-language models have advanced significantly, their application\nin language-conditioned robotic manipulation is still underexplored, especially\nfor contact-rich tasks that extend beyond visually dominant pick-and-place\nscenarios. To bridge this gap, we introduce Vision-Tactile-Language-Action\nmodel, a novel framework that enables robust policy generation in\ncontact-intensive scenarios by effectively integrating visual and tactile\ninputs through cross-modal language grounding. A low-cost, multi-modal dataset\nhas been constructed in a simulation environment, containing\nvision-tactile-action-instruction pairs specifically designed for the fingertip\ninsertion task. Furthermore, we introduce Direct Preference Optimization (DPO)\nto offer regression-like supervision for the VTLA model, effectively bridging\nthe gap between classification-based next token prediction loss and continuous\nrobotic tasks. Experimental results show that the VTLA model outperforms\ntraditional imitation learning methods (e.g., diffusion policies) and existing\nmulti-modal baselines (TLA/VLA), achieving over 90% success rates on unseen peg\nshapes. Finally, we conduct real-world peg-in-hole experiments to demonstrate\nthe exceptional Sim2Real performance of the proposed VTLA model. For\nsupplementary videos and results, please visit our project website:\nhttps://sites.google.com/view/vtla\n","authors":["Chaofan Zhang","Peng Hao","Xiaoge Cao","Xiaoshuai Hao","Shaowei Cui","Shuo Wang"],"pdf_url":"https://arxiv.org/pdf/2505.09577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09561v1","updated":"2025-05-14T17:00:47Z","published":"2025-05-14T17:00:47Z","title":"Learning Long-Context Diffusion Policies via Past-Token Prediction","summary":"  Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x.\n","authors":["Marcel Torne","Andy Tang","Yuejiang Liu","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2505.09561v1.pdf","comment":"Videos are available at https://long-context-dp.github.io"},{"id":"http://arxiv.org/abs/2505.09546v1","updated":"2025-05-14T16:45:51Z","published":"2025-05-14T16:45:51Z","title":"Distilling Realizable Students from Unrealizable Teachers","summary":"  We study policy distillation under privileged information, where a student\npolicy with only partial observations must learn from a teacher with full-state\naccess. A key challenge is information asymmetry: the student cannot directly\naccess the teacher's state space, leading to distributional shifts and policy\ndegradation. Existing approaches either modify the teacher to produce\nrealizable but sub-optimal demonstrations or rely on the student to explore\nmissing information independently, both of which are inefficient. Our key\ninsight is that the student should strategically interact with the teacher\n--querying only when necessary and resetting from recovery states --to stay on\na recoverable path within its own observation space. We introduce two methods:\n(i) an imitation learning approach that adaptively determines when the student\nshould query the teacher for corrections, and (ii) a reinforcement learning\napproach that selects where to initialize training for efficient exploration.\nWe validate our methods in both simulated and real-world robotic tasks,\ndemonstrating significant improvements over standard teacher-student baselines\nin training efficiency and final performance. The project website is available\nat : https://portal-cornell.github.io/CritiQ_ReTRy/\n","authors":["Yujin Kim","Nathaniel Chin","Arnav Vasudev","Sanjiban Choudhury"],"pdf_url":"https://arxiv.org/pdf/2505.09546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09511v1","updated":"2025-05-14T16:03:01Z","published":"2025-05-14T16:03:01Z","title":"Design of a Formation Control System to Assist Human Operators in Flying\n  a Swarm of Robotic Blimps","summary":"  Formation control is essential for swarm robotics, enabling coordinated\nbehavior in complex environments. In this paper, we introduce a novel formation\ncontrol system for an indoor blimp swarm using a specialized leader-follower\napproach enhanced with a dynamic leader-switching mechanism. This strategy\nallows any blimp to take on the leader role, distributing maneuvering demands\nacross the swarm and enhancing overall formation stability. Only the leader\nblimp is manually controlled by a human operator, while follower blimps use\nonboard monocular cameras and a laser altimeter for relative position and\naltitude estimation. A leader-switching scheme is proposed to assist the human\noperator to maintain stability of the swarm, especially when a sharp turn is\nperformed. Experimental results confirm that the leader-switching mechanism\neffectively maintains stable formations and adapts to dynamic indoor\nenvironments while assisting human operator.\n","authors":["Tianfu Wu","Jiaqi Fu","Wugang Meng","Sungjin Cho","Huanzhe Zhan","Fumin Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.09511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09477v1","updated":"2025-05-14T15:28:43Z","published":"2025-05-14T15:28:43Z","title":"Deploying Foundation Model-Enabled Air and Ground Robots in the Field:\n  Challenges and Opportunities","summary":"  The integration of foundation models (FMs) into robotics has enabled robots\nto understand natural language and reason about the semantics in their\nenvironments. However, existing FM-enabled robots primary operate in\nclosed-world settings, where the robot is given a full prior map or has a full\nview of its workspace. This paper addresses the deployment of FM-enabled robots\nin the field, where missions often require a robot to operate in large-scale\nand unstructured environments. To effectively accomplish these missions, robots\nmust actively explore their environments, navigate obstacle-cluttered terrain,\nhandle unexpected sensor inputs, and operate with compute constraints. We\ndiscuss recent deployments of SPINE, our LLM-enabled autonomy framework, in\nfield robotic settings. To the best of our knowledge, we present the first\ndemonstration of large-scale LLM-enabled robot planning in unstructured\nenvironments with several kilometers of missions. SPINE is agnostic to a\nparticular LLM, which allows us to distill small language models capable of\nrunning onboard size, weight and power (SWaP) limited platforms. Via\npreliminary model distillation work, we then present the first language-driven\nUAV planner using on-device language models. We conclude our paper by proposing\nseveral promising directions for future research.\n","authors":["Zachary Ravichandran","Fernando Cladera","Jason Hughes","Varun Murali","M. Ani Hsieh","George J. Pappas","Camillo J. Taylor","Vijay Kumar"],"pdf_url":"https://arxiv.org/pdf/2505.09477v1.pdf","comment":"Accepted to the IEEE ICRA Workshop on Field Robotics 2025"},{"id":"http://arxiv.org/abs/2505.09475v1","updated":"2025-05-14T15:25:50Z","published":"2025-05-14T15:25:50Z","title":"aUToPath: Unified Planning and Control for Autonomous Vehicles in Urban\n  Environments Using Hybrid Lattice and Free-Space Search","summary":"  This paper presents aUToPath, a unified online framework for global\npath-planning and control to address the challenge of autonomous navigation in\ncluttered urban environments. A key component of our framework is a novel\nhybrid planner that combines pre-computed lattice maps with dynamic free-space\nsampling to efficiently generate optimal driveable corridors in cluttered\nscenarios. Our system also features sequential convex programming (SCP)-based\nmodel predictive control (MPC) to refine the corridors into smooth, dynamically\nconsistent trajectories. A single optimization problem is used to both generate\na trajectory and its corresponding control commands; this addresses limitations\nof decoupled approaches by guaranteeing a safe and feasible path. Simulation\nresults of the novel planner on randomly generated obstacle-rich scenarios\ndemonstrate the success rate of a free-space Adaptively Informed Trees*\n(AIT*)-based planner, and runtimes comparable to a lattice-based planner.\nReal-world experiments of the full system on a Chevrolet Bolt EUV further\nvalidate performance in dense obstacle fields, demonstrating no violations of\ntraffic, kinematic, or vehicle constraints, and a 100% success rate across\neight trials.\n","authors":["Tanmay P. Patel","Connor Wilson","Ellina R. Zhang","Morgan Tran","Chang Keun Paik","Steven L. Waslander","Timothy D. Barfoot"],"pdf_url":"https://arxiv.org/pdf/2505.09475v1.pdf","comment":"9 pages, 10 figures. Tanmay P. Patel, Connor Wilson, and Ellina R.\n  Zhang contributed equally"},{"id":"http://arxiv.org/abs/2505.09472v1","updated":"2025-05-14T15:22:38Z","published":"2025-05-14T15:22:38Z","title":"Streaming Multi-agent Pathfinding","summary":"  The task of the multi-agent pathfinding (MAPF) problem is to navigate a team\nof agents from their start point to the goal points. However, this setup is\nunsuitable in the assembly line scenario, which is periodic with a long working\nhour. To address this issue, the study formalizes the streaming MAPF (S-MAPF)\nproblem, which assumes that the agents in the same agent stream have a periodic\nstart time and share the same action sequence. The proposed solution, Agent\nStream Conflict-Based Search (ASCBS), is designed to tackle this problem by\nincorporating a cyclic vertex/edge constraint to handle conflicts.\nAdditionally, this work explores the potential usage of the disjoint splitting\nstrategy within ASCBS. Experimental results indicate that ASCBS surpasses\ntraditional MAPF solvers in terms of runtime for scenarios with prolonged\nworking hours.\n","authors":["Mingkai Tang","Lu Gan","Kaichen Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.09472v1.pdf","comment":"to be published in IJCAI2025"},{"id":"http://arxiv.org/abs/2310.08864v9","updated":"2025-05-14T15:22:36Z","published":"2023-10-13T05:20:40Z","title":"Open X-Embodiment: Robotic Learning Datasets and RT-X Models","summary":"  Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.\n","authors":["Open X-Embodiment Collaboration","Abby O'Neill","Abdul Rehman","Abhinav Gupta","Abhiram Maddukuri","Abhishek Gupta","Abhishek Padalkar","Abraham Lee","Acorn Pooley","Agrim Gupta","Ajay Mandlekar","Ajinkya Jain","Albert Tung","Alex Bewley","Alex Herzog","Alex Irpan","Alexander Khazatsky","Anant Rai","Anchit Gupta","Andrew Wang","Andrey Kolobov","Anikait Singh","Animesh Garg","Aniruddha Kembhavi","Annie Xie","Anthony Brohan","Antonin Raffin","Archit Sharma","Arefeh Yavary","Arhan Jain","Ashwin Balakrishna","Ayzaan Wahid","Ben Burgess-Limerick","Beomjoon Kim","Bernhard Schölkopf","Blake Wulfe","Brian Ichter","Cewu Lu","Charles Xu","Charlotte Le","Chelsea Finn","Chen Wang","Chenfeng Xu","Cheng Chi","Chenguang Huang","Christine Chan","Christopher Agia","Chuer Pan","Chuyuan Fu","Coline Devin","Danfei Xu","Daniel Morton","Danny Driess","Daphne Chen","Deepak Pathak","Dhruv Shah","Dieter Büchler","Dinesh Jayaraman","Dmitry Kalashnikov","Dorsa Sadigh","Edward Johns","Ethan Foster","Fangchen Liu","Federico Ceola","Fei Xia","Feiyu Zhao","Felipe Vieira Frujeri","Freek Stulp","Gaoyue Zhou","Gaurav S. Sukhatme","Gautam Salhotra","Ge Yan","Gilbert Feng","Giulio Schiavi","Glen Berseth","Gregory Kahn","Guangwen Yang","Guanzhi Wang","Hao Su","Hao-Shu Fang","Haochen Shi","Henghui Bao","Heni Ben Amor","Henrik I Christensen","Hiroki Furuta","Homanga Bharadhwaj","Homer Walke","Hongjie Fang","Huy Ha","Igor Mordatch","Ilija Radosavovic","Isabel Leal","Jacky Liang","Jad Abou-Chakra","Jaehyung Kim","Jaimyn Drake","Jan Peters","Jan Schneider","Jasmine Hsu","Jay Vakil","Jeannette Bohg","Jeffrey Bingham","Jeffrey Wu","Jensen Gao","Jiaheng Hu","Jiajun Wu","Jialin Wu","Jiankai Sun","Jianlan Luo","Jiayuan Gu","Jie Tan","Jihoon Oh","Jimmy Wu","Jingpei Lu","Jingyun Yang","Jitendra Malik","João Silvério","Joey Hejna","Jonathan Booher","Jonathan Tompson","Jonathan Yang","Jordi Salvador","Joseph J. Lim","Junhyek Han","Kaiyuan Wang","Kanishka Rao","Karl Pertsch","Karol Hausman","Keegan Go","Keerthana Gopalakrishnan","Ken Goldberg","Kendra Byrne","Kenneth Oslund","Kento Kawaharazuka","Kevin Black","Kevin Lin","Kevin Zhang","Kiana Ehsani","Kiran Lekkala","Kirsty Ellis","Krishan Rana","Krishnan Srinivasan","Kuan Fang","Kunal Pratap Singh","Kuo-Hao Zeng","Kyle Hatch","Kyle Hsu","Laurent Itti","Lawrence Yunliang Chen","Lerrel Pinto","Li Fei-Fei","Liam Tan","Linxi \"Jim\" Fan","Lionel Ott","Lisa Lee","Luca Weihs","Magnum Chen","Marion Lepert","Marius Memmel","Masayoshi Tomizuka","Masha Itkina","Mateo Guaman Castro","Max Spero","Maximilian Du","Michael Ahn","Michael C. Yip","Mingtong Zhang","Mingyu Ding","Minho Heo","Mohan Kumar Srirama","Mohit Sharma","Moo Jin Kim","Muhammad Zubair Irshad","Naoaki Kanazawa","Nicklas Hansen","Nicolas Heess","Nikhil J Joshi","Niko Suenderhauf","Ning Liu","Norman Di Palo","Nur Muhammad Mahi Shafiullah","Oier Mees","Oliver Kroemer","Osbert Bastani","Pannag R Sanketi","Patrick \"Tree\" Miller","Patrick Yin","Paul Wohlhart","Peng Xu","Peter David Fagan","Peter Mitrano","Pierre Sermanet","Pieter Abbeel","Priya Sundaresan","Qiuyu Chen","Quan Vuong","Rafael Rafailov","Ran Tian","Ria Doshi","Roberto Martín-Martín","Rohan Baijal","Rosario Scalise","Rose Hendrix","Roy Lin","Runjia Qian","Ruohan Zhang","Russell Mendonca","Rutav Shah","Ryan Hoque","Ryan Julian","Samuel Bustamante","Sean Kirmani","Sergey Levine","Shan Lin","Sherry Moore","Shikhar Bahl","Shivin Dass","Shubham Sonawani","Shubham Tulsiani","Shuran Song","Sichun Xu","Siddhant Haldar","Siddharth Karamcheti","Simeon Adebola","Simon Guist","Soroush Nasiriany","Stefan Schaal","Stefan Welker","Stephen Tian","Subramanian Ramamoorthy","Sudeep Dasari","Suneel Belkhale","Sungjae Park","Suraj Nair","Suvir Mirchandani","Takayuki Osa","Tanmay Gupta","Tatsuya Harada","Tatsuya Matsushima","Ted Xiao","Thomas Kollar","Tianhe Yu","Tianli Ding","Todor Davchev","Tony Z. Zhao","Travis Armstrong","Trevor Darrell","Trinity Chung","Vidhi Jain","Vikash Kumar","Vincent Vanhoucke","Vitor Guizilini","Wei Zhan","Wenxuan Zhou","Wolfram Burgard","Xi Chen","Xiangyu Chen","Xiaolong Wang","Xinghao Zhu","Xinyang Geng","Xiyuan Liu","Xu Liangwei","Xuanlin Li","Yansong Pang","Yao Lu","Yecheng Jason Ma","Yejin Kim","Yevgen Chebotar","Yifan Zhou","Yifeng Zhu","Yilin Wu","Ying Xu","Yixuan Wang","Yonatan Bisk","Yongqiang Dou","Yoonyoung Cho","Youngwoon Lee","Yuchen Cui","Yue Cao","Yueh-Hua Wu","Yujin Tang","Yuke Zhu","Yunchu Zhang","Yunfan Jiang","Yunshuang Li","Yunzhu Li","Yusuke Iwasawa","Yutaka Matsuo","Zehan Ma","Zhuo Xu","Zichen Jeff Cui","Zichen Zhang","Zipeng Fu","Zipeng Lin"],"pdf_url":"https://arxiv.org/pdf/2310.08864v9.pdf","comment":"Project website: https://robotics-transformer-x.github.io"},{"id":"http://arxiv.org/abs/2504.06513v2","updated":"2025-05-14T15:20:52Z","published":"2025-04-09T01:23:44Z","title":"Safe Navigation in Uncertain Crowded Environments Using Risk Adaptive\n  CVaR Barrier Functions","summary":"  Robot navigation in dynamic, crowded environments poses a significant\nchallenge due to the inherent uncertainties in the obstacle model. In this\nwork, we propose a risk-adaptive approach based on the Conditional\nValue-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically\nadjusted to accept the minimum necessary risk, achieving a good performance in\nterms of safety and optimization feasibility under uncertainty. Additionally,\nwe introduce a dynamic zone-based barrier function which characterizes the\ncollision likelihood by evaluating the relative state between the robot and the\nobstacle. By integrating risk adaptation with this new function, our approach\nadaptively expands the safety margin, enabling the robot to proactively avoid\nobstacles in highly dynamic environments. Comparisons and ablation studies\ndemonstrate that our method outperforms existing social navigation approaches,\nand validate the effectiveness of our proposed framework.\n","authors":["Xinyi Wang","Taekyung Kim","Bardh Hoxha","Georgios Fainekos","Dimitra Panagou"],"pdf_url":"https://arxiv.org/pdf/2504.06513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11523v2","updated":"2025-05-14T15:19:54Z","published":"2024-12-16T07:59:23Z","title":"ON as ALC: Active Loop Closing Object Goal Navigation","summary":"  In simultaneous localization and mapping, active loop closing (ALC) is an\nactive vision problem that aims to visually guide a robot to maximize the\nchances of revisiting previously visited points, thereby resetting the drift\nerrors accumulated in the incrementally built map during travel. However,\ncurrent mainstream navigation strategies that leverage such incomplete maps as\nworkspace prior knowledge often fail in modern long-term autonomy long-distance\ntravel scenarios where map accumulation errors become significant. To address\nthese limitations of map-based navigation, this paper is the first to explore\nmapless navigation in the embodied AI field, in particular, to utilize\nobject-goal navigation (commonly abbreviated as ON, ObjNav, or OGN) techniques\nthat efficiently explore target objects without using such a prior map.\nSpecifically, in this work, we start from an off-the-shelf mapless ON planner,\nextend it to utilize a prior map, and further show that the performance in\nlong-distance ALC (LD-ALC) can be maximized by minimizing ``ALC loss\" and ``ON\nloss\". This study highlights a simple and effective approach, called ALC-ON\n(ALCON), to accelerate the progress of challenging long-distance ALC technology\nby leveraging the growing frontier-guided, data-driven, and LLM-guided ON\ntechnologies.\n","authors":["Daiki Iwata","Kanji Tanaka","Shoya Miyazaki","Kouki Terashima"],"pdf_url":"https://arxiv.org/pdf/2412.11523v2.pdf","comment":"Draft version of a conference paper with 7 pages, 5 figures, and 1\n  table"},{"id":"http://arxiv.org/abs/2502.18760v2","updated":"2025-05-14T14:47:40Z","published":"2025-02-26T02:36:14Z","title":"Learning Autonomy: Off-Road Navigation Enhanced by Human Input","summary":"  In the area of autonomous driving, navigating off-road terrains presents a\nunique set of challenges, from unpredictable surfaces like grass and dirt to\nunexpected obstacles such as bushes and puddles. In this work, we present a\nnovel learning-based local planner that addresses these challenges by directly\ncapturing human driving nuances from real-world demonstrations using only a\nmonocular camera. The key features of our planner are its ability to navigate\nin challenging off-road environments with various terrain types and its fast\nlearning capabilities. By utilizing minimal human demonstration data (5-10\nmins), it quickly learns to navigate in a wide array of off-road conditions.\nThe local planner significantly reduces the real world data required to learn\nhuman driving preferences. This allows the planner to apply learned behaviors\nto real-world scenarios without the need for manual fine-tuning, demonstrating\nquick adjustment and adaptability in off-road autonomous driving technology.\n","authors":["Akhil Nagariya","Dimitar Filev","Srikanth Saripalli","Gaurav Pandey"],"pdf_url":"https://arxiv.org/pdf/2502.18760v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03370v2","updated":"2025-05-14T14:34:47Z","published":"2024-03-05T23:32:26Z","title":"F$^3$Loc: Fusion and Filtering for Floorplan Localization","summary":"  In this paper we propose an efficient data-driven solution to\nself-localization within a floorplan. Floorplan data is readily available,\nlong-term persistent and inherently robust to changes in the visual appearance.\nOur method does not require retraining per map and location or demand a large\ndatabase of images of the area of interest. We propose a novel probabilistic\nmodel consisting of an observation and a novel temporal filtering module.\nOperating internally with an efficient ray-based representation, the\nobservation module consists of a single and a multiview module to predict\nhorizontal depth from images and fuses their results to benefit from advantages\noffered by either methodology. Our method operates on conventional consumer\nhardware and overcomes a common limitation of competing methods that often\ndemand upright images. Our full system meets real-time requirements, while\noutperforming the state-of-the-art by a significant margin.\n","authors":["Changan Chen","Rui Wang","Christoph Vogel","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2403.03370v2.pdf","comment":"10 pages, 11 figure, accepted to CVPR 2024 (fixed typo eq.8: s_x,s_y,\n  s_phi -> x, y, phi)"},{"id":"http://arxiv.org/abs/2505.09430v1","updated":"2025-05-14T14:34:40Z","published":"2025-05-14T14:34:40Z","title":"Train a Multi-Task Diffusion Policy on RLBench-18 in One Day with One\n  GPU","summary":"  We present a method for training multi-task vision-language robotic diffusion\npolicies that reduces training time and memory usage by an order of magnitude.\nThis improvement arises from a previously underexplored distinction between\naction diffusion and the image diffusion techniques that inspired it: image\ngeneration targets are high-dimensional, while robot actions lie in a much\nlower-dimensional space. Meanwhile, the vision-language conditions for action\ngeneration remain high-dimensional. Our approach, Mini-Diffuser, exploits this\nasymmetry by introducing Level-2 minibatching, which pairs multiple noised\naction samples with each vision-language condition, instead of the conventional\none-to-one sampling strategy. To support this batching scheme, we introduce\narchitectural adaptations to the diffusion transformer that prevent information\nleakage across samples while maintaining full conditioning access. In RLBench\nsimulations, Mini-Diffuser achieves 95\\% of the performance of state-of-the-art\nmulti-task diffusion policies, while using only 5\\% of the training time and\n7\\% of the memory. Real-world experiments further validate that Mini-Diffuser\npreserves the key strengths of diffusion-based policies, including the ability\nto model multimodal action distributions and produce behavior conditioned on\ndiverse perceptual inputs. Code available at\ngithub.com/utomm/mini-diffuse-actor.\n","authors":["Yutong Hu","Pinhao Song","Kehan Wen","Renaud Detry"],"pdf_url":"https://arxiv.org/pdf/2505.09430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09424v1","updated":"2025-05-14T14:25:32Z","published":"2025-05-14T14:25:32Z","title":"Exploring Pose-Guided Imitation Learning for Robotic Precise Insertion","summary":"  Recent studies have proved that imitation learning shows strong potential in\nthe field of robotic manipulation. However, existing methods still struggle\nwith precision manipulation task and rely on inefficient image/point cloud\nobservations. In this paper, we explore to introduce SE(3) object pose into\nimitation learning and propose the pose-guided efficient imitation learning\nmethods for robotic precise insertion task. First, we propose a precise\ninsertion diffusion policy which utilizes the relative SE(3) pose as the\nobservation-action pair. The policy models the source object SE(3) pose\ntrajectory relative to the target object. Second, we explore to introduce the\nRGBD data to the pose-guided diffusion policy. Specifically, we design a\ngoal-conditioned RGBD encoder to capture the discrepancy between the current\nstate and the goal state. In addition, a pose-guided residual gated fusion\nmethod is proposed, which takes pose features as the backbone, and the RGBD\nfeatures selectively compensate for pose feature deficiencies through an\nadaptive gating mechanism. Our methods are evaluated on 6 robotic precise\ninsertion tasks, demonstrating competitive performance with only 7-10\ndemonstrations. Experiments demonstrate that the proposed methods can\nsuccessfully complete precision insertion tasks with a clearance of about 0.01\nmm. Experimental results highlight its superior efficiency and generalization\ncapability compared to existing baselines. Code will be available at\nhttps://github.com/sunhan1997/PoseInsert.\n","authors":["Han Sun","Yizhao Wang","Zhenning Zhou","Shuai Wang","Haibo Yang","Jingyuan Sun","Qixin Cao"],"pdf_url":"https://arxiv.org/pdf/2505.09424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12716v2","updated":"2025-05-14T14:21:03Z","published":"2025-02-18T10:27:26Z","title":"Soft Arm-Motor Thrust Characterization for a Pneumatically Actuated Soft\n  Morphing Quadrotor","summary":"  In this work, an experimental characterization of the configuration space of\na soft, pneumatically actuated morphing quadrotor is presented, with a focus on\nprecise thrust characterization of its flexible arms, considering the effect of\ndownwash. Unlike traditional quadrotors, the soft drone has pneumatically\nactuated arms, introducing complex, nonlinear interactions between motor thrust\nand arm deformation, which make precise control challenging. The silicone arms\nare actuated using differential pressure to achieve flexibility and thus have a\nvariable workspace compared to their fixed counter-parts. The deflection of the\nsoft arms during compression and expansion is controlled throughout the flight.\nHowever, in real time, the downwash from the motor attached at the tip of the\nsoft arm generates a significant and random disturbance on the arm. This\ndisturbance affects both the desired deflection of the arm and the overall\nstability of the system. To address this factor, an experimental\ncharacterization of the effect of downwash on the deflection angle of the arm\nis conducted.\n","authors":["Vidya Sumathy","Jakub Haluska","George Nikolakopoulos"],"pdf_url":"https://arxiv.org/pdf/2502.12716v2.pdf","comment":"This extended abstract was accepted for RoboSoft Conference, 2025 but\n  later withdrawn"},{"id":"http://arxiv.org/abs/2505.09377v1","updated":"2025-05-14T13:27:54Z","published":"2025-05-14T13:27:54Z","title":"Strategic Jenga Play via Graph Based Dynamics Modeling","summary":"  Controlled manipulation of multiple objects whose dynamics are closely linked\nis a challenging problem within contact-rich manipulation, requiring an\nunderstanding of how the movement of one will impact the others. Using the\nJenga game as a testbed to explore this problem, we graph-based modeling to\ntackle two different aspects of the task: 1) block selection and 2) block\nextraction. For block selection, we construct graphs of the Jenga tower and\nattempt to classify, based on the tower's structure, whether removing a given\nblock will cause the tower to collapse. For block extraction, we train a\ndynamics model that predicts how all the blocks in the tower will move at each\ntimestep in an extraction trajectory, which we then use in a sampling-based\nmodel predictive control loop to safely pull blocks out of the tower with a\ngeneral-purpose parallel-jaw gripper. We train and evaluate our methods in\nsimulation, demonstrating promising results towards block selection and block\nextraction on a challenging set of full-sized Jenga towers, even at advanced\nstages of the game.\n","authors":["Kavya Puthuveetil","Xinyi Zhang","Kazuto Yokoyama","Tetsuya Narita"],"pdf_url":"https://arxiv.org/pdf/2505.09377v1.pdf","comment":"5 pages, Oral Spotlight at ICRA 2025 Workshop \"Learning Meets\n  Model-Based Methods for Contact-Rich Manipulation\""},{"id":"http://arxiv.org/abs/2505.09359v1","updated":"2025-05-14T13:08:04Z","published":"2025-05-14T13:08:04Z","title":"Improved Corner Cutting Constraints for Mixed-Integer Motion Planning of\n  a Differential Drive Micro-Mobility Vehicle","summary":"  This paper addresses the problem of motion planning for differential drive\nmicro-mobility platforms. This class of vehicle is designed to perform\nsmall-distance transportation of passengers and goods in structured\nenvironments. Our approach leverages mixed-integer linear programming (MILP) to\ncompute global optimal collision-free trajectories taking into account the\nkinematics and dynamics of the vehicle. We propose novel constraints for\nintersample collision avoidance and demonstrate its effectiveness using pick-up\nand delivery missions and statistical analysis of Monte Carlo simulations. The\nresults show that the novel formulation provides the best trajectories in terms\nof time expenditure and control effort when compared to two state-of-the-art\napproaches.\n","authors":["Angelo Caregnato-Neto","Janito Vaqueiro Ferreira"],"pdf_url":"https://arxiv.org/pdf/2505.09359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09356v1","updated":"2025-05-14T13:06:42Z","published":"2025-05-14T13:06:42Z","title":"APR-Transformer: Initial Pose Estimation for Localization in Complex\n  Environments through Absolute Pose Regression","summary":"  Precise initialization plays a critical role in the performance of\nlocalization algorithms, especially in the context of robotics, autonomous\ndriving, and computer vision. Poor localization accuracy is often a consequence\nof inaccurate initial poses, particularly noticeable in GNSS-denied\nenvironments where GPS signals are primarily relied upon for initialization.\nRecent advances in leveraging deep neural networks for pose regression have led\nto significant improvements in both accuracy and robustness, especially in\nestimating complex spatial relationships and orientations. In this paper, we\nintroduce APR-Transformer, a model architecture inspired by state-of-the-art\nmethods, which predicts absolute pose (3D position and 3D orientation) using\neither image or LiDAR data. We demonstrate that our proposed method achieves\nstate-of-the-art performance on established benchmark datasets such as the\nRadar Oxford Robot-Car and DeepLoc datasets. Furthermore, we extend our\nexperiments to include our custom complex APR-BeIntelli dataset. Additionally,\nwe validate the reliability of our approach in GNSS-denied environments by\ndeploying the model in real-time on an autonomous test vehicle. This showcases\nthe practical feasibility and effectiveness of our approach. The source code is\navailable at:https://github.com/GT-ARC/APR-Transformer.\n","authors":["Srinivas Ravuri","Yuan Xu","Martin Ludwig Zehetner","Ketan Motlag","Sahin Albayrak"],"pdf_url":"https://arxiv.org/pdf/2505.09356v1.pdf","comment":"8 pages with 6 figures"},{"id":"http://arxiv.org/abs/2505.07634v2","updated":"2025-05-14T12:56:45Z","published":"2025-05-12T15:05:34Z","title":"Neural Brain: A Neuroscience-inspired Framework for Embodied Agents","summary":"  The rapid evolution of artificial intelligence (AI) has shifted from static,\ndata-driven models to dynamic systems capable of perceiving and interacting\nwith real-world environments. Despite advancements in pattern recognition and\nsymbolic reasoning, current AI systems, such as large language models, remain\ndisembodied, unable to physically engage with the world. This limitation has\ndriven the rise of embodied AI, where autonomous agents, such as humanoid\nrobots, must navigate and manipulate unstructured environments with human-like\nadaptability. At the core of this challenge lies the concept of Neural Brain, a\ncentral intelligence system designed to drive embodied agents with human-like\nadaptability. A Neural Brain must seamlessly integrate multimodal sensing and\nperception with cognitive capabilities. Achieving this also requires an\nadaptive memory system and energy-efficient hardware-software co-design,\nenabling real-time action in dynamic environments. This paper introduces a\nunified framework for the Neural Brain of embodied agents, addressing two\nfundamental challenges: (1) defining the core components of Neural Brain and\n(2) bridging the gap between static AI models and the dynamic adaptability\nrequired for real-world deployment. To this end, we propose a biologically\ninspired architecture that integrates multimodal active sensing,\nperception-cognition-action function, neuroplasticity-based memory storage and\nupdating, and neuromorphic hardware/software optimization. Furthermore, we also\nreview the latest research on embodied agents across these four aspects and\nanalyze the gap between current AI systems and human intelligence. By\nsynthesizing insights from neuroscience, we outline a roadmap towards the\ndevelopment of generalizable, autonomous agents capable of human-level\nintelligence in real-world scenarios.\n","authors":["Jian Liu","Xiongtao Shi","Thai Duy Nguyen","Haitian Zhang","Tianxiang Zhang","Wei Sun","Yanjie Li","Athanasios V. Vasilakos","Giovanni Iacca","Arshad Ali Khan","Arvind Kumar","Jae Won Cho","Ajmal Mian","Lihua Xie","Erik Cambria","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2505.07634v2.pdf","comment":"51 pages, 17 figures, 9 tables"},{"id":"http://arxiv.org/abs/2505.05507v2","updated":"2025-05-14T12:13:27Z","published":"2025-05-07T08:18:54Z","title":"VIMPPI: Enhancing Model Predictive Path Integral Control with\n  Variational Integration for Underactuated Systems","summary":"  This paper presents VIMPPI, a novel control approach for underactuated double\npendulum systems developed for the AI Olympics competition. We enhance the\nModel Predictive Path Integral framework by incorporating variational\nintegration techniques, enabling longer planning horizons without additional\ncomputational cost. Operating at 500-700 Hz with control interpolation and\ndisturbance detection mechanisms, VIMPPI substantially outperforms both\nbaseline methods and alternative MPPI implementations\n","authors":["Igor Alentev","Lev Kozlov","Ivan Domrachev","Simeon Nedelchev","Jee-Hwan Ryu"],"pdf_url":"https://arxiv.org/pdf/2505.05507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09315v1","updated":"2025-05-14T12:10:41Z","published":"2025-05-14T12:10:41Z","title":"TransDiffuser: End-to-end Trajectory Generation with Decorrelated\n  Multi-modal Representation for Autonomous Driving","summary":"  In recent years, diffusion model has shown its potential across diverse\ndomains from vision generation to language modeling. Transferring its\ncapabilities to modern autonomous driving systems has also emerged as a\npromising direction.In this work, we propose TransDiffuser, an encoder-decoder\nbased generative trajectory planning model for end-to-end autonomous driving.\nThe encoded scene information serves as the multi-modal conditional input of\nthe denoising decoder. To tackle the mode collapse dilemma in generating\nhigh-quality diverse trajectories, we introduce a simple yet effective\nmulti-modal representation decorrelation optimization mechanism during the\ntraining process.TransDiffuser achieves PDMS of 94.85 on the NAVSIM benchmark,\nsurpassing previous state-of-the-art methods without any anchor-based prior\ntrajectories.\n","authors":["Xuefeng Jiang","Yuan Ma","Pengxiang Li","Leimeng Xu","Xin Wen","Kun Zhan","Zhongpu Xia","Peng Jia","XianPeng Lang","Sheng Sun"],"pdf_url":"https://arxiv.org/pdf/2505.09315v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2411.06542v4","updated":"2025-05-14T11:58:21Z","published":"2024-11-10T17:48:26Z","title":"Is Linear Feedback on Smoothed Dynamics Sufficient for Stabilizing\n  Contact-Rich Plans?","summary":"  Designing planners and controllers for contact-rich manipulation is extremely\nchallenging as contact violates the smoothness conditions that many\ngradient-based controller synthesis tools assume. Contact smoothing\napproximates a non-smooth system with a smooth one, allowing one to use these\nsynthesis tools more effectively. However, applying classical control synthesis\nmethods to smoothed contact dynamics remains relatively under-explored. This\npaper analyzes the efficacy of linear controller synthesis using differential\nsimulators based on contact smoothing. We introduce natural baselines for\nleveraging contact smoothing to compute (a) open-loop plans robust to uncertain\nconditions and/or dynamics, and (b) feedback gains to stabilize around\nopen-loop plans. Using robotic bimanual whole-body manipulation as a testbed,\nwe perform extensive empirical experiments on over 300 trajectories and analyze\nwhy LQR seems insufficient for stabilizing contact-rich plans. The video\nsummarizing this paper and hardware experiments is found here:\nhttps://youtu.be/HLaKi6qbwQg?si=_zCAmBBD6rGSitm9.\n","authors":["Yuki Shirai","Tong Zhao","H. J. Terry Suh","Huaijiang Zhu","Xinpei Ni","Jiuguang Wang","Max Simchowitz","Tao Pang"],"pdf_url":"https://arxiv.org/pdf/2411.06542v4.pdf","comment":"ICRA2025"},{"id":"http://arxiv.org/abs/2409.20248v2","updated":"2025-05-14T11:40:33Z","published":"2024-09-30T12:37:07Z","title":"Feature Extractor or Decision Maker: Rethinking the Role of Visual\n  Encoders in Visuomotor Policies","summary":"  An end-to-end (E2E) visuomotor policy is typically treated as a unified\nwhole, but recent approaches using out-of-domain (OOD) data to pretrain the\nvisual encoder have cleanly separated the visual encoder from the network, with\nthe remainder referred to as the policy. We propose Visual Alignment Testing,\nan experimental framework designed to evaluate the validity of this functional\nseparation. Our results indicate that in E2E-trained models, visual encoders\nactively contribute to decision-making resulting from motor data supervision,\ncontradicting the assumed functional separation. In contrast, OOD-pretrained\nmodels, where encoders lack this capability, experience an average performance\ndrop of 42\\% in our benchmark results, compared to the state-of-the-art\nperformance achieved by E2E policies. We believe this initial exploration of\nvisual encoders' role can provide a first step towards guiding future\npretraining methods to address their decision-making ability, such as\ndeveloping task-conditioned or context-aware encoders.\n","authors":["Ruiyu Wang","Zheyu Zhuang","Shutong Jin","Nils Ingelhag","Danica Kragic","Florian T. Pokorny"],"pdf_url":"https://arxiv.org/pdf/2409.20248v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09278v1","updated":"2025-05-14T10:59:09Z","published":"2025-05-14T10:59:09Z","title":"A drone that learns to efficiently find objects in agricultural fields:\n  from simulation to the real world","summary":"  Drones are promising for data collection in precision agriculture, however,\nthey are limited by their battery capacity. Efficient path planners are\ntherefore required. This paper presents a drone path planner trained using\nReinforcement Learning (RL) on an abstract simulation that uses object\ndetections and uncertain prior knowledge. The RL agent controls the flight\ndirection and can terminate the flight. By using the agent in combination with\nthe drone's flight controller and a detection network to process camera images,\nit is possible to evaluate the performance of the agent on real-world data. In\nsimulation, the agent yielded on average a 78% shorter flight path compared to\na full coverage planner, at the cost of a 14% lower recall. On real-world data,\nthe agent showed a 72% shorter flight path compared to a full coverage planner,\nhowever, at the cost of a 25% lower recall. The lower performance on real-world\ndata was attributed to the real-world object distribution and the lower\naccuracy of prior knowledge, and shows potential for improvement. Overall, we\nconcluded that for applications where it is not crucial to find all objects,\nsuch as weed detection, the learned-based path planner is suitable and\nefficient.\n","authors":["Rick van Essen","Gert Kootstra"],"pdf_url":"https://arxiv.org/pdf/2505.09278v1.pdf","comment":"Accepted to the Novel Approaches for Precision Agriculture and\n  Forestry with Autonomous Robots IEEE ICRA Workshop - 2025"},{"id":"http://arxiv.org/abs/2409.12667v3","updated":"2025-05-14T10:33:21Z","published":"2024-09-19T11:27:40Z","title":"METDrive: Multi-modal End-to-end Autonomous Driving with Temporal\n  Guidance","summary":"  Multi-modal end-to-end autonomous driving has shown promising advancements in\nrecent work. By embedding more modalities into end-to-end networks, the\nsystem's understanding of both static and dynamic aspects of the driving\nenvironment is enhanced, thereby improving the safety of autonomous driving. In\nthis paper, we introduce METDrive, an end-to-end system that leverages temporal\nguidance from the embedded time series features of ego states, including\nrotation angles, steering, throttle signals, and waypoint vectors. The\ngeometric features derived from perception sensor data and the time series\nfeatures of ego state data jointly guide the waypoint prediction with the\nproposed temporal guidance loss function. We evaluated METDrive on the CARLA\nleaderboard benchmarks, achieving a driving score of 70%, a route completion\nscore of 94%, and an infraction score of 0.78.\n","authors":["Ziang Guo","Xinhao Lin","Zakhar Yagudin","Artem Lykov","Yong Wang","Yanqiang Li","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2409.12667v3.pdf","comment":"Accepted by ICRA"},{"id":"http://arxiv.org/abs/2503.18938v3","updated":"2025-05-14T10:26:17Z","published":"2025-03-24T17:58:15Z","title":"AdaWorld: Learning Adaptable World Models with Latent Actions","summary":"  World models aim to learn action-controlled future prediction and have proven\nessential for the development of intelligent agents. However, most existing\nworld models rely heavily on substantial action-labeled data and costly\ntraining, making it challenging to adapt to novel environments with\nheterogeneous actions through limited interactions. This limitation can hinder\ntheir applicability across broader domains. To overcome this limitation, we\npropose AdaWorld, an innovative world model learning approach that enables\nefficient adaptation. The key idea is to incorporate action information during\nthe pretraining of world models. This is achieved by extracting latent actions\nfrom videos in a self-supervised manner, capturing the most critical\ntransitions between frames. We then develop an autoregressive world model that\nconditions on these latent actions. This learning paradigm enables highly\nadaptable world models, facilitating efficient transfer and learning of new\nactions even with limited interactions and finetuning. Our comprehensive\nexperiments across multiple environments demonstrate that AdaWorld achieves\nsuperior performance in both simulation quality and visual planning.\n","authors":["Shenyuan Gao","Siyuan Zhou","Yilun Du","Jun Zhang","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2503.18938v3.pdf","comment":"ICML 2025. Project page: https://adaptable-world-model.github.io/,\n  code: https://github.com/Little-Podi/AdaWorld, model:\n  https://huggingface.co/Little-Podi/AdaWorld"},{"id":"http://arxiv.org/abs/2505.09224v1","updated":"2025-05-14T08:46:24Z","published":"2025-05-14T08:46:24Z","title":"Ethical Aspects of the Use of Social Robots in Elderly Care -- A\n  Systematic Qualitative Review","summary":"  Background: The use of social robotics in elderly care is increasingly\ndiscussed as one way of meeting emerging care needs due to scarce resources.\nWhile many potential benefits are associated with robotic care technologies,\nthere is a variety of ethical challenges. To support steps towards a\nresponsible implementation and use, this review develops an overview on ethical\naspects of the use of social robots in elderly care from a decision-makers'\nperspective.\n  Methods: Electronic databases were queried using a comprehensive search\nstrategy based on the key concepts of \"ethical aspects\", \"social robotics\" and\n\"elderly care\". Abstract and title screening was conducted by two authors\nindependently. Full-text screening was conducted by one author following a\njoint consolidation phase. Data was extracted using MAXQDA24 by one author,\nbased on a consolidated coding framework. Analysis was performed through\nmodified qualitative content analysis.\n  Results: A total of 1,518 publications were screened, and 248 publications\nwere included. We have organized our analysis in a scheme of ethical hazards,\nethical opportunities and unsettled questions, identifying at least 60 broad\nethical aspects affecting three different stakeholder groups. While some\nethical issues are well-known and broadly discussed our analysis shows a\nplethora of potentially relevant aspects, often only marginally recognized,\nthat are worthy of consideration from a practical perspective.\n  Discussion: The findings highlight the need for a contextual and detailed\nevaluation of implementation scenarios. To make use of the vast knowledge of\nthe ethical discourse, we hypothesize that decision-makers need to understand\nthe specific nature of this discourse to be able to engage in careful ethical\ndeliberation.\n","authors":["Marianne Leineweber","Clara Victoria Keusgen","Marc Bubeck","Joschka Haltaufderheide","Robert Ranisch","Corinna Klingler"],"pdf_url":"https://arxiv.org/pdf/2505.09224v1.pdf","comment":"93 pages, 1 figure, 5 tables, 3 suplements"},{"id":"http://arxiv.org/abs/2503.02454v2","updated":"2025-05-14T06:03:57Z","published":"2025-03-04T10:02:53Z","title":"UAV-VLPA*: A Vision-Language-Path-Action System for Optimal Route\n  Generation on a Large Scales","summary":"  The UAV-VLPA* (Visual-Language-Planning-and-Action) system represents a\ncutting-edge advancement in aerial robotics, designed to enhance communication\nand operational efficiency for unmanned aerial vehicles (UAVs). By integrating\nadvanced planning capabilities, the system addresses the Traveling Salesman\nProblem (TSP) to optimize flight paths, reducing the total trajectory length by\n18.5\\% compared to traditional methods. Additionally, the incorporation of the\nA* algorithm enables robust obstacle avoidance, ensuring safe and efficient\nnavigation in complex environments. The system leverages satellite imagery\nprocessing combined with the Visual Language Model (VLM) and GPT's natural\nlanguage processing capabilities, allowing users to generate detailed flight\nplans through simple text commands. This seamless fusion of visual and\nlinguistic analysis empowers precise decision-making and mission planning,\nmaking UAV-VLPA* a transformative tool for modern aerial operations. With its\nunmatched operational efficiency, navigational safety, and user-friendly\nfunctionality, UAV-VLPA* sets a new standard in autonomous aerial robotics,\npaving the way for future innovations in the field.\n","authors":["Oleg Sautenkov","Aibek Akhmetkazy","Yasheerah Yaqoot","Muhammad Ahsan Mustafa","Grik Tadevosyan","Artem Lykov","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2503.02454v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2501.05014"},{"id":"http://arxiv.org/abs/2505.09145v1","updated":"2025-05-14T05:04:31Z","published":"2025-05-14T05:04:31Z","title":"Robot-Assisted Drone Recovery on a Wavy Surface Using Error-State Kalman\n  Filter and Receding Horizon Model Predictive Control","summary":"  Recovering a drone on a disturbed water surface remains a significant\nchallenge in maritime robotics. In this paper, we propose a unified framework\nfor Robot-Assisted Drone Recovery on a Wavy Surface that addresses two major\ntasks: Firstly, accurate prediction of a moving drone's position under\nwave-induced disturbances using an Error-State Kalman Filter (ESKF), and\nsecondly, effective motion planning for a manipulator via Receding Horizon\nControl (RHC). Specifically, the ESKF predicts the drone's future position 0.5s\nahead, while the manipulator plans a capture trajectory in real time, thus\novercoming not only wave-induced base motions but also limited torque\nconstraints. We provide a system design that comprises a manipulator subsystem\nand a UAV subsystem. On the UAV side, we detail how position control and\nsuspended payload strategies are implemented. On the manipulator side, we show\nhow an RHC scheme outperforms traditional low-level control algorithms.\nSimulation and real-world experiments - using wave-disturbed motion data -\ndemonstrate that our approach achieves a high success rate - above 95% and\noutperforms conventional baseline methods by up to 10% in efficiency and 20% in\nprecision. The results underscore the feasibility and robustness of our system,\nwhich achieves state-of-the-art (SOTA) performance and offers a practical\nsolution for maritime drone operations.\n","authors":["Yimou Wu","Mingyang Liang","Ruoyu Xu"],"pdf_url":"https://arxiv.org/pdf/2505.09145v1.pdf","comment":"12 pages, 15 figures"},{"id":"http://arxiv.org/abs/2505.09144v1","updated":"2025-05-14T05:03:09Z","published":"2025-05-14T05:03:09Z","title":"Latent Theory of Mind: A Decentralized Diffusion Architecture for\n  Cooperative Manipulation","summary":"  We present Latent Theory of Mind (LatentToM), a decentralized diffusion\npolicy architecture for collaborative robot manipulation. Our policy allows\nmultiple manipulators with their own perception and computation to collaborate\nwith each other towards a common task goal with or without explicit\ncommunication. Our key innovation lies in allowing each agent to maintain two\nlatent representations: an ego embedding specific to the robot, and a consensus\nembedding trained to be common to both robots, despite their different sensor\nstreams and poses. We further let each robot train a decoder to infer the other\nrobot's ego embedding from their consensus embedding, akin to theory of mind in\nlatent space. Training occurs centrally, with all the policies' consensus\nencoders supervised by a loss inspired by sheaf theory, a mathematical theory\nfor clustering data on a topological manifold. Specifically, we introduce a\nfirst-order cohomology loss to enforce sheaf-consistent alignment of the\nconsensus embeddings. To preserve the expressiveness of the consensus\nembedding, we further propose structural constraints based on theory of mind\nand a directional consensus mechanism. Execution can be fully distributed,\nrequiring no explicit communication between policies. In which case, the\ninformation is exchanged implicitly through each robot's sensor stream by\nobserving the actions of the other robots and their effects on the scene.\nAlternatively, execution can leverage direct communication to share the robots'\nconsensus embeddings, where the embeddings are shared once during each\ninference step and are aligned using the sheaf Laplacian. In our hardware\nexperiments, LatentToM outperforms a naive decentralized diffusion baseline,\nand shows comparable performance with a state-of-the-art centralized diffusion\npolicy for bi-manual manipulation. Project website:\nhttps://stanfordmsl.github.io/LatentToM/.\n","authors":["Chengyang He","Gadiel Sznaier Camps","Xu Liu","Mac Schwager","Guillaume Sartoretti"],"pdf_url":"https://arxiv.org/pdf/2505.09144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01297v2","updated":"2025-05-14T04:48:21Z","published":"2024-12-02T09:10:37Z","title":"Morphological-Symmetry-Equivariant Heterogeneous Graph Neural Network\n  for Robotic Dynamics Learning","summary":"  We present a morphological-symmetry-equivariant heterogeneous graph neural\nnetwork, namely MS-HGNN, for robotic dynamics learning, that integrates robotic\nkinematic structures and morphological symmetries into a single graph network.\nThese structural priors are embedded into the learning architecture as\nconstraints, ensuring high generalizability, sample and model efficiency. The\nproposed MS-HGNN is a versatile and general architecture that is applicable to\nvarious multi-body dynamic systems and a wide range of dynamics learning\nproblems. We formally prove the morphological-symmetry-equivariant property of\nour MS-HGNN and validate its effectiveness across multiple quadruped robot\nlearning problems using both real-world and simulated data. Our code is made\npublicly available at https://github.com/lunarlab-gatech/MorphSym-HGNN/.\n","authors":["Fengze Xie","Sizhe Wei","Yue Song","Yisong Yue","Lu Gan"],"pdf_url":"https://arxiv.org/pdf/2412.01297v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02291v2","updated":"2025-05-14T04:35:33Z","published":"2025-05-04T23:20:40Z","title":"Dexterous Contact-Rich Manipulation via the Contact Trust Region","summary":"  What is a good local description of contact dynamics for contact-rich\nmanipulation, and where can we trust this local description? While many\napproaches often rely on the Taylor approximation of dynamics with an\nellipsoidal trust region, we argue that such approaches are fundamentally\ninconsistent with the unilateral nature of contact. As a remedy, we present the\nContact Trust Region (CTR), which captures the unilateral nature of contact\nwhile remaining efficient for computation. With CTR, we first develop a\nModel-Predictive Control (MPC) algorithm capable of synthesizing local\ncontact-rich plans. Then, we extend this capability to plan globally by\nstitching together local MPC plans, enabling efficient and dexterous\ncontact-rich manipulation. To verify the performance of our method, we perform\ncomprehensive evaluations, both in high-fidelity simulation and on hardware, on\ntwo contact-rich systems: a planar IiwaBimanual system and a 3D AllegroHand\nsystem. On both systems, our method offers a significantly lower-compute\nalternative to existing RL-based approaches to contact-rich manipulation. In\nparticular, our Allegro in-hand manipulation policy, in the form of a roadmap,\ntakes fewer than 10 minutes to build offline on a standard laptop using just\nits CPU, with online inference taking just a few seconds. Experiment data,\nvideo and code are available at ctr.theaiinstitute.com.\n","authors":["H. J. Terry Suh","Tao Pang","Tong Zhao","Russ Tedrake"],"pdf_url":"https://arxiv.org/pdf/2505.02291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09119v1","updated":"2025-05-14T04:06:51Z","published":"2025-05-14T04:06:51Z","title":"Model Identification Adaptive Control with $ρ$-POMDP Planning","summary":"  Accurate system modeling is crucial for safe, effective control, as\nmisidentification can lead to accumulated errors, especially under partial\nobservability. We address this problem by formulating informative input design\n(IID) and model identification adaptive control (MIAC) as belief space planning\nproblems, modeled as partially observable Markov decision processes with\nbelief-dependent rewards ($\\rho$-POMDPs). We treat system parameters as hidden\nstate variables that must be localized while simultaneously controlling the\nsystem. We solve this problem with an adapted belief-space iterative Linear\nQuadratic Regulator (BiLQR). We demonstrate it on fully and partially\nobservable tasks for cart-pole and steady aircraft flight domains. Our method\noutperforms baselines such as regression, filtering, and local optimal control\nmethods, even under instantaneous disturbances to system parameters.\n","authors":["Michelle Ho","Arec Jamgochian","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2505.09119v1.pdf","comment":"Accepted to CoDIT 2025"},{"id":"http://arxiv.org/abs/2505.09109v1","updated":"2025-05-14T03:34:30Z","published":"2025-05-14T03:34:30Z","title":"FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding\n  via Keypoint-Driven Asset and Demonstration Synthesis","summary":"  Due to the deformability of garments, generating a large amount of\nhigh-quality data for robotic garment manipulation tasks is highly challenging.\nIn this paper, we present a synthetic garment dataset that can be used for\nrobotic garment folding. We begin by constructing geometric garment templates\nbased on keypoints and applying generative models to generate realistic texture\npatterns. Leveraging these keypoint annotations, we generate folding\ndemonstrations in simulation and train folding policies via closed-loop\nimitation learning. To improve robustness, we propose KG-DAgger, which uses a\nkeypoint-based strategy to generate demonstration data for recovering from\nfailures. KG-DAgger significantly improves the model performance, boosting the\nreal-world success rate by 25\\%. After training with 15K trajectories (about 2M\nimage-action pairs), the model achieves a 75\\% success rate in the real world.\nExperiments in both simulation and real-world settings validate the\neffectiveness of our proposed framework.\n","authors":["Yuxing Chen","Bowen Xiao","He Wang"],"pdf_url":"https://arxiv.org/pdf/2505.09109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09108v1","updated":"2025-05-14T03:33:46Z","published":"2025-05-14T03:33:46Z","title":"Air-Ground Collaboration for Language-Specified Missions in Unknown\n  Environments","summary":"  As autonomous robotic systems become increasingly mature, users will want to\nspecify missions at the level of intent rather than in low-level detail.\nLanguage is an expressive and intuitive medium for such mission specification.\nHowever, realizing language-guided robotic teams requires overcoming\nsignificant technical hurdles. Interpreting and realizing language-specified\nmissions requires advanced semantic reasoning. Successful heterogeneous robots\nmust effectively coordinate actions and share information across varying\nviewpoints. Additionally, communication between robots is typically\nintermittent, necessitating robust strategies that leverage communication\nopportunities to maintain coordination and achieve mission objectives. In this\nwork, we present a first-of-its-kind system where an unmanned aerial vehicle\n(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively\naccomplish missions specified in natural language while reacting to changes in\nspecification on the fly. We leverage a Large Language Model (LLM)-enabled\nplanner to reason over semantic-metric maps that are built online and\nopportunistically shared between an aerial and a ground robot. We consider\ntask-driven navigation in urban and rural areas. Our system must infer\nmission-relevant semantics and actively acquire information via semantic\nmapping. In both ground and air-ground teaming experiments, we demonstrate our\nsystem on seven different natural-language specifications at up to\nkilometer-scale navigation.\n","authors":["Fernando Cladera","Zachary Ravichandran","Jason Hughes","Varun Murali","Carlos Nieto-Granda","M. Ani Hsieh","George J. Pappas","Camillo J. Taylor","Vijay Kumar"],"pdf_url":"https://arxiv.org/pdf/2505.09108v1.pdf","comment":"19 pages, 24 figures, 7 tables. Submitted to T-FR"},{"id":"http://arxiv.org/abs/2505.09099v1","updated":"2025-05-14T03:09:21Z","published":"2025-05-14T03:09:21Z","title":"Imitation Learning for Adaptive Control of a Virtual Soft Exoglove","summary":"  The use of wearable robots has been widely adopted in rehabilitation training\nfor patients with hand motor impairments. However, the uniqueness of patients'\nmuscle loss is often overlooked. Leveraging reinforcement learning and a\nbiologically accurate musculoskeletal model in simulation, we propose a\ncustomized wearable robotic controller that is able to address specific muscle\ndeficits and to provide compensation for hand-object manipulation tasks. Video\ndata of a same subject performing human grasping tasks is used to train a\nmanipulation model through learning from demonstration. This manipulation model\nis subsequently fine-tuned to perform object-specific interaction tasks. The\nmuscle forces in the musculoskeletal manipulation model are then weakened to\nsimulate neurological motor impairments, which are later compensated by the\nactuation of a virtual wearable robotics glove. Results shows that integrating\nthe virtual wearable robotic glove provides shared assistance to support the\nhand manipulator with weakened muscle forces. The learned exoglove controller\nachieved an average of 90.5\\% of the original manipulation proficiency.\n","authors":["Shirui Lyu","Vittorio Caggiano","Matteo Leonetti","Dario Farina","Letizia Gionfrida"],"pdf_url":"https://arxiv.org/pdf/2505.09099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09092v1","updated":"2025-05-14T02:53:50Z","published":"2025-05-14T02:53:50Z","title":"OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models\n  under Real-world Driving Conditions","summary":"  Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its\nreal-world performance remains underexplored due to proprietary systems and\nlimited data access. This paper presents OpenLKA, the first open, large-scale\ndataset for LKA evaluation and improvement. It includes 400 hours of driving\ndata from 50+ production vehicle models, collected through extensive road\ntesting in Tampa, Florida and global contributions from the Comma.ai driving\ncommunity. The dataset spans a wide range of challenging scenarios, including\ncomplex road geometries, degraded lane markings, adverse weather, lighting\nconditions and surrounding traffic. The dataset is multimodal, comprising: i)\nfull CAN bus streams, decoded using custom reverse-engineered DBC files to\nextract key LKA events (e.g., system disengagements, lane detection failures);\nii) synchronized high-resolution dash-cam video; iii) real-time outputs from\nOpenpilot, providing accurate estimates of road curvature and lane positioning;\niv) enhanced scene annotations generated by Vision Language Models, describing\nlane visibility, pavement quality, weather, lighting, and traffic conditions.\nBy integrating vehicle-internal signals with high-fidelity perception and rich\nsemantic context, OpenLKA provides a comprehensive platform for benchmarking\nthe real-world performance of production LKA systems, identifying\nsafety-critical operational scenarios, and assessing the readiness of current\nroad infrastructure for autonomous driving. The dataset is publicly available\nat: https://github.com/OpenLKA/OpenLKA.\n","authors":["Yuhang Wang","Abdulaziz Alhuraish","Shengming Yuan","Hao Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.09092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05271v2","updated":"2025-05-14T02:44:44Z","published":"2025-02-07T19:05:04Z","title":"RobotMover: Learning to Move Large Objects From Human Demonstrations","summary":"  Moving large objects, such as furniture or appliances, is a critical\ncapability for robots operating in human environments. This task presents\nunique challenges, including whole-body coordination to avoid collisions and\nmanaging the dynamics of bulky, heavy objects. In this work, we present\nRobotMover, a learning-based system for large object manipulation that uses\nhuman-object interaction demonstrations to train robot control policies.\nRobotMover formulates the manipulation problem as imitation learning using a\nsimplified spatial representation called the Interaction Chain, which captures\nessential interaction dynamics in a way that generalizes across different robot\nbodies. We incorporate this Interaction Chain into a reward function and train\npolicies in simulation using domain randomization to enable zero-shot transfer\nto real-world robots. The resulting policies allow a Spot robot to manipulate\nvarious large objects, including chairs, tables, and standing lamps. Through\nextensive experiments in both simulation and the real world, we show that\nRobotMover achieves strong performance in terms of capability, robustness, and\ncontrollability, outperforming both learned and teleoperation baselines. The\nsystem also supports practical applications by combining learned policies with\nsimple planning modules to perform long-horizon object transport and\nrearrangement tasks.\n","authors":["Tianyu Li","Joanne Truong","Jimmy Yang","Alexander Clegg","Akshara Rai","Sehoon Ha","Xavier Puig"],"pdf_url":"https://arxiv.org/pdf/2502.05271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09074v1","updated":"2025-05-14T02:21:23Z","published":"2025-05-14T02:21:23Z","title":"Deployable and Generalizable Motion Prediction: Taxonomy, Open\n  Challenges and Future Directions","summary":"  Motion prediction, the anticipation of future agent states or scene\nevolution, is rooted in human cognition, bridging perception and\ndecision-making. It enables intelligent systems, such as robots and\nself-driving cars, to act safely in dynamic, human-involved environments, and\ninforms broader time-series reasoning challenges. With advances in methods,\nrepresentations, and datasets, the field has seen rapid progress, reflected in\nquickly evolving benchmark results. Yet, when state-of-the-art methods are\ndeployed in the real world, they often struggle to generalize to open-world\nconditions and fall short of deployment standards. This reveals a gap between\nresearch benchmarks, which are often idealized or ill-posed, and real-world\ncomplexity.\n  To address this gap, this survey revisits the generalization and\ndeployability of motion prediction models, with an emphasis on the applications\nof robotics, autonomous driving, and human motion. We first offer a\ncomprehensive taxonomy of motion prediction methods, covering representations,\nmodeling strategies, application domains, and evaluation protocols. We then\nstudy two key challenges: (1) how to push motion prediction models to be\ndeployable to realistic deployment standards, where motion prediction does not\nact in a vacuum, but functions as one module of closed-loop autonomy stacks -\nit takes input from the localization and perception, and informs downstream\nplanning and control. 2) how to generalize motion prediction models from\nlimited seen scenarios/datasets to the open-world settings. Throughout the\npaper, we highlight critical open challenges to guide future work, aiming to\nrecalibrate the community's efforts, fostering progress that is not only\nmeasurable but also meaningful for real-world applications.\n","authors":["Letian Wang","Marc-Antoine Lavoie","Sandro Papais","Barza Nisar","Yuxiao Chen","Wenhao Ding","Boris Ivanovic","Hao Shao","Abulikemu Abuduweili","Evan Cook","Yang Zhou","Peter Karkus","Jiachen Li","Changliu Liu","Marco Pavone","Steven Waslander"],"pdf_url":"https://arxiv.org/pdf/2505.09074v1.pdf","comment":"Initial draft, 162 pages, 40 figures, 13 tables"},{"id":"http://arxiv.org/abs/2505.09069v1","updated":"2025-05-14T02:09:38Z","published":"2025-05-14T02:09:38Z","title":"A Novel 6-axis Force/Torque Sensor Using Inductance Sensors","summary":"  This paper presents a novel six-axis force/torque (F/T) sensor based on\ninductive sensing technology. Unlike conventional strain gauge-based sensors\nthat require direct contact and external amplification, the proposed sensor\nutilizes non-contact inductive measurements to estimate force via displacement\nof a conductive target. A compact, fully integrated architecture is achieved by\nincorporating a CAN-FD based signal processing module directly onto the PCB,\nenabling high-speed data acquisition at up to 4~kHz without external DAQ\nsystems. The sensing mechanism is modeled and calibrated through a rational\nfunction fitting approach, which demonstrated superior performance in terms of\nroot mean square error (RMSE), coefficient of determination ($R^2$), and\nlinearity error compared to other nonlinear models. Static and repeatability\nexperiments validate the sensor's accuracy, achieving a resolution of 0.03~N\nand quantization levels exceeding 55,000 steps, surpassing that of commercial\nsensors. The sensor also exhibits low crosstalk, high sensitivity, and robust\nnoise characteristics. Its performance and structure make it suitable for\nprecision robotic applications, especially in scenarios where compactness,\nnon-contact operation, and integrated processing are essential.\n","authors":["Hyun-Bin Kim","Kyung-Soo Kim"],"pdf_url":"https://arxiv.org/pdf/2505.09069v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.09067v1","updated":"2025-05-14T02:04:57Z","published":"2025-05-14T02:04:57Z","title":"Solving Reach- and Stabilize-Avoid Problems Using Discounted\n  Reachability","summary":"  In this article, we consider the infinite-horizon reach-avoid (RA) and\nstabilize-avoid (SA) zero-sum game problems for general nonlinear\ncontinuous-time systems, where the goal is to find the set of states that can\nbe controlled to reach or stabilize to a target set, without violating\nconstraints even under the worst-case disturbance. Based on the Hamilton-Jacobi\nreachability method, we address the RA problem by designing a new Lipschitz\ncontinuous RA value function, whose zero sublevel set exactly characterizes the\nRA set. We establish that the associated Bellman backup operator is contractive\nand that the RA value function is the unique viscosity solution of a\nHamilton-Jacobi variational inequality. Finally, we develop a two-step\nframework for the SA problem by integrating our RA strategies with a recently\nproposed Robust Control Lyapunov-Value Function, thereby ensuring both target\nreachability and long-term stability. We numerically verify our RA and SA\nframeworks on a 3D Dubins car system to demonstrate the efficacy of the\nproposed approach.\n","authors":["Boyang Li","Zheng Gong","Sylvia Herbert"],"pdf_url":"https://arxiv.org/pdf/2505.09067v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2505.09058v1","updated":"2025-05-14T01:42:59Z","published":"2025-05-14T01:42:59Z","title":"Reach-Avoid-Stabilize Using Admissible Control Sets","summary":"  Hamilton-Jacobi Reachability (HJR) analysis has been successfully used in\nmany robotics and control tasks, and is especially effective in computing\nreach-avoid sets and control laws that enable an agent to reach a goal while\nsatisfying state constraints. However, the original HJR formulation provides no\nguarantees of safety after a) the prescribed time horizon, or b) goal\nsatisfaction. The reach-avoid-stabilize (RAS) problem has therefore gained a\nlot of focus: find the set of initial states (the RAS set), such that the\ntrajectory can reach the target, and stabilize to some point of interest (POI)\nwhile avoiding obstacles. Solving RAS problems using HJR usually requires\ndefining a new value function, whose zero sub-level set is the RAS set. The\nexisting methods do not consider the problem when there are a series of targets\nto reach and/or obstacles to avoid. We propose a method that uses the idea of\nadmissible control sets; we guarantee that the system will reach each target\nwhile avoiding obstacles as prescribed by the given time series. Moreover, we\nguarantee that the trajectory ultimately stabilizes to the POI. The proposed\nmethod provides an under-approximation of the RAS set, guaranteeing safety.\nNumerical examples are provided to validate the theory.\n","authors":["Zheng Gong","Boyang Li","Sylvia Herbert"],"pdf_url":"https://arxiv.org/pdf/2505.09058v1.pdf","comment":"7 pages, 5 figures, submitted to 64th IEEE Conference on Decision and\n  Control"},{"id":"http://arxiv.org/abs/2505.09040v1","updated":"2025-05-14T00:41:44Z","published":"2025-05-14T00:41:44Z","title":"RT-cache: Efficient Robot Trajectory Retrieval System","summary":"  This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation.\n","authors":["Owen Kwon","Abraham George","Alison Bartsch","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2505.09040v1.pdf","comment":"9 pages, 5 figures. Submitted to an IEEE robotics conference"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.13933v2","updated":"2025-05-14T23:53:45Z","published":"2024-07-18T23:09:14Z","title":"Unsupervised Video Highlight Detection by Learning from Audio and Visual\n  Recurrence","summary":"  With the exponential growth of video content, the need for automated video\nhighlight detection to extract key moments or highlights from lengthy videos\nhas become increasingly pressing. This technology has the potential to enhance\nuser experiences by allowing quick access to relevant content across diverse\ndomains. Existing methods typically rely either on expensive manually labeled\nframe-level annotations, or on a large external dataset of videos for weak\nsupervision through category information. To overcome this, we focus on\nunsupervised video highlight detection, eliminating the need for manual\nannotations. We propose a novel unsupervised approach which capitalizes on the\npremise that significant moments tend to recur across multiple videos of the\nsimilar category in both audio and visual modalities. Surprisingly, audio\nremains under-explored, especially in unsupervised algorithms, despite its\npotential to detect key moments. Through a clustering technique, we identify\npseudo-categories of videos and compute audio pseudo-highlight scores for each\nvideo by measuring the similarities of audio features among audio clips of all\nthe videos within each pseudo-category. Similarly, we also compute visual\npseudo-highlight scores for each video using visual features. Then, we combine\naudio and visual pseudo-highlights to create the audio-visual pseudo\nground-truth highlight of each video for training an audio-visual highlight\ndetection network. Extensive experiments and ablation studies on three\nbenchmarks showcase the superior performance of our method over prior work.\n","authors":["Zahidul Islam","Sujoy Paul","Mrigank Rochan"],"pdf_url":"https://arxiv.org/pdf/2407.13933v2.pdf","comment":"Accepted to the 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV)"},{"id":"http://arxiv.org/abs/2505.09859v1","updated":"2025-05-14T23:43:57Z","published":"2025-05-14T23:43:57Z","title":"Few-Shot Learning of Visual Compositional Concepts through Probabilistic\n  Schema Induction","summary":"  The ability to learn new visual concepts from limited examples is a hallmark\nof human cognition. While traditional category learning models represent each\nexample as an unstructured feature vector, compositional concept learning is\nthought to depend on (1) structured representations of examples (e.g., directed\ngraphs consisting of objects and their relations) and (2) the identification of\nshared relational structure across examples through analogical mapping. Here,\nwe introduce Probabilistic Schema Induction (PSI), a prototype model that\nemploys deep learning to perform analogical mapping over structured\nrepresentations of only a handful of examples, forming a compositional concept\ncalled a schema. In doing so, PSI relies on a novel conception of similarity\nthat weighs object-level similarity and relational similarity, as well as a\nmechanism for amplifying relations relevant to classification, analogous to\nselective attention parameters in traditional models. We show that PSI produces\nhuman-like learning performance and outperforms two controls: a prototype model\nthat uses unstructured feature vectors extracted from a deep learning model,\nand a variant of PSI with weaker structured representations. Notably, we find\nthat PSI's human-like performance is driven by an adaptive strategy that\nincreases relational similarity over object-level similarity and upweights the\ncontribution of relations that distinguish classes. These findings suggest that\nstructured representations and analogical mapping are critical to modeling\nrapid human-like learning of compositional visual concepts, and demonstrate how\ndeep learning can be leveraged to create psychological models.\n","authors":["Andrew Jun Lee","Taylor Webb","Trevor Bihl","Keith Holyoak","Hongjing Lu"],"pdf_url":"https://arxiv.org/pdf/2505.09859v1.pdf","comment":"Lee, A. J., Webb, T., Bihl, T., Holyoak, K. J., & Lu, H. (2025).\n  Few-shot learning of visual compositional concepts through probabilistic\n  schema induction. In A. Ruggeri, D. Barner, C. Walker, & N. Bramley (Eds.),\n  Proceedings of the 47th Annual Conference of the Cognitive Science Society.\n  Cognitive Science Society"},{"id":"http://arxiv.org/abs/2505.09858v1","updated":"2025-05-14T23:43:29Z","published":"2025-05-14T23:43:29Z","title":"Mission Balance: Generating Under-represented Class Samples using Video\n  Diffusion Models","summary":"  Computer-assisted interventions can improve intra-operative guidance,\nparticularly through deep learning methods that harness the spatiotemporal\ninformation in surgical videos. However, the severe data imbalance often found\nin surgical video datasets hinders the development of high-performing models.\nIn this work, we aim to overcome the data imbalance by synthesizing surgical\nvideos. We propose a unique two-stage, text-conditioned diffusion-based method\nto generate high-fidelity surgical videos for under-represented classes. Our\napproach conditions the generation process on text prompts and decouples\nspatial and temporal modeling by utilizing a 2D latent diffusion model to\ncapture spatial content and then integrating temporal attention layers to\nensure temporal consistency. Furthermore, we introduce a rejection sampling\nstrategy to select the most suitable synthetic samples, effectively augmenting\nexisting datasets to address class imbalance. We evaluate our method on two\ndownstream tasks-surgical action recognition and intra-operative event\nprediction-demonstrating that incorporating synthetic videos from our approach\nsubstantially enhances model performance. We open-source our implementation at\nhttps://gitlab.com/nct_tso_public/surgvgen.\n","authors":["Danush Kumar Venkatesh","Isabel Funke","Micha Pfeiffer","Fiona Kolbinger","Hanna Maria Schmeiser","Juergen Weitz","Marius Distler","Stefanie Speidel"],"pdf_url":"https://arxiv.org/pdf/2505.09858v1.pdf","comment":"Early accept at MICCAI 2025"},{"id":"http://arxiv.org/abs/2505.09831v1","updated":"2025-05-14T22:22:52Z","published":"2025-05-14T22:22:52Z","title":"ImplicitStainer: Data-Efficient Medical Image Translation for Virtual\n  Antibody-based Tissue Staining Using Local Implicit Functions","summary":"  Hematoxylin and eosin (H&E) staining is a gold standard for microscopic\ndiagnosis in pathology. However, H&E staining does not capture all the\ndiagnostic information that may be needed. To obtain additional molecular\ninformation, immunohistochemical (IHC) stains highlight proteins that mark\nspecific cell types, such as CD3 for T-cells or CK8/18 for epithelial cells.\nWhile IHC stains are vital for prognosis and treatment guidance, they are\ntypically only available at specialized centers and time consuming to acquire,\nleading to treatment delays for patients. Virtual staining, enabled by deep\nlearning-based image translation models, provides a promising alternative by\ncomputationally generating IHC stains from H&E stained images. Although many\nGAN and diffusion based image to image (I2I) translation methods have been used\nfor virtual staining, these models treat image patches as independent data\npoints, which results in increased and more diverse data requirements for\neffective generation. We present ImplicitStainer, a novel approach that\nleverages local implicit functions to improve image translation, specifically\nvirtual staining performance, by focusing on pixel-level predictions. This\nmethod enhances robustness to variations in dataset sizes, delivering\nhigh-quality results even with limited data. We validate our approach on two\ndatasets using a comprehensive set of metrics and benchmark it against over\nfifteen state-of-the-art GAN- and diffusion based models. Full Code and models\ntrained will be released publicly via Github upon acceptance.\n","authors":["Tushar Kataria","Beatrice Knudsen","Shireen Y. Elhabian"],"pdf_url":"https://arxiv.org/pdf/2505.09831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09829v1","updated":"2025-05-14T22:15:41Z","published":"2025-05-14T22:15:41Z","title":"BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image\n  Segmentation Performance for Low Data Regimes","summary":"  Obtaining large-scale medical data, annotated or unannotated, is challenging\ndue to stringent privacy regulations and data protection policies. In addition,\nannotating medical images requires that domain experts manually delineate\nanatomical structures, making the process both time-consuming and costly. As a\nresult, semi-supervised methods have gained popularity for reducing annotation\ncosts. However, the performance of semi-supervised methods is heavily dependent\non the availability of unannotated data, and their effectiveness declines when\nsuch data are scarce or absent. To overcome this limitation, we propose a\nsimple, yet effective and computationally efficient approach for medical image\nsegmentation that leverages only existing annotations. We propose BoundarySeg ,\na multi-task framework that incorporates organ boundary prediction as an\nauxiliary task to full organ segmentation, leveraging consistency between the\ntwo task predictions to provide additional supervision. This strategy improves\nsegmentation accuracy, especially in low data regimes, allowing our method to\nachieve performance comparable to or exceeding state-of-the-art semi supervised\napproaches all without relying on unannotated data or increasing computational\ndemands. Code will be released upon acceptance.\n","authors":["Tushar Kataria","Shireen Y. Elhabian"],"pdf_url":"https://arxiv.org/pdf/2505.09829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09827v1","updated":"2025-05-14T22:12:34Z","published":"2025-05-14T22:12:34Z","title":"Dyadic Mamba: Long-term Dyadic Human Motion Synthesis","summary":"  Generating realistic dyadic human motion from text descriptions presents\nsignificant challenges, particularly for extended interactions that exceed\ntypical training sequence lengths. While recent transformer-based approaches\nhave shown promising results for short-term dyadic motion synthesis, they\nstruggle with longer sequences due to inherent limitations in positional\nencoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach\nthat leverages State-Space Models (SSMs) to generate high-quality dyadic human\nmotion of arbitrary length. Our method employs a simple yet effective\narchitecture that facilitates information flow between individual motion\nsequences through concatenation, eliminating the need for complex\ncross-attention mechanisms. We demonstrate that Dyadic Mamba achieves\ncompetitive performance on standard short-term benchmarks while significantly\noutperforming transformer-based approaches on longer sequences. Additionally,\nwe propose a new benchmark for evaluating long-term motion synthesis quality,\nproviding a standardized framework for future research. Our results demonstrate\nthat SSM-based architectures offer a promising direction for addressing the\nchallenging task of long-term dyadic human motion synthesis from text\ndescriptions.\n","authors":["Julian Tanke","Takashi Shibuya","Kengo Uchida","Koichi Saito","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2505.09827v1.pdf","comment":"CVPR 2025 HuMoGen Workshop"},{"id":"http://arxiv.org/abs/2505.09819v1","updated":"2025-05-14T21:47:28Z","published":"2025-05-14T21:47:28Z","title":"Visual Feedback of Pattern Separability Improves Myoelectric Decoding\n  Performance of Upper Limb Prostheses","summary":"  State-of-the-art upper limb myoelectric prostheses often use pattern\nrecognition (PR) control systems that translate electromyography (EMG) signals\ninto desired movements. As prosthesis movement complexity increases, users\noften struggle to produce sufficiently distinct EMG patterns for reliable\nclassification. Existing training typically involves heuristic, trial-and-error\nuser adjustments to static decoder boundaries. Goal: We introduce the Reviewer,\na 3D visual interface projecting EMG signals directly into the decoder's\nclassification space, providing intuitive, real-time insight into PR algorithm\nbehavior. This structured feedback reduces cognitive load and fosters mutual,\ndata-driven adaptation between user-generated EMG patterns and decoder\nboundaries. Methods: A 10-session study with 12 able-bodied participants\ncompared PR performance after motor-based training and updating using the\nReviewer versus conventional virtual arm visualization. Performance was\nassessed using a Fitts law task that involved the aperture of the cursor and\nthe control of orientation. Results: Participants trained with the Reviewer\nachieved higher completion rates, reduced overshoot, and improved path\nefficiency and throughput compared to the standard visualization group.\nSignificance: The Reviewer introduces decoder-informed motor training,\nfacilitating immediate and consistent PR-based myoelectric control\nimprovements. By iteratively refining control through real-time feedback, this\napproach reduces reliance on trial-and-error recalibration, enabling a more\nadaptive, self-correcting training framework. Conclusion: The 3D visual\nfeedback significantly improves PR control in novice operators through\nstructured training, enabling feedback-driven adaptation and reducing reliance\non extensive heuristic adjustments.\n","authors":["Ruichen Yang","György M. Lévay","Christopher L. Hunt","Dániel Czeiner","Megan C. Hodgson","Damini Agarwal","Rahul R. Kaliki","Nitish V. Thakor"],"pdf_url":"https://arxiv.org/pdf/2505.09819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11792v2","updated":"2025-05-14T21:29:57Z","published":"2025-03-14T18:32:02Z","title":"StyleMorpheus: A Style-Based 3D-Aware Morphable Face Model","summary":"  For 3D face modeling, the recently developed 3D-aware neural rendering\nmethods are able to render photorealistic face images with arbitrary viewing\ndirections. The training of the parametric controllable 3D-aware face models,\nhowever, still relies on a large-scale dataset that is lab-collected. To\naddress this issue, this paper introduces \"StyleMorpheus\", the first\nstyle-based neural 3D Morphable Face Model (3DMM) that is trained on\nin-the-wild images. It inherits 3DMM's disentangled controllability (over face\nidentity, expression, and appearance) but without the need for accurately\nreconstructed explicit 3D shapes. StyleMorpheus employs an auto-encoder\nstructure. The encoder aims at learning a representative disentangled\nparametric code space and the decoder improves the disentanglement using shape\nand appearance-related style codes in the different sub-modules of the network.\nFurthermore, we fine-tune the decoder through style-based generative\nadversarial learning to achieve photorealistic 3D rendering quality. The\nproposed style-based design enables StyleMorpheus to achieve state-of-the-art\n3D-aware face reconstruction results, while also allowing disentangled control\nof the reconstructed face. Our model achieves real-time rendering speed,\nallowing its use in virtual reality applications. We also demonstrate the\ncapability of the proposed style-based design in face editing applications such\nas style mixing and color editing. Project homepage:\nhttps://github.com/ubc-3d-vision-lab/StyleMorpheus.\n","authors":["Peizhi Yan","Rabab K. Ward","Dan Wang","Qiang Tang","Shan Du"],"pdf_url":"https://arxiv.org/pdf/2503.11792v2.pdf","comment":"13 pages, work was completed in 2023"},{"id":"http://arxiv.org/abs/2410.16430v2","updated":"2025-05-14T19:34:10Z","published":"2024-10-21T18:50:16Z","title":"HaHeAE: Learning Generalisable Joint Representations of Human Hand and\n  Head Movements in Extended Reality","summary":"  Human hand and head movements are the most pervasive input modalities in\nextended reality (XR) and are significant for a wide range of applications.\nHowever, prior works on hand and head modelling in XR only explored a single\nmodality or focused on specific applications. We present HaHeAE - a novel\nself-supervised method for learning generalisable joint representations of hand\nand head movements in XR. At the core of our method is an autoencoder (AE) that\nuses a graph convolutional network-based semantic encoder and a diffusion-based\nstochastic encoder to learn the joint semantic and stochastic representations\nof hand-head movements. It also features a diffusion-based decoder to\nreconstruct the original signals. Through extensive evaluations on three public\nXR datasets, we show that our method 1) significantly outperforms commonly used\nself-supervised methods by up to 74.0% in terms of reconstruction quality and\nis generalisable across users, activities, and XR environments, 2) enables new\napplications, including interpretable hand-head cluster identification and\nvariable hand-head movement generation, and 3) can serve as an effective\nfeature extractor for downstream tasks. Together, these results demonstrate the\neffectiveness of our method and underline the potential of self-supervised\nmethods for jointly modelling hand-head behaviours in extended reality.\n","authors":["Zhiming Hu","Guanhua Zhang","Zheming Yin","Daniel Haeufle","Syn Schmitt","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2410.16430v2.pdf","comment":"Link: https://zhiminghu.net/hu25_haheae"},{"id":"http://arxiv.org/abs/2505.09746v1","updated":"2025-05-14T19:09:17Z","published":"2025-05-14T19:09:17Z","title":"A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the\n  Left Atrium","summary":"  The left atrium (LA) plays a pivotal role in modulating left ventricular\nfilling, but our comprehension of its hemodynamics is significantly limited by\nthe constraints of conventional ultrasound analysis. 4D flow magnetic resonance\nimaging (4D Flow MRI) holds promise for enhancing our understanding of atrial\nhemodynamics. However, the low velocities within the LA and the limited spatial\nresolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,\nthe absence of dedicated computational frameworks, combined with diverse\nacquisition protocols and vendors, complicates gathering large cohorts for\nstudying the prognostic value of hemodynamic parameters provided by 4D Flow\nMRI. In this study, we introduce the first open-source computational framework\ntailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive\nqualitative and quantitative analysis of advanced hemodynamic parameters. Our\nframework proves robust to data from different centers of varying quality,\nproducing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95\n$<$ 3 mm), even with limited training data. Additionally, we conducted the\nfirst comprehensive assessment of energy, vorticity, and pressure parameters in\nthe LA across a spectrum of disorders to investigate their potential as\nprognostic biomarkers.\n","authors":["Xabier Morales","Ayah Elsayed","Debbie Zhao","Filip Loncaric","Ainhoa Aguado","Mireia Masias","Gina Quill","Marc Ramos","Ada Doltra","Ana Garcia","Marta Sitges","David Marlevi","Alistair Young","Martyn Nash","Bart Bijnens","Oscar Camara"],"pdf_url":"https://arxiv.org/pdf/2505.09746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18418v3","updated":"2025-05-14T18:27:05Z","published":"2024-05-28T17:57:23Z","title":"Hierarchical World Models as Visual Whole-Body Humanoid Controllers","summary":"  Whole-body control for humanoids is challenging due to the high-dimensional\nnature of the problem, coupled with the inherent instability of a bipedal\nmorphology. Learning from visual observations further exacerbates this\ndifficulty. In this work, we explore highly data-driven approaches to visual\nwhole-body humanoid control based on reinforcement learning, without any\nsimplifying assumptions, reward design, or skill primitives. Specifically, we\npropose a hierarchical world model in which a high-level agent generates\ncommands based on visual observations for a low-level agent to execute, both of\nwhich are trained with rewards. Our approach produces highly performant control\npolicies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing\nmotions that are broadly preferred by humans.\n","authors":["Nicklas Hansen","Jyothir S V","Vlad Sobal","Yann LeCun","Xiaolong Wang","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2405.18418v3.pdf","comment":"Code and videos at https://nicklashansen.com/rlpuppeteer"},{"id":"http://arxiv.org/abs/2505.09615v1","updated":"2025-05-14T17:59:55Z","published":"2025-05-14T17:59:55Z","title":"UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing","summary":"  Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing\nboth uni-modal events (i.e., those occurring exclusively in either the visual\nor acoustic modality of a video) and multi-modal events (i.e., those occurring\nin both modalities concurrently). Moreover, the prohibitive cost of annotating\ntraining data with the class labels of all these events, along with their start\nand end times, imposes constraints on the scalability of AVVP techniques unless\nthey can be trained in a weakly-supervised setting, where only\nmodality-agnostic, video-level labels are available in the training data. To\nthis end, recently proposed approaches seek to generate segment-level\npseudo-labels to better guide model training. However, the absence of\ninter-segment dependencies when generating these pseudo-labels and the general\nbias towards predicting labels that are absent in a segment limit their\nperformance. This work proposes a novel approach towards overcoming these\nweaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video\nParsing (UWAV). Additionally, our innovative approach factors in the\nuncertainty associated with these estimated pseudo-labels and incorporates a\nfeature mixup based training regularization for improved training. Empirical\nresults show that UWAV outperforms state-of-the-art methods for the AVVP task\non multiple metrics, across two different datasets, attesting to its\neffectiveness and generalizability.\n","authors":["Yung-Hsuan Lai","Janek Ebbers","Yu-Chiang Frank Wang","François Germain","Michael Jeffrey Jones","Moitreya Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2505.09615v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2505.09608v1","updated":"2025-05-14T17:57:27Z","published":"2025-05-14T17:57:27Z","title":"LightLab: Controlling Light Sources in Images with Diffusion Models","summary":"  We present a simple, yet effective diffusion-based method for fine-grained,\nparametric control over light sources in an image. Existing relighting methods\neither rely on multiple input views to perform inverse rendering at inference\ntime, or fail to provide explicit control over light changes. Our method\nfine-tunes a diffusion model on a small set of real raw photograph pairs,\nsupplemented by synthetically rendered images at scale, to elicit its\nphotorealistic prior for relighting. We leverage the linearity of light to\nsynthesize image pairs depicting controlled light changes of either a target\nlight source or ambient illumination. Using this data and an appropriate\nfine-tuning scheme, we train a model for precise illumination changes with\nexplicit control over light intensity and color. Lastly, we show how our method\ncan achieve compelling light editing results, and outperforms existing methods\nbased on user preference.\n","authors":["Nadav Magar","Amir Hertz","Eric Tabellion","Yael Pritch","Alex Rav-Acha","Ariel Shamir","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2505.09608v1.pdf","comment":"Project Page: https://nadmag.github.io/LightLab/"},{"id":"http://arxiv.org/abs/2503.21696v2","updated":"2025-05-14T17:48:02Z","published":"2025-03-27T17:00:51Z","title":"Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for\n  Embodied Interactive Tasks","summary":"  Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases.\n","authors":["Wenqi Zhang","Mengna Wang","Gangao Liu","Xu Huixin","Yiwei Jiang","Yongliang Shen","Guiyang Hou","Zhe Zheng","Hang Zhang","Xin Li","Weiming Lu","Peng Li","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2503.21696v2.pdf","comment":"Code: https://github.com/zwq2018/embodied_reasoner Dataset:\n  https://huggingface.co/datasets/zwq2018/embodied_reasoner"},{"id":"http://arxiv.org/abs/2505.09591v1","updated":"2025-05-14T17:40:22Z","published":"2025-05-14T17:40:22Z","title":"Variational Visual Question Answering","summary":"  Despite remarkable progress in multimodal models for Visual Question\nAnswering (VQA), there remain major reliability concerns because the models can\noften be overconfident and miscalibrated, especially in out-of-distribution\n(OOD) settings. Plenty has been done to address such issues for unimodal\nmodels, but little work exists for multimodal cases. Here, we address\nunreliability in multimodal models by proposing a Variational VQA approach.\nSpecifically, instead of fine-tuning vision-language models by using AdamW, we\nemploy a recently proposed variational algorithm called IVON, which yields a\nposterior distribution over model parameters. Through extensive experiments, we\nshow that our approach improves calibration and abstentions without sacrificing\nthe accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce\nExpected Calibration Error by more than 50% compared to the AdamW baseline and\nraise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of\ndistribution shifts, the performance gain is even higher, achieving 8% Coverage\n(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we\npresent variational learning as a viable option to enhance the reliability of\nmultimodal models.\n","authors":["Tobias Jan Wieczorek","Nathalie Daun","Mohammad Emtiyaz Khan","Marcus Rohrbach"],"pdf_url":"https://arxiv.org/pdf/2505.09591v1.pdf","comment":"19 pages, 16 figures, under review at ICCV 2025"},{"id":"http://arxiv.org/abs/2410.07795v4","updated":"2025-05-14T17:22:25Z","published":"2024-10-10T10:24:59Z","title":"Optimal-state Dynamics Estimation for Physics-based Human Motion Capture\n  from Videos","summary":"  Human motion capture from monocular videos has made significant progress in\nrecent years. However, modern approaches often produce temporal artifacts, e.g.\nin form of jittery motion and struggle to achieve smooth and physically\nplausible motions. Explicitly integrating physics, in form of internal forces\nand exterior torques, helps alleviating these artifacts. Current\nstate-of-the-art approaches make use of an automatic PD controller to predict\ntorques and reaction forces in order to re-simulate the input kinematics, i.e.\nthe joint angles of a predefined skeleton. However, due to imperfect physical\nmodels, these methods often require simplifying assumptions and extensive\npreprocessing of the input kinematics to achieve good performance. To this end,\nwe propose a novel method to selectively incorporate the physics models with\nthe kinematics observations in an online setting, inspired by a neural\nKalman-filtering approach. We develop a control loop as a meta-PD controller to\npredict internal joint torques and external reaction forces, followed by a\nphysics-based motion simulation. A recurrent neural network is introduced to\nrealize a Kalman filter that attentively balances the kinematics input and\nsimulated motion, resulting in an optimal-state dynamics prediction. We show\nthat this filtering step is crucial to provide an online supervision that helps\nbalancing the shortcoming of the respective input motions, thus being important\nfor not only capturing accurate global motion trajectories but also producing\nphysically plausible human poses. The proposed approach excels in the\nphysics-based human pose estimation task and demonstrates the physical\nplausibility of the predictive dynamics, compared to state of the art. The code\nis available on https://github.com/cuongle1206/OSDCap\n","authors":["Cuong Le","Viktor Johansson","Manon Kok","Bastian Wandt"],"pdf_url":"https://arxiv.org/pdf/2410.07795v4.pdf","comment":"17 pages, 7 figure, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2505.09571v1","updated":"2025-05-14T17:15:03Z","published":"2025-05-14T17:15:03Z","title":"Don't Forget your Inverse DDIM for Image Editing","summary":"  The field of text-to-image generation has undergone significant advancements\nwith the introduction of diffusion models. Nevertheless, the challenge of\nediting real images persists, as most methods are either computationally\nintensive or produce poor reconstructions. This paper introduces SAGE\n(Self-Attention Guidance for image Editing) - a novel technique leveraging\npre-trained diffusion models for image editing. SAGE builds upon the DDIM\nalgorithm and incorporates a novel guidance mechanism utilizing the\nself-attention layers of the diffusion U-Net. This mechanism computes a\nreconstruction objective based on attention maps generated during the inverse\nDDIM process, enabling efficient reconstruction of unedited regions without the\nneed to precisely reconstruct the entire input image. Thus, SAGE directly\naddresses the key challenges in image editing. The superiority of SAGE over\nother methods is demonstrated through quantitative and qualitative evaluations\nand confirmed by a statistically validated comprehensive user study, in which\nall 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE\nranks as the top-performing method in seven out of 10 quantitative analyses and\nsecures second and third places in the remaining three.\n","authors":["Guillermo Gomez-Trenado","Pablo Mesejo","Oscar Cordón","Stéphane Lathuilière"],"pdf_url":"https://arxiv.org/pdf/2505.09571v1.pdf","comment":"12 pages, 12 figures, code available at\n  https://guillermogotre.github.io/sage/"},{"id":"http://arxiv.org/abs/2505.09568v1","updated":"2025-05-14T17:11:07Z","published":"2025-05-14T17:11:07Z","title":"BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset","summary":"  Unifying image understanding and generation has gained growing attention in\nrecent research on multimodal models. Although design choices for image\nunderstanding have been extensively studied, the optimal model architecture and\ntraining recipe for a unified framework with image generation remain\nunderexplored. Motivated by the strong potential of autoregressive and\ndiffusion models for high-quality generation and scalability, we conduct a\ncomprehensive study of their use in unified multimodal settings, with emphasis\non image representations, modeling objectives, and training strategies.\nGrounded in these investigations, we introduce a novel approach that employs a\ndiffusion transformer to generate semantically rich CLIP image features, in\ncontrast to conventional VAE-based representations. This design yields both\nhigher training efficiency and improved generative quality. Furthermore, we\ndemonstrate that a sequential pretraining strategy for unified models-first\ntraining on image understanding and subsequently on image generation-offers\npractical advantages by preserving image understanding capability while\ndeveloping strong image generation ability. Finally, we carefully curate a\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\nprompting GPT-4o with a diverse set of captions covering various scenes,\nobjects, human gestures, and more. Building on our innovative model design,\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\nunified multimodal models. BLIP3-o achieves superior performance across most of\nthe popular benchmarks spanning both image understanding and generation tasks.\nTo facilitate future research, we fully open-source our models, including code,\nmodel weights, training scripts, and pretraining and instruction tuning\ndatasets.\n","authors":["Jiuhai Chen","Zhiyang Xu","Xichen Pan","Yushi Hu","Can Qin","Tom Goldstein","Lifu Huang","Tianyi Zhou","Saining Xie","Silvio Savarese","Le Xue","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2505.09568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09565v1","updated":"2025-05-14T17:07:37Z","published":"2025-05-14T17:07:37Z","title":"Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using\n  Implicit Neural Representations","summary":"  High-resolution slice-to-volume reconstruction (SVR) from multiple\nmotion-corrupted low-resolution 2D slices constitutes a critical step in\nimage-based diagnostics of moving subjects, such as fetal brain Magnetic\nResonance Imaging (MRI). Existing solutions struggle with image artifacts and\nsevere subject motion or require slice pre-alignment to achieve satisfying\nreconstruction performance. We propose a novel SVR method to enable fast and\naccurate MRI reconstruction even in cases of severe image and motion\ncorruption. Our approach performs motion correction, outlier handling, and\nsuper-resolution reconstruction with all operations being entirely based on\nimplicit neural representations. The model can be initialized with\ntask-specific priors through fully self-supervised meta-learning on either\nsimulated or real-world data. In extensive experiments including over 480\nreconstructions of simulated and clinical MRI brain data from different\ncenters, we prove the utility of our method in cases of severe subject motion\nand image artifacts. Our results demonstrate improvements in reconstruction\nquality, especially in the presence of severe motion, compared to\nstate-of-the-art methods, and up to 50% reduction in reconstruction time.\n","authors":["Maik Dannecker","Thomas Sanchez","Meritxell Bach Cuadra","Özgün Turgut","Anthony N. Price","Lucilio Cordero-Grande","Vanessa Kyriakopoulou","Joseph V. Hajnal","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2505.09565v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.09564v1","updated":"2025-05-14T17:07:30Z","published":"2025-05-14T17:07:30Z","title":"Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D\n  Cardiac CT Segmentation","summary":"  Cardiac image segmentation is an important step in many cardiac image\nanalysis and modeling tasks such as motion tracking or simulations of cardiac\nmechanics. While deep learning has greatly advanced segmentation in clinical\nsettings, there is limited work on pre-clinical imaging, notably in porcine\nmodels, which are often used due to their anatomical and physiological\nsimilarity to humans. However, differences between species create a domain\nshift that complicates direct model transfer from human to pig data.\n  Recently, foundation models trained on large human datasets have shown\npromise for robust medical image segmentation; yet their applicability to\nporcine data remains largely unexplored. In this work, we investigate whether\nfoundation models can generate sufficiently accurate pseudo-labels for pig\ncardiac CT and propose a simple self-training approach to iteratively refine\nthese labels. Our method requires no manually annotated pig data, relying\ninstead on iterative updates to improve segmentation quality. We demonstrate\nthat this self-training process not only enhances segmentation accuracy but\nalso smooths out temporal inconsistencies across consecutive frames. Although\nour results are encouraging, there remains room for improvement, for example by\nincorporating more sophisticated self-training strategies and by exploring\nadditional foundation models and other cardiac imaging technologies.\n","authors":["Anne-Marie Rickmann","Stephanie L. Thorn","Shawn S. Ahn","Supum Lee","Selen Uman","Taras Lysyy","Rachel Burns","Nicole Guerrera","Francis G. Spinale","Jason A. Burdick","Albert J. Sinusas","James S. Duncan"],"pdf_url":"https://arxiv.org/pdf/2505.09564v1.pdf","comment":"accepted at FIMH 2025"},{"id":"http://arxiv.org/abs/2505.09562v1","updated":"2025-05-14T17:05:12Z","published":"2025-05-14T17:05:12Z","title":"Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through\n  Differentiable Object Shapes","summary":"  Autonomous vehicles need a complete map of their surroundings to plan and\nact. This has sparked research into the tasks of 3D occupancy prediction, 3D\nscene completion, and 3D panoptic scene completion, which predict a dense map\nof the ego vehicle's surroundings as a voxel grid. Scene completion extends\noccupancy prediction by predicting occluded regions of the voxel grid, and\npanoptic scene completion further extends this task by also distinguishing\nobject instances within the same class; both aspects are crucial for path\nplanning and decision-making. However, 3D panoptic scene completion is\ncurrently underexplored. This work introduces a novel framework for 3D panoptic\nscene completion that extends existing 3D semantic scene completion models. We\npropose an Object Module and Panoptic Module that can easily be integrated with\n3D occupancy and scene completion methods presented in the literature. Our\napproach leverages the available annotations in occupancy benchmarks, allowing\nindividual object shapes to be learned as a differentiable problem. The code is\navailable at https://github.com/nicolamarinello/OffsetOcc .\n","authors":["Nicola Marinello","Simen Cassiman","Jonas Heylen","Marc Proesmans","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2505.09562v1.pdf","comment":"Accepted to CVPR 2025 Workshop on Autonomous Driving"},{"id":"http://arxiv.org/abs/2505.09529v1","updated":"2025-05-14T16:24:22Z","published":"2025-05-14T16:24:22Z","title":"Contactless Cardiac Pulse Monitoring Using Event Cameras","summary":"  Time event cameras are a novel technology for recording scene information at\nextremely low latency and with low power consumption. Event cameras output a\nstream of events that encapsulate pixel-level light intensity changes within\nthe scene, capturing information with a higher dynamic range and temporal\nresolution than traditional cameras. This study investigates the contact-free\nreconstruction of an individual's cardiac pulse signal from time event\nrecording of their face using a supervised convolutional neural network (CNN)\nmodel. An end-to-end model is trained to extract the cardiac signal from a\ntwo-dimensional representation of the event stream, with model performance\nevaluated based on the accuracy of the calculated heart rate. The experimental\nresults confirm that physiological cardiac information in the facial region is\neffectively preserved within the event stream, showcasing the potential of this\nnovel sensor for remote heart rate monitoring. The model trained on event\nframes achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm)\ncompared to the RMSE of 2.92 bpm achieved by the baseline model trained on\nstandard camera frames. Furthermore, models trained on event frames generated\nat 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an\nRMSE of 2.54 and 2.13 bpm, respectively.\n","authors":["Mohamed Moustafa","Joseph Lemley","Peter Corcoran"],"pdf_url":"https://arxiv.org/pdf/2505.09529v1.pdf","comment":"This paper is a preprint of a paper submitted to IEEE Access and is\n  currently under review"},{"id":"http://arxiv.org/abs/2505.09528v1","updated":"2025-05-14T16:23:26Z","published":"2025-05-14T16:23:26Z","title":"Conformal Bounds on Full-Reference Image Quality for Imaging Inverse\n  Problems","summary":"  In imaging inverse problems, we would like to know how close the recovered\nimage is to the true image in terms of full-reference image quality (FRIQ)\nmetrics like PSNR, SSIM, LPIPS, etc. This is especially important in\nsafety-critical applications like medical imaging, where knowing that, say, the\nSSIM was poor could potentially avoid a costly misdiagnosis. But since we don't\nknow the true image, computing FRIQ is non-trivial. In this work, we combine\nconformal prediction with approximate posterior sampling to construct bounds on\nFRIQ that are guaranteed to hold up to a user-specified error probability. We\ndemonstrate our approach on image denoising and accelerated magnetic resonance\nimaging (MRI) problems. Code is available at\nhttps://github.com/jwen307/quality_uq.\n","authors":["Jeffrey Wen","Rizwan Ahmad","Philip Schniter"],"pdf_url":"https://arxiv.org/pdf/2505.09528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09521v1","updated":"2025-05-14T16:18:21Z","published":"2025-05-14T16:18:21Z","title":"Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI\n  Reconstruction based on Multi-directional Time-Frequency Convolutional\n  Attention Encoder and Vision-Mamba U-Net","summary":"  High-resolution functional magnetic resonance imaging (fMRI) is essential for\nmapping human brain activity; however, it remains costly and logistically\nchallenging. If comparable volumes could be generated directly from widely\navailable scalp electroencephalography (EEG), advanced neuroimaging would\nbecome significantly more accessible. Existing EEG-to-fMRI generators rely on\nplain CNNs that fail to capture cross-channel time-frequency cues or on heavy\ntransformer/GAN decoders that strain memory and stability. We propose\nSpec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts\nthese issues via a Multi-directional Time-Frequency Convolutional Attention\nEncoder, stacking temporal, spectral and joint convolutions with\nself-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space\nblocks enable efficient long-range spatial modelling. Trained end-to-end with a\nhybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on\nthree public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball\nand 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9%\nrespectively over previous best SSIM scores. Furthermore, it achieves\ncompetitive PSNR scores, particularly excelling on the CN-EPFL dataset with a\n4.6% improvement over the previous best PSNR, thus striking a better balance in\nreconstruction quality. The proposed model is lightweight and efficient, making\nit suitable for real-time applications in clinical and research settings. The\ncode is available at https://github.com/hdy6438/Spec2VolCAMU-Net.\n","authors":["Dongyi He","Shiyang Li","Bin Jiang","He Yan"],"pdf_url":"https://arxiv.org/pdf/2505.09521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00810v3","updated":"2025-05-14T16:07:36Z","published":"2023-11-01T19:49:32Z","title":"A Call to Arms: AI Should be Critical for Social Media Analysis of\n  Conflict Zones","summary":"  The massive proliferation of social media data represents a transformative\nopportunity for conflict studies and for tracking the proliferation and use of\nweaponry, as conflicts are increasingly documented in these online spaces. At\nthe same time, the scale and types of data available are problematic for\ntraditional open-source intelligence. This paper focuses on identifying\nspecific weapon systems and the insignias of the armed groups using them as\ndocumented in the Ukraine war, as these tasks are critical to operational\nintelligence and tracking weapon proliferation, especially given the scale of\ninternational military aid given to Ukraine. The large scale of social media\nmakes manual assessment difficult, however, so this paper presents early work\nthat uses computer vision models to support this task. We demonstrate that\nthese models can both identify weapons embedded in images shared in social\nmedia and how the resulting collection of military-relevant images and their\npost times interact with the offline, real-world conflict. Not only can we then\ntrack changes in the prevalence of images of tanks, land mines, military\ntrucks, etc., we find correlations among time series data associated with these\nimages and the daily fatalities in this conflict. This work shows substantial\nopportunity for examining similar online documentation of conflict contexts,\nand we also point to future avenues where computer vision can be further\nimproved for these open-source intelligence tasks.\n","authors":["Afia Abedin","Abdul Bais","Cody Buntain","Laura Courchesne","Brian McQuinn","Matthew E. Taylor","Muhib Ullah"],"pdf_url":"https://arxiv.org/pdf/2311.00810v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18769v5","updated":"2025-05-14T16:01:10Z","published":"2024-09-27T14:14:16Z","title":"State-of-the-Art Periorbital Distance Prediction and Disease\n  Classification Using Periorbital Features","summary":"  Periorbital distances are critical markers for diagnosing and monitoring a\nrange of oculoplastic and craniofacial conditions. Manual measurement, however,\nis subjective and prone to intergrader variability. Automated methods have been\ndeveloped but remain limited by standardized imaging requirements, small\ndatasets, and a narrow focus on individual measurements. We developed a\nsegmentation pipeline trained on a domain-specific dataset of healthy eyes and\ncompared its performance against the Segment Anything Model (SAM) and the prior\nbenchmark, PeriorbitAI. Segmentation accuracy was evaluated across multiple\ndisease classes and imaging conditions. We further investigated the use of\npredicted periorbital distances as features for disease classification under\nin-distribution (ID) and out-of-distribution (OOD) settings, comparing shallow\nclassifiers, CNNs, and fusion models. Our segmentation model achieved\nstate-of-the-art accuracy across all datasets, with error rates within\nintergrader variability and superior performance relative to SAM and\nPeriorbitAI. In classification tasks, models trained on periorbital distances\nmatched CNN performance on ID data (77--78\\% accuracy) and substantially\noutperformed CNNs under OOD conditions (63--68\\% accuracy vs. 14\\%). Fusion\nmodels achieved the highest ID accuracy (80\\%) but were sensitive to degraded\nCNN features under OOD shifts. Segmentation-derived periorbital distances\nprovide robust, explainable features for disease classification and generalize\nbetter under domain shift than CNN image classifiers. These results establish a\nnew benchmark for periorbital distance prediction and highlight the potential\nof anatomy-based AI pipelines for real-world deployment in oculoplastic and\ncraniofacial care.\n","authors":["George R. Nahass","Sasha Hubschman","Jeffrey C. Peterson","Ghasem Yazdanpanah","Nicholas Tomaras","Madison Cheung","Alex Palacios","Kevin Heinze","Chad A. Purnell","Pete Setabutr","Ann Q. Tran","Darvin Yi"],"pdf_url":"https://arxiv.org/pdf/2409.18769v5.pdf","comment":"25 pages, 12 figures, 16 tables"},{"id":"http://arxiv.org/abs/2505.09498v1","updated":"2025-05-14T15:45:17Z","published":"2025-05-14T15:45:17Z","title":"Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low\n  Latency and High Throughput","summary":"  In this paper, we introduce Flash-VL 2B, a novel approach to optimizing\nVision-Language Models (VLMs) for real-time applications, targeting ultra-low\nlatency and high throughput without sacrificing accuracy. Leveraging advanced\narchitectural enhancements and efficient computational strategies, Flash-VL 2B\nis designed to maximize throughput by reducing processing time while\nmaintaining competitive performance across multiple vision-language benchmarks.\nOur approach includes tailored architectural choices, token compression\nmechanisms, data curation, training schemes, and a novel image processing\ntechnique called implicit semantic stitching that effectively balances\ncomputational load and model performance. Through extensive evaluations on 11\nstandard VLM benchmarks, we demonstrate that Flash-VL 2B achieves\nstate-of-the-art results in both speed and accuracy, making it a promising\nsolution for deployment in resource-constrained environments and large-scale\nreal-time applications.\n","authors":["Bo Zhang","Shuo Li","Runhe Tian","Yang Yang","Jixin Tang","Jinhao Zhou","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2505.09498v1.pdf","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.09484v1","updated":"2025-05-14T15:36:44Z","published":"2025-05-14T15:36:44Z","title":"Denoising and Alignment: Rethinking Domain Generalization for Multimodal\n  Face Anti-Spoofing","summary":"  Face Anti-Spoofing (FAS) is essential for the security of facial recognition\nsystems in diverse scenarios such as payment processing and surveillance.\nCurrent multimodal FAS methods often struggle with effective generalization,\nmainly due to modality-specific biases and domain shifts. To address these\nchallenges, we introduce the \\textbf{M}ulti\\textbf{m}odal \\textbf{D}enoising\nand \\textbf{A}lignment (\\textbf{MMDA}) framework. By leveraging the zero-shot\ngeneralization capability of CLIP, the MMDA framework effectively suppresses\nnoise in multimodal data through denoising and alignment mechanisms, thereby\nsignificantly enhancing the generalization performance of cross-modal\nalignment. The \\textbf{M}odality-\\textbf{D}omain Joint \\textbf{D}ifferential\n\\textbf{A}ttention (\\textbf{MD2A}) module in MMDA concurrently mitigates the\nimpacts of domain and modality noise by refining the attention mechanism based\non extracted common noise features. Furthermore, the \\textbf{R}epresentation\n\\textbf{S}pace \\textbf{S}oft (\\textbf{RS2}) Alignment strategy utilizes the\npre-trained CLIP model to align multi-domain multimodal data into a generalized\nrepresentation space in a flexible manner, preserving intricate representations\nand enhancing the model's adaptability to various unseen conditions. We also\ndesign a \\textbf{U}-shaped \\textbf{D}ual \\textbf{S}pace \\textbf{A}daptation\n(\\textbf{U-DSA}) module to enhance the adaptability of representations while\nmaintaining generalization performance. These improvements not only enhance the\nframework's generalization capabilities but also boost its ability to represent\ncomplex representations. Our experimental results on four benchmark datasets\nunder different evaluation protocols demonstrate that the MMDA framework\noutperforms existing state-of-the-art methods in terms of cross-domain\ngeneralization and multimodal detection accuracy. The code will be released\nsoon.\n","authors":["Yingjie Ma","Xun Lin","Zitong Yu","Xin Liu","Xiaochen Yuan","Weicheng Xie","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2505.09484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09466v1","updated":"2025-05-14T15:17:34Z","published":"2025-05-14T15:17:34Z","title":"A 2D Semantic-Aware Position Encoding for Vision Transformers","summary":"  Vision transformers have demonstrated significant advantages in computer\nvision tasks due to their ability to capture long-range dependencies and\ncontextual relationships through self-attention. However, existing position\nencoding techniques, which are largely borrowed from natural language\nprocessing, fail to effectively capture semantic-aware positional relationships\nbetween image patches. Traditional approaches like absolute position encoding\nand relative position encoding primarily focus on 1D linear position\nrelationship, often neglecting the semantic similarity between distant yet\ncontextually related patches. These limitations hinder model generalization,\ntranslation equivariance, and the ability to effectively handle repetitive or\nstructured patterns in images. In this paper, we propose 2-Dimensional\nSemantic-Aware Position Encoding ($\\text{SaPE}^2$), a novel position encoding\nmethod with semantic awareness that dynamically adapts position representations\nby leveraging local content instead of fixed linear position relationship or\nspatial coordinates. Our method enhances the model's ability to generalize\nacross varying image resolutions and scales, improves translation equivariance,\nand better aggregates features for visually similar but spatially distant\npatches. By integrating $\\text{SaPE}^2$ into vision transformers, we bridge the\ngap between position encoding and perceptual similarity, thereby improving\nperformance on computer vision tasks.\n","authors":["Xi Chen","Shiyang Zhou","Muqi Huang","Jiaxu Feng","Yun Xiong","Kun Zhou","Biao Yang","Yuhui Zhang","Huishuai Bao","Sijia Peng","Chuan Li","Feng Shi"],"pdf_url":"https://arxiv.org/pdf/2505.09466v1.pdf","comment":"14 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2505.09455v1","updated":"2025-05-14T15:05:36Z","published":"2025-05-14T15:05:36Z","title":"Beyond Pixels: Leveraging the Language of Soccer to Improve\n  Spatio-Temporal Action Detection in Broadcast Videos","summary":"  State-of-the-art spatio-temporal action detection (STAD) methods show\npromising results for extracting soccer events from broadcast videos. However,\nwhen operated in the high-recall, low-precision regime required for exhaustive\nevent coverage in soccer analytics, their lack of contextual understanding\nbecomes apparent: many false positives could be resolved by considering a\nbroader sequence of actions and game-state information. In this work, we\naddress this limitation by reasoning at the game level and improving STAD\nthrough the addition of a denoising sequence transduction task. Sequences of\nnoisy, context-free player-centric predictions are processed alongside clean\ngame state information using a Transformer-based encoder-decoder model. By\nmodeling extended temporal context and reasoning jointly over team-level\ndynamics, our method leverages the \"language of soccer\" - its tactical\nregularities and inter-player dependencies - to generate \"denoised\" sequences\nof actions. This approach improves both precision and recall in low-confidence\nregimes, enabling more reliable event extraction from broadcast video and\ncomplementing existing pixel-based methods.\n","authors":["Jeremie Ochin","Raphael Chekroun","Bogdan Stanciulescu","Sotiris Manitsaris"],"pdf_url":"https://arxiv.org/pdf/2505.09455v1.pdf","comment":"12 pages, submitted to Advanced Concepts for Intelligent Vision\n  Systems 2025"},{"id":"http://arxiv.org/abs/2505.09450v1","updated":"2025-05-14T15:01:59Z","published":"2025-05-14T15:01:59Z","title":"MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating\n  Motion during Ultrasound-Guided Aspiration Biopsy","summary":"  Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally\ninvasive diagnostic procedure. However, an aspiration needle tracker addressing\nrapid reciprocating motion is still missing. MrTrack, an aspiration needle\ntracker with a mamba-based register mechanism, is proposed. MrTrack leverages a\nMamba-based register extractor to sequentially distill global context from each\nhistorical search map, storing these temporal cues in a register bank. The\nMamba-based register retriever then retrieves temporal prompts from the\nregister bank to provide external cues when current vision features are\ntemporarily unusable due to rapid reciprocating motion and imaging degradation.\nA self-supervised register diversify loss is proposed to encourage feature\ndiversity and dimension independence within the learned register, mitigating\nfeature collapse. Comprehensive experiments conducted on both motorized and\nmanual aspiration datasets demonstrate that MrTrack not only outperforms\nstate-of-the-art trackers in accuracy and robustness but also achieves superior\ninference efficiency.\n","authors":["Yuelin Zhang","Qingpeng Ding","Long Lei","Yongxuan Feng","Raymond Shing-Yan Tang","Shing Shin Cheng"],"pdf_url":"https://arxiv.org/pdf/2505.09450v1.pdf","comment":"Early Accepted by MICCAI 2025"},{"id":"http://arxiv.org/abs/2412.01986v2","updated":"2025-05-14T14:57:42Z","published":"2024-12-02T21:35:33Z","title":"HybridMQA: Exploring Geometry-Texture Interactions for Colored Mesh\n  Quality Assessment","summary":"  Mesh quality assessment (MQA) models play a critical role in the design,\noptimization, and evaluation of mesh operation systems in a wide variety of\napplications. Current MQA models, whether model-based methods using\ntopology-aware features or projection-based approaches working on rendered 2D\nprojections, often fail to capture the intricate interactions between texture\nand 3D geometry. We introduce HybridMQA, a first-of-its-kind hybrid\nfull-reference colored MQA framework that integrates model-based and\nprojection-based approaches, capturing complex interactions between textural\ninformation and 3D structures for enriched quality representations. Our method\nemploys graph learning to extract detailed 3D representations, which are then\nprojected to 2D using a novel feature rendering process that precisely aligns\nthem with colored projections. This enables the exploration of geometry-texture\ninteractions via cross-attention, producing comprehensive mesh quality\nrepresentations. Extensive experiments demonstrate HybridMQA's superior\nperformance across diverse datasets, highlighting its ability to effectively\nleverage geometry-texture interactions for a thorough understanding of mesh\nquality. Our implementation will be made publicly available.\n","authors":["Armin Shafiee Sarvestani","Sheyang Tang","Zhou Wang"],"pdf_url":"https://arxiv.org/pdf/2412.01986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07409v3","updated":"2025-05-14T14:57:00Z","published":"2025-02-11T09:42:13Z","title":"MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for\n  Few-Shot WSI Classification","summary":"  Whole slide pathology image classification presents challenges due to\ngigapixel image sizes and limited annotation labels, hindering model\ngeneralization. This paper introduces a prompt learning method to adapt large\nvision-language models for few-shot pathology classification. We first extend\nthe Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology\nimage tiles, into a vision-language model by adding adaptors and aligning it\nwith medical text encoders via contrastive learning on 923K image-text pairs.\nThe model is then used to extract visual features and text embeddings from\nfew-shot annotations and fine-tunes with learnable prompt embeddings. Unlike\nprior methods that combine prompts with frozen features using prefix embeddings\nor self-attention, we propose multi-granular attention that compares\ninteractions between learnable prompts with individual image patches and groups\nof them. This approach improves the model's ability to capture both\nfine-grained details and broader context, enhancing its recognition of complex\npatterns across sub-regions. To further improve accuracy, we leverage\n(unbalanced) optimal transport-based visual-text distance to secure model\nrobustness by mitigating perturbations that might occur during the data\naugmentation process. Empirical experiments on lung, kidney, and breast\npathology modalities validate the effectiveness of our approach; thereby, we\nsurpass several of the latest competitors and consistently improve performance\nacross diverse architectures, including CLIP, PLIP, and Prov-GigaPath\nintegrated PLIP. We release our implementations and pre-trained models at this\nMGPATH.\n","authors":["Anh-Tien Nguyen","Duy Minh Ho Nguyen","Nghiem Tuong Diep","Trung Quoc Nguyen","Nhat Ho","Jacqueline Michelle Metsch","Miriam Cindy Maurer","Daniel Sonntag","Hanibal Bohnenberger","Anne-Christin Hauschild"],"pdf_url":"https://arxiv.org/pdf/2502.07409v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09435v1","updated":"2025-05-14T14:43:31Z","published":"2025-05-14T14:43:31Z","title":"Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy\n  Records","summary":"  Pre-training on image-text colonoscopy records offers substantial potential\nfor improving endoscopic image analysis, but faces challenges including\nnon-informative background images, complex medical terminology, and ambiguous\nmulti-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised\nframework that enhances Contrastive Language-Image Pre-training (CLIP) for this\ndomain. Endo-CLIP's three-stage framework--cleansing, attunement, and\nunification--addresses these challenges by (1) removing background frames, (2)\nleveraging large language models to extract clinical attributes for\nfine-grained contrastive learning, and (3) employing patient-level\ncross-attention to resolve multi-polyp ambiguities. Extensive experiments\ndemonstrate that Endo-CLIP significantly outperforms state-of-the-art\npre-training methods in zero-shot and few-shot polyp detection and\nclassification, paving the way for more accurate and clinically relevant\nendoscopic analysis.\n","authors":["Yili He","Yan Zhu","Peiyao Fu","Ruijie Yang","Tianyi Chen","Zhihua Wang","Quanlin Li","Pinghong Zhou","Xian Yang","Shuo Wang"],"pdf_url":"https://arxiv.org/pdf/2505.09435v1.pdf","comment":"Early accepted to MICCAI 2025"},{"id":"http://arxiv.org/abs/2505.09433v1","updated":"2025-05-14T14:38:40Z","published":"2025-05-14T14:38:40Z","title":"Efficient LiDAR Reflectance Compression via Scanning Serialization","summary":"  Reflectance attributes in LiDAR point clouds provide essential information\nfor downstream tasks but remain underexplored in neural compression methods. To\naddress this, we introduce SerLiC, a serialization-based neural compression\nframework to fully exploit the intrinsic characteristics of LiDAR reflectance.\nSerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order\nserialization, offering a device-centric perspective for reflectance analysis.\nEach point is then tokenized into a contextual representation comprising its\nsensor scanning index, radial distance, and prior reflectance, for effective\ndependencies exploration. For efficient sequential modeling, Mamba is\nincorporated with a dual parallelization scheme, enabling simultaneous\nautoregressive dependency capture and fast processing. Extensive experiments\ndemonstrate that SerLiC attains over 2x volume reduction against the original\nreflectance data, outperforming the state-of-the-art method by up to 22%\nreduction of compressed bits while using only 2% of its parameters. Moreover, a\nlightweight version of SerLiC achieves > 10 fps (frames per second) with just\n111K parameters, which is attractive for real-world applications.\n","authors":["Jiahao Zhu","Kang You","Dandan Ding","Zhan Ma"],"pdf_url":"https://arxiv.org/pdf/2505.09433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03370v2","updated":"2025-05-14T14:34:47Z","published":"2024-03-05T23:32:26Z","title":"F$^3$Loc: Fusion and Filtering for Floorplan Localization","summary":"  In this paper we propose an efficient data-driven solution to\nself-localization within a floorplan. Floorplan data is readily available,\nlong-term persistent and inherently robust to changes in the visual appearance.\nOur method does not require retraining per map and location or demand a large\ndatabase of images of the area of interest. We propose a novel probabilistic\nmodel consisting of an observation and a novel temporal filtering module.\nOperating internally with an efficient ray-based representation, the\nobservation module consists of a single and a multiview module to predict\nhorizontal depth from images and fuses their results to benefit from advantages\noffered by either methodology. Our method operates on conventional consumer\nhardware and overcomes a common limitation of competing methods that often\ndemand upright images. Our full system meets real-time requirements, while\noutperforming the state-of-the-art by a significant margin.\n","authors":["Changan Chen","Rui Wang","Christoph Vogel","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2403.03370v2.pdf","comment":"10 pages, 11 figure, accepted to CVPR 2024 (fixed typo eq.8: s_x,s_y,\n  s_phi -> x, y, phi)"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2505.09861v1","updated":"2025-05-14T23:54:57Z","published":"2025-05-14T23:54:57Z","title":"LiDDA: Data Driven Attribution at LinkedIn","summary":"  Data Driven Attribution, which assigns conversion credits to marketing\ninteractions based on causal patterns learned from data, is the foundation of\nmodern marketing intelligence and vital to any marketing businesses and\nadvertising platform. In this paper, we introduce a unified transformer-based\nattribution approach that can handle member-level data, aggregate-level data,\nand integration of external macro factors. We detail the large scale\nimplementation of the approach at LinkedIn, showcasing significant impact. We\nalso share learning and insights that are broadly applicable to the marketing\nand ad tech fields.\n","authors":["John Bencina","Erkut Aykutlug","Yue Chen","Zerui Zhang","Stephanie Sorenson","Shao Tang","Changshuai Wei"],"pdf_url":"https://arxiv.org/pdf/2505.09861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09847v1","updated":"2025-05-14T23:12:20Z","published":"2025-05-14T23:12:20Z","title":"Causal Predictive Optimization and Generation for Business AI","summary":"  The sales process involves sales functions converting leads or opportunities\nto customers and selling more products to existing customers. The optimization\nof the sales process thus is key to success of any B2B business. In this work,\nwe introduce a principled approach to sales optimization and business AI,\nnamely the Causal Predictive Optimization and Generation, which includes three\nlayers: 1) prediction layer with causal ML 2) optimization layer with\nconstraint optimization and contextual bandit 3) serving layer with Generative\nAI and feedback-loop for system enhancement. We detail the implementation and\ndeployment of the system in LinkedIn, showcasing significant wins over legacy\nsystems and sharing learning and insight broadly applicable to this field.\n","authors":["Liyang Zhao","Olurotimi Seton","Himadeep Reddy Reddivari","Suvendu Jena","Shadow Zhao","Rachit Kumar","Changshuai Wei"],"pdf_url":"https://arxiv.org/pdf/2505.09847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08911v3","updated":"2025-05-14T22:08:01Z","published":"2024-12-12T03:47:40Z","title":"Goal-Conditioned Supervised Learning for Multi-Objective Recommendation","summary":"  Multi-objective learning endeavors to concurrently optimize multiple\nobjectives using a single model, aiming to achieve high and balanced\nperformance across diverse objectives. However, this often entails a more\ncomplex optimization problem, particularly when navigating potential conflicts\nbetween objectives, leading to solutions with higher memory requirements and\ncomputational complexity. This paper introduces a Multi-Objective\nGoal-Conditioned Supervised Learning (MOGCSL) framework for automatically\nlearning to achieve multiple objectives from offline sequential data. MOGCSL\nextends the conventional GCSL method to multi-objective scenarios by redefining\ngoals from one-dimensional scalars to multi-dimensional vectors. It benefits\nfrom naturally eliminating the need for complex architectures and optimization\nconstraints. Moreover, MOGCSL effectively filters out uninformative or noisy\ninstances that fail to achieve desirable long-term rewards across multiple\nobjectives. We also introduces a novel goal-selection algorithm for MOGCSL to\nmodel and identify \"high\" achievable goals for inference.\n  While MOGCSL is quite general, we focus on its application to the next action\nprediction problem in commercial-grade recommender systems. In this context,\nany viable solution needs to be reasonably scalable and also be robust to large\namounts of noisy data that is characteristic of this application space. We show\nthat MOGCSL performs admirably on both counts by extensive experiments on\nreal-world recommendation datasets. Also, analysis and experiments are included\nto explain its strength in discounting the noisier portions of training data in\nrecommender systems with multiple objectives.\n","authors":["Shijun Li","Hilaf Hasson","Jing Hu","Joydeep Ghosh"],"pdf_url":"https://arxiv.org/pdf/2412.08911v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09795v1","updated":"2025-05-14T20:45:29Z","published":"2025-05-14T20:45:29Z","title":"Beyond Pairwise Learning-To-Rank At Airbnb","summary":"  There are three fundamental asks from a ranking algorithm: it should scale to\nhandle a large number of items, sort items accurately by their utility, and\nimpose a total order on the items for logical consistency. But here's the\ncatch-no algorithm can achieve all three at the same time. We call this\nlimitation the SAT theorem for ranking algorithms. Given the dilemma, how can\nwe design a practical system that meets user needs? Our current work at Airbnb\nprovides an answer, with a working solution deployed at scale. We start with\npairwise learning-to-rank (LTR) models-the bedrock of search ranking tech\nstacks today. They scale linearly with the number of items ranked and perform\nstrongly on metrics like NDCG by learning from pairwise comparisons. They are\nat a sweet spot of performance vs. cost, making them an ideal choice for\nseveral industrial applications. However, they have a drawback-by ignoring\ninteractions between items, they compromise on accuracy. To improve accuracy,\nwe create a \"true\" pairwise LTR model-one that captures interactions between\nitems during pairwise comparisons. But accuracy comes at the expense of\nscalability and total order, and we discuss strategies to counter these\nchallenges. For greater accuracy, we take each item in the search result, and\ncompare it against the rest of the items along two dimensions: (1) Superiority:\nHow strongly do searchers prefer the given item over the remaining ones? (2)\nSimilarity: How similar is the given item to all the other items? This forms\nthe basis of our \"all-pairwise\" LTR framework, which factors in interactions\nacross all items at once. Looking at items on the search result page all\ntogether-superiority and similarity combined-gives us a deeper understanding of\nwhat searchers truly want. We quantify the resulting improvements in searcher\nexperience through offline and online experiments at Airbnb.\n","authors":["Malay Haldar","Daochen Zha","Huiji Gao","Liwei He","Sanjeev Katariya"],"pdf_url":"https://arxiv.org/pdf/2505.09795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09777v1","updated":"2025-05-14T20:15:52Z","published":"2025-05-14T20:15:52Z","title":"A Survey on Large Language Models in Multimodal Recommender Systems","summary":"  Multimodal recommender systems (MRS) integrate heterogeneous user and item\ndata, such as text, images, and structured information, to enhance\nrecommendation performance. The emergence of large language models (LLMs)\nintroduces new opportunities for MRS by enabling semantic reasoning, in-context\nlearning, and dynamic input handling. Compared to earlier pre-trained language\nmodels (PLMs), LLMs offer greater flexibility and generalisation capabilities\nbut also introduce challenges related to scalability and model accessibility.\nThis survey presents a comprehensive review of recent work at the intersection\nof LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and\ndata adaptation techniques. We propose a novel taxonomy to characterise\nintegration patterns, identify transferable techniques from related\nrecommendation domains, provide an overview of evaluation metrics and datasets,\nand point to possible future directions. We aim to clarify the emerging role of\nLLMs in multimodal recommendation and support future research in this rapidly\nevolving field.\n","authors":["Alejo Lopez-Avila","Jinhua Du"],"pdf_url":"https://arxiv.org/pdf/2505.09777v1.pdf","comment":"30 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.09776v1","updated":"2025-05-14T20:15:45Z","published":"2025-05-14T20:15:45Z","title":"The Impact of International Collaborations with Highly Publishing\n  Countries in Computer Science","summary":"  This paper analyzes international collaborations in Computer Science,\nfocusing on three major players: China, the European Union, and the United\nStates. Drawing from a comprehensive literature review, we examine\ncollaboration patterns, research impact, retraction rates, and the role of the\nDevelopment Index in shaping research outcomes. Our findings show that while\nChina, the EU, and the US lead global research efforts, other regions are\nnarrowing the gap in publication volume. Collaborations involving these key\nregions tend to have lower retraction rates, reflecting stronger adherence to\nscientific standards. We also find that countries with a Very High Development\nIndex contribute to research with higher citation rates and fewer retractions.\nOverall, this study highlights the value of international collaboration and the\nimportance of inclusive, ethical practices in advancing global research in\nComputer Science.\n","authors":["Alberto Gomez Espes","Michael Faerber","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2505.09776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09590v1","updated":"2025-05-14T17:39:34Z","published":"2025-05-14T17:39:34Z","title":"Distance-aware Self-adaptive Graph Convolution for Fine-grained\n  Hierarchical Recommendation","summary":"  Graph Convolutional Networks (GCNs) are widely used to improve recommendation\naccuracy and performance by effectively learning the representations of user\nand item nodes. However, two major challenges remain: (1) the lack of further\noptimization in the graph representation structure and (2) insufficient\nattention given to the varying contributions of different convolutional\nlayers.This paper proposes SAGCN, a distance-based adaptive hierarchical\naggregation method that refines the aggregation process through differentiated\nrepresentation metrics. SAGCN introduces a detailed approach to multilayer\ninformation aggregation and representation space optimization, enabling the\nmodel to learn hierarchical embedding weights based on the distance between\nhierarchical representations. This innovation allows for more precise\ncross-layer information aggregation, improves the model's ability to capture\nhierarchical embeddings, and optimizes the representation space structure.\nAdditionally, the objective loss function is refined to better align with\nrecommendation tasks.Extensive experiments conducted on four real-world\ndatasets demonstrate significant improvements, including over a 5% increase on\nYelp and a 5.58% increase in Recall@10 on the ML_1M dataset.\n","authors":["Tao Huang","Yihong Chen","Wei Fan","Wei Zhou","Junhao Wen"],"pdf_url":"https://arxiv.org/pdf/2505.09590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21813v3","updated":"2025-05-14T16:57:13Z","published":"2025-03-25T18:20:04Z","title":"OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language\n  Model Hallucinations in Ontology Matching","summary":"  Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the\nOntology Alignment Evaluation Initiative (OAEI), capturing hallucinations of\nten different LLMs performing OM tasks. These OM-specific hallucinations are\norganised into two primary categories and six sub-categories. We showcase the\nusefulness of the dataset in constructing an LLM leaderboard for OM tasks and\nfor fine-tuning LLMs used in OM tasks.\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang","Jing Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.21813v3.pdf","comment":"14 pages, 4 figures, 4 tables, 2 prompt templates"},{"id":"http://arxiv.org/abs/2505.09539v1","updated":"2025-05-14T16:32:45Z","published":"2025-05-14T16:32:45Z","title":"GlobalMood: A cross-cultural benchmark for music emotion recognition","summary":"  Human annotations of mood in music are essential for music generation and\nrecommender systems. However, existing datasets predominantly focus on Western\nsongs with mood terms derived from English, which may limit generalizability\nacross diverse linguistic and cultural backgrounds. To address this, we\nintroduce `GlobalMood', a novel cross-cultural benchmark dataset comprising\n1,180 songs sampled from 59 countries, with large-scale annotations collected\nfrom 2,519 individuals across five culturally and linguistically distinct\nlocations: U.S., France, Mexico, S. Korea, and Egypt. Rather than imposing\npredefined mood categories, we implement a bottom-up, participant-driven\napproach to organically elicit culturally specific music-related mood terms. We\nthen recruit another pool of human participants to collect 988,925 ratings for\nthese culture-specific descriptors. Our analysis confirms the presence of a\nvalence-arousal structure shared across cultures, yet also reveals significant\ndivergences in how certain mood terms, despite being dictionary equivalents,\nare perceived cross-culturally. State-of-the-art multimodal models benefit\nsubstantially from fine-tuning on our cross-culturally balanced dataset, as\nevidenced by improved alignment with human evaluations - particularly in\nnon-English contexts. More broadly, our findings inform the ongoing debate on\nthe universality versus cultural specificity of emotional descriptors, and our\nmethodology can contribute to other multimodal and cross-lingual research.\n","authors":["Harin Lee","Elif Çelen","Peter Harrison","Manuel Anglada-Tort","Pol van Rijn","Minsu Park","Marc Schönwiesner","Nori Jacoby"],"pdf_url":"https://arxiv.org/pdf/2505.09539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00762v2","updated":"2025-05-14T16:10:58Z","published":"2025-02-02T11:53:55Z","title":"On Overlap Ratio in Defocused Electron Ptychography","summary":"  Four-dimensional Scanning Transmission Electron Microscopy (4D STEM) with\ndata acquired using a defocused electron probe is a promising tool for\ncharacterising complex biological specimens and materials through a phase\nretrieval process known as Electron Ptychography (EP). The efficacy of 4D STEM\nacquisition and the resulting quality of EP reconstruction depends on the\noverlap ratio of adjacent illuminated areas. This paper demonstrates how the\noverlap ratio impacts the data redundancy and the quality of the EP\nreconstruction. We define two quantities as a function of the overlap ratio\nthat are independent of both the object and the EP algorithm. Subsequently, we\nevaluate an EP algorithm for varying overlap ratios using simulated 4D STEM\ndatasets. Notably, a 40% or greater overlap ratio yields stable, high-quality\nreconstructions.\n","authors":["Amirafshar Moshtaghpour","Angus I. Kirkland"],"pdf_url":"https://arxiv.org/pdf/2502.00762v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05009v2","updated":"2025-05-14T15:56:36Z","published":"2025-04-07T12:37:39Z","title":"Deconstructing Jazz Piano Style Using Machine Learning","summary":"  Artistic style has been studied for centuries, and recent advances in machine\nlearning create new possibilities for understanding it computationally.\nHowever, ensuring that machine-learning models produce insights aligned with\nthe interests of practitioners and critics remains a significant challenge.\nHere, we focus on musical style, which benefits from a rich theoretical and\nmathematical analysis tradition. We train a variety of supervised-learning\nmodels to identify 20 iconic jazz musicians across a carefully curated dataset\nof 84 hours of recordings, and interpret their decision-making processes. Our\nmodels include a novel multi-input architecture that enables four musical\ndomains (melody, harmony, rhythm, and dynamics) to be analysed separately.\nThese models enable us to address fundamental questions in music theory and\nalso advance the state-of-the-art in music performer identification (94%\naccuracy across 20 classes). We release open-source implementations of our\nmodels and an accompanying web application for exploring musical styles.\n","authors":["Huw Cheston","Reuben Bance","Peter M. C. Harrison"],"pdf_url":"https://arxiv.org/pdf/2504.05009v2.pdf","comment":"Paper: 40 pages, 11 figures, 1 table; added information on training\n  time + computation cost, corrections to Table 1. Supplementary material: 33\n  pages, 48 figures, 6 tables; corrections to Table S.5"},{"id":"http://arxiv.org/abs/2505.09436v1","updated":"2025-05-14T14:44:30Z","published":"2025-05-14T14:44:30Z","title":"CXMArena: Unified Dataset to benchmark performance in realistic CXM\n  Scenarios","summary":"  Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques.\n","authors":["Raghav Garg","Kapil Sharma","Karan Gupta"],"pdf_url":"https://arxiv.org/pdf/2505.09436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09414v1","updated":"2025-05-14T14:10:22Z","published":"2025-05-14T14:10:22Z","title":"FACTors: A New Dataset for Studying the Fact-checking Ecosystem","summary":"  Our fight against false information is spearheaded by fact-checkers. They\ninvestigate the veracity of claims and document their findings as fact-checking\nreports. With the rapid increase in the amount of false information circulating\nonline, the use of automation in fact-checking processes aims to strengthen\nthis ecosystem by enhancing scalability. Datasets containing fact-checked\nclaims play a key role in developing such automated solutions. However, to the\nbest of our knowledge, there is no fact-checking dataset at the ecosystem\nlevel, covering claims from a sufficiently long period of time and sourced from\na wide range of actors reflecting the entire ecosystem that admittedly follows\nwidely-accepted codes and principles of fact-checking. We present a new dataset\nFACTors, the first to fill this gap by presenting ecosystem-level data on\nfact-checking. It contains 118,112 claims from 117,993 fact-checking reports in\nEnglish (co-)authored by 1,953 individuals and published during the period of\n1995-2025 by 39 fact-checking organisations that are active signatories of the\nIFCN (International Fact-Checking Network) and/or EFCSN (European Fact-Checking\nStandards Network). It contains 7,327 overlapping claims investigated by\nmultiple fact-checking organisations, corresponding to 2,977 unique claims. It\nallows to conduct new ecosystem-level studies of the fact-checkers\n(organisations and individuals). To demonstrate the usefulness of FACTors, we\npresent three example applications, including a first-of-its-kind statistical\nanalysis of the fact-checking ecosystem, examining the political inclinations\nof the fact-checking organisations, and attempting to assign a credibility\nscore to each organisation based on the findings of the statistical analysis\nand political leanings. Our methods for constructing FACTors are generic and\ncan be used to maintain a live dataset that can be updated dynamically.\n","authors":["Enes Altuncu","Can Başkent","Sanjay Bhattacherjee","Shujun Li","Dwaipayan Roy"],"pdf_url":"https://arxiv.org/pdf/2505.09414v1.pdf","comment":"Accepted for the 48th International ACM SIGIR Conference on Research\n  and Development in Information Retrieval (SIGIR '25)"},{"id":"http://arxiv.org/abs/2505.09364v1","updated":"2025-05-14T13:13:53Z","published":"2025-05-14T13:13:53Z","title":"Diffusion Recommender Models and the Illusion of Progress: A Concerning\n  Study of Reproducibility and a Conceptual Mismatch","summary":"  Countless new machine learning models are published every year and are\nreported to significantly advance the state-of-the-art in \\emph{top-n}\nrecommendation. However, earlier reproducibility studies indicate that progress\nin this area may be quite limited. Specifically, various widespread\nmethodological issues, e.g., comparisons with untuned baseline models, have led\nto an \\emph{illusion of progress}. In this work, our goal is to examine whether\nthese problems persist in today's research. To this end, we aim to reproduce\nthe latest advancements reported from applying modern Denoising Diffusion\nProbabilistic Models to recommender systems, focusing on four models published\nat the top-ranked SIGIR conference in 2023 and 2024. Our findings are\nconcerning, revealing persistent methodological problems. Alarmingly, through\nexperiments, we find that the latest recommendation techniques based on\ndiffusion models, despite their computational complexity and substantial carbon\nfootprint, are consistently outperformed by simpler existing models.\nFurthermore, we identify key mismatches between the characteristics of\ndiffusion models and those of the traditional \\emph{top-n} recommendation task,\nraising doubts about their suitability for recommendation. We also note that,\nin the papers we analyze, the generative capabilities of these models are\nconstrained to a minimum. Overall, our results and continued methodological\nissues call for greater scientific rigor and a disruptive change in the\nresearch and publication culture in this area.\n","authors":["Michael Benigni","Maurizio Ferrari Dacrema","Dietmar Jannach"],"pdf_url":"https://arxiv.org/pdf/2505.09364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09316v1","updated":"2025-05-14T12:13:38Z","published":"2025-05-14T12:13:38Z","title":"Scent of Knowledge: Optimizing Search-Enhanced Reasoning with\n  Information Foraging","summary":"  Augmenting large language models (LLMs) with external retrieval has become a\nstandard method to address their inherent knowledge cutoff limitations.\nHowever, traditional retrieval-augmented generation methods employ static,\npre-inference retrieval strategies, making them inadequate for complex tasks\ninvolving ambiguous, multi-step, or evolving information needs. Recent advances\nin test-time scaling techniques have demonstrated significant potential in\nenabling LLMs to dynamically interact with external tools, motivating the shift\ntoward adaptive inference-time retrieval. Inspired by Information Foraging\nTheory (IFT), we propose InForage, a reinforcement learning framework that\nformalizes retrieval-augmented reasoning as a dynamic information-seeking\nprocess. Unlike existing approaches, InForage explicitly rewards intermediate\nretrieval quality, encouraging LLMs to iteratively gather and integrate\ninformation through adaptive search behaviors. To facilitate training, we\nconstruct a human-guided dataset capturing iterative search and reasoning\ntrajectories for complex, real-world web tasks. Extensive evaluations across\ngeneral question answering, multi-hop reasoning tasks, and a newly developed\nreal-time web QA dataset demonstrate InForage's superior performance over\nbaseline methods. These results highlight InForage's effectiveness in building\nrobust, adaptive, and efficient reasoning agents.\n","authors":["Hongjin Qian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2505.09316v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2505.09246v1","updated":"2025-05-14T09:35:56Z","published":"2025-05-14T09:35:56Z","title":"Focus, Merge, Rank: Improved Question Answering Based on Semi-structured\n  Knowledge Bases","summary":"  In many real-world settings, machine learning models and interactive systems\nhave access to both structured knowledge, e.g., knowledge graphs or tables, and\nunstructured content, e.g., natural language documents. However, most rely on\neither. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking\nunstructured content to nodes within structured data, thereby enabling new\nstrategies for knowledge access and use. In this work, we present\nFocusedRetriever, a modular SKB-based framework for multi-hop question\nanswering. It integrates components (VSS-based entity search, LLM-based\ngeneration of Cypher queries and pairwise re-ranking) in a way that enables it\nto outperform state-of-the-art methods across all three STaRK benchmark test\nsets, covering diverse domains and multiple performance metrics. The average\nfirst-hit rate exceeds that of the second-best method by 25.7%.\nFocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to\nextract relational facts and entity attributes from unstructured text, (2) node\nset joins to filter answer candidates based on these extracted triplets and\nconstraints, (3) vector similarity search to retrieve and rank relevant\nunstructured content, and (4) the contextual capabilities of LLMs to finally\nrank the top-k answers. For generality, we only incorporate base LLMs in\nFocusedRetriever in our evaluation. However, our analysis of intermediate\nresults highlights several opportunities for further upgrades including\nfinetuning. The source code is publicly available at\nhttps://github.com/kramerlab/FocusedRetriever .\n","authors":["Derian Boer","Stephen Roth","Stefan Kramer"],"pdf_url":"https://arxiv.org/pdf/2505.09246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09205v1","updated":"2025-05-14T07:34:36Z","published":"2025-05-14T07:34:36Z","title":"HMamba: Hyperbolic Mamba for Sequential Recommendation","summary":"  Sequential recommendation systems have become a cornerstone of personalized\nservices, adept at modeling the temporal evolution of user preferences by\ncapturing dynamic interaction sequences. Existing approaches predominantly rely\non traditional models, including RNNs and Transformers. Despite their success\nin local pattern recognition, Transformer-based methods suffer from quadratic\ncomputational complexity and a tendency toward superficial attention patterns,\nlimiting their ability to infer enduring preference hierarchies in sequential\nrecommendation data. Recent advances in Mamba-based sequential models introduce\nlinear-time efficiency but remain constrained by Euclidean geometry, failing to\nleverage the intrinsic hyperbolic structure of recommendation data. To bridge\nthis gap, we propose Hyperbolic Mamba, a novel architecture that unifies the\nefficiency of Mamba's selective state space mechanism with hyperbolic\ngeometry's hierarchical representational power. Our framework introduces (1) a\nhyperbolic selective state space that maintains curvature-aware sequence\nmodeling and (2) stabilized Riemannian operations to enable scalable training.\nExperiments across four benchmarks demonstrate that Hyperbolic Mamba achieves\n3-11% improvement while retaining Mamba's linear-time efficiency, enabling\nreal-world deployment. This work establishes a new paradigm for efficient,\nhierarchy-aware sequential modeling.\n","authors":["Qianru Zhang","Honggang Wen","Wei Yuan","Crystal Chen","Menglin Yang","Siu-Ming Yiu","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2505.09205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09065v1","updated":"2025-05-14T01:48:59Z","published":"2025-05-14T01:48:59Z","title":"Display Content, Display Methods and Evaluation Methods of the HCI in\n  Explainable Recommender Systems: A Survey","summary":"  Explainable Recommender Systems (XRS) aim to provide users with\nunderstandable reasons for the recommendations generated by these systems,\nrepresenting a crucial research direction in artificial intelligence (AI).\nRecent research has increasingly focused on the algorithms, display, and\nevaluation methodologies of XRS. While current research and reviews primarily\nemphasize the algorithmic aspects, with fewer studies addressing the\nHuman-Computer Interaction (HCI) layer of XRS. Additionally, existing reviews\nlack a unified taxonomy for XRS and there is insufficient attention given to\nthe emerging area of short video recommendations. In this study, we synthesize\nexisting literature and surveys on XRS, presenting a unified framework for its\nresearch and development. The main contributions are as follows: 1) We adopt a\nlifecycle perspective to systematically summarize the technologies and methods\nused in XRS, addressing challenges posed by the diversity and complexity of\nalgorithmic models and explanation techniques. 2) For the first time, we\nhighlight the application of multimedia, particularly video-based explanations,\nalong with its potential, technical pathways, and challenges in XRS. 3) We\nprovide a structured overview of evaluation methods from both qualitative and\nquantitative dimensions. These findings provide valuable insights for the\nsystematic design, progress, and testing of XRS.\n","authors":["Weiqing Li","Yue Xu","Yuefeng Li","Yinghui Huang"],"pdf_url":"https://arxiv.org/pdf/2505.09065v1.pdf","comment":"2 Tables, 29 figures"},{"id":"http://arxiv.org/abs/2209.13814v2","updated":"2025-05-14T01:21:55Z","published":"2022-09-28T03:39:34Z","title":"Signed Latent Factors for Spamming Activity Detection","summary":"  Due to the increasing trend of performing spamming activities (e.g., Web\nspam, deceptive reviews, fake followers, etc.) on various online platforms to\ngain undeserved benefits, spam detection has emerged as a hot research issue.\nPrevious attempts to combat spam mainly employ features related to metadata,\nuser behaviors, or relational ties. These studies have made considerable\nprogress in understanding and filtering spamming campaigns. However, this\nproblem remains far from fully solved. Almost all the proposed features focus\non a limited number of observed attributes or explainable phenomena, making it\ndifficult for existing methods to achieve further improvement. To broaden the\nvision about solving the spam problem and address long-standing challenges\n(class imbalance and graph incompleteness) in the spam detection area, we\npropose a new attempt of utilizing signed latent factors to filter fraudulent\nactivities. The spam-contaminated relational datasets of multiple online\napplications in this scenario are interpreted by the unified signed network.\nTwo competitive and highly dissimilar algorithms of latent factors mining (LFM)\nmodels are designed based on multi-relational likelihoods estimation (LFM-MRLE)\nand signed pairwise ranking (LFM-SPR), respectively. We then explore how to\napply the mined latent factors to spam detection tasks. Experiments on\nreal-world datasets of different kinds of Web applications (social media and\nWeb forum) indicate that LFM models outperform state-of-the-art baselines in\ndetecting spamming activities. By specifically manipulating experimental data,\nthe effectiveness of our methods in dealing with incomplete and imbalanced\nchallenges is validated.\n","authors":["Yuli Liu"],"pdf_url":"https://arxiv.org/pdf/2209.13814v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09033v1","updated":"2025-05-14T00:05:04Z","published":"2025-05-14T00:05:04Z","title":"Item Level Exploration Traffic Allocation in Large-scale Recommendation\n  Systems","summary":"  This paper contributes to addressing the item cold start problem in\nlarge-scale recommender systems, focusing on how to efficiently gain initial\nvisibility for newly ingested content. We propose an exploration system\ndesigned to efficiently allocate impressions to these fresh items. Our approach\nleverages a learned probabilistic model to predict an item's discoverability,\nwhich then informs a scalable and adaptive traffic allocation strategy. This\nsystem intelligently distributes exploration budgets, optimizing for the\nlong-term benefit of the recommendation platform. The impact is a demonstrably\nmore efficient cold-start process, leading to a significant increase in the\ndiscoverability of new content and ultimately enriching the item corpus\navailable for exploitation, as evidenced by its successful deployment in a\nlarge-scale production environment.\n","authors":["Dong Wang","Junyi Jiao","Arnab Bhadury","Yaping Zhang","Mingyan Gao"],"pdf_url":"https://arxiv.org/pdf/2505.09033v1.pdf","comment":"accepted by the 18th ACM Recsys Large Recsys Workshop"},{"id":"http://arxiv.org/abs/2505.11545v1","updated":"2025-05-14T19:39:46Z","published":"2025-05-14T19:39:46Z","title":"TARGET: Benchmarking Table Retrieval for Generative Tasks","summary":"  The data landscape is rich with structured data, often of high value to\norganizations, driving important applications in data analysis and machine\nlearning. Recent progress in representation learning and generative models for\nsuch data has led to the development of natural language interfaces to\nstructured data, including those leveraging text-to-SQL. Contextualizing\ninteractions, either through conversational interfaces or agentic components,\nin structured data through retrieval-augmented generation can provide\nsubstantial benefits in the form of freshness, accuracy, and comprehensiveness\nof answers. The key question is: how do we retrieve the right table(s) for the\nanalytical query or task at hand? To this end, we introduce TARGET: a benchmark\nfor evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the\nretrieval performance of different retrievers in isolation, as well as their\nimpact on downstream tasks. We find that dense embedding-based retrievers far\noutperform a BM25 baseline which is less effective than it is for retrieval\nover unstructured text. We also surface the sensitivity of retrievers across\nvarious metadata (e.g., missing table titles), and demonstrate a stark\nvariation of retrieval performance across datasets and tasks. TARGET is\navailable at https://target-benchmark.github.io.\n","authors":["Xingyu Ji","Parker Glenn","Aditya G. Parameswaran","Madelon Hulsebos"],"pdf_url":"https://arxiv.org/pdf/2505.11545v1.pdf","comment":null}]},"2025-05-13T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2504.13807v3","updated":"2025-05-13T23:33:09Z","published":"2025-04-18T17:20:27Z","title":"DiffOG: Differentiable Policy Trajectory Optimization with\n  Generalizability","summary":"  Imitation learning-based visuomotor policies excel at manipulation tasks but\noften produce suboptimal action trajectories compared to model-based methods.\nDirectly mapping camera data to actions via neural networks can result in jerky\nmotions and difficulties in meeting critical constraints, compromising safety\nand robustness in real-world deployment. For tasks that require high robustness\nor strict adherence to constraints, ensuring trajectory quality is crucial.\nHowever, the lack of interpretability in neural networks makes it challenging\nto generate constraint-compliant actions in a controlled manner. This paper\nintroduces differentiable policy trajectory optimization with generalizability\n(DiffOG), a learning-based trajectory optimization framework designed to\nenhance visuomotor policies. By leveraging the proposed differentiable\nformulation of trajectory optimization with transformer, DiffOG seamlessly\nintegrates policies with a generalizable optimization layer. DiffOG refines\naction trajectories to be smoother and more constraint-compliant while\nmaintaining alignment with the original demonstration distribution, thus\navoiding degradation in policy performance. We evaluated DiffOG across 11\nsimulated tasks and 2 real-world tasks. The results demonstrate that DiffOG\nsignificantly enhances the trajectory quality of visuomotor policies while\nhaving minimal impact on policy performance, outperforming trajectory\nprocessing baselines such as greedy constraint clipping and penalty-based\ntrajectory optimization. Furthermore, DiffOG achieves superior performance\ncompared to existing constrained visuomotor policy. Please visit the project\nwebsite for more details: https://zhengtongxu.github.io/diffog-website/.\n","authors":["Zhengtong Xu","Zichen Miao","Qiang Qiu","Zhe Zhang","Yu She"],"pdf_url":"https://arxiv.org/pdf/2504.13807v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09020v1","updated":"2025-05-13T23:26:49Z","published":"2025-05-13T23:26:49Z","title":"JcvPCA and JsvCRP : a set of metrics to evaluate changes in joint\n  coordination strategies","summary":"  Characterizing changes in inter-joint coordination presents significant\nchallenges, as it necessitates the examination of relationships between\nmultiple degrees of freedom during movements and their temporal evolution.\nExisting metrics are inadequate in providing physiologically coherent results\nthat document both the temporal and spatial aspects of inter-joint\ncoordination. In this article, we introduce two novel metrics to enhance the\nanalysis of inter-joint coordination. The first metric, Joint Contribution\nVariation based on Principal Component Analysis (JcvPCA), evaluates the\nvariation in each joint's contribution during series of movements. The second\nmetric, Joint Synchronization Variation based on Continuous Relative Phase\n(JsvCRP), measures the variation in temporal synchronization among joints\nbetween two movement datasets. We begin by presenting each metric and\nexplaining their derivation. We then demonstrate the application of these\nmetrics using simulated and experimental datasets involving identical movement\ntasks performed with distinct coordination strategies. The results show that\nthese metrics can successfully differentiate between unique coordination\nstrategies, providing meaningful insights into joint collaboration during\nmovement. These metrics hold significant potential for fields such as\nergonomics and clinical rehabilitation, where a precise understanding of the\nevolution of inter-joint coordination strategies is crucial. Potential\napplications include evaluating the effects of upper limb exoskeletons in\nindustrial settings or monitoring the progress of patients undergoing\nneurological rehabilitation.\n","authors":["Océane Dubois","Agnès Roby-Brami","Ross Parry","Nathanaël Jarrassé"],"pdf_url":"https://arxiv.org/pdf/2505.09020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08995v1","updated":"2025-05-13T22:13:48Z","published":"2025-05-13T22:13:48Z","title":"Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent\n  Reinforcement Learning","summary":"  This work presents a Hierarchical Multi-Agent Reinforcement Learning\nframework for analyzing simulated air combat scenarios involving heterogeneous\nagents. The objective is to identify effective Courses of Action that lead to\nmission success within preset simulations, thereby enabling the exploration of\nreal-world defense scenarios at low cost and in a safe-to-fail setting.\nApplying deep Reinforcement Learning in this context poses specific challenges,\nsuch as complex flight dynamics, the exponential size of the state and action\nspaces in multi-agent systems, and the capability to integrate real-time\ncontrol of individual units with look-ahead planning. To address these\nchallenges, the decision-making process is split into two levels of\nabstraction: low-level policies control individual units, while a high-level\ncommander policy issues macro commands aligned with the overall mission\ntargets. This hierarchical structure facilitates the training process by\nexploiting policy symmetries of individual agents and by separating control\nfrom command tasks. The low-level policies are trained for individual combat\ncontrol in a curriculum of increasing complexity. The high-level commander is\nthen trained on mission targets given pre-trained control policies. The\nempirical validation confirms the advantages of the proposed framework.\n","authors":["Ardian Selmonaj","Oleg Szehr","Giacomo Del Rio","Alessandro Antonucci","Adrian Schneider","Michael Rüegsegger"],"pdf_url":"https://arxiv.org/pdf/2505.08995v1.pdf","comment":"Published as journal chapter in Deep Learning Applications, Vol. 1,\n  by Taylor & Francis"},{"id":"http://arxiv.org/abs/2505.01956v2","updated":"2025-05-13T21:56:50Z","published":"2025-05-04T01:40:31Z","title":"SafeNav: Safe Path Navigation using Landmark Based Localization in a\n  GPS-denied Environment","summary":"  In battlefield environments, adversaries frequently disrupt GPS signals,\nrequiring alternative localization and navigation methods. Traditional\nvision-based approaches like Simultaneous Localization and Mapping (SLAM) and\nVisual Odometry (VO) involve complex sensor fusion and high computational\ndemand, whereas range-free methods like DV-HOP face accuracy and stability\nchallenges in sparse, dynamic networks. This paper proposes LanBLoc-BMM, a\nnavigation approach using landmark-based localization (LanBLoc) combined with a\nbattlefield-specific motion model (BMM) and Extended Kalman Filter (EKF). Its\nperformance is benchmarked against three state-of-the-art visual localization\nalgorithms integrated with BMM and Bayesian filters, evaluated on synthetic and\nreal-imitated trajectory datasets using metrics including Average Displacement\nError (ADE), Final Displacement Error (FDE), and a newly introduced Average\nWeighted Risk Score (AWRS). LanBLoc-BMM (with EKF) demonstrates superior\nperformance in ADE, FDE, and AWRS on real-imitated datasets. Additionally, two\nsafe navigation methods, SafeNav-CHull and SafeNav-Centroid, are introduced by\nintegrating LanBLoc-BMM(EKF) with a novel Risk-Aware RRT* (RAw-RRT*) algorithm\nfor obstacle avoidance and risk exposure minimization. Simulation results in\nbattlefield scenarios indicate SafeNav-Centroid excels in accuracy, risk\nexposure, and trajectory efficiency, while SafeNav-CHull provides superior\ncomputational speed.\n","authors":["Ganesh Sapkota","Sanjay Madria"],"pdf_url":"https://arxiv.org/pdf/2505.01956v2.pdf","comment":"10 pages, conference paper. arXiv admin note: text overlap with\n  arXiv:2402.14280"},{"id":"http://arxiv.org/abs/2505.08986v1","updated":"2025-05-13T21:56:44Z","published":"2025-05-13T21:56:44Z","title":"ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control\n  for Delicate, Irregular Bio-products Manipulation","summary":"  Automated poultry processing lines still rely on humans to lift slippery,\neasily bruised carcasses onto a shackle conveyor. Deformability, anatomical\nvariance, and strict hygiene rules make conventional suction and scripted\nmotions unreliable. We present ChicGrasp, an end--to--end hardware--software\nco-design for this task. An independently actuated dual-jaw pneumatic gripper\nclamps both chicken legs, while a conditional diffusion-policy controller,\ntrained from only 50 multi--view teleoperation demonstrations (RGB +\nproprioception), plans 5 DoF end--effector motion, which includes jaw commands\nin one shot. On individually presented raw broiler carcasses, our system\nachieves a 40.6\\% grasp--and--lift success rate and completes the pick to\nshackle cycle in 38 s, whereas state--of--the--art implicit behaviour cloning\n(IBC) and LSTM-GMM baselines fail entirely. All CAD, code, and datasets will be\nopen-source. ChicGrasp shows that imitation learning can bridge the gap between\nrigid hardware and variable bio--products, offering a reproducible benchmark\nand a public dataset for researchers in agricultural engineering and robot\nlearning.\n","authors":["Amirreza Davar","Zhengtong Xu","Siavash Mahmoudi","Pouya Sohrabipour","Chaitanya Pallerla","Yu She","Wan Shou","Philip Crandall","Dongyi Wang"],"pdf_url":"https://arxiv.org/pdf/2505.08986v1.pdf","comment":"Submitted for journal review"},{"id":"http://arxiv.org/abs/2204.03139v2","updated":"2025-05-13T20:31:59Z","published":"2022-04-07T00:45:26Z","title":"DiffCloud: Real-to-Sim from Point Clouds with Differentiable Simulation\n  and Rendering of Deformable Objects","summary":"  Research in manipulation of deformable objects is typically conducted on a\nlimited range of scenarios, because handling each scenario on hardware takes\nsignificant effort. Realistic simulators with support for various types of\ndeformations and interactions have the potential to speed up experimentation\nwith novel tasks and algorithms. However, for highly deformable objects it is\nchallenging to align the output of a simulator with the behavior of real\nobjects. Manual tuning is not intuitive, hence automated methods are needed. We\nview this alignment problem as a joint perception-inference challenge and\ndemonstrate how to use recent neural network architectures to successfully\nperform simulation parameter inference from real point clouds. We analyze the\nperformance of various architectures, comparing their data and training\nrequirements. Furthermore, we propose to leverage differentiable point cloud\nsampling and differentiable simulation to significantly reduce the time to\nachieve the alignment. We employ an efficient way to propagate gradients from\npoint clouds to simulated meshes and further through to the physical simulation\nparameters, such as mass and stiffness. Experiments with highly deformable\nobjects show that our method can achieve comparable or better alignment with\nreal object behavior, while reducing the time needed to achieve this by more\nthan an order of magnitude. Videos and supplementary material are available at\nhttps://diffcloud.github.io.\n","authors":["Priya Sundaresan","Rika Antonova","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2204.03139v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08949v1","updated":"2025-05-13T20:27:16Z","published":"2025-05-13T20:27:16Z","title":"Multi-step manipulation task and motion planning guided by video\n  demonstration","summary":"  This work aims to leverage instructional video to solve complex multi-step\ntask-and-motion planning tasks in robotics. Towards this goal, we propose an\nextension of the well-established Rapidly-Exploring Random Tree (RRT) planner,\nwhich simultaneously grows multiple trees around grasp and release states\nextracted from the guiding video. Our key novelty lies in combining contact\nstates and 3D object poses extracted from the guiding video with a traditional\nplanning algorithm that allows us to solve tasks with sequential dependencies,\nfor example, if an object needs to be placed at a specific location to be\ngrasped later. We also investigate the generalization capabilities of our\napproach to go beyond the scene depicted in the instructional video. To\ndemonstrate the benefits of the proposed video-guided planning approach, we\ndesign a new benchmark with three challenging tasks: (I) 3D re-arrangement of\nmultiple objects between a table and a shelf, (ii) multi-step transfer of an\nobject through a tunnel, and (iii) transferring objects using a tray similar to\na waiter transfers dishes. We demonstrate the effectiveness of our planning\nalgorithm on several robots, including the Franka Emika Panda and the KUKA KMR\niiwa. For a seamless transfer of the obtained plans to the real robot, we\ndevelop a trajectory refinement approach formulated as an optimal control\nproblem (OCP).\n","authors":["Kateryna Zorina","David Kovar","Mederic Fourmy","Florent Lamiraux","Nicolas Mansard","Justin Carpentier","Josef Sivic","Vladimir Petrik"],"pdf_url":"https://arxiv.org/pdf/2505.08949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03192v2","updated":"2025-05-13T20:06:12Z","published":"2025-03-05T05:17:15Z","title":"Distributed Certifiably Correct Range-Aided SLAM","summary":"  Reliable simultaneous localization and mapping (SLAM) algorithms are\nnecessary for safety-critical autonomous navigation. In the\ncommunication-constrained multi-agent setting, navigation systems increasingly\nuse point-to-point range sensors as they afford measurements with low bandwidth\nrequirements and known data association. The state estimation problem for these\nsystems takes the form of range-aided (RA) SLAM. However, distributed\nalgorithms for solving the RA-SLAM problem lack formal guarantees on the\nquality of the returned estimate. To this end, we present the first distributed\nalgorithm for RA-SLAM that can efficiently recover certifiably globally optimal\nsolutions. Our algorithm, distributed certifiably correct RA-SLAM (DCORA),\nachieves this via the Riemannian Staircase method, where computational\nprocedures developed for distributed certifiably correct pose graph\noptimization are generalized to the RA-SLAM problem. We demonstrate DCORA's\nefficacy on real-world multi-agent datasets by achieving absolute trajectory\nerrors comparable to those of a state-of-the-art centralized certifiably\ncorrect RA-SLAM algorithm. Additionally, we perform a parametric study on the\nstructure of the RA-SLAM problem using synthetic data, revealing how common\nparameters affect DCORA's performance.\n","authors":["Alexander Thoms","Alan Papalia","Jared Velasquez","David M. Rosen","Sriram Narasimhan"],"pdf_url":"https://arxiv.org/pdf/2503.03192v2.pdf","comment":"8 pages, 3 figures, accepted to the 2025 International Conference on\n  Robotics and Automation (ICRA). This version includes minor clerical edits to\n  the published version in the conference proceedings"},{"id":"http://arxiv.org/abs/2505.08932v1","updated":"2025-05-13T19:59:29Z","published":"2025-05-13T19:59:29Z","title":"Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest\n  Floor Segmentation from UAV Imagery","summary":"  Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation and\nforest monitoring, including seed dispersal in hard-to-reach terrains. However,\na detailed understanding of the forest floor remains a challenge due to high\nnatural variability, quickly changing environmental parameters, and ambiguous\nannotations due to unclear definitions. To address this issue, we adapt the\nSegment Anything Model (SAM), a vision foundation model with strong\ngeneralization capabilities, to segment forest floor objects such as tree\nstumps, vegetation, and woody debris. To this end, we employ\nparameter-efficient fine-tuning (PEFT) to fine-tune a small subset of\nadditional model parameters while keeping the original weights fixed. We adjust\nSAM's mask decoder to generate masks corresponding to our dataset categories,\nallowing for automatic segmentation without manual prompting. Our results show\nthat the adapter-based PEFT method achieves the highest mean intersection over\nunion (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers a\nlightweight alternative for resource-constrained UAV platforms.\n","authors":["Mohammad Wasil","Ahmad Drak","Brennan Penfold","Ludovico Scarton","Maximilian Johenneken","Alexander Asteroth","Sebastian Houben"],"pdf_url":"https://arxiv.org/pdf/2505.08932v1.pdf","comment":"Accepted to the Novel Approaches for Precision Agriculture and\n  Forestry with Autonomous Robots IEEE ICRA Workshop - 2025"},{"id":"http://arxiv.org/abs/2404.02171v2","updated":"2025-05-13T19:26:11Z","published":"2024-03-29T17:01:09Z","title":"Optimal navigation of magnetic artificial microswimmers in blood\n  capillaries with deep reinforcement learning","summary":"  Biomedical applications such as targeted drug delivery, microsurgery, and\nsensing rely on reaching precise areas within the body in a minimally invasive\nway. Artificial bacterial flagella (ABFs) have emerged as potential tools for\nthis task by navigating through the circulatory system with the help of\nexternal magnetic fields. While their swimming characteristics are well\nunderstood in simple settings, their controlled navigation through realistic\ncapillary networks remains a significant challenge due to the complexity of\nblood flow and the high computational cost of detailed simulations. We address\nthis challenge by conducting numerical simulations of ABFs in retinal\ncapillaries, propelled by an external magnetic field. The simulations are based\non a validated blood model that predicts the dynamics of individual red blood\ncells and their hydrodynamic interactions with ABFs. The magnetic field follows\na control policy that brings the ABF to a prescribed target. The control policy\nis learned with an actor-critic, off-policy reinforcement learning algorithm\ncoupled with a reduced-order model of the system. We show that the same policy\nrobustly guides the ABF to a prescribed target in both the reduced-order model\nand the fine-grained blood simulations. This approach is suitable for designing\nrobust control policies for personalized medicine at moderate computational\ncost.\n","authors":["Lucas Amoudruz","Sergey Litvinov","Petros Koumoutsakos"],"pdf_url":"https://arxiv.org/pdf/2404.02171v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00965v2","updated":"2025-05-13T19:03:52Z","published":"2024-11-01T18:32:23Z","title":"SPOT: SE(3) Pose Trajectory Diffusion for Object-Centric Manipulation","summary":"  We introduce SPOT, an object-centric imitation learning framework. The key\nidea is to capture each task by an object-centric representation, specifically\nthe SE(3) object pose trajectory relative to the target. This approach\ndecouples embodiment actions from sensory inputs, facilitating learning from\nvarious demonstration types, including both action-based and action-less human\nhand demonstrations, as well as cross-embodiment generalization. Additionally,\nobject pose trajectories inherently capture planning constraints from\ndemonstrations without the need for manually-crafted rules. To guide the robot\nin executing the task, the object trajectory is used to condition a diffusion\npolicy. We systematically evaluate our method on simulation and real-world\ntasks. In real-world evaluation, using only eight demonstrations shot on an\niPhone, our approach completed all tasks while fully complying with task\nconstraints. Project page: https://nvlabs.github.io/object_centric_diffusion\n","authors":["Cheng-Chun Hsu","Bowen Wen","Jie Xu","Yashraj Narang","Xiaolong Wang","Yuke Zhu","Joydeep Biswas","Stan Birchfield"],"pdf_url":"https://arxiv.org/pdf/2411.00965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08896v1","updated":"2025-05-13T18:38:42Z","published":"2025-05-13T18:38:42Z","title":"Deep reinforcement learning-based longitudinal control strategy for\n  automated vehicles at signalised intersections","summary":"  Developing an autonomous vehicle control strategy for signalised\nintersections (SI) is one of the challenging tasks due to its inherently\ncomplex decision-making process. This study proposes a Deep Reinforcement\nLearning (DRL) based longitudinal vehicle control strategy at SI. A\ncomprehensive reward function has been formulated with a particular focus on\n(i) distance headway-based efficiency reward, (ii) decision-making criteria\nduring amber light, and (iii) asymmetric acceleration/ deceleration response,\nalong with the traditional safety and comfort criteria. This reward function\nhas been incorporated with two popular DRL algorithms, Deep Deterministic\nPolicy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the\ncontinuous action space of acceleration/deceleration. The proposed models have\nbeen trained on the combination of real-world leader vehicle (LV) trajectories\nand simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process.\nThe overall performance of the proposed models has been tested using Cumulative\nDistribution Function (CDF) plots and compared with the real-world trajectory\ndata. The results show that the RL models successfully maintain lower distance\nheadway (i.e., higher efficiency) and jerk compared to human-driven vehicles\nwithout compromising safety. Further, to assess the robustness of the proposed\nmodels, we evaluated the model performance on diverse safety-critical\nscenarios, in terms of car-following and traffic signal compliance. Both DDPG\nand SAC models successfully handled the critical scenarios, while the DDPG\nmodel showed smoother action profiles compared to the SAC model. Overall, the\nresults confirm that DRL-based longitudinal vehicle control strategy at SI can\nhelp to improve traffic safety, efficiency, and comfort.\n","authors":["Pankaj Kumar","Aditya Mishra","Pranamesh Chakraborty","Subrahmanya Swamy Peruru"],"pdf_url":"https://arxiv.org/pdf/2505.08896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08875v1","updated":"2025-05-13T18:04:52Z","published":"2025-05-13T18:04:52Z","title":"Real-time Capable Learning-based Visual Tool Pose Correction via\n  Differentiable Simulation","summary":"  Autonomy in Minimally Invasive Robotic Surgery (MIRS) has the potential to\nreduce surgeon cognitive and task load, thereby increasing procedural\nefficiency. However, implementing accurate autonomous control can be difficult\ndue to poor end-effector proprioception, a limitation of their cable-driven\nmechanisms. Although the robot may have joint encoders for the end-effector\npose calculation, various non-idealities make the entire kinematics chain\ninaccurate. Modern vision-based pose estimation methods lack real-time\ncapability or can be hard to train and generalize. In this work, we demonstrate\na real-time capable, vision transformer-based pose estimation approach that is\ntrained using end-to-end differentiable kinematics and rendering in simulation.\nWe demonstrate the potential of this method to correct for noisy pose estimates\nin simulation, with the longer term goal of verifying the sim-to-real\ntransferability of our approach.\n","authors":["Shuyuan Yang","Zonghe Chua"],"pdf_url":"https://arxiv.org/pdf/2505.08875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08854v1","updated":"2025-05-13T17:59:20Z","published":"2025-05-13T17:59:20Z","title":"Generative AI for Autonomous Driving: Frontiers and Opportunities","summary":"  Generative Artificial Intelligence (GenAI) constitutes a transformative\ntechnological wave that reconfigures industries through its unparalleled\ncapabilities for content creation, reasoning, planning, and multimodal\nunderstanding. This revolutionary force offers the most promising path yet\ntoward solving one of engineering's grandest challenges: achieving reliable,\nfully autonomous driving, particularly the pursuit of Level 5 autonomy. This\nsurvey delivers a comprehensive and critical synthesis of the emerging role of\nGenAI across the autonomous driving stack. We begin by distilling the\nprinciples and trade-offs of modern generative modeling, encompassing VAEs,\nGANs, Diffusion Models, and Large Language Models (LLMs). We then map their\nfrontier applications in image, LiDAR, trajectory, occupancy, video generation\nas well as LLM-guided reasoning and decision making. We categorize practical\napplications, such as synthetic data workflows, end-to-end driving strategies,\nhigh-fidelity digital twin systems, smart transportation networks, and\ncross-domain transfer to embodied AI. We identify key obstacles and\npossibilities such as comprehensive generalization across rare cases,\nevaluation and safety checks, budget-limited implementation, regulatory\ncompliance, ethical concerns, and environmental effects, while proposing\nresearch plans across theoretical assurances, trust metrics, transport\nintegration, and socio-technical influence. By unifying these threads, the\nsurvey provides a forward-looking reference for researchers, engineers, and\npolicymakers navigating the convergence of generative AI and advanced\nautonomous mobility. An actively maintained repository of cited works is\navailable at https://github.com/taco-group/GenAI4AD.\n","authors":["Yuping Wang","Shuo Xing","Cui Can","Renjie Li","Hongyuan Hua","Kexin Tian","Zhaobin Mo","Xiangbo Gao","Keshu Wu","Sulong Zhou","Hengxu You","Juntong Peng","Junge Zhang","Zehao Wang","Rui Song","Mingxuan Yan","Walter Zimmer","Xingcheng Zhou","Peiran Li","Zhaohan Lu","Chia-Ju Chen","Yue Huang","Ryan A. Rossi","Lichao Sun","Hongkai Yu","Zhiwen Fan","Frank Hao Yang","Yuhao Kang","Ross Greer","Chenxi Liu","Eun Hak Lee","Xuan Di","Xinyue Ye","Liu Ren","Alois Knoll","Xiaopeng Li","Shuiwang Ji","Masayoshi Tomizuka","Marco Pavone","Tianbao Yang","Jing Du","Ming-Hsuan Yang","Hua Wei","Ziran Wang","Yang Zhou","Jiachen Li","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2505.08854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08853v1","updated":"2025-05-13T17:53:49Z","published":"2025-05-13T17:53:49Z","title":"Efficiently Manipulating Clutter via Learning and Search-Based Reasoning","summary":"  This thesis presents novel algorithms to advance robotic object\nrearrangement, a critical task for autonomous systems in applications like\nwarehouse automation and household assistance. Addressing challenges of\nhigh-dimensional planning, complex object interactions, and computational\ndemands, our work integrates deep learning for interaction prediction, tree\nsearch for action sequencing, and parallelized computation for efficiency. Key\ncontributions include the Deep Interaction Prediction Network (DIPN) for\naccurate push motion forecasting (over 90% accuracy), its synergistic\nintegration with Monte Carlo Tree Search (MCTS) for effective non-prehensile\nobject retrieval (100% completion in specific challenging scenarios), and the\nParallel MCTS with Batched Simulations (PMBS) framework, which achieves\nsubstantial planning speed-up while maintaining or improving solution quality.\nThe research further explores combining diverse manipulation primitives,\nvalidated extensively through simulated and real-world experiments.\n","authors":["Baichuan Huang"],"pdf_url":"https://arxiv.org/pdf/2505.08853v1.pdf","comment":"PhD Thesis of Baichuan Huang, written under the direction of Prof.\n  Jingjin Yu"},{"id":"http://arxiv.org/abs/2406.10060v4","updated":"2025-05-13T17:18:07Z","published":"2024-06-14T14:16:39Z","title":"PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory\n  Planner","summary":"  In decentralized multiagent trajectory planners, agents need to communicate\nand exchange their positions to generate collision-free trajectories. However,\ndue to localization errors/uncertainties, trajectory deconfliction can fail\neven if trajectories are perfectly shared between agents. To address this\nissue, we first present PARM and PARM*, perception-aware, decentralized,\nasynchronous multiagent trajectory planners that enable a team of agents to\nnavigate uncertain environments while deconflicting trajectories and avoiding\nobstacles using perception information. PARM* differs from PARM as it is less\nconservative, using more computation to find closer-to-optimal solutions. While\nthese methods achieve state-of-the-art performance, they suffer from high\ncomputational costs as they need to solve large optimization problems onboard,\nmaking it difficult for agents to replan at high rates. To overcome this\nchallenge, we present our second key contribution, PRIMER, a learning-based\nplanner trained with imitation learning (IL) using PARM* as the expert\ndemonstrator. PRIMER leverages the low computational requirements at deployment\nof neural networks and achieves a computation speed up to 5500 times faster\nthan optimization-based approaches.\n","authors":["Kota Kondo","Claudius T. Tewari","Andrea Tagliabue","Jesus Tordesillas","Parker C. Lusk","Mason B. Peterson","Jonathan P. How"],"pdf_url":"https://arxiv.org/pdf/2406.10060v4.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.03729v3","updated":"2025-05-13T16:48:41Z","published":"2025-05-06T17:57:12Z","title":"Visual Imitation Enables Contextual Humanoid Control","summary":"  How can we teach humanoids to climb staircases and sit on chairs using the\nsurrounding environment context? Arguably, the simplest way is to just show\nthem-casually capture a human motion video and feed it to humanoids. We\nintroduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday\nvideos, jointly reconstructs the humans and the environment, and produces\nwhole-body control policies for humanoid robots that perform the corresponding\nskills. We demonstrate the results of our pipeline on real humanoid robots,\nshowing robust, repeatable contextual control such as staircase ascents and\ndescents, sitting and standing from chairs and benches, as well as other\ndynamic whole-body skills-all from a single policy, conditioned on the\nenvironment and global root commands. VIDEOMIMIC offers a scalable path towards\nteaching humanoids to operate in diverse real-world environments.\n","authors":["Arthur Allshire","Hongsuk Choi","Junyi Zhang","David McAllister","Anthony Zhang","Chung Min Kim","Trevor Darrell","Pieter Abbeel","Jitendra Malik","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2505.03729v3.pdf","comment":"Project website: https://www.videomimic.net/"},{"id":"http://arxiv.org/abs/2505.08724v1","updated":"2025-05-13T16:36:20Z","published":"2025-05-13T16:36:20Z","title":"Optimal Trajectory Planning with Collision Avoidance for Autonomous\n  Vehicle Maneuvering","summary":"  To perform autonomous driving maneuvers, such as parallel or perpendicular\nparking, a vehicle requires continual speed and steering adjustments to follow\na generated path. In consequence, the path's quality is a limiting factor of\nthe vehicle maneuver's performance. While most path planning approaches include\nfinding a collision-free route, optimal trajectory planning involves solving\nthe best transition from initial to final states, minimizing the action over\nall paths permitted by a kinematic model. Here we propose a novel method based\non sequential convex optimization, which permits flexible and efficient optimal\ntrajectory generation. The objective is to achieve the fastest time, shortest\ndistance, and fewest number of path segments to satisfy motion requirements,\nwhile avoiding sensor blind-spots. In our approach, vehicle kinematics are\nrepresented by a discretized Dubins model. To avoid collisions, each waypoint\nis constrained by linear inequalities representing closest distance of\nobstacles to a polygon specifying the vehicle's extent. To promote smooth and\nvalid trajectories, the solved kinematic state and control variables are\nconstrained and regularized by penalty terms in the model's cost function,\nwhich enforces physical restrictions including limits for steering angle,\nacceleration and speed. In this paper, we analyze trajectories obtained for\nseveral parking scenarios. Results demonstrate efficient and collision-free\nmotion generated by the proposed technique.\n","authors":["Jason Zalev"],"pdf_url":"https://arxiv.org/pdf/2505.08724v1.pdf","comment":"SIAM Conference on Control and Its Applications, July 28-30th, 2025,\n  Montreal, Canada"},{"id":"http://arxiv.org/abs/2505.08712v1","updated":"2025-05-13T16:20:28Z","published":"2025-05-13T16:20:28Z","title":"NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged\n  Information Guidance","summary":"  Learning navigation in dynamic open-world environments is an important yet\nchallenging skill for robots. Most previous methods rely on precise\nlocalization and mapping or learn from expensive real-world demonstrations. In\nthis paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end\nframework trained solely in simulation and can zero-shot transfer to different\nembodiments in diverse real-world environments. The key ingredient of NavDP's\nnetwork is the combination of diffusion-based trajectory generation and a\ncritic function for trajectory selection, which are conditioned on only local\nobservation tokens encoded from a shared policy transformer. Given the\nprivileged information of the global environment in simulation, we scale up the\ndemonstrations of good quality to train the diffusion policy and formulate the\ncritic value function targets with contrastive negative samples. Our\ndemonstration generation approach achieves about 2,500 trajectories/GPU per\nday, 20$\\times$ more efficient than real-world data collection, and results in\na large-scale navigation dataset with 363.2km trajectories across 1244 scenes.\nTrained with this simulation dataset, NavDP achieves state-of-the-art\nperformance and consistently outstanding generalization capability on\nquadruped, wheeled, and humanoid robots in diverse indoor and outdoor\nenvironments. In addition, we present a preliminary attempt at using Gaussian\nSplatting to make in-domain real-to-sim fine-tuning to further bridge the\nsim-to-real gap. Experiments show that adding such real-to-sim data can improve\nthe success rate by 30\\% without hurting its generalization capability.\n","authors":["Wenzhe Cai","Jiaqi Peng","Yuqiang Yang","Yujian Zhang","Meng Wei","Hanqing Wang","Yilun Chen","Tai Wang","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2505.08712v1.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.07719v2","updated":"2025-05-13T15:30:04Z","published":"2024-11-12T11:24:18Z","title":"EMPERROR: A Flexible Generative Perception Error Model for Probing\n  Self-Driving Planners","summary":"  To handle the complexities of real-world traffic, learning planners for\nself-driving from data is a promising direction. While recent approaches have\nshown great progress, they typically assume a setting in which the ground-truth\nworld state is available as input. However, when deployed, planning needs to be\nrobust to the long-tail of errors incurred by a noisy perception system, which\nis often neglected in evaluation. To address this, previous work has proposed\ndrawing adversarial samples from a perception error model (PEM) mimicking the\nnoise characteristics of a target object detector. However, these methods use\nsimple PEMs that fail to accurately capture all failure modes of detection. In\nthis paper, we present EMPERROR, a novel transformer-based generative PEM,\napply it to stress-test an imitation learning (IL)-based planner and show that\nit imitates modern detectors more faithfully than previous work. Furthermore,\nit is able to produce realistic noisy inputs that increase the planner's\ncollision rate by up to 85%, demonstrating its utility as a valuable tool for a\nmore complete evaluation of self-driving planners.\n","authors":["Niklas Hanselmann","Simon Doll","Marius Cordts","Hendrik P. A. Lensch","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2411.07719v2.pdf","comment":"Project page: https://lasnik.github.io/emperror/"},{"id":"http://arxiv.org/abs/2505.08664v1","updated":"2025-05-13T15:26:52Z","published":"2025-05-13T15:26:52Z","title":"A Social Robot with Inner Speech for Dietary Guidance","summary":"  We explore the use of inner speech as a mechanism to enhance transparency and\ntrust in social robots for dietary advice. In humans, inner speech structures\nthought processes and decision-making; in robotics, it improves explainability\nby making reasoning explicit. This is crucial in healthcare scenarios, where\ntrust in robotic assistants depends on both accurate recommendations and\nhuman-like dialogue, which make interactions more natural and engaging.\nBuilding on this, we developed a social robot that provides dietary advice, and\nwe provided the architecture with inner speech capabilities to validate user\ninput, refine reasoning, and generate clear justifications. The system\nintegrates large language models for natural language understanding and a\nknowledge graph for structured dietary information. By making decisions more\ntransparent, our approach strengthens trust and improves human-robot\ninteraction in healthcare. We validated this by measuring the computational\nefficiency of our architecture and conducting a small user study, which\nassessed the reliability of inner speech in explaining the robot's behavior.\n","authors":["Valerio Belcamino","Alessandro Carfì","Valeria Seidita","Fulvio Mastrogiovanni","Antonio Chella"],"pdf_url":"https://arxiv.org/pdf/2505.08664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08657v1","updated":"2025-05-13T15:20:21Z","published":"2025-05-13T15:20:21Z","title":"A Comparative Study of Human Activity Recognition: Motion, Tactile, and\n  multi-modal Approaches","summary":"  Human activity recognition (HAR) is essential for effective Human-Robot\nCollaboration (HRC), enabling robots to interpret and respond to human actions.\nThis study evaluates the ability of a vision-based tactile sensor to classify\n15 activities, comparing its performance to an IMU-based data glove.\nAdditionally, we propose a multi-modal framework combining tactile and motion\ndata to leverage their complementary strengths. We examined three approaches:\nmotion-based classification (MBC) using IMU data, tactile-based classification\n(TBC) with single or dual video streams, and multi-modal classification (MMC)\nintegrating both. Offline validation on segmented datasets assessed each\nconfiguration's accuracy under controlled conditions, while online validation\non continuous action sequences tested online performance. Results showed the\nmulti-modal approach consistently outperformed single-modality methods,\nhighlighting the potential of integrating tactile and motion sensing to enhance\nHAR systems for collaborative robotics.\n","authors":["Valerio Belcamino","Nhat Minh Dinh Le","Quan Khanh Luu","Alessandro Carfì","Van Anh Ho","Fulvio Mastrogiovanni"],"pdf_url":"https://arxiv.org/pdf/2505.08657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08644v1","updated":"2025-05-13T15:03:40Z","published":"2025-05-13T15:03:40Z","title":"DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian\n  Splatting","summary":"  This work presents DLO-Splatting, an algorithm for estimating the 3D shape of\nDeformable Linear Objects (DLOs) from multi-view RGB images and gripper state\ninformation through prediction-update filtering. The DLO-Splatting algorithm\nuses a position-based dynamics model with shape smoothness and rigidity\ndampening corrections to predict the object shape. Optimization with a 3D\nGaussian Splatting-based rendering loss iteratively renders and refines the\nprediction to align it with the visual observations in the update step. Initial\nexperiments demonstrate promising results in a knot tying scenario, which is\nchallenging for existing vision-only methods.\n","authors":["Holly Dinkel","Marcel Büsching","Alberta Longhini","Brian Coltin","Trey Smith","Danica Kragic","Mårten Björkman","Timothy Bretl"],"pdf_url":"https://arxiv.org/pdf/2505.08644v1.pdf","comment":"5 pages, 2 figures, presented at the 2025 5th Workshop: Reflections\n  on Representations and Manipulating Deformable Objects at the IEEE\n  International Conference on Robotics and Automation. RMDO workshop\n  (https://deformable-workshop.github.io/icra2025/)"},{"id":"http://arxiv.org/abs/2505.08627v1","updated":"2025-05-13T14:46:23Z","published":"2025-05-13T14:46:23Z","title":"Augmented Reality for RObots (ARRO): Pointing Visuomotor Policies\n  Towards Visual Robustness","summary":"  Visuomotor policies trained on human expert demonstrations have recently\nshown strong performance across a wide range of robotic manipulation tasks.\nHowever, these policies remain highly sensitive to domain shifts stemming from\nbackground or robot embodiment changes, which limits their generalization\ncapabilities. In this paper, we present ARRO, a novel calibration-free visual\nrepresentation that leverages zero-shot open-vocabulary segmentation and object\ndetection models to efficiently mask out task-irrelevant regions of the scene\nwithout requiring additional training. By filtering visual distractors and\noverlaying virtual guides during both training and inference, ARRO improves\nrobustness to scene variations and reduces the need for additional data\ncollection. We extensively evaluate ARRO with Diffusion Policy on several\ntabletop manipulation tasks in both simulation and real-world environments, and\nfurther demonstrate its compatibility and effectiveness with generalist robot\npolicies, such as Octo and OpenVLA. Across all settings in our evaluation, ARRO\nyields consistent performance gains, allows for selective masking to choose\nbetween different objects, and shows robustness even to challenging\nsegmentation conditions. Videos showcasing our results are available at:\naugmented-reality-for-robots.github.io\n","authors":["Reihaneh Mirjalili","Tobias Jülg","Florian Walter","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2505.08627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08625v1","updated":"2025-05-13T14:45:13Z","published":"2025-05-13T14:45:13Z","title":"Beyond Predefined Actions: Integrating Behavior Trees and Dynamic\n  Movement Primitives for Robot Learning from Demonstration","summary":"  Interpretable policy representations like Behavior Trees (BTs) and Dynamic\nMotion Primitives (DMPs) enable robot skill transfer from human demonstrations,\nbut each faces limitations: BTs require expert-crafted low-level actions, while\nDMPs lack high-level task logic. We address these limitations by integrating\nDMP controllers into a BT framework, jointly learning the BT structure and DMP\nactions from single demonstrations, thereby removing the need for predefined\nactions. Additionally, by combining BT decision logic with DMP motion\ngeneration, our method enhances policy interpretability, modularity, and\nadaptability for autonomous systems. Our approach readily affords both learning\nto replicate low-level motions and combining partial demonstrations into a\ncoherent and easy-to-modify overall policy.\n","authors":["David Cáceres Domínguez","Erik Schaffernicht","Todor Stoyanov"],"pdf_url":"https://arxiv.org/pdf/2505.08625v1.pdf","comment":"14 pages, 6 figures, accepted (not yet published) at IAS19 2025\n  conference"},{"id":"http://arxiv.org/abs/2505.08619v1","updated":"2025-05-13T14:38:25Z","published":"2025-05-13T14:38:25Z","title":"Cost Function Estimation Using Inverse Reinforcement Learning with\n  Minimal Observations","summary":"  We present an iterative inverse reinforcement learning algorithm to infer\noptimal cost functions in continuous spaces. Based on a popular maximum entropy\ncriteria, our approach iteratively finds a weight improvement step and proposes\na method to find an appropriate step size that ensures learned cost function\nfeatures remain similar to the demonstrated trajectory features. In contrast to\nsimilar approaches, our algorithm can individually tune the effectiveness of\neach observation for the partition function and does not need a large sample\nset, enabling faster learning. We generate sample trajectories by solving an\noptimal control problem instead of random sampling, leading to more informative\ntrajectories. The performance of our method is compared to two state of the art\nalgorithms to demonstrate its benefits in several simulated environments.\n","authors":["Sarmad Mehrdad","Avadesh Meduri","Ludovic Righetti"],"pdf_url":"https://arxiv.org/pdf/2505.08619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08593v1","updated":"2025-05-13T14:05:07Z","published":"2025-05-13T14:05:07Z","title":"MC-Swarm: Minimal-Communication Multi-Agent Trajectory Planning and\n  Deadlock Resolution for Quadrotor Swarm","summary":"  For effective multi-agent trajectory planning, it is important to consider\nlightweight communication and its potential asynchrony. This paper presents a\ndistributed trajectory planning algorithm for a quadrotor swarm that operates\nasynchronously and requires no communication except during the initial planning\nphase. Moreover, our algorithm guarantees no deadlock under asynchronous\nupdates and absence of communication during flight. To effectively ensure these\npoints, we build two main modules: coordination state updater and trajectory\noptimizer. The coordination state updater computes waypoints for each agent\ntoward its goal and performs subgoal optimization while considering deadlocks,\nas well as safety constraints with respect to neighbor agents and obstacles.\nThen, the trajectory optimizer generates a trajectory that ensures collision\navoidance even with the asynchronous planning updates of neighboring agents. We\nprovide a theoretical guarantee of collision avoidance with deadlock resolution\nand evaluate the effectiveness of our method in complex simulation\nenvironments, including random forests and narrow-gap mazes. Additionally, to\nreduce the total mission time, we design a faster coordination state update\nusing lightweight communication. Lastly, our approach is validated through\nextensive simulations and real-world experiments with cluttered environment\nscenarios.\n","authors":["Yunwoo Lee","Jungwon Park"],"pdf_url":"https://arxiv.org/pdf/2505.08593v1.pdf","comment":"13 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.08589v1","updated":"2025-05-13T14:01:07Z","published":"2025-05-13T14:01:07Z","title":"MESSI: A Multi-Elevation Semantic Segmentation Image Dataset of an Urban\n  Environment","summary":"  This paper presents a Multi-Elevation Semantic Segmentation Image (MESSI)\ndataset comprising 2525 images taken by a drone flying over dense urban\nenvironments. MESSI is unique in two main features. First, it contains images\nfrom various altitudes, allowing us to investigate the effect of depth on\nsemantic segmentation. Second, it includes images taken from several different\nurban regions (at different altitudes). This is important since the variety\ncovers the visual richness captured by a drone's 3D flight, performing\nhorizontal and vertical maneuvers. MESSI contains images annotated with\nlocation, orientation, and the camera's intrinsic parameters and can be used to\ntrain a deep neural network for semantic segmentation or other applications of\ninterest (e.g., localization, navigation, and tracking). This paper describes\nthe dataset and provides annotation details. It also explains how semantic\nsegmentation was performed using several neural network models and shows\nseveral relevant statistics. MESSI will be published in the public domain to\nserve as an evaluation benchmark for semantic segmentation using images\ncaptured by a drone or similar vehicle flying over a dense urban environment.\n","authors":["Barak Pinkovich","Boaz Matalon","Ehud Rivlin","Hector Rotstein"],"pdf_url":"https://arxiv.org/pdf/2505.08589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08574v1","updated":"2025-05-13T13:46:35Z","published":"2025-05-13T13:46:35Z","title":"End-to-End Multi-Task Policy Learning from NMPC for Quadruped Locomotion","summary":"  Quadruped robots excel in traversing complex, unstructured environments where\nwheeled robots often fail. However, enabling efficient and adaptable locomotion\nremains challenging due to the quadrupeds' nonlinear dynamics, high degrees of\nfreedom, and the computational demands of real-time control. Optimization-based\ncontrollers, such as Nonlinear Model Predictive Control (NMPC), have shown\nstrong performance, but their reliance on accurate state estimation and high\ncomputational overhead makes deployment in real-world settings challenging. In\nthis work, we present a Multi-Task Learning (MTL) framework in which expert\nNMPC demonstrations are used to train a single neural network to predict\nactions for multiple locomotion behaviors directly from raw proprioceptive\nsensor inputs. We evaluate our approach extensively on the quadruped robot Go1,\nboth in simulation and on real hardware, demonstrating that it accurately\nreproduces expert behavior, allows smooth gait switching, and simplifies the\ncontrol pipeline for real-time deployment. Our MTL architecture enables\nlearning diverse gaits within a unified policy, achieving high $R^{2}$ scores\nfor predicted joint targets across all tasks.\n","authors":["Anudeep Sajja","Shahram Khorshidi","Sebastian Houben","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2505.08574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01486v3","updated":"2025-05-13T13:43:58Z","published":"2025-05-02T16:45:01Z","title":"Aerial Path Online Planning for Urban Scene Updation","summary":"  We present the first scene-update aerial path planning algorithm specifically\ndesigned for detecting and updating change areas in urban environments. While\nexisting methods for large-scale 3D urban scene reconstruction focus on\nachieving high accuracy and completeness, they are inefficient for scenarios\nrequiring periodic updates, as they often re-explore and reconstruct entire\nscenes, wasting significant time and resources on unchanged areas. To address\nthis limitation, our method leverages prior reconstructions and change\nprobability statistics to guide UAVs in detecting and focusing on areas likely\nto have changed. Our approach introduces a novel changeability heuristic to\nevaluate the likelihood of changes, driving the planning of two flight paths: a\nprior path informed by static priors and a dynamic real-time path that adapts\nto newly detected changes. The framework integrates surface sampling and\ncandidate view generation strategies, ensuring efficient coverage of change\nareas with minimal redundancy. Extensive experiments on real-world urban\ndatasets demonstrate that our method significantly reduces flight time and\ncomputational overhead, while maintaining high-quality updates comparable to\nfull-scene re-exploration and reconstruction. These contributions pave the way\nfor efficient, scalable, and adaptive UAV-based scene updates in complex urban\nenvironments.\n","authors":["Mingfeng Tang","Ningna Wang","Ziyuan Xie","Jianwei Hu","Ke Xie","Xiaohu Guo","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2505.01486v3.pdf","comment":"ACM SIGGRAPH 2025 (Patent Protected); Project page:\n  https://vcc.tech/research/2025/DroneUpdate ; Video:\n  https://youtu.be/3nNBoh2syTU?si=LQ04NlJNyPosdm8F"},{"id":"http://arxiv.org/abs/2502.21019v2","updated":"2025-05-13T13:42:31Z","published":"2025-02-28T13:04:36Z","title":"Nano Drone-based Indoor Crime Scene Analysis","summary":"  Technologies such as robotics, Artificial Intelligence (AI), and Computer\nVision (CV) can be applied to crime scene analysis (CSA) to help protect lives,\nfacilitate justice, and deter crime, but an overview of the tasks that can be\nautomated has been lacking. Here we follow a speculative prototyping approach:\nFirst, the STAIR tool is used to rapidly review the literature and identify\ntasks that seem to have not received much attention, like accessing crime\nscenes through a window, mapping/gathering evidence, and analyzing blood\nsmears. Secondly, we present a prototype of a small drone that implements these\nthree tasks with 75%, 85%, and 80% performance, to perform a minimal analysis\nof an indoor crime scene. Lessons learned are reported, toward guiding next\nwork.\n","authors":["Martin Cooney","Sivadinesh Ponrajan","Fernando Alonso-Fernandez"],"pdf_url":"https://arxiv.org/pdf/2502.21019v2.pdf","comment":"8 pages, 6 figures, 3 tables, accepted at ARSO 2025"},{"id":"http://arxiv.org/abs/2505.08548v1","updated":"2025-05-13T13:20:46Z","published":"2025-05-13T13:20:46Z","title":"From Seeing to Doing: Bridging Reasoning and Decision for Robotic\n  Manipulation","summary":"  Achieving generalization in robotic manipulation remains a critical\nchallenge, particularly for unseen scenarios and novel tasks. Current\nVision-Language-Action (VLA) models, while building on top of general\nVision-Language Models (VLMs), still fall short of achieving robust zero-shot\nperformance due to the scarcity and heterogeneity prevalent in embodied\ndatasets. To address these limitations, we propose FSD (From Seeing to Doing),\na novel vision-language model that generates intermediate representations\nthrough spatial relationship reasoning, providing fine-grained guidance for\nrobotic manipulation. Our approach combines a hierarchical data pipeline for\ntraining with a self-consistency mechanism that aligns spatial coordinates with\nvisual signals. Through extensive experiments, we comprehensively validated\nFSD's capabilities in both \"seeing\" and \"doing,\" achieving outstanding\nperformance across 8 benchmarks for general spatial reasoning and embodied\nreference abilities, as well as on our proposed more challenging benchmark\nVABench. We also verified zero-shot capabilities in robot manipulation,\ndemonstrating significant performance improvements over baseline methods in\nboth SimplerEnv and real robot settings. Experimental results show that FSD\nachieves 54.1% success rate in SimplerEnv and 72% success rate across 8\nreal-world tasks, outperforming the strongest baseline by 30%.\n","authors":["Yifu Yuan","Haiqin Cui","Yibin Chen","Zibin Dong","Fei Ni","Longxin Kou","Jinyi Liu","Pengyi Li","Yan Zheng","Jianye Hao"],"pdf_url":"https://arxiv.org/pdf/2505.08548v1.pdf","comment":"Early version"},{"id":"http://arxiv.org/abs/2505.08510v1","updated":"2025-05-13T12:40:19Z","published":"2025-05-13T12:40:19Z","title":"FOCI: Trajectory Optimization on Gaussian Splats","summary":"  3D Gaussian Splatting (3DGS) has recently gained popularity as a faster\nalternative to Neural Radiance Fields (NeRFs) in 3D reconstruction and view\nsynthesis methods. Leveraging the spatial information encoded in 3DGS, this\nwork proposes FOCI (Field Overlap Collision Integral), an algorithm that is\nable to optimize trajectories directly on the Gaussians themselves. FOCI\nleverages a novel and interpretable collision formulation for 3DGS using the\nnotion of the overlap integral between Gaussians. Contrary to other approaches,\nwhich represent the robot with conservative bounding boxes that underestimate\nthe traversability of the environment, we propose to represent the environment\nand the robot as Gaussian Splats. This not only has desirable computational\nproperties, but also allows for orientation-aware planning, allowing the robot\nto pass through very tight and narrow spaces. We extensively test our algorithm\nin both synthetic and real Gaussian Splats, showcasing that collision-free\ntrajectories for the ANYmal legged robot that can be computed in a few seconds,\neven with hundreds of thousands of Gaussians making up the environment. The\nproject page and code are available at\nhttps://rffr.leggedrobotics.com/works/foci/\n","authors":["Mario Gomez Andreu","Maximum Wilder-Smith","Victor Klemm","Vaishakh Patil","Jesus Tordesillas","Marco Hutter"],"pdf_url":"https://arxiv.org/pdf/2505.08510v1.pdf","comment":"7 pages, 8 figures, Mario Gomez Andreu and Maximum Wilder-Smith\n  contributed equally"},{"id":"http://arxiv.org/abs/2502.16293v2","updated":"2025-05-13T12:33:04Z","published":"2025-02-22T16:47:27Z","title":"Optimization-free Smooth Control Barrier Function for Polygonal\n  Collision Avoidance","summary":"  Polygonal collision avoidance (PCA) is short for the problem of collision\navoidance between two polygons (i.e., polytopes in planar) that own their\ndynamic equations. This problem suffers the inherent difficulty in dealing with\nnon-smooth boundaries and recently optimization-defined metrics, such as signed\ndistance field (SDF) and its variants, have been proposed as control barrier\nfunctions (CBFs) to tackle PCA problems. In contrast, we propose an\noptimization-free smooth CBF method in this paper, which is computationally\nefficient and proved to be nonconservative. It is achieved by three main steps:\na lower bound of SDF is expressed as a nested Boolean logic composition first,\nthen its smooth approximation is established by applying the latest log-sum-exp\nmethod, after which a specified CBF-based safety filter is proposed to address\nthis class of problems. To illustrate its wide applications, the\noptimization-free smooth CBF method is extended to solve distributed collision\navoidance of two underactuated nonholonomic vehicles and drive an underactuated\ncontainer crane to avoid a moving obstacle respectively, for which numerical\nsimulations are also performed.\n","authors":["Shizhen Wu","Yongchun Fang","Ning Sun","Biao Lu","Xiao Liang","Yiming Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.16293v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08492v1","updated":"2025-05-13T12:22:38Z","published":"2025-05-13T12:22:38Z","title":"Achieving Scalable Robot Autonomy via neurosymbolic planning using\n  lightweight local LLM","summary":"  PDDL-based symbolic task planning remains pivotal for robot autonomy yet\nstruggles with dynamic human-robot collaboration due to scalability,\nre-planning demands, and delayed plan availability. Although a few\nneurosymbolic frameworks have previously leveraged LLMs such as GPT-3 to\naddress these challenges, reliance on closed-source, remote models with limited\ncontext introduced critical constraints: third-party dependency, inconsistent\nresponse times, restricted plan length and complexity, and multi-domain\nscalability issues. We present Gideon, a novel framework that enables the\ntransition to modern, smaller, local LLMs with extended context length. Gideon\nintegrates a novel problem generator to systematically generate large-scale\ndatasets of realistic domain-problem-plan tuples for any domain, and adapts\nneurosymbolic planning for local LLMs, enabling on-device execution and\nextended context for multi-domain support. Preliminary experiments in\nsingle-domain scenarios performed on Qwen-2.5 1.5B and trained on 8k-32k\nsamples, demonstrate a valid plan percentage of 66.1% (32k model) and show that\nthe figure can be further scaled through additional data. Multi-domain tests on\n16k samples yield an even higher 70.6% planning validity rate, proving\nextensibility across domains and signaling that data variety can have a\npositive effect on learning efficiency. Although long-horizon planning and\nreduced model size make Gideon training much less efficient than baseline\nmodels based on larger LLMs, the results are still significant considering that\nthe trained model is about 120x smaller than baseline and that significant\nadvantages can be achieved in inference efficiency, scalability, and\nmulti-domain adaptability, all critical factors in human-robot collaboration.\nTraining inefficiency can be mitigated by Gideon's streamlined data generation\npipeline.\n","authors":["Nicholas Attolino","Alessio Capitanelli","Fulvio Mastrogiovanni"],"pdf_url":"https://arxiv.org/pdf/2505.08492v1.pdf","comment":"19 pages, 3 figures, 4 tables, accepted at IAS 2025"},{"id":"http://arxiv.org/abs/2307.05033v3","updated":"2025-05-13T12:00:41Z","published":"2023-07-11T06:15:12Z","title":"Towards Anytime Optical Flow Estimation with Event Cameras","summary":"  Event cameras respond to changes in log-brightness at the millisecond level,\nmaking them ideal for optical flow estimation. However, existing datasets from\nevent cameras provide only low frame rate ground truth for optical flow,\nlimiting the research potential of event-driven optical flow. To address this\nchallenge, we introduce a low-latency event representation, Unified Voxel Grid,\nand propose EVA-Flow, an EVent-based Anytime Flow estimation network to produce\nhigh-frame-rate event optical flow with only low-frame-rate optical flow ground\ntruth for supervision. Furthermore, we propose the Rectified Flow Warp Loss\n(RFWL) for the unsupervised assessment of intermediate optical flow. A\ncomprehensive variety of experiments on MVSEC, DESC, and our EVA-FlowSet\ndemonstrates that EVA-Flow achieves competitive performance, super-low-latency\n(5ms), time-dense motion estimation (200Hz), and strong generalization. Our\ncode will be available at https://github.com/Yaozhuwa/EVA-Flow.\n","authors":["Yaozu Ye","Hao Shi","Kailun Yang","Ze Wang","Xiaoting Yin","Lei Sun","Yaonan Wang","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2307.05033v3.pdf","comment":"Accepted to Sensors. Our code will be available at\n  https://github.com/Yaozhuwa/EVA-Flow"},{"id":"http://arxiv.org/abs/2505.08458v1","updated":"2025-05-13T11:40:21Z","published":"2025-05-13T11:40:21Z","title":"Zero-Shot Sim-to-Real Reinforcement Learning for Fruit Harvesting","summary":"  This paper presents a comprehensive sim-to-real pipeline for autonomous\nstrawberry picking from dense clusters using a Franka Panda robot. Our approach\nleverages a custom Mujoco simulation environment that integrates domain\nrandomization techniques. In this environment, a deep reinforcement learning\nagent is trained using the dormant ratio minimization algorithm. The proposed\npipeline bridges low-level control with high-level perception and decision\nmaking, demonstrating promising performance in both simulation and in a real\nlaboratory environment, laying the groundwork for successful transfer to\nreal-world autonomous fruit harvesting.\n","authors":["Emlyn Williams","Athanasios Polydoros"],"pdf_url":"https://arxiv.org/pdf/2505.08458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08453v1","updated":"2025-05-13T11:30:51Z","published":"2025-05-13T11:30:51Z","title":"Parameter Estimation using Reinforcement Learning Causal Curiosity:\n  Limits and Challenges","summary":"  Causal understanding is important in many disciplines of science and\nengineering, where we seek to understand how different factors in the system\ncausally affect an experiment or situation and pave a pathway towards creating\neffective or optimising existing models. Examples of use cases are autonomous\nexploration and modelling of unknown environments or assessing key variables in\noptimising large complex systems. In this paper, we analyse a Reinforcement\nLearning approach called Causal Curiosity, which aims to estimate as accurately\nand efficiently as possible, without directly measuring them, the value of\nfactors that causally determine the dynamics of a system. Whilst the idea\npresents a pathway forward, measurement accuracy is the foundation of\nmethodology effectiveness. Focusing on the current causal curiosity's robotic\nmanipulator, we present for the first time a measurement accuracy analysis of\nthe future potentials and current limitations of this technique and an analysis\nof its sensitivity and confounding factor disentanglement capability - crucial\nfor causal analysis. As a result of our work, we promote proposals for an\nimproved and efficient design of Causal Curiosity methods to be applied to\nreal-world complex scenarios.\n","authors":["Miguel Arana-Catania","Weisi Guo"],"pdf_url":"https://arxiv.org/pdf/2505.08453v1.pdf","comment":"24 pages, 10 figures, 9 tables"},{"id":"http://arxiv.org/abs/2505.08444v1","updated":"2025-05-13T11:13:00Z","published":"2025-05-13T11:13:00Z","title":"Symbolically-Guided Visual Plan Inference from Uncurated Video Data","summary":"  Visual planning, by offering a sequence of intermediate visual subgoals to a\ngoal-conditioned low-level policy, achieves promising performance on\nlong-horizon manipulation tasks. To obtain the subgoals, existing methods\ntypically resort to video generation models but suffer from model hallucination\nand computational cost. We present Vis2Plan, an efficient, explainable and\nwhite-box visual planning framework powered by symbolic guidance. From raw,\nunlabeled play data, Vis2Plan harnesses vision foundation models to\nautomatically extract a compact set of task symbols, which allows building a\nhigh-level symbolic transition graph for multi-goal, multi-stage planning. At\ntest time, given a desired task goal, our planner conducts planning at the\nsymbolic level and assembles a sequence of physically consistent intermediate\nsub-goal images grounded by the underlying symbolic representation. Our\nVis2Plan outperforms strong diffusion video generation-based visual planners by\ndelivering 53\\% higher aggregate success rate in real robot settings while\ngenerating visual plans 35$\\times$ faster. The results indicate that Vis2Plan\nis able to generate physically consistent image goals while offering fully\ninspectable reasoning steps.\n","authors":["Wenyan Yang","Ahmet Tikna","Yi Zhao","Yuying Zhang","Luigi Palopoli","Marco Roveri","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2505.08444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12514v5","updated":"2025-05-13T11:02:20Z","published":"2024-09-19T07:10:18Z","title":"TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation","summary":"  Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io.\n","authors":["Junjie Wen","Yichen Zhu","Jinming Li","Minjie Zhu","Kun Wu","Zhiyuan Xu","Ning Liu","Ran Cheng","Chaomin Shen","Yaxin Peng","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2409.12514v5.pdf","comment":"add more citations"},{"id":"http://arxiv.org/abs/2502.05855v2","updated":"2025-05-13T10:55:53Z","published":"2025-02-09T11:25:56Z","title":"DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General\n  Robot Control","summary":"  Enabling robots to perform diverse tasks across varied environments is a\ncentral challenge in robot learning. While vision-language-action (VLA) models\nhave shown promise for generalizable robot skills, realizing their full\npotential requires addressing limitations in action representation and\nefficient training. Current VLA models often focus on scaling the\nvision-language model (VLM) component, while the action space representation\nremains a critical bottleneck. This paper introduces DexVLA, a novel framework\ndesigned to enhance the efficiency and generalization capabilities of VLAs for\ncomplex, long-horizon tasks across diverse robot embodiments. DexVLA features a\nnovel diffusion-based action expert, scaled to one billion parameters, designed\nfor cross-embodiment learning. A novel embodiment curriculum learning strategy\nfacilitates efficient training: (1) pre-training the diffusion expert that is\nseparable from the VLA on cross-embodiment data, (2) aligning the VLA model to\nspecific embodiments, and (3) post-training for rapid adaptation to new tasks.\nWe conduct comprehensive experiments across multiple embodiments, including\nsingle-arm, bimanual, and dexterous hand, demonstrating DexVLA's adaptability\nto challenging tasks without task-specific adaptation, its ability to learn\ndexterous skills on novel embodiments with limited data, and its capacity to\ncomplete complex, long-horizon tasks using only direct language prompting, such\nas laundry folding. In all settings, our method demonstrates superior\nperformance compared to state-of-the-art models like Octo, OpenVLA, and\nDiffusion Policy.\n","authors":["Junjie Wen","Yichen Zhu","Jinming Li","Zhibin Tang","Chaomin Shen","Feifei Feng"],"pdf_url":"https://arxiv.org/pdf/2502.05855v2.pdf","comment":"The webpage is at https://dex-vla.github.io/"},{"id":"http://arxiv.org/abs/2505.08419v1","updated":"2025-05-13T10:22:51Z","published":"2025-05-13T10:22:51Z","title":"HMR-ODTA: Online Diverse Task Allocation for a Team of Heterogeneous\n  Mobile Robots","summary":"  Coordinating time-sensitive deliveries in environments like hospitals poses a\ncomplex challenge, particularly when managing multiple online pickup and\ndelivery requests within strict time windows using a team of heterogeneous\nrobots. Traditional approaches fail to address dynamic rescheduling or diverse\nservice requirements, typically restricting robots to single-task types. This\npaper tackles the Multi-Pickup and Delivery Problem with Time Windows (MPDPTW),\nwhere autonomous mobile robots are capable of handling varied service requests.\nThe objective is to minimize late delivery penalties while maximizing task\ncompletion rates. To achieve this, we propose a novel framework leveraging a\nheterogeneous robot team and an efficient dynamic scheduling algorithm that\nsupports dynamic task rescheduling. Users submit requests with specific time\nconstraints, and our decentralized algorithm, Heterogeneous Mobile Robots\nOnline Diverse Task Allocation (HMR-ODTA), optimizes task assignments to ensure\ntimely service while addressing delays or task rejections. Extensive\nsimulations validate the algorithm's effectiveness. For smaller task sets\n(40-160 tasks), penalties were reduced by nearly 63%, while for larger sets\n(160-280 tasks), penalties decreased by approximately 50%. These results\nhighlight the algorithm's effectiveness in improving task scheduling and\ncoordination in multi-robot systems, offering a robust solution for enhancing\ndelivery performance in structured, time-critical environments.\n","authors":["Ashish Verma","Avinash Gautam","Tanishq Duhan","V. S. Shekhawat","Sudeept Mohan"],"pdf_url":"https://arxiv.org/pdf/2505.08419v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2505.00039v2","updated":"2025-05-13T17:19:55Z","published":"2025-04-29T18:36:57Z","title":"Graph RAG for Legal Norms: A Hierarchical and Temporal Approach","summary":"  This article proposes an adaptation of Graph Retrieval Augmented Generation\n(Graph RAG) specifically designed for the analysis and comprehension of legal\nnorms, which are characterized by their predefined hierarchical structure,\nextensive network of internal and external references and multiple temporal\nversions. By combining structured knowledge graphs with contextually enriched\ntext segments, Graph RAG offers a promising solution to address the inherent\ncomplexity and vast volume of legal data. The integration of hierarchical\nstructure and temporal evolution into knowledge graphs - along with the concept\nof comprehensive Text Units - facilitates the construction of richer,\ninterconnected representations of legal knowledge. Through a detailed analysis\nof Graph RAG and its application to legal norm datasets, this article aims to\nadvance the field of Artificial Intelligence applied to Law, creating\nopportunities for more effective systems in legal research, legislative\nanalysis, and decision support.\n","authors":["Hudson de Martim"],"pdf_url":"https://arxiv.org/pdf/2505.00039v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08728v1","updated":"2025-05-13T16:39:00Z","published":"2025-05-13T16:39:00Z","title":"Securing RAG: A Risk Assessment and Mitigation Framework","summary":"  Retrieval Augmented Generation (RAG) has emerged as the de facto industry\nstandard for user-facing NLP applications, offering the ability to integrate\ndata without re-training or fine-tuning Large Language Models (LLMs). This\ncapability enhances the quality and accuracy of responses but also introduces\nnovel security and privacy challenges, particularly when sensitive data is\nintegrated. With the rapid adoption of RAG, securing data and services has\nbecome a critical priority. This paper first reviews the vulnerabilities of RAG\npipelines, and outlines the attack surface from data pre-processing and data\nstorage management to integration with LLMs. The identified risks are then\npaired with corresponding mitigations in a structured overview. In a second\nstep, the paper develops a framework that combines RAG-specific security\nconsiderations, with existing general security guidelines, industry standards,\nand best practices. The proposed framework aims to guide the implementation of\nrobust, compliant, secure, and trustworthy RAG systems.\n","authors":["Lukas Ammann","Sara Ott","Christoph R. Landolt","Marco P. Lehmann"],"pdf_url":"https://arxiv.org/pdf/2505.08728v1.pdf","comment":"8 pages, 3 figures, Sara Ott and Lukas Ammann contributed equally"},{"id":"http://arxiv.org/abs/2503.21098v3","updated":"2025-05-13T11:54:26Z","published":"2025-03-27T02:36:48Z","title":"Alleviating LLM-based Generative Retrieval Hallucination in Alipay\n  Search","summary":"  Generative retrieval (GR) has revolutionized document retrieval with the\nadvent of large language models (LLMs), and LLM-based GR is gradually being\nadopted by the industry. Despite its remarkable advantages and potential,\nLLM-based GR suffers from hallucination and generates documents that are\nirrelevant to the query in some instances, severely challenging its credibility\nin practical applications. We thereby propose an optimized GR framework\ndesigned to alleviate retrieval hallucination, which integrates knowledge\ndistillation reasoning in model training and incorporate decision agent to\nfurther improve retrieval precision. Specifically, we employ LLMs to assess and\nreason GR retrieved query-document (q-d) pairs, and then distill the reasoning\ndata as transferred knowledge to the GR model. Moreover, we utilize a decision\nagent as post-processing to extend the GR retrieved documents through retrieval\nmodel and select the most relevant ones from multi perspectives as the final\ngenerative retrieval result. Extensive offline experiments on real-world\ndatasets and online A/B tests on Fund Search and Insurance Search in Alipay\ndemonstrate our framework's superiority and effectiveness in improving search\nquality and conversion gains.\n","authors":["Yedan Shen","Kaixin Wu","Yuechen Ding","Jingyuan Wen","Hong Liu","Mingjie Zhong","Zhouhan Lin","Jia Xu","Linjian Mo"],"pdf_url":"https://arxiv.org/pdf/2503.21098v3.pdf","comment":"Accepted by SIGIR 2025"},{"id":"http://arxiv.org/abs/2505.08471v1","updated":"2025-05-13T11:53:26Z","published":"2025-05-13T11:53:26Z","title":"Interest Changes: Considering User Interest Life Cycle in Recommendation\n  System","summary":"  In recommendation systems, user interests are always in a state of constant\nflux. Typically, a user interest experiences a emergent phase, a stable phase,\nand a declining phase, which are referred to as the \"user interest life-cycle\".\nRecent papers on user interest modeling have primarily focused on how to\ncompute the correlation between the target item and user's historical\nbehaviors, without thoroughly considering the life-cycle features of user\ninterest. In this paper, we propose an effective method called Deep Interest\nLife-cycle Network (DILN), which not only captures the interest life-cycle\nfeatures efficiently, but can also be easily integrated to existing ranking\nmodels. DILN contains two key components: Interest Life-cycle Encoder Module\nconstructs historical activity histograms of the user interest and then encodes\nthem into dense representation. Interest Life-cycle Fusion Module injects the\nencoded dense representation into multiple expert networks, with the aim of\nenabling the specific phase of interest life-cycle to activate distinct\nexperts. Online A/B testing reveals that DILN achieves significant improvements\nof +0.38% in CTR, +1.04% in CVR and +0.25% in duration per user, which\ndemonstrates its effectiveness. In addition, DILN inherently increase the\nexposure of users' emergent and stable interests while decreasing the exposure\nof declining interests. DILN has been deployed on the Lofter App.\n","authors":["Yinjiang Cai","Jiangpan Hou","Yangping Zhu","Yuan Nie"],"pdf_url":"https://arxiv.org/pdf/2505.08471v1.pdf","comment":"Accepted by SIGIR 2025"},{"id":"http://arxiv.org/abs/2504.04752v3","updated":"2025-05-13T11:02:51Z","published":"2025-04-07T05:58:01Z","title":"Investigating Popularity Bias Amplification in Recommender Systems\n  Employed in the Entertainment Domain","summary":"  Recommender systems have become an integral part of our daily online\nexperience by analyzing past user behavior to suggest relevant content in\nentertainment domains such as music, movies, and books. Today, they are among\nthe most widely used applications of AI and machine learning. Consequently,\nregulations and guidelines for trustworthy AI, such as the European AI Act,\nwhich addresses issues like bias and fairness, are highly relevant to the\ndesign, development, and evaluation of recommender systems. One particularly\nimportant type of bias in this context is popularity bias, which results in the\nunfair underrepresentation of less popular content in recommendation lists.\nThis work summarizes our research on investigating the amplification of\npopularity bias in recommender systems within the entertainment sector.\nAnalyzing datasets from three entertainment domains, music, movies, and anime,\nwe demonstrate that an item's recommendation frequency is positively correlated\nwith its popularity. As a result, user groups with little interest in popular\ncontent receive less accurate recommendations compared to those who prefer\nwidely popular items. Furthermore, this work contributes to a better\nunderstanding of the connection between recommendation accuracy, calibration\nquality of algorithms, and popularity bias amplification.\n","authors":["Dominik Kowald"],"pdf_url":"https://arxiv.org/pdf/2504.04752v3.pdf","comment":"Accepted at EWAF'25, summarizes fairness and popularity bias research\n  presented in Dr. Kowald's habilitation:\n  https://domkowald.github.io/documents/others/2024habilitation_recsys.pdf"},{"id":"http://arxiv.org/abs/2505.08411v1","updated":"2025-05-13T10:09:51Z","published":"2025-05-13T10:09:51Z","title":"Lost in Transliteration: Bridging the Script Gap in Neural IR","summary":"  Most human languages use scripts other than the Latin alphabet. Search users\nin these languages often formulate their information needs in a transliterated\n-- usually Latinized -- form for ease of typing. For example, Greek speakers\nmight use Greeklish, and Arabic speakers might use Arabizi. This paper shows\nthat current search systems, including those that use multilingual dense\nembeddings such as BGE-M3, do not generalise to this setting, and their\nperformance rapidly deteriorates when exposed to transliterated queries. This\ncreates a ``script gap\" between the performance of the same queries when\nwritten in their native or transliterated form. We explore whether adapting the\npopular ``translate-train\" paradigm to transliterations can enhance the\nrobustness of multilingual Information Retrieval (IR) methods and bridge the\ngap between native and transliterated scripts. By exploring various\ncombinations of non-Latin and Latinized query text for training, we investigate\nwhether we can enhance the capacity of existing neural retrieval techniques and\nenable them to apply to this important setting. We show that by further\nfine-tuning IR models on an even mixture of native and Latinized text, they can\nperform this cross-script matching at nearly the same performance as when the\nquery was formulated in the native script. Out-of-domain evaluation and further\nqualitative analysis show that transliterations can also cause queries to lose\nsome of their nuances, motivating further research in this direction.\n","authors":["Andreas Chari","Iadh Ounis","Sean MacAvaney"],"pdf_url":"https://arxiv.org/pdf/2505.08411v1.pdf","comment":"6 pages, 2 tables. paper accepted at the Short Paper track of The\n  48th International ACM SIGIR Conference on Research and Development in\n  Information Retrieval"},{"id":"http://arxiv.org/abs/2505.08385v1","updated":"2025-05-13T09:32:09Z","published":"2025-05-13T09:32:09Z","title":"TikTok Search Recommendations: Governance and Research Challenges","summary":"  Like other social media, TikTok is embracing its use as a search engine,\ndeveloping search products to steer users to produce searchable content and\nengage in content discovery. Their recently developed product search\nrecommendations are preformulated search queries recommended to users on\nvideos. However, TikTok provides limited transparency about how search\nrecommendations are generated and moderated, despite requirements under\nregulatory frameworks like the European Union's Digital Services Act. By\nsuggesting that the platform simply aggregates comments and common searches\nlinked to videos, it sidesteps responsibility and issues that arise from\ncontextually problematic recommendations, reigniting long-standing concerns\nabout platform liability and moderation. This position paper addresses the\nnovelty of search recommendations on TikTok by highlighting the challenges that\nthis feature poses for platform governance and offering a computational\nresearch agenda, drawing on preliminary qualitative analysis. It sets out the\nneed for transparency in platform documentation, data access and research to\nstudy search recommendations.\n","authors":["Taylor Annabell","Robert Gorwa","Rebecca Scharlach","Jacob van de Kerkhof","Thales Bertaglia"],"pdf_url":"https://arxiv.org/pdf/2505.08385v1.pdf","comment":"to appear in The 1st international Workshop on Computational\n  Approaches to Content Moderation and Platform Governance (COMPASS), held at\n  ICWSM 2025"},{"id":"http://arxiv.org/abs/2407.00104v2","updated":"2025-05-13T09:29:47Z","published":"2024-06-27T07:33:34Z","title":"Clinically inspired enhance Explainability and Interpretability of an\n  AI-Tool for BCC diagnosis based on expert annotation","summary":"  An AI tool has been developed to provide interpretable support for the\ndiagnosis of BCC via teledermatology, thus speeding up referrals and optimizing\nresource utilization. The interpretability is provided in two ways: on the one\nhand, the main BCC dermoscopic patterns are found in the image to justify the\nBCC/Non BCC classification. Secondly, based on the common visual XAI Grad-CAM,\na clinically inspired visual explanation is developed where the relevant\nfeatures for diagnosis are located. Since there is no established ground truth\nfor BCC dermoscopic features, a standard reference is inferred from the\ndiagnosis of four dermatologists using an Expectation Maximization (EM) based\nalgorithm. The results demonstrate significant improvements in classification\naccuracy and interpretability, positioning this approach as a valuable tool for\nearly BCC detection and referral to dermatologists. The BCC/non-BCC\nclassification achieved an accuracy rate of 90%. For Clinically-inspired XAI\nresults, the detection of BCC patterns useful to clinicians reaches 99%\naccuracy. As for the Clinically-inspired Visual XAI results, the mean of the\nGrad-CAM normalized value within the manually segmented clinical features is\n0.57, while outside this region it is 0.16. This indicates that the model\nstruggles to accurately identify the regions of the BCC patterns. These results\nprove the ability of the AI tool to provide a useful explanation.\n","authors":["Iván Matas","Carmen Serrano","Francisca Silva","Amalia Serrano","Tomás Toledo-Pastrana","Begoña Acha"],"pdf_url":"https://arxiv.org/pdf/2407.00104v2.pdf","comment":"8 pages, 4 figures, 4 tables, under review"},{"id":"http://arxiv.org/abs/2505.07577v2","updated":"2025-05-13T09:27:19Z","published":"2025-05-12T13:57:47Z","title":"From raw affiliations to organization identifiers","summary":"  Accurate affiliation matching, which links affiliation strings to\nstandardized organization identifiers, is critical for improving research\nmetadata quality, facilitating comprehensive bibliometric analyses, and\nsupporting data interoperability across scholarly knowledge bases. Existing\napproaches fail to handle the complexity of affiliation strings that often\ninclude mentions of multiple organizations or extraneous information. In this\npaper, we present AffRo, a novel approach designed to address these challenges,\nleveraging advanced parsing and disambiguation techniques. We also introduce\nAffRoDB, an expert-curated dataset to systematically evaluate affiliation\nmatching algorithms, ensuring robust benchmarking. Results demonstrate the\neffectiveness of AffRp in accurately identifying organizations from complex\naffiliation strings.\n","authors":["Myrto Kallipoliti","Serafeim Chatzopoulos","Miriam Baglioni","Eleni Adamidi","Paris Koloveas","Thanasis Vergoulis"],"pdf_url":"https://arxiv.org/pdf/2505.07577v2.pdf","comment":"16 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2408.10613v3","updated":"2025-05-13T05:08:02Z","published":"2024-08-20T07:48:19Z","title":"Task-level Distributionally Robust Optimization for Large Language\n  Model-based Dense Retrieval","summary":"  Large Language Model-based Dense Retrieval (LLM-DR) optimizes over numerous\nheterogeneous fine-tuning collections from different domains. However, the\ndiscussion about its training data distribution is still minimal. Previous\nstudies rely on empirically assigned dataset choices or sampling ratios, which\ninevitably lead to sub-optimal retrieval performances. In this paper, we\npropose a new task-level Distributionally Robust Optimization (tDRO) algorithm\nfor LLM-DR fine-tuning, targeted at improving the universal domain\ngeneralization ability by end-to-end reweighting the data distribution of each\ntask. The tDRO parameterizes the domain weights and updates them with scaled\ndomain gradients. The optimized weights are then transferred to the LLM-DR\nfine-tuning to train more robust retrievers. Experiments show optimal\nimprovements in large-scale retrieval benchmarks and reduce up to 30% dataset\nusage after applying our optimization algorithm with a series of\ndifferent-sized LLM-DR models.\n","authors":["Guangyuan Ma","Yongliang Ma","Xing Wu","Zhenpeng Su","Ming Zhou","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2408.10613v3.pdf","comment":"Accepted by AAAI25. Source code is available at\n  https://github.com/ma787639046/tdro"},{"id":"http://arxiv.org/abs/2501.02968v3","updated":"2025-05-13T02:16:51Z","published":"2025-01-06T12:24:57Z","title":"FlippedRAG: Black-Box Opinion Manipulation Adversarial Attacks to\n  Retrieval-Augmented Generation Models","summary":"  Retrieval-Augmented Generation (RAG) enriches LLMs by dynamically retrieving\nexternal knowledge, reducing hallucinations and satisfying real-time\ninformation needs. While existing research mainly targets RAG's performance and\nefficiency, emerging studies highlight critical security concerns. Yet, current\nadversarial approaches remain limited, mostly addressing white-box scenarios or\nheuristic black-box attacks without fully investigating vulnerabilities in the\nretrieval phase. Additionally, prior works mainly focus on factoid QA tasks,\ntheir attacks lack complexity and can be easily corrected by advanced LLMs. In\nthis paper, we investigate a more realistic and critical threat scenario:\nadversarial attacks intended for opinion manipulation against black-box RAG\nmodels, particularly on controversial topics. Specifically, we propose\nFlippedRAG, a transfer-based adversarial attack against black-box RAG systems.\nWe first demonstrate that the underlying retriever of a black-box RAG system\ncan be reverse-engineered, enabling us to train a surrogate retriever.\nLeveraging the surrogate retriever, we further craft target poisoning triggers,\naltering vary few documents to effectively manipulate both retrieval and\nsubsequent generation. Extensive empirical results show that FlippedRAG\nsubstantially outperforms baseline methods, improving the average attack\nsuccess rate by 16.7%. FlippedRAG achieves on average a 50% directional shift\nin the opinion polarity of RAG-generated responses, ultimately causing a\nnotable 20% shift in user cognition. Furthermore, we evaluate the performance\nof several potential defensive measures, concluding that existing mitigation\nstrategies remain insufficient against such sophisticated manipulation attacks.\nThese results highlight an urgent need for developing innovative defensive\nsolutions to ensure the security and trustworthiness of RAG systems.\n","authors":["Zhuo Chen","Jiawei Liu","Yuyang Gong","Miaokun Chen","Haotan Liu","Qikai Cheng","Fan Zhang","Wei Lu","Xiaozhong Liu","Xiaofeng Wang"],"pdf_url":"https://arxiv.org/pdf/2501.02968v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2407.13757"},{"id":"http://arxiv.org/abs/2405.13238v6","updated":"2025-05-13T02:16:29Z","published":"2024-05-21T22:53:00Z","title":"Enhancing User Interest based on Stream Clustering and Memory Networks\n  in Large-Scale Recommender Systems","summary":"  Recommender Systems (RSs) provide personalized recommendation service based\non user interest, which are widely used in various platforms. However, there\nare lots of users with sparse interest due to lacking consumption behaviors,\nwhich leads to poor recommendation results for them. This problem is widespread\nin large-scale RSs and is particularly difficult to address. To solve this\nchallenging problem, we propose an innovative solution called User Interest\nEnhancement (UIE). UIE enhances user interest including user profile and user\nhistory behavior sequences by leveraging the enhancement vectors and\npersonalized enhancement vectors generated based on dynamic streaming\nclustering of similar users and items from multiple perspectives, which are\nstored and updated in memory networks. UIE not only remarkably improves model\nperformance for users with sparse interest, but also delivers notable gains for\nother users. As an end-to-end solution, UIE is easy to implement on top of\nexisting ranking models. Furthermore, we extend our approach to long-tail items\nusing similar methods, which also yields excellent improvements. We conduct\nextensive offline and online experiments in an industrial RS. The results\ndemonstrate that UIE substantially outperforms other existing approaches,\nespecially for users with sparse interest. UIE has been deployed in several\nlarge-scale RSs at Tencent since 2022, which was made public on 21 May 2024. In\naddition, UIE-based methods have also been successfully applied in candidate\ngeneration, pre-ranking, and context-DNN stages. Multiple teams have developed\nsolutions based on UIE, focusing on updating clustering algorithms and\nattention mechanisms. As far as we know, UIE has been applied in multiple RSs,\nadvertising systems and search engines. The thoughts of UIE, dynamic streaming\nclustering and similarity enhancement, have inspired subsequent relevant works.\n","authors":["Peng Liu","Nian Wang","Cong Xu","Ming Zhao","Bin Wang","Yi Ren"],"pdf_url":"https://arxiv.org/pdf/2405.13238v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08157v1","updated":"2025-05-13T01:30:27Z","published":"2025-05-13T01:30:27Z","title":"Hyperbolic Contrastive Learning with Model-augmentation for\n  Knowledge-aware Recommendation","summary":"  Benefiting from the effectiveness of graph neural networks (GNNs) and\ncontrastive learning, GNN-based contrastive learning has become mainstream for\nknowledge-aware recommendation. However, most existing contrastive\nlearning-based methods have difficulties in effectively capturing the\nunderlying hierarchical structure within user-item bipartite graphs and\nknowledge graphs. Moreover, they commonly generate positive samples for\ncontrastive learning by perturbing the graph structure, which may lead to a\nshift in user preference learning. To overcome these limitations, we propose\nhyperbolic contrastive learning with model-augmentation for knowledge-aware\nrecommendation. To capture the intrinsic hierarchical graph structures, we\nfirst design a novel Lorentzian knowledge aggregation mechanism, which enables\nmore effective representations of users and items. Then, we propose three\nmodel-level augmentation techniques to assist Hyperbolic contrastive learning.\nDifferent from the classical structure-level augmentation (e.g., edge\ndropping), the proposed model-augmentations can avoid preference shifts between\nthe augmented positive pair. Finally, we conduct extensive experiments to\ndemonstrate the superiority (maximum improvement of $11.03\\%$) of proposed\nmethods over existing baselines.\n","authors":["Shengyin Sun","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2505.08157v1.pdf","comment":"18 pages"}]},"2025-05-20T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2409.15250v3","updated":"2025-05-20T17:23:45Z","published":"2024-09-23T17:47:59Z","title":"ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models","summary":"  Recent progress in large language models and access to large-scale robotic\ndatasets has sparked a paradigm shift in robotics models transforming them into\ngeneralists able to adapt to various tasks, scenes, and robot modalities. A\nlarge step for the community are open Vision Language Action models which\nshowcase strong performance in a wide variety of tasks. In this work, we study\nthe visual generalization capabilities of three existing robotic foundation\nmodels, and propose a corresponding evaluation framework. Our study shows that\nthe existing models do not exhibit robustness to visual out-of-domain\nscenarios. This is potentially caused by limited variations in the training\ndata and/or catastrophic forgetting, leading to domain limitations in the\nvision foundation models. We further explore OpenVLA, which uses two\npre-trained vision foundation models and is, therefore, expected to generalize\nto out-of-domain experiments. However, we showcase catastrophic forgetting by\nDINO-v2 in OpenVLA through its failure to fulfill the task of depth regression.\nTo overcome the aforementioned issue of visual catastrophic forgetting, we\npropose a gradual backbone reversal approach founded on model merging. This\nenables OpenVLA -- which requires the adaptation of the visual backbones during\ninitial training -- to regain its visual generalization ability. Regaining this\ncapability enables our ReVLA model to improve over OpenVLA by a factor of 77\\%\nand 66\\% for grasping and lifting in visual OOD tasks. Comprehensive\nevaluations, episode rollouts and model weights are available on the ReVLA Page\n","authors":["Sombit Dey","Jan-Nico Zaech","Nikolay Nikolov","Luc Van Gool","Danda Pani Paudel"],"pdf_url":"https://arxiv.org/pdf/2409.15250v3.pdf","comment":"Accepted at ICRA-2025, Atlanta"},{"id":"http://arxiv.org/abs/2406.14761v2","updated":"2025-05-20T17:21:03Z","published":"2024-06-20T22:22:28Z","title":"Diffusion-Based Failure Sampling for Evaluating Safety-Critical\n  Autonomous Systems","summary":"  Validating safety-critical autonomous systems in high-dimensional domains\nsuch as robotics presents a significant challenge. Existing black-box\napproaches based on Markov chain Monte Carlo may require an enormous number of\nsamples, while methods based on importance sampling often rely on simple\nparametric families that may struggle to represent the distribution over\nfailures. We propose to sample the distribution over failures using a\nconditional denoising diffusion model, which has shown success in complex\nhigh-dimensional problems such as robotic task planning. We iteratively train a\ndiffusion model to produce state trajectories closer to failure. We demonstrate\nthe effectiveness of our approach on high-dimensional robotic validation tasks,\nimproving sample efficiency and mode coverage compared to existing black-box\ntechniques.\n","authors":["Harrison Delecki","Marc R. Schlichting","Mansur Arief","Anthony Corso","Marcell Vazquez-Chanlatte","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2406.14761v2.pdf","comment":"Appears in IEEE International Conference on Engineering Reliable\n  Autonomous Systems (ERAS) 2025"},{"id":"http://arxiv.org/abs/2505.14580v1","updated":"2025-05-20T16:38:00Z","published":"2025-05-20T16:38:00Z","title":"Traversability-aware path planning in dynamic environments","summary":"  Planning in environments with moving obstacles remains a significant\nchallenge in robotics. While many works focus on navigation and path planning\nin obstacle-dense spaces, traversing such congested regions is often avoidable\nby selecting alternative routes. This paper presents Traversability-aware FMM\n(Tr-FMM), a path planning method that computes paths in dynamic environments,\navoiding crowded regions. The method operates in two steps: first, it\ndiscretizes the environment, identifying regions and their distribution;\nsecond, it computes the traversability of regions, aiming to minimize both\nobstacle risks and goal deviation. The path is then computed by propagating the\nwavefront through regions with higher traversability. Simulated and real-world\nexperiments demonstrate that the approach enhances significant safety by\nkeeping the robot away from regions with obstacles while reducing unnecessary\ndeviations from the goal.\n","authors":["Yaroslav Marchukov","Luis Montano"],"pdf_url":"https://arxiv.org/pdf/2505.14580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14526v1","updated":"2025-05-20T15:48:23Z","published":"2025-05-20T15:48:23Z","title":"NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based\n  Autonomous Navigation","summary":"  Autonomous robots must navigate and operate in diverse environments, from\nterrestrial and aquatic settings to aerial and space domains. While\nReinforcement Learning (RL) has shown promise in training policies for specific\nautonomous robots, existing benchmarks are often constrained to unique\nplatforms, limiting generalization and fair comparisons across different\nmobility systems. In this paper, we present NavBench, a multi-domain benchmark\nfor training and evaluating RL-based navigation policies across diverse robotic\nplatforms and operational environments. Built on IsaacLab, our framework\nstandardizes task definitions, enabling different robots to tackle various\nnavigation challenges without the need for ad-hoc task redesigns or custom\nevaluation metrics. Our benchmark addresses three key challenges: (1) Unified\ncross-medium benchmarking, enabling direct evaluation of diverse actuation\nmethods (thrusters, wheels, water-based propulsion) in realistic environments;\n(2) Scalable and modular design, facilitating seamless robot-task\ninterchangeability and reproducible training pipelines; and (3) Robust\nsim-to-real validation, demonstrated through successful policy transfer to\nmultiple real-world robots, including a satellite robotic simulator, an\nunmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency\nbetween simulation and real-world deployment, NavBench simplifies the\ndevelopment of adaptable RL-based navigation strategies. Its modular design\nallows researchers to easily integrate custom robots and tasks by following the\nframework's predefined templates, making it accessible for a wide range of\napplications. Our code is publicly available at NavBench.\n","authors":["Matteo El-Hariry","Antoine Richard","Ricard M. Castan","Luis F. W. Batista","Matthieu Geist","Cedric Pradalier","Miguel Olivares-Mendez"],"pdf_url":"https://arxiv.org/pdf/2505.14526v1.pdf","comment":"Submitted for publication. Under review (2025)"},{"id":"http://arxiv.org/abs/2505.14486v1","updated":"2025-05-20T15:17:03Z","published":"2025-05-20T15:17:03Z","title":"Robust Immersive Bilateral Teleoperation of Dissimilar Systems with\n  Enhanced Transparency and Sense of Embodiment","summary":"  In human-in-the-loop systems such as teleoperation, especially those\ninvolving heavy-duty manipulators, achieving high task performance requires\nboth robust control and strong human engagement. This paper presents a\nbilateral teleoperation framework that enhances the operator's Sense of\nEmbodiment (SoE), specifically, the senses of agency and self-location, through\nan immersive virtual reality interface and distributed haptic feedback via an\nexoskeleton. To support this embodiment and stablish high level of motion and\nforce transparency, we develop a force-sensorless, robust control architecture\nthat tackles input nonlinearities, master-slave asymmetries, unknown\nuncertainties, and arbitrary time delays. A human-robot augmented dynamic model\nis integrated into the control loop to enhance human-adaptability of the\ncontroller. Theoretical analysis confirms semi-global uniform ultimate\nboundedness of the closed-loop system. Extensive real-world experiments\ndemonstrate high accuracy tracking under up to 1:13 motion scaling and 1:1000\nforce scaling, showcasing the significance of the results. Additionally, the\nstability-transparency tradeoff for motion tracking and force\nreflection-tracking is establish up to 150 ms of one-way fix and time-varying\ncommunication delay. The results of user study with 10 participants (9 male and\n1 female) demonstrated that the system can imply a good level of SoE (76.4%),\nat the same time is very user friendly with no gender limitation. These results\nare significant given the scale and weight of the heavy-duty manipulators.\n","authors":["Mahdi Hejrati","Jouni Mattila"],"pdf_url":"https://arxiv.org/pdf/2505.14486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14440v2","updated":"2025-05-20T15:16:55Z","published":"2025-04-20T01:22:40Z","title":"SG-Reg: Generalizable and Efficient Scene Graph Registration","summary":"  This paper addresses the challenges of registering two rigid semantic scene\ngraphs, an essential capability when an autonomous agent needs to register its\nmap against a remote agent, or against a prior map. The hand-crafted\ndescriptors in classical semantic-aided registration, or the ground-truth\nannotation reliance in learning-based scene graph registration, impede their\napplication in practical real-world environments. To address the challenges, we\ndesign a scene graph network to encode multiple modalities of semantic nodes:\nopen-set semantic feature, local topology with spatial awareness, and shape\nfeature. These modalities are fused to create compact semantic node features.\nThe matching layers then search for correspondences in a coarse-to-fine manner.\nIn the back-end, we employ a robust pose estimator to decide transformation\naccording to the correspondences. We manage to maintain a sparse and\nhierarchical scene representation. Our approach demands fewer GPU resources and\nfewer communication bandwidth in multi-agent tasks. Moreover, we design a new\ndata generation approach using vision foundation models and a semantic mapping\nmodule to reconstruct semantic scene graphs. It differs significantly from\nprevious works, which rely on ground-truth semantic annotations to generate\ndata. We validate our method in a two-agent SLAM benchmark. It significantly\noutperforms the hand-crafted baseline in terms of registration success rate.\nCompared to visual loop closure networks, our method achieves a slightly higher\nregistration recall while requiring only 52 KB of communication bandwidth for\neach query frame. Code available at:\n\\href{http://github.com/HKUST-Aerial-Robotics/SG-Reg}{http://github.com/HKUST-Aerial-Robotics/SG-Reg}.\n","authors":["Chuhao Liu","Zhijian Qiao","Jieqi Shi","Ke Wang","Peize Liu","Shaojie Shen"],"pdf_url":"https://arxiv.org/pdf/2504.14440v2.pdf","comment":"IEEE Transactions Robotics Regular Paper"},{"id":"http://arxiv.org/abs/2505.13431v2","updated":"2025-05-20T15:08:55Z","published":"2025-05-19T17:55:28Z","title":"A Practical Guide for Incorporating Symmetry in Diffusion Policy","summary":"  Recently, equivariant neural networks for policy learning have shown\npromising improvements in sample efficiency and generalization, however, their\nwide adoption faces substantial barriers due to implementation complexity.\nEquivariant architectures typically require specialized mathematical\nformulations and custom network design, posing significant challenges when\nintegrating with modern policy frameworks like diffusion-based models. In this\npaper, we explore a number of straightforward and practical approaches to\nincorporate symmetry benefits into diffusion policies without the overhead of\nfull equivariant designs. Specifically, we investigate (i) invariant\nrepresentations via relative trajectory actions and eye-in-hand perception,\n(ii) integrating equivariant vision encoders, and (iii) symmetric feature\nextraction with pretrained encoders using Frame Averaging. We first prove that\ncombining eye-in-hand perception with relative or delta action parameterization\nyields inherent SE(3)-invariance, thus improving policy generalization. We then\nperform a systematic experimental study on those design choices for integrating\nsymmetry in diffusion policies, and conclude that an invariant representation\nwith equivariant feature extraction significantly improves the policy\nperformance. Our method achieves performance on par with or exceeding fully\nequivariant architectures while greatly simplifying implementation.\n","authors":["Dian Wang","Boce Hu","Shuran Song","Robin Walters","Robert Platt"],"pdf_url":"https://arxiv.org/pdf/2505.13431v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14443v1","updated":"2025-05-20T14:45:16Z","published":"2025-05-20T14:45:16Z","title":"Semantically-driven Deep Reinforcement Learning for Inspection Path\n  Planning","summary":"  This paper introduces a novel semantics-aware inspection planning policy\nderived through deep reinforcement learning. Reflecting the fact that within\nautonomous informative path planning missions in unknown environments, it is\noften only a sparse set of objects of interest that need to be inspected, the\nmethod contributes an end-to-end policy that simultaneously performs semantic\nobject visual inspection combined with collision-free navigation. Assuming\naccess only to the instantaneous depth map, the associated segmentation image,\nthe ego-centric local occupancy, and the history of past positions in the\nrobot's neighborhood, the method demonstrates robust generalizability and\nsuccessful crossing of the sim2real gap. Beyond simulations and extensive\ncomparison studies, the approach is verified in experimental evaluations\nonboard a flying robot deployed in novel environments with previously unseen\nsemantics and overall geometric configurations.\n","authors":["Grzegorz Malczyk","Mihir Kulkarni","Kostas Alexis"],"pdf_url":"https://arxiv.org/pdf/2505.14443v1.pdf","comment":"Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)"},{"id":"http://arxiv.org/abs/2412.05873v3","updated":"2025-05-20T14:18:41Z","published":"2024-12-08T09:37:32Z","title":"AC-LIO: Towards Asymptotic Compensation for Distortion in LiDAR-Inertial\n  Odometry via Selective Intra-Frame Smoothing","summary":"  Existing LiDAR-Inertial Odometry (LIO) methods typically utilize the prior\ntrajectory derived from the IMU integration to compensate for the motion\ndistortion within LiDAR frames. However, discrepancies between the prior and\ntrue trajectory can lead to residual motion distortions that compromise the\nconsistency of LiDAR frame with its corresponding geometric environment. This\nimbalance may result in pointcloud registration becoming trapped in local\noptima, thereby exacerbating drift during long-term and large-scale\nlocalization. To this end, we propose a novel LIO framework with selective\nintra-frame smoothing dubbed AC-LIO. Our core idea is to asymptotically\nbackpropagate current update term and compensate for residual motion distortion\nunder the guidance of convergence criteria, aiming to improve the accuracy of\ndiscrete-state LIO system with minimal computational increase. Extensive\nexperiments demonstrate that our AC-LIO framework further enhances odometry\naccuracy compared to prior arts, with about 30.4% reduction in average RMSE\nover the second best result, leading to marked improvements in the accuracy of\nlong-term and large-scale localization and mapping.\n","authors":["Tianxiang Zhang","Xuanxuan Zhang","Wenlei Fan","Xin Xia","Huai Yu","Lin Wang","You Li"],"pdf_url":"https://arxiv.org/pdf/2412.05873v3.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2409.07107v2","updated":"2025-05-20T14:18:24Z","published":"2024-09-11T08:55:40Z","title":"End-to-End and Highly-Efficient Differentiable Simulation for Robotics","summary":"  Over the past few years, robotics simulators have largely improved in\nefficiency and scalability, enabling them to generate years of simulated data\nin a few hours. Yet, efficiently and accurately computing the simulation\nderivatives remains an open challenge, with potentially high gains on the\nconvergence speed of reinforcement learning and trajectory optimization\nalgorithms, especially for problems involving physical contact interactions.\nThis paper contributes to this objective by introducing a unified and efficient\nalgorithmic solution for computing the analytical derivatives of robotic\nsimulators. The approach considers both the collision and frictional stages,\naccounting for their intrinsic nonsmoothness and also exploiting the sparsity\ninduced by the underlying multibody systems. These derivatives have been\nimplemented in C++, and the code will be open-sourced in the Simple simulator.\nThey depict state-of-the-art timings ranging from 5 microseconds for a 7-dof\nmanipulator up to 95 microseconds for 36-dof humanoid, outperforming\nalternative solutions by a factor of at least 100.\n","authors":["Quentin Le Lidec","Louis Montaut","Yann de Mont-Marin","Fabian Schramm","Justin Carpentier"],"pdf_url":"https://arxiv.org/pdf/2409.07107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14366v1","updated":"2025-05-20T13:49:09Z","published":"2025-05-20T13:49:09Z","title":"Towards Embodied Cognition in Robots via Spatially Grounded Synthetic\n  Worlds","summary":"  We present a conceptual framework for training Vision-Language Models (VLMs)\nto perform Visual Perspective Taking (VPT), a core capability for embodied\ncognition essential for Human-Robot Interaction (HRI). As a first step toward\nthis goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,\nthat enables supervised learning for spatial reasoning tasks. Each instance\nincludes an RGB image, a natural language description, and a ground-truth 4X4\ntransformation matrix representing object pose. We focus on inferring Z-axis\ndistance as a foundational skill, with future extensions targeting full 6\nDegrees Of Freedom (DOFs) reasoning. The dataset is publicly available to\nsupport further research. This work serves as a foundational step toward\nembodied AI systems capable of spatial understanding in interactive human-robot\nscenarios.\n","authors":["Joel Currie","Gioele Migno","Enrico Piacenti","Maria Elena Giannaccini","Patric Bach","Davide De Tommaso","Agnieszka Wykowska"],"pdf_url":"https://arxiv.org/pdf/2505.14366v1.pdf","comment":"Accepted to: Intelligent Autonomous Systems (IAS) 2025 as Late\n  Breaking Report"},{"id":"http://arxiv.org/abs/2505.14337v1","updated":"2025-05-20T13:23:46Z","published":"2025-05-20T13:23:46Z","title":"Local Minima Prediction using Dynamic Bayesian Filtering for UGV\n  Navigation in Unstructured Environments","summary":"  Path planning is crucial for the navigation of autonomous vehicles, yet these\nvehicles face challenges in complex and real-world environments. Although a\nglobal view may be provided, it is often outdated, necessitating the reliance\nof Unmanned Ground Vehicles (UGVs) on real-time local information. This\nreliance on partial information, without considering the global context, can\nlead to UGVs getting stuck in local minima. This paper develops a method to\nproactively predict local minima using Dynamic Bayesian filtering, based on the\ndetected obstacles in the local view and the global goal. This approach aims to\nenhance the autonomous navigation of self-driving vehicles by allowing them to\npredict potential pitfalls before they get stuck, and either ask for help from\na human, or re-plan an alternate trajectory.\n","authors":["Seung Hun Lee","Wonse Jo","Lionel P. Robert Jr.","Dawn M. Tilbury"],"pdf_url":"https://arxiv.org/pdf/2505.14337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14266v1","updated":"2025-05-20T12:16:26Z","published":"2025-05-20T12:16:26Z","title":"Sampling-Based System Identification with Active Exploration for Legged\n  Robot Sim2Real Learning","summary":"  Sim-to-real discrepancies hinder learning-based policies from achieving\nhigh-precision tasks in the real world. While Domain Randomization (DR) is\ncommonly used to bridge this gap, it often relies on heuristics and can lead to\noverly conservative policies with degrading performance when not properly\ntuned. System Identification (Sys-ID) offers a targeted approach, but standard\ntechniques rely on differentiable dynamics and/or direct torque measurement,\nassumptions that rarely hold for contact-rich legged systems. To this end, we\npresent SPI-Active (Sampling-based Parameter Identification with Active\nExploration), a two-stage framework that estimates physical parameters of\nlegged robots to minimize the sim-to-real gap. SPI-Active robustly identifies\nkey physical parameters through massive parallel sampling, minimizing state\nprediction errors between simulated and real-world trajectories. To further\nimprove the informativeness of collected data, we introduce an active\nexploration strategy that maximizes the Fisher Information of the collected\nreal-world trajectories via optimizing the input commands of an exploration\npolicy. This targeted exploration leads to accurate identification and better\ngeneralization across diverse tasks. Experiments demonstrate that SPI-Active\nenables precise sim-to-real transfer of learned policies to the real world,\noutperforming baselines by 42-63% in various locomotion tasks.\n","authors":["Nikhil Sobanbabu","Guanqi He","Tairan He","Yuxiang Yang","Guanya Shi"],"pdf_url":"https://arxiv.org/pdf/2505.14266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09382v4","updated":"2025-05-20T11:51:50Z","published":"2024-02-14T18:26:58Z","title":"Safe Distributed Control of Multi-Robot Systems with Communication\n  Delays","summary":"  Safe operation of multi-robot systems is critical, especially in\ncommunication-degraded environments such as underwater for seabed mapping,\nunderground caves for navigation, and in extraterrestrial missions for assembly\nand construction. We address safety of networked autonomous systems where the\ninformation exchanged between robots incurs communication delays. We formalize\na notion of distributed control barrier function for multi-robot systems, a\nsafety certificate amenable to a distributed implementation, which provides\nformal ground to using graph neural networks to learn safe distributed\ncontrollers. Further, we observe that learning a distributed controller\nignoring delays can severely degrade safety. We finally propose a\npredictor-based framework to train a safe distributed controller under\ncommunication delays, where the current state of nearby robots is predicted\nfrom received data and age-of-information. Numerical experiments on multi-robot\ncollision avoidance show that our predictor-based approach can significantly\nimprove the safety of a learned distributed controller under communication\ndelays. A video abstract is available at https://youtu.be/Hcu1Ri32Spk.\n","authors":["Luca Ballotta","Rajat Talak"],"pdf_url":"https://arxiv.org/pdf/2402.09382v4.pdf","comment":"Copyright (c) 2025 IEEE. Personal use of this material is permitted.\n  However, permission to use this material for any other purposes must be\n  obtained from the IEEE by sending a request to pubs-permissions@ieee.org"},{"id":"http://arxiv.org/abs/2505.14159v1","updated":"2025-05-20T10:13:00Z","published":"2025-05-20T10:13:00Z","title":"M3Depth: Wavelet-Enhanced Depth Estimation on Mars via Mutual Boosting\n  of Dual-Modal Data","summary":"  Depth estimation plays a great potential role in obstacle avoidance and\nnavigation for further Mars exploration missions. Compared to traditional\nstereo matching, learning-based stereo depth estimation provides a data-driven\napproach to infer dense and precise depth maps from stereo image pairs.\nHowever, these methods always suffer performance degradation in environments\nwith sparse textures and lacking geometric constraints, such as the\nunstructured terrain of Mars. To address these challenges, we propose M3Depth,\na depth estimation model tailored for Mars rovers. Considering the sparse and\nsmooth texture of Martian terrain, which is primarily composed of low-frequency\nfeatures, our model incorporates a convolutional kernel based on wavelet\ntransform that effectively captures low-frequency response and expands the\nreceptive field. Additionally, we introduce a consistency loss that explicitly\nmodels the complementary relationship between depth map and surface normal map,\nutilizing the surface normal as a geometric constraint to enhance the accuracy\nof depth estimation. Besides, a pixel-wise refinement module with mutual\nboosting mechanism is designed to iteratively refine both depth and surface\nnormal predictions. Experimental results on synthetic Mars datasets with depth\nannotations show that M3Depth achieves a significant 16% improvement in depth\nestimation accuracy compared to other state-of-the-art methods in depth\nestimation. Furthermore, the model demonstrates strong applicability in\nreal-world Martian scenarios, offering a promising solution for future Mars\nexploration missions.\n","authors":["Junjie Li","Jiawei Wang","Miyu Li","Yu Liu","Yumei Wang","Haitao Xu"],"pdf_url":"https://arxiv.org/pdf/2505.14159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10973v2","updated":"2025-05-20T09:59:57Z","published":"2025-05-16T08:17:01Z","title":"GRoQ-Loco: Generalist and Robot-agnostic Quadruped Locomotion Control\n  using Offline Datasets","summary":"  Recent advancements in large-scale offline training have demonstrated the\npotential of generalist policy learning for complex robotic tasks. However,\napplying these principles to legged locomotion remains a challenge due to\ncontinuous dynamics and the need for real-time adaptation across diverse\nterrains and robot morphologies. In this work, we propose GRoQ-Loco, a\nscalable, attention-based framework that learns a single generalist locomotion\npolicy across multiple quadruped robots and terrains, relying solely on offline\ndatasets. Our approach leverages expert demonstrations from two distinct\nlocomotion behaviors - stair traversal (non-periodic gaits) and flat terrain\ntraversal (periodic gaits) - collected across multiple quadruped robots, to\ntrain a generalist model that enables behavior fusion for both behaviors.\nCrucially, our framework operates directly on proprioceptive data from all\nrobots without incorporating any robot-specific encodings. The policy is\ndirectly deployable on an Intel i7 nuc, producing low-latency control outputs\nwithout any test-time optimization. Our extensive experiments demonstrate\nstrong zero-shot transfer across highly diverse quadruped robots and terrains,\nincluding hardware deployment on the Unitree Go1, a commercially available 12kg\nrobot. Notably, we evaluate challenging cross-robot training setups where\ndifferent locomotion skills are unevenly distributed across robots, yet observe\nsuccessful transfer of both flat walking and stair traversal behaviors to all\nrobots at test time. We also show preliminary walking on Stoch 5, a 70kg\nquadruped, on flat and outdoor terrains without requiring any fine tuning.\nThese results highlight the potential for robust generalist locomotion across\ndiverse robots and terrains.\n","authors":["Narayanan PP","Sarvesh Prasanth Venkatesan","Srinivas Kantha Reddy","Shishir Kolathaya"],"pdf_url":"https://arxiv.org/pdf/2505.10973v2.pdf","comment":"18pages, 16figures, 6tables"},{"id":"http://arxiv.org/abs/2410.15879v2","updated":"2025-05-20T09:53:08Z","published":"2024-10-21T10:59:27Z","title":"Triplane Grasping: Efficient 6-DoF Grasping with Single RGB Images","summary":"  Reliable object grasping is one of the fundamental tasks in robotics.\nHowever, determining grasping pose based on single-image input has long been a\nchallenge due to limited visual information and the complexity of real-world\nobjects. In this paper, we propose Triplane Grasping, a fast grasping\ndecision-making method that relies solely on a single RGB-only image as input.\nTriplane Grasping creates a hybrid Triplane-Gaussian 3D representation through\na point decoder and a triplane decoder, which produce an efficient and\nhigh-quality reconstruction of the object to be grasped to meet real-time\ngrasping requirements. We propose to use an end-to-end network to generate\n6-DoF parallel-jaw grasp distributions directly from 3D points in the point\ncloud as potential grasp contacts and anchor the grasp pose in the observed\ndata. Experiments on the OmniObject3D and GraspNet-1Billion datasets\ndemonstrate that our method achieves rapid modeling and grasping pose\ndecision-making for daily objects, and strong generalization capability.\n","authors":["Yiming Li","Hanchi Ren","Yue Yang","Jingjing Deng","Xianghua Xie"],"pdf_url":"https://arxiv.org/pdf/2410.15879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14139v1","updated":"2025-05-20T09:43:05Z","published":"2025-05-20T09:43:05Z","title":"FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning","summary":"  The use of guidance to steer sampling toward desired outcomes has been widely\nexplored within diffusion models, especially in applications such as image and\ntrajectory generation. However, incorporating guidance during training remains\nrelatively underexplored. In this work, we introduce energy-guided flow\nmatching, a novel approach that enhances the training of flow models and\neliminates the need for guidance at inference time. We learn a conditional\nvelocity field corresponding to the flow policy by approximating an\nenergy-guided probability path as a Gaussian path. Learning guided trajectories\nis appealing for tasks where the target distribution is defined by a\ncombination of data and an energy function, as in reinforcement learning.\nDiffusion-based policies have recently attracted attention for their expressive\npower and ability to capture multi-modal action distributions. Typically, these\npolicies are optimized using weighted objectives or by back-propagating\ngradients through actions sampled by the policy. As an alternative, we propose\nFlowQ, an offline reinforcement learning algorithm based on energy-guided flow\nmatching. Our method achieves competitive performance while the policy training\ntime is constant in the number of flow sampling steps.\n","authors":["Marvin Alles","Nutan Chen","Patrick van der Smagt","Botond Cseke"],"pdf_url":"https://arxiv.org/pdf/2505.14139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15863v2","updated":"2025-05-20T09:42:11Z","published":"2024-10-21T10:43:49Z","title":"Task-oriented Robotic Manipulation with Vision Language Models","summary":"  Vision Language Models (VLMs) play a crucial role in robotic manipulation by\nenabling robots to understand and interpret the visual properties of objects\nand their surroundings, allowing them to perform manipulation based on this\nmultimodal understanding. Accurately understanding spatial relationships\nremains a non-trivial challenge, yet it is essential for effective robotic\nmanipulation. In this work, we introduce a novel framework that integrates VLMs\nwith a structured spatial reasoning pipeline to perform object manipulation\nbased on high-level, task-oriented input. Our approach is the transformation of\nvisual scenes into tree-structured representations that encode the spatial\nrelations. These trees are subsequently processed by a Large Language Model\n(LLM) to infer restructured configurations that determine how these objects\nshould be organised for a given high-level task. To support our framework, we\nalso present a new dataset containing manually annotated captions that describe\nspatial relations among objects, along with object-level attribute annotations\nsuch as fragility, mass, material, and transparency. We demonstrate that our\nmethod not only improves the comprehension of spatial relationships among\nobjects in the visual environment but also enables robots to interact with\nthese objects more effectively. As a result, this approach significantly\nenhances spatial reasoning in robotic manipulation tasks. To our knowledge,\nthis is the first method of its kind in the literature, offering a novel\nsolution that allows robots to more efficiently organize and utilize objects in\ntheir surroundings.\n","authors":["Nurhan Bulus Guran","Hanchi Ren","Jingjing Deng","Xianghua Xie"],"pdf_url":"https://arxiv.org/pdf/2410.15863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14129v1","updated":"2025-05-20T09:34:38Z","published":"2025-05-20T09:34:38Z","title":"Unconventional Hexacopters via Evolution and Learning: Performance Gains\n  and New Insights","summary":"  Evolution and learning have historically been interrelated topics, and their\ninterplay is attracting increased interest lately. The emerging new factor in\nthis trend is morphological evolution, the evolution of physical forms within\nembodied AI systems such as robots. In this study, we investigate a system of\nhexacopter-type drones with evolvable morphologies and learnable controllers\nand make contributions to two fields. For aerial robotics, we demonstrate that\nthe combination of evolution and learning can deliver non-conventional drones\nthat significantly outperform the traditional hexacopter on several tasks that\nare more complex than previously considered in the literature. For the field of\nEvolutionary Computing, we introduce novel metrics and perform new analyses\ninto the interaction of morphological evolution and learning, uncovering\nhitherto unidentified effects. Our analysis tools are domain-agnostic, making a\nmethodological contribution towards building solid foundations for embodied AI\nsystems that integrate evolution and learning.\n","authors":["Jed Muff","Keiichi Ito","Elijah H. W. Ang","Karine Miras","A. E. Eiben"],"pdf_url":"https://arxiv.org/pdf/2505.14129v1.pdf","comment":"16 pages, 14 figures, currently under review"},{"id":"http://arxiv.org/abs/2505.11886v2","updated":"2025-05-20T08:51:38Z","published":"2025-05-17T07:34:56Z","title":"Aux-Think: Exploring Reasoning Strategies for Data-Efficient\n  Vision-Language Navigation","summary":"  Vision-Language Navigation (VLN) is a critical task for developing embodied\nagents that can follow natural language instructions to navigate in complex\nreal-world environments. Recent advances in VLN by large pretrained models have\nsignificantly improved generalization and instruction grounding compared to\ntraditional approaches. However, the role of reasoning strategies in\nnavigation-an action-centric, long-horizon task-remains underexplored, despite\nChain-of-Thought (CoT) reasoning's demonstrated success in static tasks like\nvisual question answering. To address this gap, we conduct the first systematic\nevaluation of reasoning strategies for VLN, including No-Think (direct action\nprediction), Pre-Think (reason before action), and Post-Think (reason after\naction). Surprisingly, our findings reveal the Inference-time Reasoning\nCollapse issue, where inference-time reasoning degrades navigation accuracy,\nhighlighting the challenges of integrating reasoning into VLN. Based on this\ninsight, we propose Aux-Think, a framework that trains models to internalize\nstructured reasoning patterns through CoT supervision, while inferring action\ndirectly without reasoning in online prediction. To support this framework, we\nrelease R2R-CoT-320k, the first Chain-of-Thought annotated dataset for VLN.\nExtensive experiments show that Aux-Think reduces training effort greatly and\nachieves the best performance under the same data scale.\n","authors":["Shuo Wang","Yongcai Wang","Wanting Li","Xudong Cai","Yucheng Wang","Maiyue Chen","Kaihui Wang","Zhizhong Su","Deying Li","Zhaoxin Fan"],"pdf_url":"https://arxiv.org/pdf/2505.11886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14191v2","updated":"2025-05-20T07:55:44Z","published":"2024-10-18T05:55:28Z","title":"A Probabilistic Model for Skill Acquisition with Switching Latent\n  Feedback Controllers","summary":"  Manipulation tasks often consist of subtasks, each representing a distinct\nskill. Mastering these skills is essential for robots, as it enhances their\nautonomy, efficiency, adaptability, and ability to work in their environment.\nLearning from demonstrations allows robots to rapidly acquire new skills\nwithout starting from scratch, with demonstrations typically sequencing skills\nto achieve tasks. Behaviour cloning approaches to learning from demonstration\ncommonly rely on mixture density network output heads to predict robot actions.\nIn this work, we first reinterpret the mixture density network as a library of\nfeedback controllers (or skills) conditioned on latent states. This arises from\nthe observation that a one-layer linear network is functionally equivalent to a\nclassical feedback controller, with network weights corresponding to controller\ngains. We use this insight to derive a probabilistic graphical model that\ncombines these elements, describing the skill acquisition process as\nsegmentation in a latent space, where each skill policy functions as a feedback\ncontrol law in this latent space. Our approach significantly improves not only\ntask success rate, but also robustness to observation noise when trained with\nhuman demonstrations. Our physical robot experiments further show that the\ninduced robustness improves model deployment on robots.\n","authors":["Juyan Zhang","Dana Kulic","Michael Burke"],"pdf_url":"https://arxiv.org/pdf/2410.14191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14053v1","updated":"2025-05-20T07:55:36Z","published":"2025-05-20T07:55:36Z","title":"On-Demand Scenario Generation for Testing Automated Driving Systems","summary":"  The safety and reliability of Automated Driving Systems (ADS) are paramount,\nnecessitating rigorous testing methodologies to uncover potential failures\nbefore deployment. Traditional testing approaches often prioritize either\nnatural scenario sampling or safety-critical scenario generation, resulting in\noverly simplistic or unrealistic hazardous tests. In practice, the demand for\nnatural scenarios (e.g., when evaluating the ADS's reliability in real-world\nconditions), critical scenarios (e.g., when evaluating safety in critical\nsituations), or somewhere in between (e.g., when testing the ADS in regions\nwith less civilized drivers) varies depending on the testing objectives. To\naddress this issue, we propose the On-demand Scenario Generation (OSG)\nFramework, which generates diverse scenarios with varying risk levels.\nAchieving the goal of OSG is challenging due to the complexity of quantifying\nthe criticalness and naturalness stemming from intricate vehicle-environment\ninteractions, as well as the need to maintain scenario diversity across various\nrisk levels. OSG learns from real-world traffic datasets and employs a Risk\nIntensity Regulator to quantitatively control the risk level. It also leverages\nan improved heuristic search method to ensure scenario diversity. We evaluate\nOSG on the Carla simulators using various ADSs. We verify OSG's ability to\ngenerate scenarios with different risk levels and demonstrate its necessity by\ncomparing accident types across risk levels. With the help of OSG, we are now\nable to systematically and objectively compare the performance of different\nADSs based on different risk levels.\n","authors":["Songyang Yan","Xiaodong Zhang","Kunkun Hao","haojie xin","Yonggang Luo","Jucheng Yang","Ming Fan","Chao Yang","Jun Sun","Zijiang Yang"],"pdf_url":"https://arxiv.org/pdf/2505.14053v1.pdf","comment":"20 pages, 9 figures. Accepted by FSE 2025"},{"id":"http://arxiv.org/abs/2505.12634v2","updated":"2025-05-20T07:47:35Z","published":"2025-05-19T02:39:18Z","title":"MSCEKF-MIO: Magnetic-Inertial Odometry Based on Multi-State Constraint\n  Extended Kalman Filter","summary":"  To overcome the limitation of existing indoor odometry technologies which\noften cannot simultaneously meet requirements for accuracy cost-effectiveness,\nand robustness-this paper proposes a novel magnetometer array-aided inertial\nodometry approach, MSCEKF-MIO (Multi-State Constraint Extended Kalman\nFilter-based Magnetic-Inertial Odometry). We construct a magnetic field model\nby fitting measurements from the magnetometer array and then use temporal\nvariations in this model-extracted from continuous observations-to estimate the\ncarrier's absolute velocity. Furthermore, we implement the MSCEKF framework to\nfuse observed magnetic field variations with position and attitude estimates\nfrom inertial navigation system (INS) integration, thereby enabling autonomous,\nhigh-precision indoor relative positioning. Experimental results demonstrate\nthat the proposed algorithm achieves superior velocity estimation accuracy and\nhorizontal positioning precision relative to state-of-the-art magnetic\narray-aided INS algorithms (MAINS). On datasets with trajectory lengths of\n150-250m, the proposed method yields an average horizontal position RMSE of\napproximately 2.5m. In areas with distinctive magnetic features, the\nmagneto-inertial odometry achieves a velocity estimation accuracy of 0.07m/s.\nConsequently, the proposed method offers a novel positioning solution\ncharacterized by low power consumption, cost-effectiveness, and high\nreliability in complex indoor environments.\n","authors":["Jiazhu Li","Jian Kuang","Xiaoji Niu"],"pdf_url":"https://arxiv.org/pdf/2505.12634v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2505.14030v1","updated":"2025-05-20T07:29:26Z","published":"2025-05-20T07:29:26Z","title":"AutoBio: A Simulation and Benchmark for Robotic Automation in Digital\n  Biology Laboratory","summary":"  Vision-language-action (VLA) models have shown promise as generalist robotic\npolicies by jointly leveraging visual, linguistic, and proprioceptive\nmodalities to generate action trajectories. While recent benchmarks have\nadvanced VLA research in domestic tasks, professional science-oriented domains\nremain underexplored. We introduce AutoBio, a simulation framework and\nbenchmark designed to evaluate robotic automation in biology laboratory\nenvironments--an application domain that combines structured protocols with\ndemanding precision and multimodal interaction. AutoBio extends existing\nsimulation capabilities through a pipeline for digitizing real-world laboratory\ninstruments, specialized physics plugins for mechanisms ubiquitous in\nlaboratory workflows, and a rendering stack that support dynamic instrument\ninterfaces and transparent materials through physically based rendering. Our\nbenchmark comprises biologically grounded tasks spanning three difficulty\nlevels, enabling standardized evaluation of language-guided robotic\nmanipulation in experimental protocols. We provide infrastructure for\ndemonstration generation and seamless integration with VLA models. Baseline\nevaluations with two SOTA VLA models reveal significant gaps in precision\nmanipulation, visual reasoning, and instruction following in scientific\nworkflows. By releasing AutoBio, we aim to catalyze research on generalist\nrobotic systems for complex, high-precision, and multimodal professional\nenvironments. The simulator and benchmark are publicly available to facilitate\nreproducible research.\n","authors":["Zhiqian Lan","Yuxuan Jiang","Ruiqi Wang","Xuanbing Xie","Rongkui Zhang","Yicheng Zhu","Peihang Li","Tianshuo Yang","Tianxing Chen","Haoyu Gao","Xiaokang Yang","Xuelong Li","Hongyuan Zhang","Yao Mu","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2505.14030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13982v1","updated":"2025-05-20T06:29:20Z","published":"2025-05-20T06:29:20Z","title":"Adaptive Visuo-Tactile Fusion with Predictive Force Attention for\n  Dexterous Manipulation","summary":"  Effectively utilizing multi-sensory data is important for robots to\ngeneralize across diverse tasks. However, the heterogeneous nature of these\nmodalities makes fusion challenging. Existing methods propose strategies to\nobtain comprehensively fused features but often ignore the fact that each\nmodality requires different levels of attention at different manipulation\nstages. To address this, we propose a force-guided attention fusion module that\nadaptively adjusts the weights of visual and tactile features without human\nlabeling. We also introduce a self-supervised future force prediction auxiliary\ntask to reinforce the tactile modality, improve data imbalance, and encourage\nproper adjustment. Our method achieves an average success rate of 93% across\nthree fine-grained, contactrich tasks in real-world experiments. Further\nanalysis shows that our policy appropriately adjusts attention to each modality\nat different manipulation stages. The videos can be viewed at\nhttps://adaptac-dex.github.io/.\n","authors":["Jinzhou Li","Tianhao Wu","Jiyao Zhang","Zeyuan Chen","Haotian Jin","Mingdong Wu","Yujun Shen","Yaodong Yang","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2505.13982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12827v2","updated":"2025-05-20T06:22:06Z","published":"2025-05-19T08:12:35Z","title":"Practical Equivalence Testing and Its Application in Synthetic Pre-Crash\n  Scenario Validation","summary":"  The use of representative pre-crash scenarios is critical for assessing the\nsafety impact of driving automation systems through simulation. However, a gap\nremains in the robust evaluation of the similarity between synthetic and\nreal-world pre-crash scenarios and their crash characteristics. Without proper\nvalidation, it cannot be ensured that the synthetic test scenarios adequately\nrepresent real-world driving behaviors and crash characteristics. One reason\nfor this validation gap is the lack of focus on methods to confirm that the\nsynthetic test scenarios are practically equivalent to real-world ones, given\nthe assessment scope. Traditional statistical methods, like significance\ntesting, focus on detecting differences rather than establishing equivalence;\nsince failure to detect a difference does not imply equivalence, they are of\nlimited applicability for validating synthetic pre-crash scenarios and crash\ncharacteristics. This study addresses this gap by proposing an equivalence\ntesting method based on the Bayesian Region of Practical Equivalence (ROPE)\nframework. This method is designed to assess the practical equivalence of\nscenario characteristics that are most relevant for the intended assessment,\nmaking it particularly appropriate for the domain of virtual safety\nassessments. We first review existing equivalence testing methods. Then we\npropose and demonstrate the Bayesian ROPE-based method by testing the\nequivalence of two rear-end pre-crash datasets. Our approach focuses on the\nmost relevant scenario characteristics. Our analysis provides insights into the\npracticalities and effectiveness of equivalence testing in synthetic test\nscenario validation and demonstrates the importance of testing for improving\nthe credibility of synthetic data for automated vehicle safety assessment, as\nwell as the credibility of subsequent safety impact assessments.\n","authors":["Jian Wu","Ulrich Sander","Carol Flannagan","Minxiang Zhao","Jonas Bärgman"],"pdf_url":"https://arxiv.org/pdf/2505.12827v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13969v1","updated":"2025-05-20T06:07:21Z","published":"2025-05-20T06:07:21Z","title":"Hypothesis on the Functional Advantages of the Selection-Broadcast Cycle\n  Structure: Global Workspace Theory and Dealing with a Real-Time World","summary":"  This paper discusses the functional advantages of the Selection-Broadcast\nCycle structure proposed by Global Workspace Theory (GWT), inspired by human\nconsciousness, particularly focusing on its applicability to artificial\nintelligence and robotics in dynamic, real-time scenarios. While previous\nstudies often examined the Selection and Broadcast processes independently,\nthis research emphasizes their combined cyclic structure and the resulting\nbenefits for real-time cognitive systems. Specifically, the paper identifies\nthree primary benefits: Dynamic Thinking Adaptation, Experience-Based\nAdaptation, and Immediate Real-Time Adaptation. This work highlights GWT's\npotential as a cognitive architecture suitable for sophisticated\ndecision-making and adaptive performance in unsupervised, dynamic environments.\nIt suggests new directions for the development and implementation of robust,\ngeneral-purpose AI and robotics systems capable of managing complex, real-world\ntasks.\n","authors":["Junya Nakanishi","Jun Baba","Yuichiro Yoshikawa","Hiroko Kamide","Hiroshi Ishiguro"],"pdf_url":"https://arxiv.org/pdf/2505.13969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13959v1","updated":"2025-05-20T05:44:04Z","published":"2025-05-20T05:44:04Z","title":"MultiDrive: A Co-Simulation Framework Bridging 2D and 3D Driving\n  Simulation for AV Software Validation","summary":"  Scenario-based testing using simulations is a cornerstone of Autonomous\nVehicles (AVs) software validation. So far, developers needed to choose between\nlow-fidelity 2D simulators to explore the scenario space efficiently, and\nhigh-fidelity 3D simulators to study relevant scenarios in more detail, thus\nreducing testing costs while mitigating the sim-to-real gap. This paper\npresents a novel framework that leverages multi-agent co-simulation and\nprocedural scenario generation to support scenario-based testing across low-\nand high-fidelity simulators for the development of motion planning algorithms.\nOur framework limits the effort required to transition scenarios between\nsimulators and automates experiment execution, trajectory analysis, and\nvisualization. Experiments with a reference motion planner show that our\nframework uncovers discrepancies between the planner's intended and actual\nbehavior, thus exposing weaknesses in planning assumptions under more realistic\nconditions. Our framework is available at:\nhttps://github.com/TUM-AVS/MultiDrive\n","authors":["Marc Kaufeld","Korbinian Moller","Alessio Gambi","Paolo Arcaini","Johannes Betz"],"pdf_url":"https://arxiv.org/pdf/2505.13959v1.pdf","comment":"7 pages, Submitted to the IEEE International Conference on\n  Intelligent Transportation Systems (ITSC 2025), Australia"},{"id":"http://arxiv.org/abs/2505.12224v2","updated":"2025-05-20T05:16:05Z","published":"2025-05-18T03:57:08Z","title":"RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and\n  Correction","summary":"  Vision-Language-Action (VLA) models have recently advanced robotic\nmanipulation by translating natural-language instructions and image information\ninto sequential control actions. However, these models often underperform in\nopen-world scenarios, as they are predominantly trained on successful expert\ndemonstrations and exhibit a limited capacity for failure recovery. In this\nwork, we present a Robotic Failure Analysis and Correction (RoboFAC) framework\nto address this issue. Firstly, we construct RoboFAC dataset comprising 9,440\nerroneous manipulation trajectories and 78,623 QA pairs across 16 diverse tasks\nand 53 scenes in both simulation and real-world environments. Leveraging our\ndataset, we develop RoboFAC model, which is capable of Task Understanding,\nFailure Analysis and Failure Correction. Experimental results demonstrate that\nthe RoboFAC model outperforms GPT-4o by 34.1% on our evaluation benchmark.\nFurthermore, we integrate the RoboFAC model into a real-world VLA control\npipeline as an external supervision providing correction instructions, yielding\na 29.1% relative improvement on average on four real-world tasks. The results\nshow that our RoboFAC framework effectively handles robotic failures and\nassists the VLA model in recovering from failures.\n","authors":["Weifeng Lu","Minghao Ye","Zewei Ye","Ruihan Tao","Shuo Yang","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.12224v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10141v2","updated":"2025-05-20T05:11:26Z","published":"2025-03-13T08:00:58Z","title":"Mapless Collision-Free Flight via MPC using Dual KD-Trees in Cluttered\n  Environments","summary":"  Collision-free flight in cluttered environments is a critical capability for\nautonomous quadrotors. Traditional methods often rely on detailed 3D map\nconstruction, trajectory generation, and tracking. However, this cascade\npipeline can introduce accumulated errors and computational delays, limiting\nflight agility and safety. In this paper, we propose a novel method for\nenabling collision-free flight in cluttered environments without explicitly\nconstructing 3D maps or generating and tracking collision-free trajectories.\nInstead, we leverage Model Predictive Control (MPC) to directly produce safe\nactions from sparse waypoints and point clouds from a depth camera. These\nsparse waypoints are dynamically adjusted online based on nearby obstacles\ndetected from point clouds. To achieve this, we introduce a dual KD-Tree\nmechanism: the Obstacle KD-Tree quickly identifies the nearest obstacle for\navoidance, while the Edge KD-Tree provides a robust initial guess for the MPC\nsolver, preventing it from getting stuck in local minima during obstacle\navoidance. We validate our approach through extensive simulations and\nreal-world experiments. The results show that our approach significantly\noutperforms the mapping-based methods and is also superior to imitation\nlearning-based methods, demonstrating reliable obstacle avoidance at up to 12\nm/s in simulations and 6 m/s in real-world tests. Our method provides a simple\nand robust alternative to existing methods.\n","authors":["Linzuo Zhang","Yu Hu","Yang Deng","Feng Yu","Danping Zou"],"pdf_url":"https://arxiv.org/pdf/2503.10141v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13931v1","updated":"2025-05-20T04:56:14Z","published":"2025-05-20T04:56:14Z","title":"Sketch Interface for Teleoperation of Mobile Manipulator to Enable\n  Intuitive and Intended Operation: A Proof of Concept","summary":"  Recent advancements in robotics have underscored the need for effective\ncollaboration between humans and robots. Traditional interfaces often struggle\nto balance robot autonomy with human oversight, limiting their practical\napplication in complex tasks like mobile manipulation. This study aims to\ndevelop an intuitive interface that enables a mobile manipulator to\nautonomously interpret user-provided sketches, enhancing user experience while\nminimizing burden. We implemented a web-based application utilizing machine\nlearning algorithms to process sketches, making the interface accessible on\nmobile devices for use anytime, anywhere, by anyone. In the first validation,\nwe examined natural sketches drawn by users for 27 selected manipulation and\nnavigation tasks, gaining insights into trends related to sketch instructions.\nThe second validation involved comparative experiments with five grasping\ntasks, showing that the sketch interface reduces workload and enhances\nintuitiveness compared to conventional axis control interfaces. These findings\nsuggest that the proposed sketch interface improves the efficiency of mobile\nmanipulators and opens new avenues for integrating intuitive human-robot\ncollaboration in various applications.\n","authors":["Yuka Iwanaga","Masayoshi Tsuchinaga","Kosei Tanada","Yuji Nakamura","Takemitsu Mori","Takashi Yamamoto"],"pdf_url":"https://arxiv.org/pdf/2505.13931v1.pdf","comment":"This paper has been accepted to the the 20th edition of the IEEE/ACM\n  International Conference on Human-Robot Interaction (HRI'25), which will be\n  held in Melbourne, Australia on March 4-6, 2025"},{"id":"http://arxiv.org/abs/2505.13925v1","updated":"2025-05-20T04:40:49Z","published":"2025-05-20T04:40:49Z","title":"Time Reversal Symmetry for Efficient Robotic Manipulations in Deep\n  Reinforcement Learning","summary":"  Symmetry is pervasive in robotics and has been widely exploited to improve\nsample efficiency in deep reinforcement learning (DRL). However, existing\napproaches primarily focus on spatial symmetries, such as reflection, rotation,\nand translation, while largely neglecting temporal symmetries. To address this\ngap, we explore time reversal symmetry, a form of temporal symmetry commonly\nfound in robotics tasks such as door opening and closing. We propose Time\nReversal symmetry enhanced Deep Reinforcement Learning (TR-DRL), a framework\nthat combines trajectory reversal augmentation and time reversal guided reward\nshaping to efficiently solve temporally symmetric tasks. Our method generates\nreversed transitions from fully reversible transitions, identified by a\nproposed dynamics-consistent filter, to augment the training data. For\npartially reversible transitions, we apply reward shaping to guide learning,\naccording to successful trajectories from the reversed task. Extensive\nexperiments on the Robosuite and MetaWorld benchmarks demonstrate that TR-DRL\nis effective in both single-task and multi-task settings, achieving higher\nsample efficiency and stronger final performance compared to baseline methods.\n","authors":["Yunpeng Jiang","Jianshu Hu","Paul Weng","Yutong Ban"],"pdf_url":"https://arxiv.org/pdf/2505.13925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13921v1","updated":"2025-05-20T04:34:58Z","published":"2025-05-20T04:34:58Z","title":"APEX: Empowering LLMs with Physics-Based Task Planning for Real-time\n  Insight","summary":"  Large Language Models (LLMs) demonstrate strong reasoning and task planning\ncapabilities but remain fundamentally limited in physical interaction modeling.\nExisting approaches integrate perception via Vision-Language Models (VLMs) or\nadaptive decision-making through Reinforcement Learning (RL), but they fail to\ncapture dynamic object interactions or require task-specific training, limiting\ntheir real-world applicability. We introduce APEX (Anticipatory\nPhysics-Enhanced Execution), a framework that equips LLMs with physics-driven\nforesight for real-time task planning. APEX constructs structured graphs to\nidentify and model the most relevant dynamic interactions in the environment,\nproviding LLMs with explicit physical state updates. Simultaneously, APEX\nprovides low-latency forward simulations of physically feasible actions,\nallowing LLMs to select optimal strategies based on predictive outcomes rather\nthan static observations. We evaluate APEX on three benchmarks designed to\nassess perception, prediction, and decision-making: (1) Physics Reasoning\nBenchmark, testing causal inference and object motion prediction; (2) Tetris,\nevaluating whether physics-informed prediction enhances decision-making\nperformance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance,\nassessing the immediate integration of perception and action feasibility\nanalysis. APEX significantly outperforms standard LLMs and VLM-based models,\ndemonstrating the necessity of explicit physics reasoning for bridging the gap\nbetween language-based intelligence and real-world task execution. The source\ncode and experiment setup are publicly available at\nhttps://github.com/hwj20/APEX_EXP .\n","authors":["Wanjing Huang","Weixiang Yan","Zhen Zhang","Ambuj Singh"],"pdf_url":"https://arxiv.org/pdf/2505.13921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13916v1","updated":"2025-05-20T04:26:45Z","published":"2025-05-20T04:26:45Z","title":"Robotic Monitoring of Colorimetric Leaf Sensors for Precision\n  Agriculture","summary":"  Current remote sensing technologies that measure crop health e.g. RGB,\nmultispectral, hyperspectral, and LiDAR, are indirect, and cannot capture plant\nstress indicators directly. Instead, low-cost leaf sensors that directly\ninterface with the crop surface present an opportunity to advance real-time\ndirect monitoring. To this end, we co-design a sensor-detector system, where\nthe sensor is a novel colorimetric leaf sensor that directly measures crop\nhealth in a precision agriculture setting, and the detector autonomously\nobtains optical signals from these leaf sensors. This system integrates a\nground robot platform with an on-board monocular RGB camera and object detector\nto localize the leaf sensor, and a hyperspectral camera with motorized mirror\nand an on-board halogen light to acquire a hyperspectral reflectance image of\nthe leaf sensor, from which a spectral response characterizing crop health can\nbe extracted. We show a successful demonstration of our co-designed system\noperating in outdoor environments, obtaining spectra that are interpretable\nwhen compared to controlled laboratory-grade spectrometer measurements. The\nsystem is demonstrated in row-crop environments both indoors and outdoors where\nit is able to autonomously navigate, locate and obtain a hyperspectral image of\nall leaf sensors present, and retrieve interpretable spectral resonance from\nleaf sensors.\n","authors":["Malakhi Hopkins","Alice Kate Li","Shobhita Kramadhati","Jackson Arnold","Akhila Mallavarapu","Chavez Lawrence","Varun Murali","Sanjeev J. Koppal","Cherie Kagan","Vijay Kumar"],"pdf_url":"https://arxiv.org/pdf/2505.13916v1.pdf","comment":"Accepted to the Novel Approaches for Precision Agriculture and\n  Forestry with Autonomous Robots IEEE ICRA Workshop - 2025"},{"id":"http://arxiv.org/abs/2505.12222v2","updated":"2025-05-20T04:16:52Z","published":"2025-05-18T03:46:47Z","title":"Learning Impact-Rich Rotational Maneuvers via Centroidal Velocity\n  Rewards and Sim-to-Real Techniques: A One-Leg Hopper Flip Case Study","summary":"  Dynamic rotational maneuvers, such as front flips, inherently involve large\nangular momentum generation and intense impact forces, presenting major\nchallenges for reinforcement learning and sim-to-real transfer. In this work,\nwe propose a general framework for learning and deploying impact-rich,\nrotation-intensive behaviors through centroidal velocity-based rewards and\nactuator-aware sim-to-real techniques. We identify that conventional link-level\nreward formulations fail to induce true whole-body rotation and introduce a\ncentroidal angular velocity reward that accurately captures system-wide\nrotational dynamics. To bridge the sim-to-real gap under extreme conditions, we\nmodel motor operating regions (MOR) and apply transmission load regularization\nto ensure realistic torque commands and mechanical robustness. Using the\none-leg hopper front flip as a representative case study, we demonstrate the\nfirst successful hardware realization of a full front flip. Our results\nhighlight that incorporating centroidal dynamics and actuator constraints is\ncritical for reliably executing highly dynamic motions. A supplementary video\nis available at: https://youtu.be/atMAVI4s1RY\n","authors":["Dongyun Kang","Gijeong Kim","JongHun Choe","Hajun Kim","Hae-Won Park"],"pdf_url":"https://arxiv.org/pdf/2505.12222v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13905v1","updated":"2025-05-20T04:12:44Z","published":"2025-05-20T04:12:44Z","title":"4D-ROLLS: 4D Radar Occupancy Learning via LiDAR Supervision","summary":"  A comprehensive understanding of 3D scenes is essential for autonomous\nvehicles (AVs), and among various perception tasks, occupancy estimation plays\na central role by providing a general representation of drivable and occupied\nspace. However, most existing occupancy estimation methods rely on LiDAR or\ncameras, which perform poorly in degraded environments such as smoke, rain,\nsnow, and fog. In this paper, we propose 4D-ROLLS, the first weakly supervised\noccupancy estimation method for 4D radar using the LiDAR point cloud as the\nsupervisory signal. Specifically, we introduce a method for generating\npseudo-LiDAR labels, including occupancy queries and LiDAR height maps, as\nmulti-stage supervision to train the 4D radar occupancy estimation model. Then\nthe model is aligned with the occupancy map produced by LiDAR, fine-tuning its\naccuracy in occupancy estimation. Extensive comparative experiments validate\nthe exceptional performance of 4D-ROLLS. Its robustness in degraded\nenvironments and effectiveness in cross-dataset training are qualitatively\ndemonstrated. The model is also seamlessly transferred to downstream tasks BEV\nsegmentation and point cloud occupancy prediction, highlighting its potential\nfor broader applications. The lightweight network enables 4D-ROLLS model to\nachieve fast inference speeds at about 30 Hz on a 4060 GPU. The code of\n4D-ROLLS will be made available at https://github.com/CLASS-Lab/4D-ROLLS.\n","authors":["Ruihan Liu","Xiaoyi Wu","Xijun Chen","Liang Hu","Yunjiang Lou"],"pdf_url":"https://arxiv.org/pdf/2505.13905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13904v1","updated":"2025-05-20T04:10:50Z","published":"2025-05-20T04:10:50Z","title":"Learning to Insert for Constructive Neural Vehicle Routing Solver","summary":"  Neural Combinatorial Optimisation (NCO) is a promising learning-based\napproach for solving Vehicle Routing Problems (VRPs) without extensive manual\ndesign. While existing constructive NCO methods typically follow an\nappending-based paradigm that sequentially adds unvisited nodes to partial\nsolutions, this rigid approach often leads to suboptimal results. To overcome\nthis limitation, we explore the idea of insertion-based paradigm and propose\nLearning to Construct with Insertion-based Paradigm (L2C-Insert), a novel\nlearning-based method for constructive NCO. Unlike traditional approaches,\nL2C-Insert builds solutions by strategically inserting unvisited nodes at any\nvalid position in the current partial solution, which can significantly enhance\nthe flexibility and solution quality. The proposed framework introduces three\nkey components: a novel model architecture for precise insertion position\nprediction, an efficient training scheme for model optimization, and an\nadvanced inference technique that fully exploits the insertion paradigm's\nflexibility. Extensive experiments on both synthetic and real-world instances\nof the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) demonstrate that L2C-Insert consistently achieves superior\nperformance across various problem sizes.\n","authors":["Fu Luo","Xi Lin","Mengyuan Zhong","Fei Liu","Zhenkun Wang","Jianyong Sun","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.13904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13889v1","updated":"2025-05-20T03:50:29Z","published":"2025-05-20T03:50:29Z","title":"Certifiably Safe Manipulation of Deformable Linear Objects via Joint\n  Shape and Tension Prediction","summary":"  Manipulating deformable linear objects (DLOs) is challenging due to their\ncomplex dynamics and the need for safe interaction in contact-rich\nenvironments. Most existing models focus on shape prediction alone and fail to\naccount for contact and tension constraints, which can lead to damage to both\nthe DLO and the robot. In this work, we propose a certifiably safe motion\nplanning and control framework for DLO manipulation. At the core of our method\nis a predictive model that jointly estimates the DLO's future shape and\ntension. These predictions are integrated into a real-time trajectory optimizer\nbased on polynomial zonotopes, allowing us to enforce safety constraints\nthroughout the execution. We evaluate our framework on a simulated wire harness\nassembly task using a 7-DOF robotic arm. Compared to state-of-the-art methods,\nour approach achieves a higher task success rate while avoiding all safety\nviolations. The results demonstrate that our method enables robust and safe DLO\nmanipulation in contact-rich environments.\n","authors":["Yiting Zhang","Shichen Li"],"pdf_url":"https://arxiv.org/pdf/2505.13889v1.pdf","comment":"Accepted to ICRA 2025 Workshop on Learning Meets Model-Based Methods\n  for Contact-Rich Manipulation"},{"id":"http://arxiv.org/abs/2505.13888v1","updated":"2025-05-20T03:48:34Z","published":"2025-05-20T03:48:34Z","title":"InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning","summary":"  Leveraging pretrained Vision-Language Models (VLMs) to map language\ninstruction and visual observations to raw low-level actions,\nVision-Language-Action models (VLAs) hold great promise for achieving\ngeneral-purpose robotic systems. Despite their advancements, existing VLAs tend\nto spuriously correlate task-irrelevant visual features with actions, limiting\ntheir generalization capacity beyond the training data. To tackle this\nchallenge, we propose Intrinsic Spatial Reasoning (InSpire), a simple yet\neffective approach that mitigates the adverse effects of spurious correlations\nby boosting the spatial reasoning ability of VLAs. Specifically, InSpire\nredirects the VLA's attention to task-relevant factors by prepending the\nquestion \"In which direction is the [object] relative to the robot?\" to the\nlanguage instruction and aligning the answer\n\"right/left/up/down/front/back/grasped\" and predicted actions with the\nground-truth. Notably, InSpire can be used as a plugin to enhance existing\nautoregressive VLAs, requiring no extra training data or interaction with other\nlarge models. Extensive experimental results in both simulation and real-world\nenvironments demonstrate the effectiveness and flexibility of our approach. Our\ncode, pretrained models and demos are publicly available at:\nhttps://Koorye.github.io/proj/Inspire.\n","authors":["Ji Zhang","Shihan Wu","Xu Luo","Hao Wu","Lianli Gao","Heng Tao Shen","Jingkuan Song"],"pdf_url":"https://arxiv.org/pdf/2505.13888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13872v1","updated":"2025-05-20T03:27:06Z","published":"2025-05-20T03:27:06Z","title":"Safety2Drive: Safety-Critical Scenario Benchmark for the Evaluation of\n  Autonomous Driving","summary":"  Autonomous Driving (AD) systems demand the high levels of safety assurance.\nDespite significant advancements in AD demonstrated on open-source benchmarks\nlike Longest6 and Bench2Drive, existing datasets still lack\nregulatory-compliant scenario libraries for closed-loop testing to\ncomprehensively evaluate the functional safety of AD. Meanwhile, real-world AD\naccidents are underrepresented in current driving datasets. This scarcity leads\nto inadequate evaluation of AD performance, posing risks to safety validation\nand practical deployment. To address these challenges, we propose Safety2Drive,\na safety-critical scenario library designed to evaluate AD systems.\nSafety2Drive offers three key contributions. (1) Safety2Drive comprehensively\ncovers the test items required by standard regulations and contains 70 AD\nfunction test items. (2) Safety2Drive supports the safety-critical scenario\ngeneralization. It has the ability to inject safety threats such as natural\nenvironment corruptions and adversarial attacks cross camera and LiDAR sensors.\n(3) Safety2Drive supports multi-dimensional evaluation. In addition to the\nevaluation of AD systems, it also supports the evaluation of various perception\ntasks, such as object detection and lane detection. Safety2Drive provides a\nparadigm from scenario construction to validation, establishing a standardized\ntest framework for the safe deployment of AD.\n","authors":["Jingzheng Li","Tiancheng Wang","Xingyu Peng","Jiacheng Chen","Zhijun Chen","Bing Li","Xianglong Liu"],"pdf_url":"https://arxiv.org/pdf/2505.13872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08452v2","updated":"2025-05-20T02:44:12Z","published":"2025-02-12T14:46:27Z","title":"Learning to Group and Grasp Multiple Objects","summary":"  Simultaneously grasping and delivering multiple objects can significantly\nenhance robotic work efficiency and has been a key research focus for decades.\nThe primary challenge lies in determining how to push objects, group them, and\nexecute simultaneous grasping for respective groups while considering object\ndistribution and the hardware constraints of the robot. Traditional rule-based\nmethods struggle to flexibly adapt to diverse scenarios. To address this\nchallenge, this paper proposes an imitation learning-based approach. We collect\na series of expert demonstrations through teleoperation and train a diffusion\npolicy network, enabling the robot to dynamically generate action sequences for\npushing, grouping, and grasping, thereby facilitating efficient multi-object\ngrasping and delivery. We conducted experiments to evaluate the method under\ndifferent training dataset sizes, varying object quantities, and real-world\nobject scenarios. The results demonstrate that the proposed approach can\neffectively and adaptively generate multi-object grouping and grasping\nstrategies. With the support of more training data, imitation learning is\nexpected to be an effective approach for solving the multi-object grasping\nproblem.\n","authors":["Takahiro Yonemaru","Weiwei Wan","Tatsuki Nishimura","Kensuke Harada"],"pdf_url":"https://arxiv.org/pdf/2502.08452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11755v2","updated":"2025-05-20T02:30:21Z","published":"2025-05-16T23:30:13Z","title":"Reachability Barrier Networks: Learning Hamilton-Jacobi Solutions for\n  Smooth and Flexible Control Barrier Functions","summary":"  Recent developments in autonomous driving and robotics underscore the\nnecessity of safety-critical controllers. Control barrier functions (CBFs) are\na popular method for appending safety guarantees to a general control\nframework, but they are notoriously difficult to generate beyond low\ndimensions. Existing methods often yield non-differentiable or inaccurate\napproximations that lack integrity, and thus fail to ensure safety. In this\nwork, we use physics-informed neural networks (PINNs) to generate smooth\napproximations of CBFs by computing Hamilton-Jacobi (HJ) optimal control\nsolutions. These reachability barrier networks (RBNs) avoid traditional\ndimensionality constraints and support the tuning of their conservativeness\npost-training through a parameterized discount term. To ensure robustness of\nthe discounted solutions, we leverage conformal prediction methods to derive\nprobabilistic safety guarantees for RBNs. We demonstrate that RBNs are highly\naccurate in low dimensions, and safer than the standard neural CBF approach in\nhigh dimensions. Namely, we showcase the RBNs in a 9D multi-vehicle collision\navoidance problem where it empirically proves to be 5.5x safer and 1.9x less\nconservative than the neural CBFs, offering a promising method to synthesize\nCBFs for general nonlinear autonomous systems.\n","authors":["Matthew Kim","William Sharpless","Hyun Joe Jeong","Sander Tonkens","Somil Bansal","Sylvia Herbert"],"pdf_url":"https://arxiv.org/pdf/2505.11755v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.13837v1","updated":"2025-05-20T02:23:15Z","published":"2025-05-20T02:23:15Z","title":"Enhancing Robot Navigation Policies with Task-Specific Uncertainty\n  Managements","summary":"  Robots navigating complex environments must manage uncertainty from sensor\nnoise, environmental changes, and incomplete information, with different tasks\nrequiring varying levels of precision in different areas. For example, precise\nlocalization may be crucial near obstacles but less critical in open spaces. We\npresent GUIDE (Generalized Uncertainty Integration for Decision-Making and\nExecution), a framework that integrates these task-specific requirements into\nnavigation policies via Task-Specific Uncertainty Maps (TSUMs). By assigning\nacceptable uncertainty levels to different locations, TSUMs enable robots to\nadapt uncertainty management based on context. When combined with reinforcement\nlearning, GUIDE learns policies that balance task completion and uncertainty\nmanagement without extensive reward engineering. Real-world tests show\nsignificant performance gains over methods lacking task-specific uncertainty\nawareness.\n","authors":["Gokul Puthumanaillam","Paulo Padrao","Jose Fuentes","Leonardo Bobadilla","Melkior Ornik"],"pdf_url":"https://arxiv.org/pdf/2505.13837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13836v1","updated":"2025-05-20T02:21:34Z","published":"2025-05-20T02:21:34Z","title":"Duawlfin: A Drone with Unified Actuation for Wheeled Locomotion and\n  Flight Operation","summary":"  This paper presents Duawlfin, a drone with unified actuation for wheeled\nlocomotion and flight operation that achieves efficient, bidirectional ground\nmobility. Unlike existing hybrid designs, Duawlfin eliminates the need for\nadditional actuators or propeller-driven ground propulsion by leveraging only\nits standard quadrotor motors and introducing a differential drivetrain with\none-way bearings. This innovation simplifies the mechanical system,\nsignificantly reduces energy usage, and prevents the disturbance caused by\npropellers spinning near the ground, such as dust interference with sensors.\nBesides, the one-way bearings minimize the power transfer from motors to\npropellers in the ground mode, which enables the vehicle to operate safely near\nhumans. We provide a detailed mechanical design, present control strategies for\nrapid and smooth mode transitions, and validate the concept through extensive\nexperimental testing. Flight-mode tests confirm stable aerial performance\ncomparable to conventional quadcopters, while ground-mode experiments\ndemonstrate efficient slope climbing (up to 30{\\deg}) and agile turning\nmaneuvers approaching 1g lateral acceleration. The seamless transitions between\naerial and ground modes further underscore the practicality and effectiveness\nof our approach for applications like urban logistics and indoor navigation.\nAll the materials including 3-D model files, demonstration video and other\nassets are open-sourced at https://sites.google.com/view/Duawlfin.\n","authors":["Jerry Tang","Ruiqi Zhang","Kaan Beyduz","Yiwei Jiang","Cody Wiebe","Haoyu Zhang","Osaruese Asoro","Mark W. Mueller"],"pdf_url":"https://arxiv.org/pdf/2505.13836v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.13834v1","updated":"2025-05-20T02:20:54Z","published":"2025-05-20T02:20:54Z","title":"Toward Real-World Cooperative and Competitive Soccer with Quadrupedal\n  Robot Teams","summary":"  Achieving coordinated teamwork among legged robots requires both fine-grained\nlocomotion control and long-horizon strategic decision-making. Robot soccer\noffers a compelling testbed for this challenge, combining dynamic, competitive,\nand multi-agent interactions. In this work, we present a hierarchical\nmulti-agent reinforcement learning (MARL) framework that enables fully\nautonomous and decentralized quadruped robot soccer. First, a set of highly\ndynamic low-level skills is trained for legged locomotion and ball\nmanipulation, such as walking, dribbling, and kicking. On top of these, a\nhigh-level strategic planning policy is trained with Multi-Agent Proximal\nPolicy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning\nframework allows agents to adapt to diverse opponent strategies and gives rise\nto sophisticated team behaviors, including coordinated passing, interception,\nand dynamic role allocation. With an extensive ablation study, the proposed\nlearning method shows significant advantages in the cooperative and competitive\nmulti-agent soccer game. We deploy the learned policies to real quadruped\nrobots relying solely on onboard proprioception and decentralized localization,\nwith the resulting system supporting autonomous robot-robot and robot-human\nsoccer matches on indoor and outdoor soccer courts.\n","authors":["Zhi Su","Yuman Gao","Emily Lukas","Yunfei Li","Jiaze Cai","Faris Tulbah","Fei Gao","Chao Yu","Zhongyu Li","Yi Wu","Koushil Sreenath"],"pdf_url":"https://arxiv.org/pdf/2505.13834v1.pdf","comment":"11 pages, 12 figures"},{"id":"http://arxiv.org/abs/2411.16959v2","updated":"2025-05-20T01:17:09Z","published":"2024-11-25T21:57:15Z","title":"RoCoDA: Counterfactual Data Augmentation for Data-Efficient Robot\n  Learning from Demonstrations","summary":"  Imitation learning in robotics faces significant challenges in generalization\ndue to the complexity of robotic environments and the high cost of data\ncollection. We introduce RoCoDA, a novel method that unifies the concepts of\ninvariance, equivariance, and causality within a single framework to enhance\ndata augmentation for imitation learning. RoCoDA leverages causal invariance by\nmodifying task-irrelevant subsets of the environment state without affecting\nthe policy's output. Simultaneously, we exploit SE(3) equivariance by applying\nrigid body transformations to object poses and adjusting corresponding actions\nto generate synthetic demonstrations. We validate RoCoDA through extensive\nexperiments on five robotic manipulation tasks, demonstrating improvements in\npolicy performance, generalization, and sample efficiency compared to\nstate-of-the-art data augmentation methods. Our policies exhibit robust\ngeneralization to unseen object poses, textures, and the presence of\ndistractors. Furthermore, we observe emergent behavior such as re-grasping,\nindicating policies trained with RoCoDA possess a deeper understanding of task\ndynamics. By leveraging invariance, equivariance, and causality, RoCoDA\nprovides a principled approach to data augmentation in imitation learning,\nbridging the gap between geometric symmetries and causal reasoning. Project\nPage: https://rocoda.github.io\n","authors":["Ezra Ameperosa","Jeremy A. Collins","Mrinal Jain","Animesh Garg"],"pdf_url":"https://arxiv.org/pdf/2411.16959v2.pdf","comment":"Accepted to 2025 IEEE International Conference on Robotics and\n  Automation (ICRA)"},{"id":"http://arxiv.org/abs/2505.13782v1","updated":"2025-05-20T00:03:21Z","published":"2025-05-20T00:03:21Z","title":"C*: A Coverage Path Planning Algorithm for Unknown Environments using\n  Rapidly Covering Graphs","summary":"  The paper presents a novel sample-based algorithm, called C*, for real-time\ncoverage path planning (CPP) of unknown environments. The C* algorithm is built\nupon the concept of Rapidly Covering Graph (RCGs). The RCG is constructed\nincrementally via progressive sampling during robot navigation, which\neliminates the need for cellular decomposition of the search space. The RCG has\na sparse-graph structure formed by efficient sampling and pruning techniques,\nwhich produces non-myopic waypoints of the coverage trajectory. While C*\nproduces the desired back and forth coverage pattern, it adapts to the\nTSP-based locally optimal coverage of small uncovered regions, called coverage\nholes, that are surrounded by obstacles and covered regions. Thus, C*\nproactively detects and covers the coverage holes in situ, which reduces the\ncoverage time by preventing the longer return trajectories from distant regions\nto cover such holes later. The algorithmic simplicity and low computational\ncomplexity of C* makes it easy to implement and suitable for real-time onboard\napplications. It is analytically proven that C* provides complete coverage of\nunknown environments. The performance of C* is validated by 1) extensive\nhigh-fidelity simulations and 2) real laboratory experiments using autonomous\nrobots. A comparative evaluation with seven existing CPP methods demonstrate\nthat C* yields significant performance improvements in terms of coverage time,\nnumber of turns, trajectory length and overlap ratio, while preventing the\nformation of coverage holes. Finally, C* is evaluated on two different\napplications of CPP using 1) energy-constrained robots and 2) multi-robot\nteams.\n","authors":["Zongyuan Shen","James P. Wilson","Shalabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2505.13782v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2505.14684v1","updated":"2025-05-20T17:59:31Z","published":"2025-05-20T17:59:31Z","title":"Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning","summary":"  Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.\n","authors":["Haolei Xu","Yuchen Yan","Yongliang Shen","Wenqi Zhang","Guiyang Hou","Shengpei Jiang","Kaitao Song","Weiming Lu","Jun Xiao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2505.14684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14681v1","updated":"2025-05-20T17:59:16Z","published":"2025-05-20T17:59:16Z","title":"Two Experts Are All You Need for Steering Thinking: Reinforcing\n  Cognitive Effort in MoE Reasoning Models Without Additional Training","summary":"  Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models.\n","authors":["Mengru Wang","Xingyu Chen","Yue Wang","Zhiwei He","Jiahao Xu","Tian Liang","Qiuzhi Liu","Yunzhi Yao","Wenxuan Wang","Ruotian Ma","Haitao Mi","Ningyu Zhang","Zhaopeng Tu","Xiaolong Li","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2505.14681v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2505.14680v1","updated":"2025-05-20T17:59:13Z","published":"2025-05-20T17:59:13Z","title":"NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search","summary":"  Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.\n","authors":["Sunhao Dai","Wenjie Wang","Liang Pang","Jun Xu","See-Kiong Ng","Ji-Rong Wen","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2505.14680v1.pdf","comment":"SIGIR 2025 Perspective Paper"},{"id":"http://arxiv.org/abs/2505.14673v1","updated":"2025-05-20T17:58:02Z","published":"2025-05-20T17:58:02Z","title":"Training-Free Watermarking for Autoregressive Image Generation","summary":"  Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.\n","authors":["Yu Tong","Zihao Pan","Shuai Yang","Kaiyang Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.14673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14668v1","updated":"2025-05-20T17:55:25Z","published":"2025-05-20T17:55:25Z","title":"ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory\n  Perceptions","summary":"  Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants.\n","authors":["Bufang Yang","Lilin Xu","Liekang Zeng","Kaiwei Liu","Siyang Jiang","Wenrui Lu","Hongkai Chen","Xiaofan Jiang","Guoliang Xing","Zhenyu Yan"],"pdf_url":"https://arxiv.org/pdf/2505.14668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14667v1","updated":"2025-05-20T17:54:54Z","published":"2025-05-20T17:54:54Z","title":"SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early\n  Alignment","summary":"  Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI.\n","authors":["Wonje Jeung","Sangyeon Yoon","Minsuk Kahng","Albert No"],"pdf_url":"https://arxiv.org/pdf/2505.14667v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2502.07158v3","updated":"2025-05-20T17:53:08Z","published":"2025-02-11T00:53:36Z","title":"Early Risk Prediction of Pediatric Cardiac Arrest from Electronic Health\n  Records via Multimodal Fused Transformer","summary":"  Early prediction of pediatric cardiac arrest (CA) is critical for timely\nintervention in high-risk intensive care settings. We introduce PedCA-FT, a\nnovel transformer-based framework that fuses tabular view of EHR with the\nderived textual view of EHR to fully unleash the interactions of\nhigh-dimensional risk factors and their dynamics. By employing dedicated\ntransformer modules for each modality view, PedCA-FT captures complex temporal\nand contextual patterns to produce robust CA risk estimates. Evaluated on a\ncurated pediatric cohort from the CHOA-CICU database, our approach outperforms\nten other artificial intelligence models across five key performance metrics\nand identifies clinically meaningful risk factors. These findings underscore\nthe potential of multimodal fusion techniques to enhance early CA detection and\nimprove patient care.\n","authors":["Jiaying Lu","Stephanie R. Brown","Songyuan Liu","Shifan Zhao","Kejun Dong","Del Bold","Michael Fundora","Alaa Aljiffry","Alex Fedorov","Jocelyn Grunwell","Xiao Hu"],"pdf_url":"https://arxiv.org/pdf/2502.07158v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14664v1","updated":"2025-05-20T17:52:03Z","published":"2025-05-20T17:52:03Z","title":"AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of\n  Cross-Modal Embeddings","summary":"  Cross-modal embeddings form the foundation for multi-modal models. However,\nvisualization methods for interpreting cross-modal embeddings have been\nprimarily confined to traditional dimensionality reduction (DR) techniques like\nPCA and t-SNE. These DR methods primarily focus on feature distributions within\na single modality, whilst failing to incorporate metrics (e.g., CLIPScore)\nacross multiple modalities.This paper introduces AKRMap, a new DR technique\ndesigned to visualize cross-modal embeddings metric with enhanced accuracy by\nlearning kernel regression of the metric landscape in the projection space.\nSpecifically, AKRMap constructs a supervised projection network guided by a\npost-projection kernel regression loss, and employs adaptive generalized\nkernels that can be jointly optimized with the projection. This approach\nenables AKRMap to efficiently generate visualizations that capture complex\nmetric distributions, while also supporting interactive features such as zoom\nand overlay for deeper exploration. Quantitative experiments demonstrate that\nAKRMap outperforms existing DR methods in generating more accurate and\ntrustworthy visualizations. We further showcase the effectiveness of AKRMap in\nvisualizing and comparing cross-modal embeddings for text-to-image models. Code\nand demo are available at https://github.com/yilinye/AKRMap.\n","authors":["Yilin Ye","Junchao Huang","Xingchen Zeng","Jiazhi Xia","Wei Zeng"],"pdf_url":"https://arxiv.org/pdf/2505.14664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15364v3","updated":"2025-05-20T17:50:11Z","published":"2025-04-21T18:12:46Z","title":"KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments","summary":"  We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods.\n","authors":["Junyoung Park","Dalton Jones","Matthew J Morse","Raghavv Goel","Mingu Lee","Chris Lott"],"pdf_url":"https://arxiv.org/pdf/2504.15364v3.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.14661v1","updated":"2025-05-20T17:49:46Z","published":"2025-05-20T17:49:46Z","title":"Abacus: A Cost-Based Optimizer for Semantic Operator Systems","summary":"  LLMs enable an exciting new class of data processing applications over large\ncollections of unstructured documents. Several new programming frameworks have\nenabled developers to build these applications by composing them out of\nsemantic operators: a declarative set of AI-powered data transformations with\nnatural language specifications. These include LLM-powered maps, filters,\njoins, etc. used for document processing tasks such as information extraction,\nsummarization, and more. While systems of semantic operators have achieved\nstrong performance on benchmarks, they can be difficult to optimize. An\noptimizer for this setting must determine how to physically implement each\nsemantic operator in a way that optimizes the system globally. Existing\noptimizers are limited in the number of optimizations they can apply, and most\n(if not all) cannot optimize system quality, cost, or latency subject to\nconstraint(s) on the other dimensions. In this paper we present Abacus, an\nextensible, cost-based optimizer which searches for the best implementation of\na semantic operator system given a (possibly constrained) optimization\nobjective. Abacus estimates operator performance by leveraging a minimal set of\nvalidation examples and, if available, prior beliefs about operator\nperformance. We evaluate Abacus on document processing workloads in the\nbiomedical and legal domains (BioDEX; CUAD) and multi-modal question answering\n(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%\nbetter quality and up to 23.6x lower cost and 4.2x lower latency than the next\nbest system.\n","authors":["Matthew Russo","Sivaprasad Sudhir","Gerardo Vitagliano","Chunwei Liu","Tim Kraska","Samuel Madden","Michael Cafarella"],"pdf_url":"https://arxiv.org/pdf/2505.14661v1.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.14660v1","updated":"2025-05-20T17:47:04Z","published":"2025-05-20T17:47:04Z","title":"EmoGist: Efficient In-Context Learning for Visual Emotion Understanding","summary":"  In this paper, we introduce EmoGist, a training-free, in-context learning\nmethod for performing visual emotion classification with LVLMs. The key\nintuition of our approach is that context-dependent definition of emotion\nlabels could allow more accurate predictions of emotions, as the ways in which\nemotions manifest within images are highly context dependent and nuanced.\nEmoGist pre-generates multiple explanations of emotion labels, by analyzing the\nclusters of example images belonging to each category. At test time, we\nretrieve a version of explanation based on embedding similarity, and feed it to\na fast VLM for classification. Through our experiments, we show that EmoGist\nallows up to 13 points improvement in micro F1 scores with the multi-label\nMemotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.\n","authors":["Ronald Seoh","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2505.14660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14659v1","updated":"2025-05-20T17:46:09Z","published":"2025-05-20T17:46:09Z","title":"Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless\n  Networks","summary":"  As healthcare systems increasingly adopt advanced wireless networks and\nconnected devices, securing medical applications has become critical. The\nintegration of Internet of Medical Things devices, such as robotic surgical\ntools, intensive care systems, and wearable monitors has enhanced patient care\nbut introduced serious security risks. Cyberattacks on these devices can lead\nto life threatening consequences, including surgical errors, equipment failure,\nand data breaches. While the ITU IMT 2030 vision highlights 6G's transformative\nrole in healthcare through AI and cloud integration, it also raises new\nsecurity concerns. This paper explores how explainable AI techniques like SHAP,\nLIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve\ntrust and transparency in 6G enabled healthcare. We support our approach with\nexperimental analysis and highlight promising results.\n","authors":["Navneet Kaur","Lav Gupta"],"pdf_url":"https://arxiv.org/pdf/2505.14659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14656v1","updated":"2025-05-20T17:43:33Z","published":"2025-05-20T17:43:33Z","title":"Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning","summary":"  While LLMs excel at open-ended reasoning, they often struggle with\ncost-sensitive planning, either treating all actions as having equal cost or\nfailing to stay within strict budgets. In this paper, we introduce\nCost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings\nexplicit cost-awareness into LLM-guided planning. Tight cost constraints push\nthe planner to quickly identify infeasible solutions, while looser constraints\nencourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,\nClaude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their\nperformance in cost-sensitive scenarios. Our experiments suggest that raw LLMs\nsuch as GPT-4.1 often falter under tight budgets, whereas CATS consistently\ndelivers strong performance, achieving higher task success rates and better\ncost efficiency. CATS provides an effective solution for budget-aware\ndecision-making by combining the reasoning power of LLMs with structured\nsearch.\n","authors":["Zihao Zhang","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2505.14656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14654v1","updated":"2025-05-20T17:42:34Z","published":"2025-05-20T17:42:34Z","title":"Beyond Words: Multimodal LLM Knows When to Speak","summary":"  While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI.\n","authors":["Zikai Liao","Yi Ouyang","Yi-Lun Lee","Chen-Ping Yu","Yi-Hsuan Tsai","Zhaozheng Yin"],"pdf_url":"https://arxiv.org/pdf/2505.14654v1.pdf","comment":"Project page: https://github.com/lzk901372/MM-When2Speak"},{"id":"http://arxiv.org/abs/2505.11750v2","updated":"2025-05-20T17:42:26Z","published":"2025-05-16T23:22:07Z","title":"Improving Medium Range Severe Weather Prediction through Transformer\n  Post-processing of AI Weather Forecasts","summary":"  Improving the skill of medium-range (1-8 day) severe weather prediction is\ncrucial for mitigating societal impacts. This study introduces a novel approach\nleveraging decoder-only transformer networks to post-process AI-based weather\nforecasts, specifically from the Pangu-Weather model, for improved severe\nweather guidance. Unlike traditional post-processing methods that use a dense\nneural network to predict the probability of severe weather using discrete\nforecast samples, our method treats forecast lead times as sequential\n``tokens'', enabling the transformer to learn complex temporal relationships\nwithin the evolving atmospheric state. We compare this approach against\npost-processing of the Global Forecast System (GFS) using both a traditional\ndense neural network and our transformer, as well as configurations that\nexclude convective parameters to fairly evaluate the impact of using the\nPangu-Weather AI model. Results demonstrate that the transformer-based\npost-processing significantly enhances forecast skill compared to dense neural\nnetworks. Furthermore, AI-driven forecasts, particularly Pangu-Weather\ninitialized from high resolution analysis, exhibit superior performance to GFS\nin the medium-range, even without explicit convective parameters. Our approach\noffers improved accuracy, and reliability, which also provides interpretability\nthrough feature attribution analysis, advancing medium-range severe weather\nprediction capabilities.\n","authors":["Zhanxiang Hua","Ryan Sobash","David John Gagne II","Yingkai Sha","Alexandra Anderson-Frey"],"pdf_url":"https://arxiv.org/pdf/2505.11750v2.pdf","comment":"16 pages, 10 figures; update fix issues with section reference number"},{"id":"http://arxiv.org/abs/2505.14646v1","updated":"2025-05-20T17:34:44Z","published":"2025-05-20T17:34:44Z","title":"CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided\n  Design Code Generation","summary":"  Efficient creation of accurate and editable 3D CAD models is critical in\nengineering design, significantly impacting cost and time-to-market in product\ninnovation. Current manual workflows remain highly time-consuming and demand\nextensive user expertise. While recent developments in AI-driven CAD generation\nshow promise, existing models are limited by incomplete representations of CAD\noperations, inability to generalize to real-world images, and low output\naccuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model\n(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)\ndirectly from visual input. Leveraging a novel dataset that we\ncreated--GenCAD-Code, consisting of over 163k CAD-model image and code\npairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and\nQwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in\n3D solid similarity. Notably, our VLM demonstrates some signs of\ngeneralizability, successfully generating CAD code from real-world images and\nexecuting CAD operations unseen during fine-tuning. The performance and\nadaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code\nto streamline CAD workflows for engineers and designers. CAD-Coder is publicly\navailable at: https://github.com/anniedoris/CAD-Coder.\n","authors":["Anna C. Doris","Md Ferdous Alam","Amin Heyrani Nobari","Faez Ahmed"],"pdf_url":"https://arxiv.org/pdf/2505.14646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14633v1","updated":"2025-05-20T17:24:09Z","published":"2025-05-20T17:24:09Z","title":"Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values\n  Prioritization with AIRiskDilemmas","summary":"  Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.\n","authors":["Yu Ying Chiu","Zhilin Wang","Sharan Maiya","Yejin Choi","Kyle Fish","Sydney Levine","Evan Hubinger"],"pdf_url":"https://arxiv.org/pdf/2505.14633v1.pdf","comment":"34 pages, 11 figures, see associated data at\n  https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at\n  https://github.com/kellycyy/LitmusValues"},{"id":"http://arxiv.org/abs/2504.14783v2","updated":"2025-05-20T17:22:21Z","published":"2025-04-21T00:46:31Z","title":"How Effective Can Dropout Be in Multiple Instance Learning ?","summary":"  Multiple Instance Learning (MIL) is a popular weakly-supervised method for\nvarious applications, with a particular interest in histological whole slide\nimage (WSI) classification. Due to the gigapixel resolution of WSI,\napplications of MIL in WSI typically necessitate a two-stage training scheme:\nfirst, extract features from the pre-trained backbone and then perform MIL\naggregation. However, it is well-known that this suboptimal training scheme\nsuffers from \"noisy\" feature embeddings from the backbone and inherent weak\nsupervision, hindering MIL from learning rich and generalizable features.\nHowever, the most commonly used technique (i.e., dropout) for mitigating this\nissue has yet to be explored in MIL. In this paper, we empirically explore how\neffective the dropout can be in MIL. Interestingly, we observe that dropping\nthe top-k most important instances within a bag leads to better performance and\ngeneralization even under noise attack. Based on this key observation, we\npropose a novel MIL-specific dropout method, termed MIL-Dropout, which\nsystematically determines which instances to drop. Experiments on five MIL\nbenchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the\nperformance of current MIL methods with a negligible computational cost. The\ncode is available at https://github.com/ChongQingNoSubway/MILDropout.\n","authors":["Wenhui Zhu","Peijie Qiu","Xiwen Chen","Zhangsihao Yang","Aristeidis Sotiras","Abolfazl Razi","Yalin Wang"],"pdf_url":"https://arxiv.org/pdf/2504.14783v2.pdf","comment":"Accepted by ICML2025"},{"id":"http://arxiv.org/abs/2406.14761v2","updated":"2025-05-20T17:21:03Z","published":"2024-06-20T22:22:28Z","title":"Diffusion-Based Failure Sampling for Evaluating Safety-Critical\n  Autonomous Systems","summary":"  Validating safety-critical autonomous systems in high-dimensional domains\nsuch as robotics presents a significant challenge. Existing black-box\napproaches based on Markov chain Monte Carlo may require an enormous number of\nsamples, while methods based on importance sampling often rely on simple\nparametric families that may struggle to represent the distribution over\nfailures. We propose to sample the distribution over failures using a\nconditional denoising diffusion model, which has shown success in complex\nhigh-dimensional problems such as robotic task planning. We iteratively train a\ndiffusion model to produce state trajectories closer to failure. We demonstrate\nthe effectiveness of our approach on high-dimensional robotic validation tasks,\nimproving sample efficiency and mode coverage compared to existing black-box\ntechniques.\n","authors":["Harrison Delecki","Marc R. Schlichting","Mansur Arief","Anthony Corso","Marcell Vazquez-Chanlatte","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2406.14761v2.pdf","comment":"Appears in IEEE International Conference on Engineering Reliable\n  Autonomous Systems (ERAS) 2025"},{"id":"http://arxiv.org/abs/2505.14629v1","updated":"2025-05-20T17:19:57Z","published":"2025-05-20T17:19:57Z","title":"KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large\n  Language Models","summary":"  Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL.\n","authors":["Fnu Mohbat","Mohammed J Zaki"],"pdf_url":"https://arxiv.org/pdf/2505.14629v1.pdf","comment":"Accepted at ACL 2025"},{"id":"http://arxiv.org/abs/2505.14627v1","updated":"2025-05-20T17:18:17Z","published":"2025-05-20T17:18:17Z","title":"Debating for Better Reasoning: An Unsupervised Multimodal Approach","summary":"  As Large Language Models (LLMs) gain expertise across diverse domains and\nmodalities, scalable oversight becomes increasingly challenging, particularly\nwhen their capabilities may surpass human evaluators. Debate has emerged as a\npromising mechanism for enabling such oversight. In this work, we extend the\ndebate paradigm to a multimodal setting, exploring its potential for weaker\nmodels to supervise and enhance the performance of stronger models. We focus on\nvisual question answering (VQA), where two \"sighted\" expert vision-language\nmodels debate an answer, while a \"blind\" (text-only) judge adjudicates based\nsolely on the quality of the arguments. In our framework, the experts defend\nonly answers aligned with their beliefs, thereby obviating the need for\nexplicit role-playing and concentrating the debate on instances of expert\ndisagreement. Experiments on several multimodal tasks demonstrate that the\ndebate framework consistently outperforms individual expert models. Moreover,\njudgments from weaker LLMs can help instill reasoning capabilities in\nvision-language models through finetuning.\n","authors":["Ashutosh Adhikari","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2505.14627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14625v1","updated":"2025-05-20T17:16:44Z","published":"2025-05-20T17:16:44Z","title":"TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning","summary":"  Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.\n","authors":["Zhangchen Xu","Yuetai Li","Fengqing Jiang","Bhaskar Ramasubramanian","Luyao Niu","Bill Yuchen Lin","Radha Poovendran"],"pdf_url":"https://arxiv.org/pdf/2505.14625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07482v2","updated":"2025-05-20T17:09:53Z","published":"2025-01-13T16:58:32Z","title":"TiEBe: Tracking Language Model Recall of Notable Worldwide Events\n  Through Time","summary":"  As the knowledge landscape evolves and large language models (LLMs) become\nincreasingly widespread, there is a growing need to keep these models updated\nwith current events. While existing benchmarks assess general factual recall,\nfew studies explore how LLMs retain knowledge over time or across different\nregions. To address these gaps, we present the Timely Events Benchmark (TiEBe),\na dataset of over 23,000 question-answer pairs centered on notable global and\nregional events, spanning more than 10 years of events, 23 regions, and 13\nlanguages. TiEBe leverages structured retrospective data from Wikipedia to\nidentify notable events through time. These events are then used to construct a\nbenchmark to evaluate LLMs' understanding of global and regional developments,\ngrounded in factual evidence beyond Wikipedia itself. Our results reveal\nsignificant geographic disparities in factual recall, emphasizing the need for\nmore balanced global representation in LLM training. We also observe a Pearson\ncorrelation of more than 0.7 between models' performance in TiEBe and various\ncountries' socioeconomic indicators, such as HDI. In addition, we examine the\nimpact of language on factual recall by posing questions in the native language\nof the region where each event occurred, uncovering substantial performance\ngaps for low-resource languages.\n","authors":["Thales Sales Almeida","Giovana Kerche Bonás","João Guilherme Alves Santos","Hugo Abonizio","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2501.07482v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14615v1","updated":"2025-05-20T17:00:22Z","published":"2025-05-20T17:00:22Z","title":"SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle\n  Generation from SAT Formulas","summary":"  We introduce SATBench, a benchmark for evaluating the logical reasoning\ncapabilities of large language models (LLMs) through logical puzzles derived\nfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses on\ninference rule-based reasoning, which often involves deducing conclusions from\na set of premises, our approach leverages the search-based nature of SAT\nproblems, where the objective is to find a solution that fulfills a specified\nset of logical constraints. Each instance in SATBench is generated from a SAT\nformula, then translated into a story context and conditions using LLMs. The\ngeneration process is fully automated and allows for adjustable difficulty by\nvarying the number of clauses. All 2100 puzzles are validated through both\nLLM-assisted and solver-based consistency checks, with human validation on a\nsubset. Experimental results show that even the strongest model, o4-mini,\nachieves only 65.0% accuracy on hard UNSAT problems, close to the random\nbaseline of 50%. SATBench exposes fundamental limitations in the search-based\nlogical reasoning abilities of current LLMs and provides a scalable testbed for\nfuture research in logical reasoning.\n","authors":["Anjiang Wei","Yuheng Wu","Yingjia Wan","Tarun Suresh","Huanmi Tan","Zhanke Zhou","Sanmi Koyejo","Ke Wang","Alex Aiken"],"pdf_url":"https://arxiv.org/pdf/2505.14615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14608v1","updated":"2025-05-20T16:55:44Z","published":"2025-05-20T16:55:44Z","title":"Language Models Optimized to Fool Detectors Still Have a Distinct Style\n  (And How to Change It)","summary":"  Despite considerable progress in the development of machine-text detectors,\nit has been suggested that the problem is inherently hard, and therefore, that\nstakeholders should proceed under the assumption that machine-generated text\ncannot be reliably detected as such. We examine a recent such claim by Nicks et\nal. (2024) regarding the ease with which language models can be optimized to\ndegrade the performance of machine-text detectors, including detectors not\nspecifically optimized against. We identify a feature space$\\unicode{x2013}$the\nstylistic feature space$\\unicode{x2013}$that is robust to such optimization,\nand show that it may be used to reliably detect samples from language models\noptimized to prevent detection. Furthermore, we show that even when models are\nexplicitly optimized against stylistic detectors, detection performance remains\nsurprisingly unaffected. We then seek to understand if stylistic detectors are\ninherently more robust. To study this question, we explore a new paraphrasing\napproach that simultaneously aims to close the gap between human writing and\nmachine writing in stylistic feature space while avoiding detection using\ntraditional features. We show that when only a single sample is available for\ndetection, this attack is universally effective across all detectors\nconsidered, including those that use writing style. However, as the number of\nsamples available for detection grows, the human and machine distributions\nbecome distinguishable. This observation encourages us to introduce AURA, a\nmetric that estimates the overlap between human and machine-generated\ndistributions by analyzing how detector performance improves as more samples\nbecome available. Overall, our findings underscore previous recommendations to\navoid reliance on machine-text detection.\n","authors":["Rafael Rivera Soto","Barry Chen","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2505.14608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14604v1","updated":"2025-05-20T16:53:40Z","published":"2025-05-20T16:53:40Z","title":"Let LLMs Break Free from Overthinking via Self-Braking Tuning","summary":"  Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.\n","authors":["Haoran Zhao","Yuchen Yan","Yongliang Shen","Haolei Xu","Wenqi Zhang","Kaitao Song","Jian Shao","Weiming Lu","Jun Xiao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2505.14604v1.pdf","comment":"Github:https://github.com/CCAI-Lab/Self-Braking-Tuning; Project:\n  https://CCAI-Lab.github.io/SBT"},{"id":"http://arxiv.org/abs/2504.10368v2","updated":"2025-05-20T16:52:53Z","published":"2025-04-14T16:13:23Z","title":"S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models","summary":"  We introduce S1-Bench, a novel benchmark designed to evaluate the performance\nof Large Reasoning Models (LRMs) on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their heavy reliance on system 2 thinking may limit their system 1\nthinking capabilities. However, there is a lack of an appropriate benchmark for\nevaluating LRM's system 1 thinking capabilities. To fill this gap, S1-Bench\nintroduces a suite of simple, diverse, and natural questions across multiple\ndomains and languages, specifically designed to assess LRMs' performance on\nquestions more suitable for system 1 . We conduct extensive evaluations across\n28 LRMs, revealing their inefficiency, inadequate accuracy, and limited\nrobustness when handling simple questions. Additionally, we observe a gap\nbetween their difficulty perception and generation length. Overall, this work\npaves the way toward dual-system compatibility in the development of LRMs.\n","authors":["Wenyuan Zhang","Shuaiyi Nie","Xinghua Zhang","Zefeng Zhang","Tingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10368v2.pdf","comment":"31 pages, 9 figures, 16 tables"},{"id":"http://arxiv.org/abs/2505.14603v1","updated":"2025-05-20T16:52:11Z","published":"2025-05-20T16:52:11Z","title":"Towards a Foundation Model for Communication Systems","summary":"  Artificial Intelligence (AI) has demonstrated unprecedented performance\nacross various domains, and its application to communication systems is an\nactive area of research. While current methods focus on task-specific\nsolutions, the broader trend in AI is shifting toward large general models\ncapable of supporting multiple applications. In this work, we take a step\ntoward a foundation model for communication data--a transformer-based,\nmulti-modal model designed to operate directly on communication data. We\npropose methodologies to address key challenges, including tokenization,\npositional embedding, multimodality, variable feature sizes, and normalization.\nFurthermore, we empirically demonstrate that such a model can successfully\nestimate multiple features, including transmission rank, selected precoder,\nDoppler spread, and delay profile.\n","authors":["Davide Buffelli","Sowmen Das","Yu-Wei Lin","Sattar Vakili","Chien-Yi Wang","Masoud Attarifar","Pritthijit Nath","Da-shan Shiu"],"pdf_url":"https://arxiv.org/pdf/2505.14603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14599v1","updated":"2025-05-20T16:49:40Z","published":"2025-05-20T16:49:40Z","title":"Toward Reliable Biomedical Hypothesis Generation: Evaluating\n  Truthfulness and Hallucination in Large Language Models","summary":"  Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.\n","authors":["Guangzhi Xiong","Eric Xie","Corey Williams","Myles Kim","Amir Hassan Shariatmadari","Sikun Guo","Stefan Bekiranov","Aidong Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.14599v1.pdf","comment":"Accepted to IJCAI 2025"},{"id":"http://arxiv.org/abs/2502.16901v2","updated":"2025-05-20T16:45:00Z","published":"2025-02-24T06:54:50Z","title":"Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in\n  Multilingual LLMs","summary":"  We explore \\textbf{C}ross-lingual \\textbf{B}ackdoor \\textbf{AT}tacks (X-BAT)\nin multilingual Large Language Models (mLLMs), revealing how backdoors inserted\nin one language can automatically transfer to others through shared embedding\nspaces. Using toxicity classification as a case study, we demonstrate that\nattackers can compromise multilingual systems by poisoning data in a single\nlanguage, with rare and high-occurring tokens serving as specific, effective\ntriggers. Our findings expose a critical vulnerability that influences the\nmodel's architecture, resulting in a concealed backdoor effect during the\ninformation flow. Our code and data are publicly available\nhttps://github.com/himanshubeniwal/X-BAT.\n","authors":["Himanshu Beniwal","Sailesh Panda","Birudugadda Srivibhav","Mayank Singh"],"pdf_url":"https://arxiv.org/pdf/2502.16901v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07115v4","updated":"2025-05-20T16:29:52Z","published":"2025-02-10T23:11:44Z","title":"Online Scheduling for LLM Inference with KV Cache Constraints","summary":"  Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment.\n","authors":["Patrick Jaillet","Jiashuo Jiang","Konstantina Mellou","Marco Molinaro","Chara Podimata","Zijie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07115v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17401v3","updated":"2025-05-20T16:29:43Z","published":"2024-08-30T16:36:53Z","title":"Exploring the Effect of Explanation Content and Format on User\n  Comprehension and Trust in Healthcare","summary":"  AI-driven tools for healthcare are widely acknowledged as potentially\nbeneficial to health practitioners and patients, e.g. the QCancer regression\ntool for cancer risk prediction. However, for these tools to be trusted, they\nneed to be supplemented with explanations. We examine how explanations' content\nand format affect user comprehension and trust when explaining QCancer's\npredictions. Regarding content, we deploy SHAP and Occlusion-1. Regarding\nformat, we present SHAP explanations, conventionally, as charts (SC) and\nOcclusion-1 explanations as charts (OC) as well as text (OT), to which their\nsimpler nature lends itself. We conduct experiments with two sets of\nstakeholders: the general public (representing patients) and medical students\n(representing healthcare practitioners). Our experiments showed higher\nsubjective comprehension and trust for Occlusion-1 over SHAP explanations based\non content. However, when controlling for format, only OT outperformed SC,\nsuggesting this trend is driven by preferences for text. Other findings\ncorroborated that explanation format, rather than content, is often the\ncritical factor.\n","authors":["Antonio Rago","Bence Palfi","Purin Sukpanichnant","Hannibal Nabli","Kavyesh Vivek","Olga Kostopoulou","James Kinross","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2408.17401v3.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2311.16515v4","updated":"2025-05-20T16:29:22Z","published":"2023-11-25T14:24:49Z","title":"Automatic Synthetic Data and Fine-grained Adaptive Feature Alignment for\n  Composed Person Retrieval","summary":"  Person retrieval has attracted rising attention. Existing methods are mainly\ndivided into two retrieval modes, namely image-only and text-only. However,\nthey are unable to make full use of the available information and are difficult\nto meet diverse application requirements. To address the above limitations, we\npropose a new Composed Person Retrieval (CPR) task, which combines visual and\ntextual queries to identify individuals of interest from large-scale person\nimage databases. Nevertheless, the foremost difficulty of the CPR task is the\nlack of available annotated datasets. Therefore, we first introduce a scalable\nautomatic data synthesis pipeline, which decomposes complex multimodal data\ngeneration into the creation of textual quadruples followed by\nidentity-consistent image synthesis using fine-tuned generative models.\nMeanwhile, a multimodal filtering method is designed to ensure the resulting\nSynCPR dataset retains 1.15 million high-quality and fully synthetic triplets.\nAdditionally, to improve the representation of composed person queries, we\npropose a novel Fine-grained Adaptive Feature Alignment (FAFA) framework\nthrough fine-grained dynamic alignment and masked feature reasoning. Moreover,\nfor objective evaluation, we manually annotate the Image-Text Composed Person\nRetrieval (ITCPR) test set. The extensive experiments demonstrate the\neffectiveness of the SynCPR dataset and the superiority of the proposed FAFA\nframework when compared with the state-of-the-art methods. All code and data\nwill be provided at\nhttps://github.com/Delong-liu-bupt/Composed_Person_Retrieval.\n","authors":["Delong Liu","Haiwen Li","Zhaohui Hou","Zhicheng Zhao","Fei Su","Yuan Dong"],"pdf_url":"https://arxiv.org/pdf/2311.16515v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14569v1","updated":"2025-05-20T16:28:08Z","published":"2025-05-20T16:28:08Z","title":"Agent Context Protocols Enhance Collective Inference","summary":"  AI agents have become increasingly adept at complex tasks such as coding,\nreasoning, and multimodal understanding. However, building generalist systems\nrequires moving beyond individual agents to collective inference -- a paradigm\nwhere multi-agent systems with diverse, task-specialized agents complement one\nanother through structured communication and collaboration. Today, coordination\nis usually handled with imprecise, ad-hoc natural language, which limits\ncomplex interaction and hinders interoperability with domain-specific agents.\nWe introduce Agent context protocols (ACPs): a domain- and agent-agnostic\nfamily of structured protocols for agent-agent communication, coordination, and\nerror handling. ACPs combine (i) persistent execution blueprints -- explicit\ndependency graphs that store intermediate agent outputs -- with (ii)\nstandardized message schemas, enabling robust and fault-tolerant multi-agent\ncollective inference. ACP-powered generalist systems reach state-of-the-art\nperformance: 28.3 % accuracy on AssistantBench for long-horizon web assistance\nand best-in-class multimodal technical reports, outperforming commercial AI\nsystems in human evaluation. ACPs are highly modular and extensible, allowing\npractitioners to build top-tier generalist agents quickly.\n","authors":["Devansh Bhardwaj","Arjun Beniwal","Shreyas Chaudhari","Ashwin Kalyan","Tanmay Rajpurohit","Karthik R. Narasimhan","Ameet Deshpande","Vishvak Murahari"],"pdf_url":"https://arxiv.org/pdf/2505.14569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10940v2","updated":"2025-05-20T16:27:05Z","published":"2025-02-16T01:05:16Z","title":"CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation","summary":"  The full-size MLPs and the projection layers in attention introduce\ntremendous model sizes of large language models (LLMs), imposing extremely\ndemanding needs of computational resources in the pre-training stage. However,\nwe empirically observe that the activations of pre-trained LLMs exhibit\nlow-rank property. Motivated by such observations, we propose CoLA and its\nmemory-efficient implementation, CoLA-M, to replace these full-size layers with\ncompute-efficient auto-encoders that naturally enforce low-rank activations\nthroughout training. This fundamental architectural change eliminates the\nactivation redundancy and significantly boosts model capacity and training\nefficiency. Experiments on LLaMA models with 60 million to 7 billion parameters\nshow that CoLA reduces the computing cost by $\\bf 2\\pmb{\\times}$ and improves\ntraining throughput by $\\bf 1.86\\pmb{\\times}$ while maintaining full-rank level\nperformance. CoLA-M further squeezes memory cost without sacrificing\nthroughput, offering a pre-training approach with collectively superior\nparameter, computing, and memory efficiency. The LLMs produced are also $\\bf\n2\\pmb{\\times}$ smaller, enabling faster inference with lower memory cost on\nresource-constrained platforms.\n","authors":["Ziyue Liu","Ruijie Zhang","Zhengyang Wang","Zi Yang","Paul Hovland","Bogdan Nicolae","Franck Cappello","Zheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.10940v2.pdf","comment":"v2"},{"id":"http://arxiv.org/abs/2505.14566v1","updated":"2025-05-20T16:25:41Z","published":"2025-05-20T16:25:41Z","title":"KIPPO: Koopman-Inspired Proximal Policy Optimization","summary":"  Reinforcement Learning (RL) has made significant strides in various domains,\nand policy gradient methods like Proximal Policy Optimization (PPO) have gained\npopularity due to their balance in performance, training stability, and\ncomputational efficiency. These methods directly optimize policies through\ngradient-based updates. However, developing effective control policies for\nenvironments with complex and non-linear dynamics remains a challenge. High\nvariance in gradient estimates and non-convex optimization landscapes often\nlead to unstable learning trajectories. Koopman Operator Theory has emerged as\na powerful framework for studying non-linear systems through an\ninfinite-dimensional linear operator that acts on a higher-dimensional space of\nmeasurement functions. In contrast with their non-linear counterparts, linear\nsystems are simpler, more predictable, and easier to analyze. In this paper, we\npresent Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an\napproximately linear latent-space representation of the underlying system's\ndynamics while retaining essential features for effective policy learning. This\nis achieved through a Koopman-approximation auxiliary network that can be added\nto the baseline policy optimization algorithms without altering the\narchitecture of the core policy or value function. Extensive experimental\nresults demonstrate consistent improvements over the PPO baseline with 6-60%\nincreased performance while reducing variability by up to 91% when evaluated on\nvarious continuous control tasks.\n","authors":["Andrei Cozma","Landon Harris","Hairong Qi"],"pdf_url":"https://arxiv.org/pdf/2505.14566v1.pdf","comment":"Accepted for IJCAI 2025. This arXiv submission is the full version of\n  the conference paper, including the appendix and supplementary material\n  omitted from the IJCAI proceedings"},{"id":"http://arxiv.org/abs/2505.14564v1","updated":"2025-05-20T16:24:42Z","published":"2025-05-20T16:24:42Z","title":"Bellman operator convergence enhancements in reinforcement learning\n  algorithms","summary":"  This paper reviews the topological groundwork for the study of reinforcement\nlearning (RL) by focusing on the structure of state, action, and policy spaces.\nWe begin by recalling key mathematical concepts such as complete metric spaces,\nwhich form the foundation for expressing RL problems. By leveraging the Banach\ncontraction principle, we illustrate how the Banach fixed-point theorem\nexplains the convergence of RL algorithms and how Bellman operators, expressed\nas operators on Banach spaces, ensure this convergence. The work serves as a\nbridge between theoretical mathematics and practical algorithm design, offering\nnew approaches to enhance the efficiency of RL. In particular, we investigate\nalternative formulations of Bellman operators and demonstrate their impact on\nimproving convergence rates and performance in standard RL environments such as\nMountainCar, CartPole, and Acrobot. Our findings highlight how a deeper\nmathematical understanding of RL can lead to more effective algorithms for\ndecision-making problems.\n","authors":["David Krame Kadurha","Domini Jocema Leko Moutouo","Yae Ulrich Gaba"],"pdf_url":"https://arxiv.org/pdf/2505.14564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17388v3","updated":"2025-05-20T16:24:22Z","published":"2024-11-26T12:46:57Z","title":"Can LLMs be Good Graph Judge for Knowledge Graph Construction?","summary":"  In real-world scenarios, most of the data obtained from the information\nretrieval (IR) system is unstructured. Converting natural language sentences\ninto structured Knowledge Graphs (KGs) remains a critical challenge. We\nidentified three limitations with respect to existing KG construction methods:\n(1) There could be a large amount of noise in real-world documents, which could\nresult in extracting messy information. (2) Naive LLMs usually extract\ninaccurate knowledge from some domain-specific documents. (3) Hallucination\nphenomenon cannot be overlooked when directly using LLMs to construct KGs. In\nthis paper, we propose \\textbf{GraphJudge}, a KG construction framework to\naddress the aforementioned challenges. In this framework, we designed an\nentity-centric strategy to eliminate the noise information in the documents.\nAnd we fine-tuned a LLM as a graph judge to finally enhance the quality of\ngenerated KGs. Experiments conducted on two general and one domain-specific\ntext-graph pair datasets demonstrate state-of-the-art performance against\nvarious baseline methods with strong generalization abilities. Our code is\navailable at\n\\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}.\n","authors":["Haoyu Huang","Chong Chen","Zeang Sheng","Yang Li","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.17388v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14561v1","updated":"2025-05-20T16:19:34Z","published":"2025-05-20T16:19:34Z","title":"SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised\n  Speaker Verification","summary":"  Self-Supervised Learning (SSL) has led to considerable progress in Speaker\nVerification (SV). The standard framework uses same-utterance positive sampling\nand data-augmentation to generate anchor-positive pairs of the same speaker.\nThis is a major limitation, as this strategy primarily encodes channel\ninformation from the recording condition, shared by the anchor and positive. We\npropose a new positive sampling technique to address this bottleneck:\nSelf-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find\nan appropriate positive, i.e., of the same speaker identity but a different\nrecording condition, in the latent space using clustering assignments and a\nmemory queue of positive embeddings. SSPS improves SV performance for both\nSimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods\non VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by\nlowering intra-speaker variance, providing comparable performance to DINO-SSPS.\n","authors":["Theo Lepage","Reda Dehak"],"pdf_url":"https://arxiv.org/pdf/2505.14561v1.pdf","comment":"accepted at Interspeech 2025"},{"id":"http://arxiv.org/abs/2502.12466v2","updated":"2025-05-20T16:19:18Z","published":"2025-02-18T02:54:25Z","title":"EquiBench: Benchmarking Large Language Models' Understanding of Program\n  Semantics via Equivalence Checking","summary":"  As large language models (LLMs) become integral to code-related tasks, a\ncentral question emerges: do LLMs truly understand program execution semantics?\nWe introduce EquiBench, a new benchmark for evaluating LLMs through equivalence\nchecking, i.e., determining whether two programs produce identical outputs for\nall possible inputs. Unlike prior code generation benchmarks, this task\ndirectly tests a model's understanding of code execution semantics. EquiBench\nconsists of 2400 program pairs across four languages and six categories. These\npairs are generated through program analysis, compiler scheduling, and\nsuperoptimization, ensuring high-confidence labels, nontrivial difficulty, and\nfull automation. The transformations span syntactic edits, structural\nmodifications, and algorithmic changes, covering a broad spectrum of semantic\nvariation. We evaluate 19 state-of-the-art LLMs and find that in the most\nchallenging categories, the best accuracies are 63.8% and 76.2%, only modestly\nabove the 50% random baseline. Further analysis reveals that models often rely\non syntactic similarity rather than exhibiting robust reasoning over execution\nsemantics, highlighting fundamental limitations.\n","authors":["Anjiang Wei","Jiannan Cao","Ran Li","Hongyu Chen","Yuhui Zhang","Ziheng Wang","Yuan Liu","Thiago S. F. X. Teixeira","Diyi Yang","Ke Wang","Alex Aiken"],"pdf_url":"https://arxiv.org/pdf/2502.12466v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18169v4","updated":"2025-05-20T16:15:13Z","published":"2024-12-24T05:07:46Z","title":"KunServe: Efficient Parameter-centric Memory Management for LLM Serving","summary":"  Serving LLMs with a cluster of GPUs is common nowadays, where the serving\nsystem must meet strict latency SLOs required by applications. However, the\nstateful nature of LLM serving requires maintaining huge states (i.e., KVCache)\nin limited GPU memory. Under spikes in real-world workloads, GPU memory can be\neasily throttled, leading to orders of magnitude higher response latency due to\nqueuing introduced by waiting for KVCache to be reclaimed. Prior\nKVCache-centric approaches handle load throttling by dropping, migrating, or\nswapping KVCache. These methods fail to release sufficient memory quickly with\nrequests still queued.\n  This paper proposes the first parameter-centric approach to handling\nthrottling by selectively dropping replicated parameters to instantly free\nmemory for requests, based on an unnoticed observation that model parameters\nare commonly replicated across GPUs for serving LLMs. With additional memory,\nall requests can be served with a larger batch without queuing. To make the\nparameter-centric approach correct and efficient, we cooperatively execute\nrequests on GPUs with a complete copy of parameters using pipeline parallelism,\nand derive an appropriate drop plan without unnecessary cooperation. We also\ndesign techniques to minimize the performance overhead due to pipeline\nparallelism with the execution patterns of requests under drop. Evaluations\nshow that {\\sys} reduces the tail TTFT of requests under throttling by up to\n72.2 times compared to the state-of-the-art systems including Llumnix, vLLM and\nInferCept.\n","authors":["Rongxin Cheng","Yuxin Lai","Xingda Wei","Rong Chen","Haibo Chen"],"pdf_url":"https://arxiv.org/pdf/2412.18169v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14555v1","updated":"2025-05-20T16:13:20Z","published":"2025-05-20T16:13:20Z","title":"Physics-Guided Learning of Meteorological Dynamics for Weather\n  Downscaling and Forecasting","summary":"  Weather forecasting is essential but remains computationally intensive and\nphysically incomplete in traditional numerical weather prediction (NWP)\nmethods. Deep learning (DL) models offer efficiency and accuracy but often\nignore physical laws, limiting interpretability and generalization. We propose\nPhyDL-NWP, a physics-guided deep learning framework that integrates physical\nequations with latent force parameterization into data-driven models. It\npredicts weather variables from arbitrary spatiotemporal coordinates, computes\nphysical terms via automatic differentiation, and uses a physics-informed loss\nto align predictions with governing dynamics. PhyDL-NWP enables resolution-free\ndownscaling by modeling weather as a continuous function and fine-tunes\npre-trained models with minimal overhead, achieving up to 170x faster inference\nwith only 55K parameters. Experiments show that PhyDL-NWP improves both\nforecasting performance and physical consistency.\n","authors":["Yingtao Luo","Shikai Fang","Binqing Wu","Qingsong Wen","Liang Sun"],"pdf_url":"https://arxiv.org/pdf/2505.14555v1.pdf","comment":"Published/Accepted in KDD 2025 (February Cycle)"},{"id":"http://arxiv.org/abs/2505.14552v1","updated":"2025-05-20T16:06:32Z","published":"2025-05-20T16:06:32Z","title":"KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation","summary":"  Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.\n","authors":["Jiajun Shi","Jian Yang","Jiaheng Liu","Xingyuan Bu","Jiangjie Chen","Junting Zhou","Kaijing Ma","Zhoufutu Wen","Bingli Wang","Yancheng He","Liang Song","Hualei Zhu","Shilong Li","Xingjian Wang","Wei Zhang","Ruibin Yuan","Yifan Yao","Wenjun Yang","Yunli Wang","Siyuan Fang","Siyu Yuan","Qianyu He","Xiangru Tang","Yingshui Tan","Wangchunshu Zhou","Zhaoxiang Zhang","Zhoujun Li","Wenhao Huang","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.14552v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2505.14551v1","updated":"2025-05-20T16:06:25Z","published":"2025-05-20T16:06:25Z","title":"Trustworthy Reputation Games and Applications to Proof-of-Reputation\n  Blockchains","summary":"  Reputation systems play an essential role in the Internet era, as they enable\npeople to decide whom to trust, by collecting and aggregating data about users'\nbehavior. Recently, several works proposed the use of reputation for the design\nand scalability improvement of decentralized (blockchain) ledgers; however,\nsuch systems are prone to manipulation and to our knowledge no game-theoretic\ntreatment exists that can support their economic robustness.\n  In this work we put forth a new model for the design of what we call, {\\em\ntrustworthy reputation systems}. Concretely, we describe a class of games,\nwhich we term {\\em trustworthy reputation games}, that enable a set of users to\nreport a function of their beliefs about the trustworthiness of each server in\na set -- i.e., their estimate of the probability that this server will behave\naccording to its specified strategy -- in a way that satisfies the following\nproperties:\n  1. It is $(\\epsilon$-)best response for any rational user in the game to play\na prescribed (truthful) strategy according to their true belief.\n  2. Assuming that the users' beliefs are not too far from the {\\em true}\ntrustworthiness of the servers, playing the above ($\\epsilon-$)Nash equilibrium\nallows anyone who observes the users' strategies to estimate the relative\ntrustworthiness of any two servers.\n  Our utilities and decoding function build on a connection between the well\nknown PageRank algorithm and the problem of trustworthiness discovery, which\ncan be of independent interest. Finally, we show how the above games are\nmotivated by and can be leveraged in proof-of-reputation (PoR) blockchains.\n","authors":["Petros Drineas","Rohit Nema","Rafail Ostrovsky","Vassilis Zikas"],"pdf_url":"https://arxiv.org/pdf/2505.14551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14549v1","updated":"2025-05-20T16:05:05Z","published":"2025-05-20T16:05:05Z","title":"Can Large Language Models Really Recognize Your Name?","summary":"  Large language models (LLMs) are increasingly being used to protect sensitive\nuser data. However, current LLM-based privacy solutions assume that these\nmodels can reliably detect personally identifiable information (PII),\nparticularly named entities. In this paper, we challenge that assumption by\nrevealing systematic failures in LLM-based privacy tasks. Specifically, we show\nthat modern LLMs regularly overlook human names even in short text snippets due\nto ambiguous contexts, which cause the names to be misinterpreted or\nmishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous\nhuman names, leveraging the name regularity bias phenomenon, embedded within\nconcise text snippets along with benign prompt injections. Our experiments on\nmodern LLMs tasked to detect PII as well as specialized tools show that recall\nof ambiguous names drops by 20--40% compared to more recognizable names.\nFurthermore, ambiguous human names are four times more likely to be ignored in\nsupposedly privacy-preserving summaries generated by LLMs when benign prompt\ninjections are present. These findings highlight the underexplored risks of\nrelying solely on LLMs to safeguard user privacy and underscore the need for a\nmore systematic investigation into their privacy failure modes.\n","authors":["Dzung Pham","Peter Kairouz","Niloofar Mireshghallah","Eugene Bagdasarian","Chau Minh Pham","Amir Houmansadr"],"pdf_url":"https://arxiv.org/pdf/2505.14549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14544v1","updated":"2025-05-20T15:59:44Z","published":"2025-05-20T15:59:44Z","title":"Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic\n  Signal Optimization: A Simulation Study","summary":"  Urban traffic congestion, particularly at intersections, significantly\nimpacts travel time, fuel consumption, and emissions. Traditional fixed-time\nsignal control systems often lack the adaptability to manage dynamic traffic\npatterns effectively. This study explores the application of multi-agent\nreinforcement learning (MARL) to optimize traffic signal coordination across\nmultiple intersections within a simulated environment. Utilizing Pygame, a\nsimulation was developed to model a network of interconnected intersections\nwith randomly generated vehicle flows to reflect realistic traffic variability.\nA decentralized MARL controller was implemented, in which each traffic signal\noperates as an autonomous agent, making decisions based on local observations\nand information from neighboring agents. Performance was evaluated against a\nbaseline fixed-time controller using metrics such as average vehicle wait time\nand overall throughput. The MARL approach demonstrated statistically\nsignificant improvements, including reduced average waiting times and improved\nthroughput. These findings suggest that MARL-based dynamic control strategies\nhold substantial promise for improving urban traffic management efficiency.\nMore research is recommended to address scalability and real-world\nimplementation challenges.\n","authors":["Saahil Mahato"],"pdf_url":"https://arxiv.org/pdf/2505.14544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14539v1","updated":"2025-05-20T15:56:34Z","published":"2025-05-20T15:56:34Z","title":"A Logic of General Attention Using Edge-Conditioned Event Models\n  (Extended Version)","summary":"  In this work, we present the first general logic of attention. Attention is a\npowerful cognitive ability that allows agents to focus on potentially complex\ninformation, such as logically structured propositions, higher-order beliefs,\nor what other agents pay attention to. This ability is a strength, as it helps\nto ignore what is irrelevant, but it can also introduce biases when some types\nof information or agents are systematically ignored. Existing dynamic epistemic\nlogics for attention cannot model such complex attention scenarios, as they\nonly model attention to atomic formulas. Additionally, such logics quickly\nbecome cumbersome, as their size grows exponentially in the number of agents\nand announced literals. Here, we introduce a logic that overcomes both\nlimitations. First, we generalize edge-conditioned event models, which we show\nto be as expressive as standard event models yet exponentially more succinct\n(generalizing both standard event models and generalized arrow updates).\nSecond, we extend attention to arbitrary formulas, allowing agents to also\nattend to other agents' beliefs or attention. Our work treats attention as a\nmodality, like belief or awareness. We introduce attention principles that\nimpose closure properties on that modality and that can be used in its\naxiomatization. Throughout, we illustrate our framework with examples of AI\nagents reasoning about human attentional biases, demonstrating how such agents\ncan discover attentional biases.\n","authors":["Gaia Belardinelli","Thomas Bolander","Sebastian Watzl"],"pdf_url":"https://arxiv.org/pdf/2505.14539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12909v2","updated":"2025-05-20T15:54:36Z","published":"2025-05-19T09:45:18Z","title":"Sinusoidal Initialization, Time for a New Start","summary":"  Initialization plays a critical role in Deep Neural Network training,\ndirectly influencing convergence, stability, and generalization. Common\napproaches such as Glorot and He initializations rely on randomness, which can\nproduce uneven weight distributions across layer connections. In this paper, we\nintroduce the Sinusoidal initialization, a novel deterministic method that\nemploys sinusoidal functions to construct structured weight matrices expressly\nto improve the spread and balance of weights throughout the network while\nsimultaneously fostering a more uniform, well-conditioned distribution of\nneuron activation states from the very first forward pass. Because Sinusoidal\ninitialization begins with weights and activations that are already evenly and\nefficiently utilized, it delivers consistently faster convergence, greater\ntraining stability, and higher final accuracy across a wide range of models,\nincluding convolutional neural networks, vision transformers, and large\nlanguage models. On average, our experiments show an increase of 4.9% in final\nvalidation accuracy and 20.9% in convergence speed. By replacing randomness\nwith structure, this initialization provides a stronger and more reliable\nfoundation for Deep Learning systems.\n","authors":["Alberto Fernández-Hernández","Jose I. Mestre","Manuel F. Dolz","Jose Duato","Enrique S. Quintana-Ortí"],"pdf_url":"https://arxiv.org/pdf/2505.12909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14533v1","updated":"2025-05-20T15:52:43Z","published":"2025-05-20T15:52:43Z","title":"Energy-Efficient Deep Reinforcement Learning with Spiking Transformers","summary":"  Agent-based Transformers have been widely adopted in recent reinforcement\nlearning advances due to their demonstrated ability to solve complex tasks.\nHowever, the high computational complexity of Transformers often results in\nsignificant energy consumption, limiting their deployment in real-world\nautonomous systems. Spiking neural networks (SNNs), with their biologically\ninspired structure, offer an energy-efficient alternative for machine learning.\nIn this paper, a novel Spike-Transformer Reinforcement Learning (STRL)\nalgorithm that combines the energy efficiency of SNNs with the powerful\ndecision-making capabilities of reinforcement learning is developed.\nSpecifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons\nand attention mechanisms capable of processing spatio-temporal patterns over\nmultiple time steps is designed. The architecture is further enhanced with\nstate, action, and reward encodings to create a Transformer-like structure\noptimized for reinforcement learning tasks. Comprehensive numerical experiments\nconducted on state-of-the-art benchmarks demonstrate that the proposed SNN\nTransformer achieves significantly improved policy performance compared to\nconventional agent-based Transformers. With both enhanced energy efficiency and\npolicy optimality, this work highlights a promising direction for deploying\nbio-inspired, low-cost machine learning models in complex real-world\ndecision-making scenarios.\n","authors":["Mohammad Irfan Uddin","Nishad Tasnim","Md Omor Faruk","Zejian Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.14533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14526v1","updated":"2025-05-20T15:48:23Z","published":"2025-05-20T15:48:23Z","title":"NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based\n  Autonomous Navigation","summary":"  Autonomous robots must navigate and operate in diverse environments, from\nterrestrial and aquatic settings to aerial and space domains. While\nReinforcement Learning (RL) has shown promise in training policies for specific\nautonomous robots, existing benchmarks are often constrained to unique\nplatforms, limiting generalization and fair comparisons across different\nmobility systems. In this paper, we present NavBench, a multi-domain benchmark\nfor training and evaluating RL-based navigation policies across diverse robotic\nplatforms and operational environments. Built on IsaacLab, our framework\nstandardizes task definitions, enabling different robots to tackle various\nnavigation challenges without the need for ad-hoc task redesigns or custom\nevaluation metrics. Our benchmark addresses three key challenges: (1) Unified\ncross-medium benchmarking, enabling direct evaluation of diverse actuation\nmethods (thrusters, wheels, water-based propulsion) in realistic environments;\n(2) Scalable and modular design, facilitating seamless robot-task\ninterchangeability and reproducible training pipelines; and (3) Robust\nsim-to-real validation, demonstrated through successful policy transfer to\nmultiple real-world robots, including a satellite robotic simulator, an\nunmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency\nbetween simulation and real-world deployment, NavBench simplifies the\ndevelopment of adaptable RL-based navigation strategies. Its modular design\nallows researchers to easily integrate custom robots and tasks by following the\nframework's predefined templates, making it accessible for a wide range of\napplications. Our code is publicly available at NavBench.\n","authors":["Matteo El-Hariry","Antoine Richard","Ricard M. Castan","Luis F. W. Batista","Matthieu Geist","Cedric Pradalier","Miguel Olivares-Mendez"],"pdf_url":"https://arxiv.org/pdf/2505.14526v1.pdf","comment":"Submitted for publication. Under review (2025)"},{"id":"http://arxiv.org/abs/2502.11051v3","updated":"2025-05-20T15:47:22Z","published":"2025-02-16T09:23:50Z","title":"MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of\n  Multimodal Large Language Models","summary":"  Recent progress in Machine Unlearning (MU) has introduced solutions for the\nselective removal of private or sensitive information encoded within deep\nneural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs)\nremains in its nascent phase. Therefore, we propose to reformulate the task of\nmultimodal MU in the era of MLLMs, which aims to erase only the visual patterns\nassociated with a given entity while preserving the corresponding textual\nknowledge encoded within the original parameters of the language model\nbackbone. Furthermore, we develop a novel geometry-constrained gradient ascent\nmethod MMUnlearner. It updates the weights of MLLMs with a weight saliency map\njointly restricted by the remaining concepts and textual knowledge during\nunlearning, thereby preserving parameters essential for non-target knowledge.\nExtensive experiments demonstrate that MMUnlearner surpasses baselines that\nfinetuning MLLMs with VQA data directly through Gradient Ascent (GA) or\nNegative Preference Optimization (NPO), across all evaluation dimensions. Our\ncode will be released upon acceptance.\n","authors":["Jiahao Huo","Yibo Yan","Xu Zheng","Yuanhuiyi Lyu","Xin Zou","Zhihua Wei","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2502.11051v3.pdf","comment":"Accepted as ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2505.14524v1","updated":"2025-05-20T15:46:59Z","published":"2025-05-20T15:46:59Z","title":"Guarded Query Routing for Large Language Models","summary":"  Query routing, the task to route user queries to different large language\nmodel (LLM) endpoints, can be considered as a text classification problem.\nHowever, out-of-distribution queries must be handled properly, as those could\nbe questions about unrelated domains, queries in other languages, or even\ncontain unsafe text. Here, we thus study a \\emph{guarded} query routing\nproblem, for which we first introduce the Guarded Query Routing Benchmark\n(GQR-Bench), which covers three exemplary target domains (law, finance, and\nhealthcare), and seven datasets to test robustness against out-of-distribution\nqueries. We then use GQR-Bench to contrast the effectiveness and efficiency of\nLLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),\nstandard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo\nGuardrails), continuous bag-of-words classifiers (WideMLP, fastText), and\ntraditional machine learning models (SVM, XGBoost). Our results show that\nWideMLP, enhanced with out-of-domain detection capabilities, yields the best\ntrade-off between accuracy (88\\%) and speed (<4ms). The embedding-based\nfastText excels at speed (<1ms) with acceptable accuracy (80\\%), whereas LLMs\nyield the highest accuracy (91\\%) but are comparatively slow (62ms for local\nLlama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge\nthe automatic reliance on LLMs for (guarded) query routing and provide concrete\nrecommendations for practical applications. GQR-Bench will be released as a\nPython package -- \\texttt{gqr}.\n","authors":["Richard Šléher","William Brach","Tibor Sloboda","Kristián Košťál","Lukas Galke"],"pdf_url":"https://arxiv.org/pdf/2505.14524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14523v1","updated":"2025-05-20T15:46:44Z","published":"2025-05-20T15:46:44Z","title":"Exploring Graph Representations of Logical Forms for Language Modeling","summary":"  We make the case for language models over logical forms (LFLMs), arguing that\nsuch models are more data-efficient than their textual counterparts. To that\nend, we introduce the Graph-based Formal-Logical Distributional Semantics\n(GFoLDS) prototype, a pretrained LM over graph representations of logical\nforms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong\nexperimental evidence that LFLMs can leverage the built-in, basic linguistic\nknowledge inherent in such models to immediately begin learning more complex\npatterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,\ntransformer LMs pretrained on similar amounts of data, indicating that LFLMs\ncan learn with substantially less data than models over plain text.\nFurthermore, we show that the performance of this model is likely to scale with\nadditional parameters and pretraining data, suggesting the viability of LFLMs\nin real-world applications.\n","authors":["Michael Sullivan"],"pdf_url":"https://arxiv.org/pdf/2505.14523v1.pdf","comment":"To be published in ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2505.14513v1","updated":"2025-05-20T15:41:05Z","published":"2025-05-20T15:41:05Z","title":"Latent Flow Transformer","summary":"  Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in \\textit{preserving\ncoupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms.\n","authors":["Yen-Chen Wu","Feng-Ting Liao","Meng-Hsi Chen","Pei-Chen Ho","Farhang Nabiei","Da-shan Shiu"],"pdf_url":"https://arxiv.org/pdf/2505.14513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14510v1","updated":"2025-05-20T15:39:05Z","published":"2025-05-20T15:39:05Z","title":"BACON: A fully explainable AI model with graded logic for decision\n  making problems","summary":"  As machine learning models and autonomous agents are increasingly deployed in\nhigh-stakes, real-world domains such as healthcare, security, finance, and\nrobotics, the need for transparent and trustworthy explanations has become\ncritical. To ensure end-to-end transparency of AI decisions, we need models\nthat are not only accurate but also fully explainable and human-tunable. We\nintroduce BACON, a novel framework for automatically training explainable AI\nmodels for decision making problems using graded logic. BACON achieves high\npredictive accuracy while offering full structural transparency and precise,\nlogic-based symbolic explanations, enabling effective human-AI collaboration\nand expert-guided refinement. We evaluate BACON with a diverse set of\nscenarios: classic Boolean approximation, Iris flower classification, house\npurchasing decisions and breast cancer diagnosis. In each case, BACON provides\nhigh-performance models while producing compact, human-verifiable decision\nlogic. These results demonstrate BACON's potential as a practical and\nprincipled approach for delivering crisp, trustworthy explainable AI.\n","authors":["Haishi Bai","Jozo Dujmovic","Jianwu Wang"],"pdf_url":"https://arxiv.org/pdf/2505.14510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14505v1","updated":"2025-05-20T15:34:36Z","published":"2025-05-20T15:34:36Z","title":"ModRWKV: Transformer Multimodality in Linear Time","summary":"  Currently, most multimodal studies are based on large language models (LLMs)\nwith quadratic-complexity Transformer architectures. While linear models like\nRNNs enjoy low inference costs, their application has been largely limited to\nthe text-only modality. This work explores the capabilities of modern RNN\narchitectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal\nframework built upon the RWKV7 architecture as its LLM backbone-which achieves\nmulti-source information fusion through dynamically adaptable heterogeneous\nmodality encoders. We designed the multimodal modules in ModRWKV with an\nextremely lightweight architecture and, through extensive experiments,\nidentified a configuration that achieves an optimal balance between performance\nand computational efficiency. ModRWKV leverages the pretrained weights of the\nRWKV7 LLM for initialization, which significantly accelerates multimodal\ntraining. Comparative experiments with different pretrained checkpoints further\ndemonstrate that such initialization plays a crucial role in enhancing the\nmodel's ability to understand multimodal signals. Supported by extensive\nexperiments, we conclude that modern RNN architectures present a viable\nalternative to Transformers in the domain of multimodal large language models\n(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV\narchitecture through systematic exploration.\n","authors":["Jiale Kang","Ziyin Yue","Qingyu Yin","Jiang Rui","Weile Li","Zening Lu","Zhouran Ji"],"pdf_url":"https://arxiv.org/pdf/2505.14505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09083v2","updated":"2025-05-20T15:29:37Z","published":"2024-10-06T08:33:39Z","title":"Evaluating the Correctness of Inference Patterns Used by LLMs for\n  Judgment","summary":"  This paper presents a method to analyze the inference patterns used by Large\nLanguage Models (LLMs) for judgment in a case study on legal LLMs, so as to\nidentify potential incorrect representations of the LLM, according to human\ndomain knowledge. Unlike traditional evaluations on language generation\nresults, we propose to evaluate the correctness of the detailed inference\npatterns of an LLM behind its seemingly correct outputs. To this end, we\nquantify the interactions between input phrases used by the LLM as primitive\ninference patterns, because recent theoretical achievements have proven several\nmathematical guarantees of the faithfulness of the interaction-based\nexplanation. We design a set of metrics to evaluate the detailed inference\npatterns of LLMs. Experiments show that even when the language generation\nresults appear correct, a significant portion of the inference patterns used by\nthe LLM for the legal judgment may represent misleading or irrelevant logic.\n","authors":["Lu Chen","Yuxuan Huang","Yixing Li","Dongrui Liu","Qihan Ren","Shuai Zhao","Kun Kuang","Zilong Zheng","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.09083v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14499v1","updated":"2025-05-20T15:28:26Z","published":"2025-05-20T15:28:26Z","title":"Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated\n  Rationales","summary":"  There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA.\n","authors":["Jun Cao","Jiyi Li","Ziwei Yang","Renjie Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.14499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06993v2","updated":"2025-05-20T15:25:12Z","published":"2025-05-11T14:37:30Z","title":"Technical Report: Quantifying and Analyzing the Generalization Power of\n  a DNN","summary":"  This paper proposes a new perspective for analyzing the generalization power\nof deep neural networks (DNNs), i.e., directly disentangling and analyzing the\ndynamics of generalizable and non-generalizable interaction encoded by a DNN\nthrough the training process. Specifically, this work builds upon the recent\ntheoretical achievement in explainble AI, which proves that the detailed\ninference logic of DNNs can be can be strictly rewritten as a small number of\nAND-OR interaction patterns. Based on this, we propose an efficient method to\nquantify the generalization power of each interaction, and we discover a\ndistinct three-phase dynamics of the generalization power of interactions\nduring training. In particular, the early phase of training typically removes\nnoisy and non-generalizable interactions and learns simple and generalizable\nones. The second and the third phases tend to capture increasingly complex\ninteractions that are harder to generalize. Experimental results verify that\nthe learning of non-generalizable interactions is the the direct cause for the\ngap between the training and testing losses.\n","authors":["Yuxuan He","Junpeng Zhang","Lei Cheng","Hongyuan Zhang","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.06993v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09948v3","updated":"2025-05-20T15:19:09Z","published":"2024-03-15T01:18:08Z","title":"RadCLIP: Enhancing Radiologic Image Analysis through Contrastive\n  Language-Image Pre-training","summary":"  The integration of artificial intelligence (AI) with radiology marks a\ntransformative era in medicine. Vision foundation models have been adopted to\nenhance radiologic imaging analysis. However, the distinct complexities of\nradiologic 2D and 3D radiologic data pose unique challenges that existing\nmodels, pre-trained on general non-medical images, fail to address adequately.\nTo bridge this gap and capitalize on the diagnostic precision required in\nradiologic imaging, we introduce Radiologic Contrastive Language-Image\nPre-training (RadCLIP): a cross-modal vision-language foundational model that\nharnesses Vision Language Pre-training (VLP) framework to improve radiologic\nimage analysis. Building upon Contrastive Language-Image Pre-training (CLIP),\nRadCLIP incorporates a slice pooling mechanism tailored for volumetric image\nanalysis and is pre-trained using a large and diverse dataset of radiologic\nimage-text pairs. The RadCLIP was pre-trained to effectively align radiologic\nimages with their corresponding text annotations, creating a robust vision\nbackbone for radiologic images. Extensive experiments demonstrate RadCLIP's\nsuperior performance in both uni-modal radiologic image classification and\ncross-modal image-text matching, highlighting its significant promise for\nimproving diagnostic accuracy and efficiency in clinical settings. Our Key\ncontributions include curating a large dataset with diverse radiologic 2D/3D\nradiologic image-text pairs, a slice pooling adapter using an attention\nmechanism for integrating 2D images, and comprehensive evaluations of RadCLIP\non various radiologic downstream tasks.\n","authors":["Zhixiu Lu","Hailong Li","Nehal A. Parikh","Jonathan R. Dillman","Lili He"],"pdf_url":"https://arxiv.org/pdf/2403.09948v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14489v1","updated":"2025-05-20T15:19:00Z","published":"2025-05-20T15:19:00Z","title":"Reasoning Models Better Express Their Confidence","summary":"  Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.\n","authors":["Dongkeun Yoon","Seungone Kim","Sohee Yang","Sunkyoung Kim","Soyeon Kim","Yongil Kim","Eunbi Choi","Yireun Kim","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2505.14489v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2412.02508v2","updated":"2025-05-20T15:17:39Z","published":"2024-12-03T15:39:05Z","title":"Towards Rich Emotions in 3D Avatars: A Text-to-3D Avatar Generation\n  Benchmark","summary":"  Producing emotionally dynamic 3D facial avatars with text derived from spoken\nwords (Emo3D) has been a pivotal research topic in 3D avatar generation. While\nprogress has been made in general-purpose 3D avatar generation, the exploration\nof generating emotional 3D avatars remains scarce, primarily due to the\ncomplexities of identifying and rendering rich emotions from spoken words. This\npaper reexamines Emo3D generation and draws inspiration from human processes,\nbreaking down Emo3D into two cascading steps: Text-to-3D Expression Mapping\n(T3DEM) and 3D Avatar Rendering (3DAR). T3DEM is the most crucial step in\ndetermining the quality of Emo3D generation and encompasses three key\nchallenges: Expression Diversity, Emotion-Content Consistency, and Expression\nFluidity. To address these challenges, we introduce a novel benchmark to\nadvance research in Emo3D generation. First, we present EmoAva, a large-scale,\nhigh-quality dataset for T3DEM, comprising 15,000 text-to-3D expression\nmappings that characterize the aforementioned three challenges in Emo3D\ngeneration. Furthermore, we develop various metrics to effectively evaluate\nmodels against these identified challenges. Next, to effectively model the\nconsistency, diversity, and fluidity of human expressions in the T3DEM step, we\npropose the Continuous Text-to-Expression Generator, which employs an\nautoregressive Conditional Variational Autoencoder for expression code\ngeneration, enhanced with Latent Temporal Attention and Expression-wise\nAttention mechanisms. Finally, to further enhance the 3DAR step on rendering\nhigher-quality subtle expressions, we present the Globally-informed Gaussian\nAvatar (GiGA) model. GiGA incorporates a global information mechanism into 3D\nGaussian representations, enabling the capture of subtle micro-expressions and\nseamless transitions between emotional states.\n","authors":["Haidong Xu","Meishan Zhang","Hao Ju","Zhedong Zheng","Erik Cambria","Min Zhang","Hao Fei"],"pdf_url":"https://arxiv.org/pdf/2412.02508v2.pdf","comment":"19 pages. Project website: https://github.com/WalkerMitty/EmoAva"},{"id":"http://arxiv.org/abs/2505.14479v1","updated":"2025-05-20T15:13:32Z","published":"2025-05-20T15:13:32Z","title":"Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach","summary":"  Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness.\n","authors":["Oren Sultan","Eitan Stern","Dafna Shahaf"],"pdf_url":"https://arxiv.org/pdf/2505.14479v1.pdf","comment":"long paper"},{"id":"http://arxiv.org/abs/2409.17994v5","updated":"2025-05-20T15:08:41Z","published":"2024-09-26T16:06:38Z","title":"CRoP: Context-wise Robust Static Human-Sensing Personalization","summary":"  The advancement in deep learning and internet-of-things have led to diverse\nhuman sensing applications. However, distinct patterns in human sensing,\ninfluenced by various factors or contexts, challenge the generic neural network\nmodel's performance due to natural distribution shifts. To address this,\npersonalization tailors models to individual users. Yet most personalization\nstudies overlook intra-user heterogeneity across contexts in sensory data,\nlimiting intra-user generalizability. This limitation is especially critical in\nclinical applications, where limited data availability hampers both\ngeneralizability and personalization. Notably, intra-user sensing attributes\nare expected to change due to external factors such as treatment progression,\nfurther complicating the challenges. To address the intra-user generalization\nchallenge, this work introduces CRoP, a novel static personalization approach.\nCRoP leverages off-the-shelf pre-trained models as generic starting points and\ncaptures user-specific traits through adaptive pruning on a minimal sub-network\nwhile allowing generic knowledge to be incorporated in remaining parameters.\nCRoP demonstrates superior personalization effectiveness and intra-user\nrobustness across four human-sensing datasets, including two from real-world\nhealth domains, underscoring its practical and social impact. Additionally, to\nsupport CRoP's generalization ability and design choices, we provide empirical\njustification through gradient inner product analysis, ablation studies, and\ncomparisons against state-of-the-art baselines.\n","authors":["Sawinder Kaur","Avery Gump","Yi Xiao","Jingyu Xin","Harshit Sharma","Nina R Benway","Jonathan L Preston","Asif Salekin"],"pdf_url":"https://arxiv.org/pdf/2409.17994v5.pdf","comment":"34 pages, 6 figues and 15 tables"},{"id":"http://arxiv.org/abs/2505.14469v1","updated":"2025-05-20T15:05:03Z","published":"2025-05-20T15:05:03Z","title":"Attributional Safety Failures in Large Language Models under Code-Mixed\n  Perturbations","summary":"  Recent advancements in LLMs have raised significant safety concerns,\nparticularly when dealing with code-mixed inputs and outputs. Our study\nsystematically investigates the increased susceptibility of LLMs to produce\nunsafe outputs from code-mixed prompts compared to monolingual English prompts.\nUtilizing explainability methods, we dissect the internal attribution shifts\ncausing model's harmful behaviors. In addition, we explore cultural dimensions\nby distinguishing between universally unsafe and culturally-specific unsafe\nqueries. This paper presents novel experimental insights, clarifying the\nmechanisms driving this phenomenon.\n","authors":["Somnath Banerjee","Pratyush Chatterjee","Shanu Kumar","Sayan Layek","Parag Agrawal","Rima Hazra","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2505.14469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10657v2","updated":"2025-05-20T14:59:21Z","published":"2025-03-08T04:07:07Z","title":"RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore\n  Model-level Scaling Up in LLMs","summary":"  Routing large language models (LLMs) is a new paradigm that uses a router to\nrecommend the best LLM from a pool of candidates for a given input. In this\npaper, our comprehensive analysis with more than 8,500 LLMs reveals a novel\nmodel-level scaling up phenomenon in Routing LLMs, i.e., a capable router can\nsignificantly enhance the performance of this paradigm as the number of\ncandidates increases. This improvement can even surpass the performance of the\nbest single model in the pool and many existing strong LLMs, confirming it a\nhighly promising paradigm. However, the lack of comprehensive and open-source\nbenchmarks for Routing LLMs has hindered the development of routers. In this\npaper, we introduce RouterEval, a benchmark tailored for router research, which\nincludes over 200,000,000 performance records for 12 popular LLM evaluations\nacross various areas such as commonsense reasoning, semantic understanding,\netc., based on over 8,500 various LLMs. Using RouterEval, extensive evaluations\nof existing Routing LLM methods reveal that most still have significant room\nfor improvement. See https://github.com/MilkThink-Lab/RouterEval for all data,\ncode and tutorial.\n","authors":["Zhongzhan Huang","Guoming Ling","Yupei Lin","Yandong Chen","Shanshan Zhong","Hefeng Wu","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2503.10657v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.13346v2","updated":"2025-05-20T14:57:18Z","published":"2025-05-19T16:50:35Z","title":"J4R: Learning to Judge with Equivalent Initial State Group Relative\n  Policy Optimization","summary":"  To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.\n","authors":["Austin Xu","Yilun Zhou","Xuan-Phi Nguyen","Caiming Xiong","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2505.13346v2.pdf","comment":"25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark"},{"id":"http://arxiv.org/abs/2411.18463v3","updated":"2025-05-20T14:55:01Z","published":"2024-11-26T15:13:17Z","title":"Hotspot-Driven Peptide Design via Multi-Fragment Autoregressive\n  Extension","summary":"  Peptides, short chains of amino acids, interact with target proteins, making\nthem a unique class of protein-based therapeutics for treating human diseases.\nRecently, deep generative models have shown great promise in peptide\ngeneration. However, several challenges remain in designing effective peptide\nbinders. First, not all residues contribute equally to peptide-target\ninteractions. Second, the generated peptides must adopt valid geometries due to\nthe constraints of peptide bonds. Third, realistic tasks for peptide drug\ndevelopment are still lacking. To address these challenges, we introduce\nPepHAR, a hot-spot-driven autoregressive generative model for designing\npeptides targeting specific proteins. Building on the observation that certain\nhot spot residues have higher interaction potentials, we first use an\nenergy-based density model to fit and sample these key residues. Next, to\nensure proper peptide geometry, we autoregressively extend peptide fragments by\nestimating dihedral angles between residue frames. Finally, we apply an\noptimization process to iteratively refine fragment assembly, ensuring correct\npeptide structures. By combining hot spot sampling with fragment-based\nextension, our approach enables de novo peptide design tailored to a target\nprotein and allows the incorporation of key hot spot residues into peptide\nscaffolds. Extensive experiments, including peptide design and peptide scaffold\ngeneration, demonstrate the strong potential of PepHAR in computational peptide\nbinder design. Source code will be available at\nhttps://github.com/Ced3-han/PepHAR.\n","authors":["Jiahan Li","Tong Chen","Shitong Luo","Chaoran Cheng","Jiaqi Guan","Ruihan Guo","Sheng Wang","Ge Liu","Jian Peng","Jianzhu Ma"],"pdf_url":"https://arxiv.org/pdf/2411.18463v3.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2505.14455v1","updated":"2025-05-20T14:52:41Z","published":"2025-05-20T14:52:41Z","title":"CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block\n  Prediction and Controllable Generation","summary":"  Although autoregressive models have dominated language modeling in recent\nyears, there has been a growing interest in exploring alternative paradigms to\nthe conventional next-token prediction framework. Diffusion-based language\nmodels have emerged as a compelling alternative due to their powerful parallel\ngeneration capabilities and inherent editability. However, these models are\noften constrained by fixed-length generation. A promising direction is to\ncombine the strengths of both paradigms, segmenting sequences into blocks,\nmodeling autoregressive dependencies across blocks while leveraging discrete\ndiffusion to estimate the conditional distribution within each block given the\npreceding context. Nevertheless, their practical application is often hindered\nby two key limitations: rigid fixed-length outputs and a lack of flexible\ncontrol mechanisms. In this work, we address the critical limitations of fixed\ngranularity and weak controllability in current large diffusion language\nmodels. We propose CtrlDiff, a dynamic and controllable semi-autoregressive\nframework that adaptively determines the size of each generation block based on\nlocal semantics using reinforcement learning. Furthermore, we introduce a\nclassifier-guided control mechanism tailored to discrete diffusion, which\nsignificantly reduces computational overhead while facilitating efficient\npost-hoc conditioning without retraining. Extensive experiments demonstrate\nthat CtrlDiff sets a new standard among hybrid diffusion models, narrows the\nperformance gap to state-of-the-art autoregressive approaches, and enables\neffective conditional text generation across diverse tasks.\n","authors":["Chihan Huang","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2505.14455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14452v1","updated":"2025-05-20T14:51:27Z","published":"2025-05-20T14:51:27Z","title":"How Managers Perceive AI-Assisted Conversational Training for Workplace\n  Communication","summary":"  Effective workplace communication is essential for managerial success, yet\nmany managers lack access to tailored and sustained training. Although\nAI-assisted communication systems may offer scalable training solutions, little\nis known about how managers envision the role of AI in helping them improve\ntheir communication skills. To investigate this, we designed a conversational\nrole-play system, CommCoach, as a functional probe to understand how managers\nanticipate using AI to practice their communication skills. Through\nsemi-structured interviews, participants emphasized the value of adaptive,\nlow-risk simulations for practicing difficult workplace conversations. They\nalso highlighted opportunities, including human-AI teaming, transparent and\ncontext-aware feedback, and greater control over AI-generated personas.\nAI-assisted communication training should balance personalization, structured\nlearning objectives, and adaptability to different user styles and contexts.\nHowever, achieving this requires carefully navigating tensions between adaptive\nand consistent AI feedback, realism and potential bias, and the open-ended\nnature of AI conversations versus structured workplace discourse.\n","authors":["Lance T Wilhelm","Xiaohan Ding","Kirk McInnis Knutsen","Buse Carik","Eugenia H Rho"],"pdf_url":"https://arxiv.org/pdf/2505.14452v1.pdf","comment":"accepted to CUI '25"},{"id":"http://arxiv.org/abs/2505.07078v2","updated":"2025-05-20T14:51:24Z","published":"2025-05-11T18:02:21Z","title":"Can LLM-based Financial Investing Strategies Outperform the Market in\n  Long Run?","summary":"  Large Language Models (LLMs) have recently been leveraged for asset pricing\ntasks and stock trading applications, enabling AI agents to generate investment\ndecisions from unstructured financial data. However, most evaluations of LLM\ntiming-based investing strategies are conducted on narrow timeframes and\nlimited stock universes, overstating effectiveness due to survivorship and\ndata-snooping biases. We critically assess their generalizability and\nrobustness by proposing FINSABER, a backtesting framework evaluating\ntiming-based strategies across longer periods and a larger universe of symbols.\nSystematic backtests over two decades and 100+ symbols reveal that previously\nreported LLM advantages deteriorate significantly under broader cross-section\nand over a longer-term evaluation. Our market regime analysis further\ndemonstrates that LLM strategies are overly conservative in bull markets,\nunderperforming passive benchmarks, and overly aggressive in bear markets,\nincurring heavy losses. These findings highlight the need to develop LLM\nstrategies that are able to prioritise trend detection and regime-aware risk\ncontrols over mere scaling of framework complexity.\n","authors":["Weixian Waylon Li","Hyeonjun Kim","Mihai Cucuringu","Tiejun Ma"],"pdf_url":"https://arxiv.org/pdf/2505.07078v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2505.14451v1","updated":"2025-05-20T14:51:07Z","published":"2025-05-20T14:51:07Z","title":"RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data\n  Imputation","summary":"  Missing values in high-dimensional, mixed-type datasets pose significant\nchallenges for data imputation, particularly under Missing Not At Random (MNAR)\nmechanisms. Existing methods struggle to integrate local and global data\ncharacteristics, limiting performance in MNAR and high-dimensional settings. We\npropose an innovative framework, RefiDiff, combining local machine learning\npredictions with a novel Mamba-based denoising network capturing\ninterrelationships among distant features and samples. Our approach leverages\npre-refinement for initial warm-up imputations and post-refinement to polish\nresults, enhancing stability and accuracy. By encoding mixed-type data into\nunified tokens, RefiDiff enables robust imputation without architectural or\nhyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods\nacross missing-value settings, excelling in MNAR with a 4x faster training time\nthan SOTA DDPM-based approaches. Extensive evaluations on nine real-world\ndatasets demonstrate its robustness, scalability, and effectiveness in handling\ncomplex missingness patterns.\n","authors":["Md Atik Ahamed","Qiang Ye","Qiang Cheng"],"pdf_url":"https://arxiv.org/pdf/2505.14451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08828v2","updated":"2025-05-20T14:49:55Z","published":"2025-01-15T14:30:13Z","title":"MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents","summary":"  Multimodal document retrieval aims to identify and retrieve various forms of\nmultimodal content, such as figures, tables, charts, and layout information\nfrom extensive documents. Despite its increasing popularity, there is a notable\nlack of a comprehensive and robust benchmark to effectively evaluate the\nperformance of systems in such tasks. To address this gap, this work introduces\na new benchmark, named MMDocIR, that encompasses two distinct tasks: page-level\nand layout-level retrieval. The former evaluates the performance of identifying\nthe most relevant pages within a long document, while the later assesses the\nability of detecting specific layouts, providing a more fine-grained measure\nthan whole-page analysis. A layout refers to a variety of elements, including\ntextual paragraphs, equations, figures, tables, or charts. The MMDocIR\nbenchmark comprises a rich dataset featuring 1,685 questions annotated by\nexperts and 173,843 questions with bootstrapped labels, making it a valuable\nresource in multimodal document retrieval for both training and evaluation.\nThrough rigorous experiments, we demonstrate that (i) visual retrievers\nsignificantly outperform their text counterparts, (ii) MMDocIR training set\neffectively enhances the performance of multimodal document retrieval and (iii)\ntext retrievers leveraging VLM-text significantly outperforms retrievers\nrelying on OCR-text. Our dataset is available at\nhttps://mmdocrag.github.io/MMDocIR/.\n","authors":["Kuicai Dong","Yujing Chang","Xin Deik Goh","Dexun Li","Ruiming Tang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08828v2.pdf","comment":"https://huggingface.co/MMDocIR"},{"id":"http://arxiv.org/abs/2505.11983v2","updated":"2025-05-20T14:49:41Z","published":"2025-05-17T12:31:12Z","title":"Online Iterative Self-Alignment for Radiology Report Generation","summary":"  Radiology Report Generation (RRG) is an important research topic for\nrelieving radiologist' heavy workload. Existing RRG models mainly rely on\nsupervised fine-tuning (SFT) based on different model architectures using data\npairs of radiological images and corresponding radiologist-annotated reports.\nRecent research has shifted focus to post-training improvements, aligning RRG\nmodel outputs with human preferences using reinforcement learning (RL).\nHowever, the limited data coverage of high-quality annotated data poses risks\nof overfitting and generalization. This paper proposes a novel Online Iterative\nSelf-Alignment (OISA) method for RRG that consists of four stages:\nself-generation of diverse data, self-evaluation for multi-objective preference\ndata,self-alignment for multi-objective optimization and self-iteration for\nfurther improvement. Our approach allows for generating varied reports tailored\nto specific clinical objectives, enhancing the overall performance of the RRG\nmodel iteratively. Unlike existing methods, our frame-work significantly\nincreases data quality and optimizes performance through iterative\nmulti-objective optimization. Experimental results demonstrate that our method\nsurpasses previous approaches, achieving state-of-the-art performance across\nmultiple evaluation metrics.\n","authors":["Ting Xiao","Lei Shi","Yang Zhang","HaoFeng Yang","Zhe Wang","Chenjia Bai"],"pdf_url":"https://arxiv.org/pdf/2505.11983v2.pdf","comment":"Accepted by ACL 2025 Main"},{"id":"http://arxiv.org/abs/2505.14442v1","updated":"2025-05-20T14:43:41Z","published":"2025-05-20T14:43:41Z","title":"Creative Preference Optimization","summary":"  While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality.\n","authors":["Mete Ismayilzada","Antonio Laverghetta Jr.","Simone A. Luchini","Reet Patel","Antoine Bosselut","Lonneke van der Plas","Roger Beaty"],"pdf_url":"https://arxiv.org/pdf/2505.14442v1.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2505.14436v1","updated":"2025-05-20T14:42:03Z","published":"2025-05-20T14:42:03Z","title":"Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric\n  Knowledge Transfer in Large Language Models","summary":"  Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility.\n","authors":["Yuqiao Tan","Shizhu He","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.14436v1.pdf","comment":"Accepted by ACL'25 Main. Code link:\n  https://github.com/Trae1ounG/Neural_Incompatibility"},{"id":"http://arxiv.org/abs/2505.14435v1","updated":"2025-05-20T14:41:56Z","published":"2025-05-20T14:41:56Z","title":"Choosing a Model, Shaping a Future: Comparing LLM Perspectives on\n  Sustainability and its Relationship with AI","summary":"  As organizations increasingly rely on AI systems for decision support in\nsustainability contexts, it becomes critical to understand the inherent biases\nand perspectives embedded in Large Language Models (LLMs). This study\nsystematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek,\nGPT, LLaMA, and Mistral - conceptualize sustainability and its relationship\nwith AI. We administered validated, psychometric sustainability-related\nquestionnaires - each 100 times per model -- to capture response patterns and\nvariability. Our findings revealed significant inter-model differences: For\nexample, GPT exhibited skepticism about the compatibility of AI and\nsustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect\nscores for several Sustainable Development Goals (SDGs). Models also diverged\nin attributing institutional responsibility for AI and sustainability\nintegration, a results that holds implications for technology governance\napproaches. Our results demonstrate that model selection could substantially\ninfluence organizational sustainability strategies, highlighting the need for\nawareness of model-specific biases when deploying LLMs for\nsustainability-related decision-making.\n","authors":["Annika Bush","Meltem Aksoy","Markus Pauly","Greta Ontrup"],"pdf_url":"https://arxiv.org/pdf/2505.14435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14428v1","updated":"2025-05-20T14:38:39Z","published":"2025-05-20T14:38:39Z","title":"Interpretable Neural System Dynamics: Combining Deep Learning with\n  System Dynamics Modeling to Support Critical Applications","summary":"  The objective of this proposal is to bridge the gap between Deep Learning\n(DL) and System Dynamics (SD) by developing an interpretable neural system\ndynamics framework. While DL excels at learning complex models and making\naccurate predictions, it lacks interpretability and causal reliability.\nTraditional SD approaches, on the other hand, provide transparency and causal\ninsights but are limited in scalability and require extensive domain knowledge.\nTo overcome these limitations, this project introduces a Neural System Dynamics\npipeline, integrating Concept-Based Interpretability, Mechanistic\nInterpretability, and Causal Machine Learning. This framework combines the\npredictive power of DL with the interpretability of traditional SD models,\nresulting in both causal reliability and scalability. The efficacy of the\nproposed pipeline will be validated through real-world applications of the\nEU-funded AutoMoTIF project, which is focused on autonomous multimodal\ntransportation systems. The long-term goal is to collect actionable insights\nthat support the integration of explainability and safety in autonomous\nsystems.\n","authors":["Riccardo D'Elia"],"pdf_url":"https://arxiv.org/pdf/2505.14428v1.pdf","comment":"To be submitted to CEUR-WS.org for publication in the Doctoral\n  Consortium Proceedings of XAI 2025, The World Conference on Explainable\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2504.14150v2","updated":"2025-05-20T14:36:36Z","published":"2025-04-19T02:51:20Z","title":"Walk the Talk? Measuring the Faithfulness of Large Language Model\n  Explanations","summary":"  Large language models (LLMs) are capable of generating plausible explanations\nof how they arrived at an answer to a question. However, these explanations can\nmisrepresent the model's \"reasoning\" process, i.e., they can be unfaithful.\nThis, in turn, can lead to over-trust and misuse. We introduce a new approach\nfor measuring the faithfulness of LLM explanations. First, we provide a\nrigorous definition of faithfulness. Since LLM explanations mimic human\nexplanations, they often reference high-level concepts in the input question\nthat purportedly influenced the model. We define faithfulness in terms of the\ndifference between the set of concepts that LLM explanations imply are\ninfluential and the set that truly are. Second, we present a novel method for\nestimating faithfulness that is based on: (1) using an auxiliary LLM to modify\nthe values of concepts within model inputs to create realistic counterfactuals,\nand (2) using a Bayesian hierarchical model to quantify the causal effects of\nconcepts at both the example- and dataset-level. Our experiments show that our\nmethod can be used to quantify and discover interpretable patterns of\nunfaithfulness. On a social bias task, we uncover cases where LLM explanations\nhide the influence of social bias. On a medical question answering task, we\nuncover cases where LLM explanations provide misleading claims about which\npieces of evidence influenced the model's decisions.\n","authors":["Katie Matton","Robert Osazuwa Ness","John Guttag","Emre Kıcıman"],"pdf_url":"https://arxiv.org/pdf/2504.14150v2.pdf","comment":"66 pages, 14 figures, 40 tables; ICLR 2025 (spotlight) camera ready"},{"id":"http://arxiv.org/abs/2502.13160v3","updated":"2025-05-20T14:34:54Z","published":"2025-02-16T03:02:48Z","title":"Attention Mechanism for LLM-based Agents Dynamic Diffusion under\n  Information Asymmetry","summary":"  Large language models have been used to simulate human society using\nmulti-agent systems. Most current social simulation research emphasizes\ninteractive behaviors in fixed environments, ignoring information opacity,\nrelationship variability, and diffusion diversity. In this paper, we first\npropose a general framework for exploring multi-agent information diffusion. We\nidentified LLMs' deficiency in the perception and utilization of social\nrelationships, as well as diverse actions. Then, we designed a dynamic\nattention mechanism to help agents allocate attention to different information,\naddressing the limitations of the LLM attention mechanism. Agents start by\nresponding to external information stimuli within a five-agent group,\nincreasing group size and forming information circles while developing\nrelationships and sharing information. Additionally, we explore the information\ndiffusion features in the asymmetric open environment by observing the\nevolution of information gaps, diffusion patterns, and the accumulation of\nsocial capital, which are closely linked to psychological, sociological, and\ncommunication theories.\n","authors":["Yiwen Zhang","Yifu Wu","Wenyue Hua","Xiang Lu","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2502.13160v3.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.14419v1","updated":"2025-05-20T14:31:15Z","published":"2025-05-20T14:31:15Z","title":"SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated\n  Process Annotation","summary":"  Process Reward Models (PRMs) have demonstrated promising results in\nmathematical reasoning, but existing process annotation approaches, whether\nthrough human annotations or Monte Carlo simulations, remain computationally\nexpensive. In this paper, we introduce Step COmpression for Process Estimation\n(SCOPE), a novel compression-based approach that significantly reduces\nannotation costs. We first translate natural language reasoning steps into code\nand normalize them through Abstract Syntax Tree, then merge equivalent steps to\nconstruct a prefix tree. Unlike simulation-based methods that waste numerous\nsamples on estimation, SCOPE leverages a compression-based prefix tree where\neach root-to-leaf path serves as a training sample, reducing the complexity\nfrom $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K\nsamples with only 5% of the computational resources required by previous\nmethods. Empirical results demonstrate that PRMs trained on our dataset\nconsistently outperform existing automated annotation approaches on both\nBest-of-N strategy and ProcessBench.\n","authors":["Huimin Xu","Xin Mao","Feng-Lin Li","Xiaobao Wu","Wang Chen","Wei Zhang","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2505.14419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14412v1","updated":"2025-05-20T14:26:19Z","published":"2025-05-20T14:26:19Z","title":"PRL: Prompts from Reinforcement Learning","summary":"  Effective prompt engineering remains a central challenge in fully harnessing\nthe capabilities of LLMs. While well-designed prompts can dramatically enhance\nperformance, crafting them typically demands expert intuition and a nuanced\nunderstanding of the task. Moreover, the most impactful prompts often hinge on\nsubtle semantic cues, ones that may elude human perception but are crucial for\nguiding LLM behavior. In this paper, we introduce PRL (Prompts from\nReinforcement Learning), a novel RL-based approach for automatic prompt\ngeneration. Unlike previous methods, PRL can produce novel few-shot examples\nthat were not seen during training. Our approach achieves state-of-the-art\nperformance across a range of benchmarks, including text classification,\nsimplification, and summarization. On the classification task, it surpasses\nprior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it\nimproves the average ROUGE scores on the summarization task by 4.32 over APE\nand by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over\nAPE and by 6.01 over EvoPrompt. Our code is available at\nhttps://github.com/Batorskq/prl .\n","authors":["Paweł Batorski","Adrian Kosmala","Paul Swoboda"],"pdf_url":"https://arxiv.org/pdf/2505.14412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13794v2","updated":"2025-05-20T14:22:45Z","published":"2025-03-18T00:50:40Z","title":"LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation","summary":"  Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design.\n","authors":["Yang Zhou","Shiyu Zhao","Yuxiao Chen","Zhenting Wang","Can Jin","Dimitris N. Metaxas"],"pdf_url":"https://arxiv.org/pdf/2503.13794v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12938v2","updated":"2025-05-20T14:22:15Z","published":"2025-05-19T10:22:04Z","title":"Leveraging LLM Inconsistency to Boost Pass@k Performance","summary":"  Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations.\n","authors":["Uri Dalal","Meirav Segal","Zvika Ben-Haim","Dan Lahav","Omer Nevo"],"pdf_url":"https://arxiv.org/pdf/2505.12938v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03941v2","updated":"2025-05-20T14:21:41Z","published":"2025-05-06T19:38:07Z","title":"GRAML: Goal Recognition As Metric Learning","summary":"  Goal Recognition (GR) is the problem of recognizing an agent's objectives\nbased on observed actions. Recent data-driven approaches for GR alleviate the\nneed for costly, manually crafted domain models. However, these approaches can\nonly reason about a pre-defined set of goals, and time-consuming training is\nneeded for new emerging goals. To keep this model-learning automated while\nenabling quick adaptation to new goals, this paper introduces GRAML: Goal\nRecognition As Metric Learning. GRAML uses a Siamese network to treat GR as a\ndeep metric learning task, employing an RNN that learns a metric over an\nembedding space, where the embeddings for observation traces leading to\ndifferent goals are distant, and embeddings of traces leading to the same goals\nare close. This metric is especially useful when adapting to new goals, even if\ngiven just one example observation trace per goal. Evaluated on a versatile set\nof environments, GRAML shows speed, flexibility, and runtime improvements over\nthe state-of-the-art GR while maintaining accurate recognition.\n","authors":["Matan Shamir","Reuth Mirsky"],"pdf_url":"https://arxiv.org/pdf/2505.03941v2.pdf","comment":"Accepted for publication in International Joint Conference on\n  Artificial Intelligence (IJCAI) 2025"},{"id":"http://arxiv.org/abs/2505.14403v1","updated":"2025-05-20T14:16:49Z","published":"2025-05-20T14:16:49Z","title":"Unearthing Gems from Stones: Policy Optimization with Negative Sample\n  Augmentation for LLM Reasoning","summary":"  Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations.\n","authors":["Zhaohui Yang","Shilei Jiang","Chen Hu","Linjing Li","Shihong Deng","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2505.14403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14398v1","updated":"2025-05-20T14:14:38Z","published":"2025-05-20T14:14:38Z","title":"Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable\n  Computation","summary":"  While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.\n","authors":["Peter Baile Chen","Yi Zhang","Dan Roth","Samuel Madden","Jacob Andreas","Michael Cafarella"],"pdf_url":"https://arxiv.org/pdf/2505.14398v1.pdf","comment":"Data and code are available at https://peterbaile.github.io/lag/"},{"id":"http://arxiv.org/abs/2505.14396v1","updated":"2025-05-20T14:14:05Z","published":"2025-05-20T14:14:05Z","title":"Causal Cartographer: From Mapping to Reasoning Over Counterfactual\n  Worlds","summary":"  Causal world models are systems that can answer counterfactual questions\nabout an environment of interest, i.e. predict how it would have evolved if an\narbitrary subset of events had been realized differently. It requires\nunderstanding the underlying causes behind chains of events and conducting\ncausal inference for arbitrary unseen distributions. So far, this task eludes\nfoundation models, notably large language models (LLMs), which do not have\ndemonstrated causal reasoning capabilities beyond the memorization of existing\ncausal relationships. Furthermore, evaluating counterfactuals in real-world\napplications is challenging since only the factual world is observed, limiting\nevaluation to synthetic datasets. We address these problems by explicitly\nextracting and modeling causal relationships and propose the Causal\nCartographer framework. First, we introduce a graph retrieval-augmented\ngeneration agent tasked to retrieve causal relationships from data. This\napproach allows us to construct a large network of real-world causal\nrelationships that can serve as a repository of causal knowledge and build\nreal-world counterfactuals. In addition, we create a counterfactual reasoning\nagent constrained by causal relationships to perform reliable step-by-step\ncausal inference. We show that our approach can extract causal knowledge and\nimprove the robustness of LLMs for causal reasoning tasks while reducing\ninference costs and spurious correlations.\n","authors":["Gaël Gendron","Jože M. Rožanec","Michael Witbrock","Gillian Dobbie"],"pdf_url":"https://arxiv.org/pdf/2505.14396v1.pdf","comment":"29 pages, 9 pages for the main paper, 20 pages for the references and\n  appendix, 25 figures"},{"id":"http://arxiv.org/abs/2505.14395v1","updated":"2025-05-20T14:14:00Z","published":"2025-05-20T14:14:00Z","title":"MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation\n  Capabilities in Any Language","summary":"  Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.\n","authors":["Seyoung Song","Seogyeong Jeong","Eunsu Kim","Jiho Jin","Dongkwan Kim","Jay Shin","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2505.14395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14394v1","updated":"2025-05-20T14:13:59Z","published":"2025-05-20T14:13:59Z","title":"Knowledge Graph Based Repository-Level Code Generation","summary":"  Recent advancements in Large Language Models (LLMs) have transformed code\ngeneration from natural language queries. However, despite their extensive\nknowledge and ability to produce high-quality code, LLMs often struggle with\ncontextual accuracy, particularly in evolving codebases. Current code search\nand retrieval methods frequently lack robustness in both the quality and\ncontextual relevance of retrieved results, leading to suboptimal code\ngeneration. This paper introduces a novel knowledge graph-based approach to\nimprove code search and retrieval leading to better quality of code generation\nin the context of repository-level tasks. The proposed approach represents code\nrepositories as graphs, capturing structural and relational information for\nenhanced context-aware code generation. Our framework employs a hybrid approach\nfor code retrieval to improve contextual relevance, track inter-file modular\ndependencies, generate more robust code and ensure consistency with the\nexisting codebase. We benchmark the proposed approach on the Evolutionary Code\nBenchmark (EvoCodeBench) dataset, a repository-level code generation benchmark,\nand demonstrate that our method significantly outperforms the baseline\napproach. These findings suggest that knowledge graph based code generation\ncould advance robust, context-sensitive coding assistance tools.\n","authors":["Mihir Athale","Vishal Vaddina"],"pdf_url":"https://arxiv.org/pdf/2505.14394v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.14391v1","updated":"2025-05-20T14:12:05Z","published":"2025-05-20T14:12:05Z","title":"Beyond the First Error: Process Reward Models for Reflective\n  Mathematical Reasoning","summary":"  Many studies focus on data annotation techniques for training effective PRMs.\nHowever, current methods encounter a significant issue when applied to long CoT\nreasoning processes: they tend to focus solely on the first incorrect step and\nall preceding steps, assuming that all subsequent steps are incorrect. These\nmethods overlook the unique self-correction and reflection mechanisms inherent\nin long CoT, where correct reasoning steps may still occur after initial\nreasoning mistakes. To address this issue, we propose a novel data annotation\nmethod for PRMs specifically designed to score the long CoT reasoning process.\nGiven that under the reflection pattern, correct and incorrect steps often\nalternate, we introduce the concepts of Error Propagation and Error Cessation,\nenhancing PRMs' ability to identify both effective self-correction behaviors\nand reasoning based on erroneous steps. Leveraging an LLM-based judger for\nannotation, we collect 1.7 million data samples to train a 7B PRM and evaluate\nit at both solution and step levels. Experimental results demonstrate that\ncompared to existing open-source PRMs and PRMs trained on open-source datasets,\nour PRM achieves superior performance across various metrics, including search\nguidance, BoN, and F1 scores. Compared to widely used MC-based annotation\nmethods, our annotation approach not only achieves higher data efficiency but\nalso delivers superior performance. Detailed analysis is also conducted to\ndemonstrate the stability and generalizability of our method.\n","authors":["Zhaohui Yang","Chenghua He","Xiaowen Shi","Linjing Li","Qiyue Yin","Shihong Deng","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2505.14391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.23326v2","updated":"2025-05-20T14:09:58Z","published":"2025-03-30T05:48:53Z","title":"Exploring Explainable Multi-player MCTS-minimax Hybrids in Board Game\n  Using Process Mining","summary":"  Monte-Carlo Tree Search (MCTS) is a family of sampling-based search\nalgorithms widely used for online planning in sequential decision-making\ndomains and at the heart of many recent advances in artificial intelligence.\nUnderstanding the behavior of MCTS agents is difficult for developers and users\ndue to the frequently large and complex search trees that result from the\nsimulation of many possible futures, their evaluations, and their\nrelationships. This paper presents our ongoing investigation into potential\nexplanations for the decision-making and behavior of MCTS. A weakness of MCTS\nis that it constructs a highly selective tree and, as a result, can miss\ncrucial moves and fall into tactical traps. Full-width minimax search\nconstitutes the solution. We integrate shallow minimax search into the rollout\nphase of multi-player MCTS and use process mining technique to explain agents'\nstrategies in 3v3 checkers.\n","authors":["Yiyu Qian","Tim Miller","Zheng Qian","Liyuan Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.23326v2.pdf","comment":"38 pages, AAAI 2025 PRL"},{"id":"http://arxiv.org/abs/2505.13358v2","updated":"2025-05-20T14:05:02Z","published":"2025-05-19T16:59:47Z","title":"One-Step Offline Distillation of Diffusion-based Models via Koopman\n  Modeling","summary":"  Diffusion-based generative models have demonstrated exceptional performance,\nyet their iterative sampling procedures remain computationally expensive. A\nprominent strategy to mitigate this cost is distillation, with offline\ndistillation offering particular advantages in terms of efficiency, modularity,\nand flexibility. In this work, we identify two key observations that motivate a\nprincipled distillation framework: (1) while diffusion models have been viewed\nthrough the lens of dynamical systems theory, powerful and underexplored tools\ncan be further leveraged; and (2) diffusion models inherently impose\nstructured, semantically coherent trajectories in latent space. Building on\nthese observations, we introduce the Koopman Distillation Model KDM, a novel\noffline distillation approach grounded in Koopman theory-a classical framework\nfor representing nonlinear dynamics linearly in a transformed space. KDM\nencodes noisy inputs into an embedded space where a learned linear operator\npropagates them forward, followed by a decoder that reconstructs clean samples.\nThis enables single-step generation while preserving semantic fidelity. We\nprovide theoretical justification for our approach: (1) under mild assumptions,\nthe learned diffusion dynamics admit a finite-dimensional Koopman\nrepresentation; and (2) proximity in the Koopman latent space correlates with\nsemantic similarity in the generated outputs, allowing for effective trajectory\nalignment. Empirically, KDM achieves state-of-the-art performance across\nstandard offline distillation benchmarks, improving FID scores by up to 40% in\na single generation step. All implementation details and code for the\nexperimental setups are provided in our GitHub -\nhttps://github.com/azencot-group/KDM, or in our project page -\nhttps://sites.google.com/view/koopman-distillation-model.\n","authors":["Nimrod Berman","Ilan Naiman","Moshe Eliasof","Hedi Zisling","Omri Azencot"],"pdf_url":"https://arxiv.org/pdf/2505.13358v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14381v1","updated":"2025-05-20T14:03:24Z","published":"2025-05-20T14:03:24Z","title":"SCAN: Semantic Document Layout Analysis for Textual and Visual\n  Retrieval-Augmented Generation","summary":"  With the increasing adoption of Large Language Models (LLMs) and\nVision-Language Models (VLMs), rich document analysis technologies for\napplications like Retrieval-Augmented Generation (RAG) and visual RAG are\ngaining significant attention. Recent research indicates that using VLMs can\nachieve better RAG performance, but processing rich documents still remains a\nchallenge since a single page contains large amounts of information. In this\npaper, we present SCAN (\\textbf{S}emanti\\textbf{C} Document Layout\n\\textbf{AN}alysis), a novel approach enhancing both textual and visual\nRetrieval-Augmented Generation (RAG) systems working with visually rich\ndocuments. It is a VLM-friendly approach that identifies document components\nwith appropriate semantic granularity, balancing context preservation with\nprocessing efficiency. SCAN uses a coarse-grained semantic approach that\ndivides documents into coherent regions covering continuous components. We\ntrained the SCAN model by fine-tuning object detection models with\nsophisticated annotation datasets. Our experimental results across English and\nJapanese datasets demonstrate that applying SCAN improves end-to-end textual\nRAG performance by up to 9.0\\% and visual RAG performance by up to 6.4\\%,\noutperforming conventional approaches and even commercial document processing\nsolutions.\n","authors":["Yuyang Dong","Nobuhiro Ueda","Krisztián Boros","Daiki Ito","Takuya Sera","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2505.14381v1.pdf","comment":"v1"},{"id":"http://arxiv.org/abs/2505.14377v1","updated":"2025-05-20T14:00:28Z","published":"2025-05-20T14:00:28Z","title":"When Bias Backfires: The Modulatory Role of Counterfactual Explanations\n  on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making","summary":"  Although the integration of artificial intelligence (AI) into everyday tasks\nimproves efficiency and objectivity, it also risks transmitting bias to human\ndecision-making. In this study, we conducted a controlled experiment that\nsimulated hiring decisions to examine how biased AI recommendations - augmented\nwith or without counterfactual explanations - influence human judgment over\ntime. Participants, acting as hiring managers, completed 60 decision trials\ndivided into a baseline phase without AI, followed by a phase with biased (X)AI\nrecommendations (favoring either male or female candidates), and a final\npost-interaction phase without AI. Our results indicate that the participants\nfollowed the AI recommendations 70% of the time when the qualifications of the\ngiven candidates were comparable. Yet, only a fraction of participants detected\nthe gender bias (8 out of 294). Crucially, exposure to biased AI altered\nparticipants' inherent preferences: in the post-interaction phase,\nparticipants' independent decisions aligned with the bias when no\ncounterfactual explanations were provided before, but reversed the bias when\nexplanations were given. Reported trust did not differ significantly across\nconditions. Confidence varied throughout the study phases after exposure to\nmale-biased AI, indicating nuanced effects of AI bias on decision certainty.\nOur findings point to the importance of calibrating XAI to avoid unintended\nbehavioral shifts in order to safeguard equitable decision-making and prevent\nthe adoption of algorithmic bias.\n","authors":["Ulrike Kuhl","Annika Bush"],"pdf_url":"https://arxiv.org/pdf/2505.14377v1.pdf","comment":"Accepted for XAI2025"},{"id":"http://arxiv.org/abs/2505.13126v2","updated":"2025-05-20T13:53:50Z","published":"2025-05-19T13:58:15Z","title":"Zero-Shot Iterative Formalization and Planning in Partially Observable\n  Environments","summary":"  Using LLMs not to predict plans but to formalize an environment into the\nPlanning Domain Definition Language (PDDL) has been shown to improve\nperformance and control. Existing work focuses on fully observable\nenvironments; we tackle the more realistic and challenging partially observable\nenvironments that lack of complete, reliable information. We propose PDDLego+,\na framework to iteratively formalize, plan, grow, and refine PDDL\nrepresentations in a zero-shot manner, without needing access to any existing\ntrajectories. On two textual simulated environments, we show that PDDLego+\nimproves goal reaching success and exhibits robustness against problem\ncomplexity. We also show that the domain knowledge captured after a successful\ntrial can benefit future tasks.\n","authors":["Liancheng Gong","Wang Zhu","Jesse Thomason","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.13126v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14366v1","updated":"2025-05-20T13:49:09Z","published":"2025-05-20T13:49:09Z","title":"Towards Embodied Cognition in Robots via Spatially Grounded Synthetic\n  Worlds","summary":"  We present a conceptual framework for training Vision-Language Models (VLMs)\nto perform Visual Perspective Taking (VPT), a core capability for embodied\ncognition essential for Human-Robot Interaction (HRI). As a first step toward\nthis goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,\nthat enables supervised learning for spatial reasoning tasks. Each instance\nincludes an RGB image, a natural language description, and a ground-truth 4X4\ntransformation matrix representing object pose. We focus on inferring Z-axis\ndistance as a foundational skill, with future extensions targeting full 6\nDegrees Of Freedom (DOFs) reasoning. The dataset is publicly available to\nsupport further research. This work serves as a foundational step toward\nembodied AI systems capable of spatial understanding in interactive human-robot\nscenarios.\n","authors":["Joel Currie","Gioele Migno","Enrico Piacenti","Maria Elena Giannaccini","Patric Bach","Davide De Tommaso","Agnieszka Wykowska"],"pdf_url":"https://arxiv.org/pdf/2505.14366v1.pdf","comment":"Accepted to: Intelligent Autonomous Systems (IAS) 2025 as Late\n  Breaking Report"},{"id":"http://arxiv.org/abs/2505.14351v1","updated":"2025-05-20T13:35:55Z","published":"2025-05-20T13:35:55Z","title":"FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis\n  for Ü-Tsang, Amdo and Kham Speech Dataset Generation","summary":"  Tibetan is a low-resource language with minimal parallel speech corpora\nspanning its three major dialects-\\\"U-Tsang, Amdo, and Kham-limiting progress\nin speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,\nmulti-speaker, multi-dialect text-to-speech framework that synthesizes parallel\ndialectal speech from limited reference audio and explicit dialect labels. Our\nmethod features a novel speaker-dialect fusion module and a Dialect-Specialized\nDynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and\nlinguistic variations across dialects while preserving speaker identity.\nExtensive objective and subjective evaluations demonstrate that FMSD-TTS\nsignificantly outperforms baselines in both dialectal expressiveness and\nspeaker similarity. We further validate the quality and utility of the\nsynthesized speech through a challenging speech-to-speech dialect conversion\ntask. Our contributions include: (1) a novel few-shot TTS system tailored for\nTibetan multi-dialect speech synthesis, (2) the public release of a large-scale\nsynthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source\nevaluation toolkit for standardized assessment of speaker similarity, dialect\nconsistency, and audio quality.\n","authors":["Yutong Liu","Ziyue Zhang","Ban Ma-bao","Yuqing Cai","Yongbin Yu","Renzeng Duojie","Xiangxiang Wang","Fan Gao","Cheng Huang","Nyima Tashi"],"pdf_url":"https://arxiv.org/pdf/2505.14351v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2505.14349v1","updated":"2025-05-20T13:31:43Z","published":"2025-05-20T13:31:43Z","title":"Upgrading Democracies with Fairer Voting Methods","summary":"  Voting methods are instrumental design element of democracies. Citizens use\nthem to express and aggregate their preferences to reach a collective decision.\nHowever, voting outcomes can be as sensitive to voting rules as they are to\npeople's voting choices. Despite the significance and inter-disciplinary\nscientific progress on voting methods, several democracies keep relying on\noutdated voting methods that do not fit modern, pluralistic societies well,\nwhile lacking social innovation. Here, we demonstrate how one can upgrade\nreal-world democracies, namely by using alternative preferential voting methods\nsuch as cumulative voting and the method of equal shares designed for a\nproportional representation of voters' preferences. By rigorously assessing a\nnew participatory budgeting approach applied in the city of Aarau, Switzerland,\nwe unravel the striking voting outcomes of fair voting methods: more winning\nprojects with the same budget and broader geographic and preference\nrepresentation of citizens by the elected projects, in particular for voters\nwho used to be under-represented, while promoting novel project ideas. We\nprovide profound causal evidence showing that citizens prefer proportional\nvoting methods, which possess strong legitimacy without the need of very\ntechnical specialized explanations. We also reveal strong underlying democratic\nvalues exhibited by citizens who support fair voting methods such as altruism\nand compromise. These findings come with a global momentum to unleash a new and\nlong-awaited participation blueprint of how to upgrade democracies.\n","authors":["Evangelos Pournaras","Srijoni Majumdar","Thomas Wellings","Joshua C. Yang","Fatemeh B. Heravan","Regula Hänggli Fricker","Dirk Helbing"],"pdf_url":"https://arxiv.org/pdf/2505.14349v1.pdf","comment":"Includes Supplementary Information"},{"id":"http://arxiv.org/abs/2505.14345v1","updated":"2025-05-20T13:29:04Z","published":"2025-05-20T13:29:04Z","title":"Enhancing Classification with Semi-Supervised Deep Learning Using\n  Distance-Based Sample Weights","summary":"  Recent advancements in semi-supervised deep learning have introduced\neffective strategies for leveraging both labeled and unlabeled data to improve\nclassification performance. This work proposes a semi-supervised framework that\nutilizes a distance-based weighting mechanism to prioritize critical training\nsamples based on their proximity to test data. By focusing on the most\ninformative examples, the method enhances model generalization and robustness,\nparticularly in challenging scenarios with noisy or imbalanced datasets.\nBuilding on techniques such as uncertainty consistency and graph-based\nrepresentations, the approach addresses key challenges of limited labeled data\nwhile maintaining scalability. Experiments on twelve benchmark datasets\ndemonstrate significant improvements across key metrics, including accuracy,\nprecision, and recall, consistently outperforming existing methods. This\nframework provides a robust and practical solution for semi-supervised\nlearning, with potential applications in domains such as healthcare and\nsecurity where data limitations pose significant challenges.\n","authors":["Aydin Abedinia","Shima Tabakhi","Vahid Seydi"],"pdf_url":"https://arxiv.org/pdf/2505.14345v1.pdf","comment":"5 pages, 6 figures. This paper has been accepted for publication and\n  oral presentation at the 2025 10th IEEE International Conference on Machine\n  Learning Technologies (ICMLT 2025). The final authenticated version will be\n  available in IEEE Xplore following the conference"},{"id":"http://arxiv.org/abs/2505.14341v1","updated":"2025-05-20T13:27:52Z","published":"2025-05-20T13:27:52Z","title":"Replace in Translation: Boost Concept Alignment in Counterfactual\n  Text-to-Image","summary":"  Text-to-Image (T2I) has been prevalent in recent years, with most common\ncondition tasks having been optimized nicely. Besides, counterfactual\nText-to-Image is obstructing us from a more versatile AIGC experience. For\nthose scenes that are impossible to happen in real world and anti-physics, we\nshould spare no efforts in increasing the factual feel, which means\nsynthesizing images that people think very likely to be happening, and concept\nalignment, which means all the required objects should be in the same frame. In\nthis paper, we focus on concept alignment. As controllable T2I models have\nachieved satisfactory performance for real applications, we utilize this\ntechnology to replace the objects in a synthesized image in latent space\nstep-by-step to change the image from a common scene to a counterfactual scene\nto meet the prompt. We propose a strategy to instruct this replacing process,\nwhich is called as Explicit Logical Narrative Prompt (ELNP), by using the newly\nSoTA language model DeepSeek to generate the instructions. Furthermore, to\nevaluate models' performance in counterfactual T2I, we design a metric to\ncalculate how many required concepts in the prompt can be covered averagely in\nthe synthesized images. The extensive experiments and qualitative comparisons\ndemonstrate that our strategy can boost the concept alignment in counterfactual\nT2I.\n","authors":["Sifan Li","Ming Tao","Hao Zhao","Ling Shao","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2505.14341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14425v2","updated":"2025-05-20T13:26:45Z","published":"2024-10-18T12:39:32Z","title":"Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge\n  Distillation","summary":"  Parameter-efficient fine-tuning (PEFT) can bridge the gap between large\nlanguage models (LLMs) and downstream tasks. However, PEFT has been proven\nvulnerable to malicious attacks. Research indicates that poisoned LLMs, even\nafter PEFT, retain the capability to activate internalized backdoors when input\nsamples contain predefined triggers. In this paper, we introduce a novel\nweak-to-strong unlearning algorithm to defend against backdoor attacks based on\nfeature alignment knowledge distillation, named W2SDefense. Specifically, we\nfirst train a small-scale language model through full-parameter fine-tuning to\nserve as the clean teacher model. Then, this teacher model guides the\nlarge-scale poisoned student model in unlearning the backdoor, leveraging PEFT.\nTheoretical analysis suggests that W2SDefense has the potential to enhance the\nstudent model's ability to unlearn backdoor features, preventing the activation\nof the backdoor. We conduct comprehensive experiments on three state-of-the-art\nlarge language models and several different backdoor attack algorithms. Our\nempirical results demonstrate the outstanding performance of W2SDefense in\ndefending against backdoor attacks without compromising model performance.\n","authors":["Shuai Zhao","Xiaobao Wu","Cong-Duy Nguyen","Yanhao Jia","Meihuizi Jia","Yichao Feng","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2410.14425v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14330v1","updated":"2025-05-20T13:16:55Z","published":"2025-05-20T13:16:55Z","title":"Handloom Design Generation Using Generative Networks","summary":"  This paper proposes deep learning techniques of generating designs for\nclothing, focused on handloom fabric and discusses the associated challenges\nalong with its application. The capability of generative neural network models\nin understanding artistic designs and synthesizing those is not yet explored\nwell. In this work, multiple methods are employed incorporating the current\nstate of the art generative models and style transfer algorithms to study and\nobserve their performance for the task. The results are then evaluated through\nuser score. This work also provides a new dataset NeuralLoom for the task of\nthe design generation.\n","authors":["Rajat Kanti Bhattacharjee","Meghali Nandi","Amrit Jha","Gunajit Kalita","Ferdous Ahmed Barbhuiya"],"pdf_url":"https://arxiv.org/pdf/2505.14330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00379v4","updated":"2025-05-20T13:10:54Z","published":"2025-02-01T09:35:51Z","title":"Latent Action Learning Requires Supervision in the Presence of\n  Distractors","summary":"  Recently, latent action learning, pioneered by Latent Action Policies (LAPO),\nhave shown remarkable pre-training efficiency on observation-only data,\noffering potential for leveraging vast amounts of video available on the web\nfor embodied AI. However, prior work has focused on distractor-free data, where\nchanges between observations are primarily explained by ground-truth actions.\nUnfortunately, real-world videos contain action-correlated distractors that may\nhinder latent action learning. Using Distracting Control Suite (DCS) we\nempirically investigate the effect of distractors on latent action learning and\ndemonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO\nmodification that improves the quality of latent actions by 8x, as measured by\nlinear probing. Importantly, we show that providing supervision with\nground-truth actions, as few as 2.5% of the full dataset, during latent action\nlearning improves downstream performance by 4.2x on average. Our findings\nsuggest that integrating supervision during Latent Action Models (LAM) training\nis critical in the presence of distractors, challenging the conventional\npipeline of first learning LAM and only then decoding from latent to\nground-truth actions.\n","authors":["Alexander Nikulin","Ilya Zisman","Denis Tarasov","Nikita Lyubaykin","Andrei Polubarov","Igor Kiselev","Vladislav Kurenkov"],"pdf_url":"https://arxiv.org/pdf/2502.00379v4.pdf","comment":"ICML 2025, Poster, Source code: https://github.com/dunnolab/laom"},{"id":"http://arxiv.org/abs/2505.11568v2","updated":"2025-05-20T13:09:44Z","published":"2025-05-16T09:46:08Z","title":"BioCube: A Multimodal Dataset for Biodiversity Research","summary":"  Biodiversity research requires complete and detailed information to study\necosystem dynamics at different scales. Employing data-driven methods like\nMachine Learning is getting traction in ecology and more specific biodiversity,\noffering alternative modelling pathways. For these methods to deliver accurate\nresults there is the need for large, curated and multimodal datasets that offer\ngranular spatial and temporal resolutions. In this work, we introduce BioCube,\na multimodal, fine-grained global dataset for ecology and biodiversity\nresearch. BioCube incorporates species observations through images, audio\nrecordings and descriptions, environmental DNA, vegetation indices,\nagricultural, forest, land indicators, and high-resolution climate variables.\nAll observations are geospatially aligned under the WGS84 geodetic system,\nspanning from 2000 to 2020. The dataset will become available at\nhttps://huggingface.co/datasets/BioDT/BioCube while the acquisition and\nprocessing code base at https://github.com/BioDT/bfm-data.\n","authors":["Stylianos Stasinos","Martino Mensio","Elena Lazovik","Athanasios Trantas"],"pdf_url":"https://arxiv.org/pdf/2505.11568v2.pdf","comment":"submitted to BiDS'25, 5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2409.13652v3","updated":"2025-05-20T13:06:00Z","published":"2024-09-20T17:02:00Z","title":"OATS: Outlier-Aware Pruning Through Sparse and Low Rank Decomposition","summary":"  The recent paradigm shift to large-scale foundation models has brought about\na new era for deep learning that, while has found great success in practice,\nhas also been plagued by prohibitively expensive costs in terms of high memory\nconsumption and compute. To mitigate these issues, there has been a concerted\neffort in post-hoc neural network pruning techniques that do not require costly\nretraining. Despite the considerable progress being made, existing methods\noften exhibit a steady drop in model performance as the compression increases.\nIn this paper, we present a novel approach to compressing large transformers,\ncoined OATS, that utilizes the second moment information in the input\nembeddings to decompose the model weights into a sum of sparse and low-rank\nmatrices. Without any retraining, OATS achieves state-of-the-art performance\nwhen compressing models by up to $60\\%$ on large language models such as\nLlama-3 and Phi-3 and vision transformers such as ViT and DINOv2 while\ndelivering up to $1.37\\times$ the CPU acceleration versus a model that was\ncomparably pruned.\n","authors":["Stephen Zhang","Vardan Papyan"],"pdf_url":"https://arxiv.org/pdf/2409.13652v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14316v1","updated":"2025-05-20T13:03:15Z","published":"2025-05-20T13:03:15Z","title":"Exploring Jailbreak Attacks on LLMs through Intent Concealment and\n  Diversion","summary":"  Although large language models (LLMs) have achieved remarkable advancements,\ntheir security remains a pressing concern. One major threat is jailbreak\nattacks, where adversarial prompts bypass model safeguards to generate harmful\nor objectionable content. Researchers study jailbreak attacks to understand\nsecurity and robustness of LLMs. However, existing jailbreak attack methods\nface two main challenges: (1) an excessive number of iterative queries, and (2)\npoor generalization across models. In addition, recent jailbreak evaluation\ndatasets focus primarily on question-answering scenarios, lacking attention to\ntext generation tasks that require accurate regeneration of toxic content. To\ntackle these challenges, we propose two contributions: (1) ICE, a novel\nblack-box jailbreak method that employs Intent Concealment and divErsion to\neffectively circumvent security constraints. ICE achieves high attack success\nrates (ASR) with a single query, significantly improving efficiency and\ntransferability across different models. (2) BiSceneEval, a comprehensive\ndataset designed for assessing LLM robustness in question-answering and\ntext-generation tasks. Experimental results demonstrate that ICE outperforms\nexisting jailbreak techniques, revealing critical vulnerabilities in current\ndefense mechanisms. Our findings underscore the necessity of a hybrid security\nstrategy that integrates predefined security mechanisms with real-time semantic\ndecomposition to enhance the security of LLMs.\n","authors":["Tiehan Cui","Yanxu Mao","Peipei Liu","Congying Liu","Datao You"],"pdf_url":"https://arxiv.org/pdf/2505.14316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14312v1","updated":"2025-05-20T13:00:43Z","published":"2025-05-20T13:00:43Z","title":"MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional\n  Evaluation in Tabular Domains","summary":"  Despite the widespread use of tabular data in real-world applications, most\nbenchmarks rely on average-case metrics, which fail to reveal how model\nbehavior varies across diverse data regimes. To address this, we propose\nMultiTab, a benchmark suite and evaluation framework for multi-dimensional,\ndata-aware analysis of tabular learning algorithms. Rather than comparing\nmodels only in aggregate, MultiTab categorizes 196 publicly available datasets\nalong key data characteristics, including sample size, label imbalance, and\nfeature interaction, and evaluates 13 representative models spanning a range of\ninductive biases. Our analysis shows that model performance is highly sensitive\nto such regimes: for example, models using sample-level similarity excel on\ndatasets with large sample sizes or high inter-feature correlation, while\nmodels encoding inter-feature dependencies perform best with weakly correlated\nfeatures. These findings reveal that inductive biases do not always behave as\nintended, and that regime-aware evaluation is essential for understanding and\nimproving model behavior. MultiTab enables more principled model design and\noffers practical guidance for selecting models tailored to specific data\ncharacteristics. All datasets, code, and optimization logs are publicly\navailable at https://huggingface.co/datasets/LGAI-DILab/Multitab.\n","authors":["Kyungeun Lee","Moonjung Eo","Hye-Seung Cho","Dongmin Kim","Ye Seul Sim","Seoyoon Kim","Min-Kook Suh","Woohyung Lim"],"pdf_url":"https://arxiv.org/pdf/2505.14312v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2505.12711v2","updated":"2025-05-20T12:57:58Z","published":"2025-05-19T05:07:34Z","title":"Any-to-Any Learning in Computational Pathology via Triplet Multimodal\n  Pretraining","summary":"  Recent advances in computational pathology and artificial intelligence have\nsignificantly enhanced the utilization of gigapixel whole-slide images and and\nadditional modalities (e.g., genomics) for pathological diagnosis. Although\ndeep learning has demonstrated strong potential in pathology, several key\nchallenges persist: (1) fusing heterogeneous data types requires sophisticated\nstrategies beyond simple concatenation due to high computational costs; (2)\ncommon scenarios of missing modalities necessitate flexible strategies that\nallow the model to learn robustly in the absence of certain modalities; (3) the\ndownstream tasks in CPath are diverse, ranging from unimodal to multimodal,\ncnecessitating a unified model capable of handling all modalities. To address\nthese challenges, we propose ALTER, an any-to-any tri-modal pretraining\nframework that integrates WSIs, genomics, and pathology reports. The term \"any\"\nemphasizes ALTER's modality-adaptive design, enabling flexible pretraining with\nany subset of modalities, and its capacity to learn robust, cross-modal\nrepresentations beyond WSI-centric approaches. We evaluate ALTER across\nextensive clinical tasks including survival prediction, cancer subtyping, gene\nmutation prediction, and report generation, achieving superior or comparable\nperformance to state-of-the-art baselines.\n","authors":["Qichen Sun","Zhengrui Guo","Rui Peng","Hao Chen","Jinzhuo Wang"],"pdf_url":"https://arxiv.org/pdf/2505.12711v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14300v1","updated":"2025-05-20T12:49:58Z","published":"2025-05-20T12:49:58Z","title":"SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring\n  Deceptive Behaviors","summary":"  High-risk industries like nuclear and aviation use real-time monitoring to\ndetect dangerous system conditions. Similarly, Large Language Models (LLMs)\nneed monitoring safeguards. We propose a real-time framework to predict harmful\nAI outputs before they occur by using an unsupervised approach that treats\nnormal behavior as the baseline and harmful outputs as outliers. Our study\nfocuses specifically on backdoor-triggered responses -- where specific input\nphrases activate hidden vulnerabilities causing the model to generate unsafe\ncontent like violence, pornography, or hate speech. We address two key\nchallenges: (1) identifying true causal indicators rather than surface\ncorrelations, and (2) preventing advanced models from deception -- deliberately\nevading monitoring systems. Hence, we approach this problem from an\nunsupervised lens by drawing parallels to human deception: just as humans\nexhibit physical indicators while lying, we investigate whether LLMs display\ndistinct internal behavioral signatures when generating harmful content. Our\nstudy addresses two critical challenges: 1) designing monitoring systems that\ncapture true causal indicators rather than superficial correlations; and\n2)preventing intentional evasion by increasingly capable \"Future models''. Our\nfindings show that models can produce harmful content through causal mechanisms\nand can become deceptive by: (a) alternating between linear and non-linear\nrepresentations, and (b) modifying feature relationships. To counter this, we\ndeveloped Safety-Net -- a multi-detector framework that monitors different\nrepresentation dimensions, successfully detecting harmful behavior even when\ninformation is shifted across representational spaces to evade individual\nmonitors. Our evaluation shows 96% accuracy in detecting harmful cases using\nour unsupervised ensemble approach.\n","authors":["Maheep Chaudhary","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2505.14300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17980v2","updated":"2025-05-20T12:48:26Z","published":"2024-10-23T15:51:13Z","title":"Scaling Stick-Breaking Attention: An Efficient Implementation and\n  In-depth Study","summary":"  The self-attention mechanism traditionally relies on the softmax operator,\nnecessitating positional embeddings like RoPE, or position biases to account\nfor token order. But current methods using still face length generalisation\nchallenges. We investigate an alternative attention mechanism based on the\nstick-breaking process in larger scale settings. The method works as follows:\nFor each token before the current, we determine a break point, which represents\nthe proportion of the stick, the weight of the attention, to allocate to the\ncurrent token. We repeat this on the remaining stick, until all tokens are\nallocated a weight, resulting in a sequence of attention weights. This process\nnaturally incorporates recency bias, which has linguistic motivations for\ngrammar parsing. We study the implications of replacing the conventional\nsoftmax-based attention mechanism with stick-breaking attention. We then\ndiscuss implementation of numerically stable stick-breaking attention and adapt\nFlash Attention to accommodate this mechanism. When used as a drop-in\nreplacement for current softmax+RoPE attention systems, we find that\nstick-breaking attention performs competitively with current methods on length\ngeneralisation and downstream tasks. Stick-breaking also performs well at\nlength generalisation, allowing a model trained with $2^{11}$ context window to\nperform well at $2^{14}$ with perplexity improvements.\n","authors":["Shawn Tan","Songlin Yang","Aaron Courville","Rameswar Panda","Yikang Shen"],"pdf_url":"https://arxiv.org/pdf/2410.17980v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14232v2","updated":"2025-05-20T12:46:42Z","published":"2025-03-18T13:09:01Z","title":"CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion\n  Models","summary":"  Text-to-Image diffusion models can produce undesirable content that\nnecessitates concept erasure. However, existing methods struggle with\nunder-erasure, leaving residual traces of targeted concepts, or over-erasure,\nmistakenly eliminating unrelated but visually similar concepts. To address\nthese limitations, we introduce CRCE, a novel concept erasure framework that\nleverages Large Language Models to identify both semantically related concepts\nthat should be erased alongside the target and distinct concepts that should be\npreserved. By explicitly modelling coreferential and retained concepts\nsemantically, CRCE enables more precise concept removal, without unintended\nerasure. Experiments demonstrate that CRCE outperforms existing methods on\ndiverse erasure tasks, including real-world object, person identities, and\nabstract intellectual property characteristics. The constructed dataset\nCorefConcept and the source code will be release upon acceptance.\n","authors":["Yuyang Xue","Edward Moroshko","Feng Chen","Jingyu Sun","Steven McDonagh","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2503.14232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14295v1","updated":"2025-05-20T12:44:14Z","published":"2025-05-20T12:44:14Z","title":"Benchmarking data encoding methods in Quantum Machine Learning","summary":"  Data encoding plays a fundamental and distinctive role in Quantum Machine\nLearning (QML). While classical approaches process data directly as vectors,\nQML may require transforming classical data into quantum states through\nencoding circuits, known as quantum feature maps or quantum embeddings. This\nstep leverages the inherently high-dimensional and non-linear nature of Hilbert\nspace, enabling more efficient data separation in complex feature spaces that\nmay be inaccessible to classical methods. This encoding part significantly\naffects the performance of the QML model, so it is important to choose the\nright encoding method for the dataset to be encoded. However, this choice is\ngenerally arbitrary, since there is no \"universal\" rule for knowing which\nencoding to choose based on a specific set of data. There are currently a\nvariety of encoding methods using different quantum logic gates. We studied the\nmost commonly used types of encoding methods and benchmarked them using\ndifferent datasets.\n","authors":["Orlane Zang","Grégoire Barrué","Tony Quertier"],"pdf_url":"https://arxiv.org/pdf/2505.14295v1.pdf","comment":"30 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.06330v2","updated":"2025-05-20T12:43:04Z","published":"2025-05-09T15:35:11Z","title":"Prompting Large Language Models for Training-Free Non-Intrusive Load\n  Monitoring","summary":"  Non-intrusive load monitoring (NILM) aims to disaggregate aggregate household\nelectricity consumption into individual appliance usage and thus enables more\neffective energy management. While deep learning has advanced NILM, it remains\nlimited by its dependence on labeled data, restricted generalization, and lack\nof explainability. This paper introduces the first prompt-based NILM framework\nthat leverages large language models (LLMs) with in-context learning. We design\nand evaluate prompt strategies that integrate appliance features, timestamps\nand contextual information, as well as representative time-series examples on\nwidely used open datasets. With optimized prompts, LLMs achieve competitive\nstate detection accuracy and demonstrate robust generalization without the need\nfor fine-tuning. LLMs also enhance explainability by providing clear,\nhuman-readable explanations for their predictions. Our results show that LLMs\ncan reduce data requirements, improve adaptability, and provide transparent\nenergy disaggregation in NILM applications.\n","authors":["Junyu Xue","Xudong Wang","Xiaoling He","Shicheng Liu","Yi Wang","Guoming Tang"],"pdf_url":"https://arxiv.org/pdf/2505.06330v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10271v3","updated":"2025-05-20T12:41:51Z","published":"2024-05-16T17:27:41Z","title":"Federated Hybrid Model Pruning through Loss Landscape Exploration","summary":"  As the era of connectivity and unprecedented data generation expands,\ncollaborative intelligence emerges as a key driver for machine learning,\nencouraging global-scale model development. Federated learning (FL) stands at\nthe heart of this transformation, enabling distributed systems to work\ncollectively on complex tasks while respecting strict constraints on privacy\nand security. Despite its vast potential, specially in the age of complex\nmodels, FL encounters challenges such as elevated communication costs,\ncomputational constraints, and the heterogeneous data distributions. In this\ncontext, we present AutoFLIP, a novel framework that optimizes FL through an\nadaptive hybrid pruning approach, grounded in a federated loss exploration\nphase. By jointly analyzing diverse non-IID client loss landscapes, AutoFLIP\nefficiently identifies model substructures for pruning both at structured and\nunstructured levels. This targeted optimization fosters a symbiotic\nintelligence loop, reducing computational burdens and boosting model\nperformance on resource-limited devices for a more inclusive and democratized\nmodel usage. Our extensive experiments across multiple datasets and FL tasks\nshow that AutoFLIP delivers quantifiable benefits: a 48.8% reduction in\ncomputational overhead, a 35.5% decrease in communication costs, and a notable\nimprovement in global accuracy. By significantly reducing these overheads,\nAutoFLIP offer the way for efficient FL deployment in real-world applications\nfor a scalable and broad applicability.\n","authors":["Christian Internò","Elena Raponi","Niki van Stein","Thomas Bäck","Markus Olhofer","Yaochu Jin","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2405.10271v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14289v1","updated":"2025-05-20T12:41:05Z","published":"2025-05-20T12:41:05Z","title":"EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection","summary":"  As multimodal agents are increasingly trained to operate graphical user\ninterfaces (GUIs) to complete user tasks, they face a growing threat from\nindirect prompt injection, attacks in which misleading instructions are\nembedded into the agent's visual environment, such as popups or chat messages,\nand misinterpreted as part of the intended task. A typical example is\nenvironmental injection, in which GUI elements are manipulated to influence\nagent behavior without directly modifying the user prompt. To address these\nemerging attacks, we propose EVA, a red teaming framework for indirect prompt\ninjection which transforms the attack into a closed loop optimization by\ncontinuously monitoring an agent's attention distribution over the GUI and\nupdating adversarial cues, keywords, phrasing, and layout, in response.\nCompared with prior one shot methods that generate fixed prompts without regard\nfor how the model allocates visual attention, EVA dynamically adapts to\nemerging attention hotspots, yielding substantially higher attack success rates\nand far greater transferability across diverse GUI scenarios. We evaluate EVA\non six widely used generalist and specialist GUI agents in realistic settings\nsuch as popup manipulation, chat based phishing, payments, and email\ncomposition. Experimental results show that EVA substantially improves success\nrates over static baselines. Under goal agnostic constraints, where the\nattacker does not know the agent's task intent, EVA still discovers effective\npatterns. Notably, we find that injection styles transfer well across models,\nrevealing shared behavioral biases in GUI agents. These results suggest that\nevolving indirect prompt injection is a powerful tool not only for red teaming\nagents, but also for uncovering common vulnerabilities in their multimodal\ndecision making.\n","authors":["Yijie Lu","Tianjie Ju","Manman Zhao","Xinbei Ma","Yuan Guo","ZhuoSheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.14289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08399v2","updated":"2025-05-20T12:38:58Z","published":"2025-04-11T10:03:55Z","title":"Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in\n  Large Language Models","summary":"  Self-report questionnaires have long been used to assess LLM personality\ntraits, yet they fail to capture behavioral nuances due to biases and\nmeta-knowledge contamination. This paper proposes a novel multi-observer\nframework for personality trait assessments in LLM agents that draws on\ninformant-report methods in psychology. Instead of relying on self-assessments,\nwe employ multiple observer agents. Each observer is configured with a specific\nrelational context (e.g., family member, friend, or coworker) and engages the\nsubject LLM in dialogue before evaluating its behavior across the Big Five\ndimensions. We show that these observer-report ratings align more closely with\nhuman judgments than traditional self-reports and reveal systematic biases in\nLLM self-assessments. We also found that aggregating responses from 5 to 7\nobservers reduces systematic biases and achieves optimal reliability. Our\nresults highlight the role of relationship context in perceiving personality\nand demonstrate that a multi-observer paradigm offers a more reliable,\ncontext-sensitive approach to evaluating LLM personality traits.\n","authors":["Yin Jou Huang","Rafik Hadfi"],"pdf_url":"https://arxiv.org/pdf/2504.08399v2.pdf","comment":"16 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.05863v2","updated":"2025-05-20T12:37:05Z","published":"2025-02-09T11:46:05Z","title":"Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education","summary":"  In AI-facilitated teaching, leveraging various query styles to interpret\nabstract text descriptions is crucial for ensuring high-quality teaching.\nHowever, current retrieval models primarily focus on natural text-image\nretrieval, making them insufficiently tailored to educational scenarios due to\nthe ambiguities in the retrieval process. In this paper, we propose a diverse\nexpression retrieval task tailored to educational scenarios, supporting\nretrieval based on multiple query styles and expressions. We introduce the STEM\nEducation Retrieval Dataset (SER), which contains over 24,000 query pairs of\ndifferent styles, and the Uni-Retrieval, an efficient and style-diversified\nretrieval vision-language model based on prompt tuning. Uni-Retrieval extracts\nquery style features as prototypes and builds a continuously updated Prompt\nBank containing prompt tokens for diverse queries. This bank can updated during\ntest time to represent domain-specific knowledge for different subject\nretrieval scenarios. Our framework demonstrates scalability and robustness by\ndynamically retrieving prompt tokens based on prototype similarity, effectively\nfacilitating learning for unknown queries. Experimental results indicate that\nUni-Retrieval outperforms existing retrieval models in most retrieval tasks.\nThis advancement provides a scalable and precise solution for diverse\neducational needs.\n","authors":["Yanhao Jia","Xinyi Wu","Hao Li","Qinglin Zhang","Yuxiao Hu","Shuai Zhao","Wenqi Fan"],"pdf_url":"https://arxiv.org/pdf/2502.05863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14285v1","updated":"2025-05-20T12:35:43Z","published":"2025-05-20T12:35:43Z","title":"AquaSignal: An Integrated Framework for Robust Underwater Acoustic\n  Analysis","summary":"  This paper presents AquaSignal, a modular and scalable pipeline for\npreprocessing, denoising, classification, and novelty detection of underwater\nacoustic signals. Designed to operate effectively in noisy and dynamic marine\nenvironments, AquaSignal integrates state-of-the-art deep learning\narchitectures to enhance the reliability and accuracy of acoustic signal\nanalysis. The system is evaluated on a combined dataset from the Deepship and\nOcean Networks Canada (ONC) benchmarks, providing a diverse set of real-world\nunderwater scenarios. AquaSignal employs a U-Net architecture for denoising, a\nResNet18 convolutional neural network for classifying known acoustic events,\nand an AutoEncoder-based model for unsupervised detection of novel or anomalous\nsignals. To our knowledge, this is the first comprehensive study to apply and\nevaluate this combination of techniques on maritime vessel acoustic data.\nExperimental results show that AquaSignal improves signal clarity and task\nperformance, achieving 71% classification accuracy and 91% accuracy in novelty\ndetection. Despite slightly lower classification performance compared to some\nstate-of-the-art models, differences in data partitioning strategies limit\ndirect comparisons. Overall, AquaSignal demonstrates strong potential for\nreal-time underwater acoustic monitoring in scientific, environmental, and\nmaritime domains.\n","authors":["Eirini Panteli","Paulo E. Santos","Nabil Humphrey"],"pdf_url":"https://arxiv.org/pdf/2505.14285v1.pdf","comment":"8 pages; 9 figures"},{"id":"http://arxiv.org/abs/2505.14279v1","updated":"2025-05-20T12:30:46Z","published":"2025-05-20T12:30:46Z","title":"YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering","summary":"  Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry and artificial general intelligence.\n","authors":["Jennifer D'Souza","Hamed Babaei Giglou","Quentin Münch"],"pdf_url":"https://arxiv.org/pdf/2505.14279v1.pdf","comment":"8 pages, 3 figures, Accepted as a Long Paper at the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)"},{"id":"http://arxiv.org/abs/2505.07447v2","updated":"2025-05-20T12:27:53Z","published":"2025-05-12T11:15:39Z","title":"Unified Continuous Generative Models","summary":"  Recent advances in continuous generative models, including multi-step\napproaches like diffusion and flow-matching (typically requiring 8-1000\nsampling steps) and few-step methods such as consistency models (typically 1-8\nsteps), have demonstrated impressive generative performance. However, existing\nwork often treats these approaches as distinct paradigms, resulting in separate\ntraining and sampling methodologies. We introduce a unified framework for\ntraining, sampling, and analyzing these models. Our implementation, the Unified\nContinuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves\nstate-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a\n675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID\nin 20 steps and a few-step model reaching 1.42 FID in just 2 steps.\nAdditionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at\n250 steps) improves performance to 1.06 FID in only 40 steps. Code is available\nat: https://github.com/LINs-lab/UCGM.\n","authors":["Peng Sun","Yi Jiang","Tao Lin"],"pdf_url":"https://arxiv.org/pdf/2505.07447v2.pdf","comment":"https://github.com/LINs-lab/UCGM"},{"id":"http://arxiv.org/abs/2505.13232v2","updated":"2025-05-20T12:27:33Z","published":"2025-05-19T15:15:35Z","title":"StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment","summary":"  Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions. We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features. Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance.\n","authors":["Younghyun Kim","Jongheon Jeong","Sangkyung Kwak","Kyungmin Lee","Juho Lee","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2505.13232v2.pdf","comment":"IJCAI 2025; Code is available at https://github.com/alinlab/StarFT"},{"id":"http://arxiv.org/abs/2505.14273v1","updated":"2025-05-20T12:26:03Z","published":"2025-05-20T12:26:03Z","title":"X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary\n  Rule-Based Machine Learning","summary":"  Function approximation is a critical task in various fields. However,\nexisting neural network approaches struggle with locally complex or\ndiscontinuous functions due to their reliance on a single global model covering\nthe entire problem space. We propose X-KAN, a novel method that optimizes\nmultiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary\nrule-based machine learning framework called XCSF. X-KAN combines KAN's high\nexpressiveness with XCSF's adaptive partitioning capability by implementing\nlocal KAN models as rule consequents and defining local regions via rule\nantecedents. Our experimental results on artificial test functions and\nreal-world datasets demonstrate that X-KAN significantly outperforms\nconventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms\nof approximation accuracy. Notably, X-KAN effectively handles functions with\nlocally complex or discontinuous structures that are challenging for\nconventional KAN, using a compact set of rules (average 7.2 $\\pm$ 2.3 rules).\nThese results validate the effectiveness of using KAN as a local model in XCSF,\nwhich evaluates the rule fitness based on both accuracy and generality. Our\nX-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.\n","authors":["Hiroki Shiraishi","Hisao Ishibuchi","Masaya Nakata"],"pdf_url":"https://arxiv.org/pdf/2505.14273v1.pdf","comment":"Accepted by the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)"},{"id":"http://arxiv.org/abs/2505.14268v1","updated":"2025-05-20T12:19:10Z","published":"2025-05-20T12:19:10Z","title":"Think-J: Learning to Think for Generative LLM-as-a-Judge","summary":"  LLM-as-a-Judge refers to the automatic modeling of preferences for responses\ngenerated by Large Language Models (LLMs), which is of significant importance\nfor both LLM evaluation and reward modeling. Although generative LLMs have made\nsubstantial progress in various tasks, their performance as LLM-Judge still\nfalls short of expectations. In this work, we propose Think-J, which improves\ngenerative LLM-as-a-Judge by learning how to think. We first utilized a small\namount of curated data to develop the model with initial judgment thinking\ncapabilities. Subsequently, we optimize the judgment thinking traces based on\nreinforcement learning (RL). We propose two methods for judgment thinking\noptimization, based on offline and online RL, respectively. The offline RL\nrequires training a critic model to construct positive and negative examples\nfor learning. The online method defines rule-based reward as feedback for\noptimization. Experimental results showed that our approach can significantly\nenhance the evaluation capability of generative LLM-Judge, surpassing both\ngenerative and classifier-based LLM-Judge without requiring extra human\nannotations.\n","authors":["Hui Huang","Yancheng He","Hongli Zhou","Rui Zhang","Wei Liu","Weixun Wang","Wenbo Su","Bo Zheng","Jiaheng Liu"],"pdf_url":"https://arxiv.org/pdf/2505.14268v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2504.05047v2","updated":"2025-05-20T12:17:15Z","published":"2025-04-07T13:17:52Z","title":"Debate Only When Necessary: Adaptive Multiagent Collaboration for\n  Efficient LLM Reasoning","summary":"  Multiagent collaboration has emerged as a promising framework for enhancing\nthe reasoning capabilities of large language models (LLMs). Despite\nimprovements in reasoning, the approach introduces substantial computational\noverhead resulting from iterative agent interactions. Furthermore, engaging in\nunnecessary debates increases the risk of generating erroneous responses. To\naddress these challenges, we propose Debate Only When Necessary (DOWN), an\nadaptive multiagent debate framework that selectively activates debate based on\nthe confidence score of the agent's initial response. Debate is activated only\nfor queries requiring further deliberation, during which agents refine their\noutputs by referencing peer responses and associated confidence scores.\nEvaluations on benchmarks show that DOWN improves efficiency by up to six times\nwhile preserving or even outperforming the performance of existing methods.\nFurther analysis indicates that DOWN effectively mitigates the risk of error\npropagation stemming from the unnecessary debate process. These findings\ndemonstrate the effectiveness of our approach in delivering high-performance\nLLM solutions at a lower computational cost.\n","authors":["Sugyeong Eo","Hyeonseok Moon","Evelyn Hayoon Zi","Chanjun Park","Heuiseok Lim"],"pdf_url":"https://arxiv.org/pdf/2504.05047v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16747v2","updated":"2025-05-20T12:14:20Z","published":"2025-02-23T23:23:51Z","title":"SQLong: Enhanced NL2SQL for Longer Contexts with LLMs","summary":"  Open-weight large language models (LLMs) have significantly advanced\nperformance in the Natural Language to SQL (NL2SQL) task. However, their\neffectiveness diminishes when dealing with large database schemas, as the\ncontext length increases. To address this limitation, we present SQLong, a\nnovel and efficient data augmentation framework designed to enhance LLM\nperformance in long-context scenarios for the NL2SQL task. SQLong generates\naugmented datasets by extending existing database schemas with additional\nsynthetic CREATE TABLE commands and corresponding data rows, sampled from\ndiverse schemas in the training data. This approach effectively simulates\nlong-context scenarios during finetuning and evaluation. Through experiments on\nthe Spider and BIRD datasets, we demonstrate that LLMs finetuned with\nSQLong-augmented data significantly outperform those trained on standard\ndatasets. These imply SQLong's practical implementation and its impact on\nimproving NL2SQL capabilities in real-world settings with complex database\nschemas.\n","authors":["Dai Quoc Nguyen","Cong Duy Vu Hoang","Duy Vu","Gioacchino Tangari","Thanh Tien Vu","Don Dharmasiri","Yuan-Fang Li","Long Duong"],"pdf_url":"https://arxiv.org/pdf/2502.16747v2.pdf","comment":"Accepted to Table Representation Learning Workshop at ACL 2025"},{"id":"http://arxiv.org/abs/2505.14260v1","updated":"2025-05-20T12:12:17Z","published":"2025-05-20T12:12:17Z","title":"Speculative Decoding Reimagined for Multimodal Large Language Models","summary":"  This paper introduces Multimodal Speculative Decoding (MSD) to accelerate\nMultimodal Large Language Models (MLLMs) inference. Speculative decoding has\nbeen shown to accelerate Large Language Models (LLMs) without sacrificing\naccuracy. However, current speculative decoding methods for MLLMs fail to\nachieve the same speedup as they do for LLMs. To address this, we reimagine\nspeculative decoding specifically for MLLMs. Our analysis of MLLM\ncharacteristics reveals two key design principles for MSD: (1) Text and visual\ntokens have fundamentally different characteristics and need to be processed\nseparately during drafting. (2) Both language modeling ability and visual\nperception capability are crucial for the draft model. For the first principle,\nMSD decouples text and visual tokens in the draft model, allowing each to be\nhandled based on its own characteristics. For the second principle, MSD uses a\ntwo-stage training strategy: In stage one, the draft model is trained on\ntext-only instruction-tuning datasets to improve its language modeling ability.\nIn stage two, MSD gradually introduces multimodal data to enhance the visual\nperception capability of the draft model. Experiments show that MSD boosts\ninference speed by up to $2.29\\times$ for LLaVA-1.5-7B and up to $2.46\\times$\nfor LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.\nOur code is available at https://github.com/Lyn-Lucy/MSD.\n","authors":["Luxi Lin","Zhihang Lin","Zhanpeng Zeng","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2505.14260v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2505.08155v3","updated":"2025-05-20T12:09:36Z","published":"2025-05-13T01:24:09Z","title":"Efficient and Scalable Neural Symbolic Search for Knowledge Graph\n  Complex Query Answering","summary":"  Complex Query Answering (CQA) aims to retrieve answer sets for complex\nlogical formulas from incomplete knowledge graphs, which is a crucial yet\nchallenging task in knowledge graph reasoning. While neuro-symbolic search\nutilized neural link predictions achieve superior accuracy, they encounter\nsignificant complexity bottlenecks: (i) Data complexity typically scales\nquadratically with the number of entities in the knowledge graph, and (ii)\nQuery complexity becomes NP-hard for cyclic queries. Consequently, these\napproaches struggle to effectively scale to larger knowledge graphs and more\ncomplex queries. To address these challenges, we propose an efficient and\nscalable symbolic search framework. First, we propose two constraint strategies\nto compute neural logical indices to reduce the domain of variables, thereby\ndecreasing the data complexity of symbolic search. Additionally, we introduce\nan approximate algorithm based on local search to tackle the NP query\ncomplexity of cyclic queries. Experiments on various CQA benchmarks demonstrate\nthat our framework reduces the computational load of symbolic methods by 90\\%\nwhile maintaining nearly the same performance, thus alleviating both efficiency\nand scalability issues.\n","authors":["Weizhi Fei","Zihao Wang","hang Yin","Shukai Zhao","Wei Zhang","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2505.08155v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14256v1","updated":"2025-05-20T12:09:17Z","published":"2025-05-20T12:09:17Z","title":"FuxiMT: Sparsifying Large Language Models for Chinese-Centric\n  Multilingual Machine Translation","summary":"  In this paper, we present FuxiMT, a novel Chinese-centric multilingual\nmachine translation model powered by a sparsified large language model (LLM).\nWe adopt a two-stage strategy to train FuxiMT. We first pre-train the model on\na massive Chinese corpus and then conduct multilingual fine-tuning on a large\nparallel dataset encompassing 65 languages. FuxiMT incorporates\nMixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust\nperformance across various resource levels. Experimental results demonstrate\nthat FuxiMT significantly outperforms strong baselines, including\nstate-of-the-art LLMs and machine translation models, particularly under\nlow-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot\ntranslation capabilities for unseen language pairs, indicating its potential to\nbridge communication gaps where parallel data are scarce or unavailable.\n","authors":["Shaolin Zhu","Tianyu Dong","Bo Li","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2505.14256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14252v1","updated":"2025-05-20T12:05:17Z","published":"2025-05-20T12:05:17Z","title":"Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence\n  Encoders and Physics-Informed Neural Networks","summary":"  In this work, we explore the integration of Sequence Encoding for Online\nParameter Identification with Physics-Informed Neural Networks to create a\nmodel that, once trained, can be utilized for real time applications with\nvariable parameters, boundary conditions, and initial conditions. Recently, the\ncombination of PINNs with Sparse Regression has emerged as a method for\nperforming dynamical system identification through supervised learning and\nsparse regression optimization, while also solving the dynamics using PINNs.\nHowever, this approach can be limited by variations in parameters or boundary\nand initial conditions, requiring retraining of the model whenever changes\noccur. In this work, we introduce an architecture that employs Deep Sets or\nSequence Encoders to encode dynamic parameters, boundary conditions, and\ninitial conditions, using these encoded features as inputs for the PINN,\nenabling the model to adapt to changes in parameters, BCs, and ICs. We apply\nthis approach to three different problems. First, we analyze the Rossler ODE\nsystem, demonstrating the robustness of the model with respect to noise and its\nability to generalize. Next, we explore the model's capability in a 2D\nNavier-Stokes PDE problem involving flow past a cylinder with a parametric\nsinusoidal inlet velocity function, showing that the model can encode pressure\ndata from a few points to identify the inlet velocity profile and utilize\nphysics to compute velocity and pressure throughout the domain. Finally, we\naddress a 1D heat monitoring problem using real data from the heating of glass\nfiber and thermoplastic composite plates.\n","authors":["Mouad Elaarabi","Domenico Borzacchiello","Philippe Le Bot","Nathan Lauzeral","Sebastien Comas-Cardona"],"pdf_url":"https://arxiv.org/pdf/2505.14252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14246v1","updated":"2025-05-20T11:59:25Z","published":"2025-05-20T11:59:25Z","title":"Visual Agentic Reinforcement Fine-Tuning","summary":"  A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\nagentic ability to use external tools such as web browsers for searching and\nwriting/executing code for image manipulation to think with images. In the\nopen-source research community, while significant progress has been made in\nlanguage-only agentic abilities such as function calling and tool integration,\nthe development of multi-modal agentic capabilities that involve truly thinking\nwith images, and their corresponding benchmarks, are still less explored. This\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\nability to browse websites for real-time information updates and write code to\nmanipulate and analyze input images through cropping, rotation, and other image\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\nagentic search and coding abilities. Our experimental results demonstrate that\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\nOur findings suggest that Visual-ARFT offers a promising path toward building\nrobust and generalizable multimodal agents.\n","authors":["Ziyu Liu","Yuhang Zang","Yushan Zou","Zijian Liang","Xiaoyi Dong","Yuhang Cao","Haodong Duan","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2505.14246v1.pdf","comment":"project url:\n  https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT"},{"id":"http://arxiv.org/abs/2504.12324v2","updated":"2025-05-20T11:52:01Z","published":"2025-04-11T13:18:26Z","title":"Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and\n  Interpretability Prediction","summary":"  Natural Language Inference (NLI) is a fundamental task in natural language\nprocessing. While NLI has developed many sub-directions such as sentence-level\nNLI, document-level NLI and cross-lingual NLI, Cross-Document Cross-Lingual NLI\n(CDCL-NLI) remains largely unexplored. In this paper, we propose a novel\nparadigm: CDCL-NLI, which extends traditional NLI capabilities to\nmulti-document, multilingual scenarios. To support this task, we construct a\nhigh-quality CDCL-NLI dataset including 25,410 instances and spanning 26\nlanguages. To address the limitations of previous methods on CDCL-NLI task, we\nfurther propose an innovative method that integrates RST-enhanced graph fusion\nwith interpretability-aware prediction. Our approach leverages RST (Rhetorical\nStructure Theory) within heterogeneous graph neural networks for cross-document\ncontext modeling, and employs a structure-aware semantic alignment based on\nlexical chains for cross-lingual understanding. For NLI interpretability, we\ndevelop an EDU (Elementary Discourse Unit)-level attribution framework that\nproduces extractive explanations. Extensive experiments demonstrate our\napproach's superior performance, achieving significant improvements over both\nconventional NLI models as well as large language models. Our work sheds light\non the study of NLI and will bring research interest on cross-document\ncross-lingual context understanding, hallucination elimination and\ninterpretability inference. Our code and datasets are available at\n\\href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-link} for peer\nreview.\n","authors":["Mengying Yuan","Wenhao Wang","Zixuan Wang","Yujie Huang","Kangli Wei","Fei Li","Chong Teng","Donghong Ji"],"pdf_url":"https://arxiv.org/pdf/2504.12324v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12442v2","updated":"2025-05-20T11:48:36Z","published":"2025-05-18T14:31:45Z","title":"IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems","summary":"  The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses.\n","authors":["Liwen Wang","Wenxuan Wang","Shuai Wang","Zongjie Li","Zhenlan Ji","Zongyi Lyu","Daoyuan Wu","Shing-Chi Cheung"],"pdf_url":"https://arxiv.org/pdf/2505.12442v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11537v3","updated":"2025-05-20T11:45:24Z","published":"2025-02-17T08:06:10Z","title":"Uncovering Untapped Potential in Sample-Efficient World Model Agents","summary":"  World model (WM) agents enable sample-efficient reinforcement learning by\nlearning policies entirely from simulated experience. However, existing\ntoken-based world models (TBWMs) are limited to visual inputs and discrete\nactions, restricting their adoption and applicability. Moreover, although both\nintrinsic motivation and prioritized WM replay have shown promise in improving\nWM performance and generalization, they remain underexplored in this setting,\nparticularly in combination. We introduce Simulus, a highly modular TBWM agent\nthat integrates (1) a modular multi-modality tokenization framework, (2)\nintrinsic motivation, (3) prioritized WM replay, and (4)\nregression-as-classification for reward and return prediction. Simulus achieves\nstate-of-the-art sample efficiency for planning-free WMs across three diverse\nbenchmarks. Ablation studies reveal the individual contribution of each\ncomponent while highlighting their synergy. Our code and model weights are\npublicly available at https://github.com/leor-c/Simulus.\n","authors":["Lior Cohen","Kaixin Wang","Bingyi Kang","Uri Gadot","Shie Mannor"],"pdf_url":"https://arxiv.org/pdf/2502.11537v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14238v1","updated":"2025-05-20T11:43:25Z","published":"2025-05-20T11:43:25Z","title":"ABBA: Highly Expressive Hadamard Product Adaptation for Large Language\n  Models","summary":"  Large Language Models have demonstrated strong performance across a wide\nrange of tasks, but adapting them efficiently to new domains remains a key\nchallenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nintroducing lightweight, trainable modules while keeping most pre-trained\nweights fixed. The prevailing approach, LoRA, models updates using a low-rank\ndecomposition, but its expressivity is inherently constrained by the rank.\nRecent methods like HiRA aim to increase expressivity by incorporating a\nHadamard product with the frozen weights, but still rely on the structure of\nthe pre-trained model. We introduce ABBA, a new PEFT architecture that\nreparameterizes the update as a Hadamard product of two independently learnable\nlow-rank matrices. In contrast to prior work, ABBA fully decouples the update\nfrom the pre-trained weights, enabling both components to be optimized freely.\nThis leads to significantly higher expressivity under the same parameter\nbudget. We formally analyze ABBA's expressive capacity and validate its\nadvantages through matrix reconstruction experiments. Empirically, ABBA\nachieves state-of-the-art results on arithmetic and commonsense reasoning\nbenchmarks, consistently outperforming existing PEFT methods by a significant\nmargin across multiple models. Our code is publicly available at:\nhttps://github.com/CERT-Lab/abba.\n","authors":["Raghav Singhal","Kaustubh Ponkshe","Rohit Vartak","Praneeth Vepakomma"],"pdf_url":"https://arxiv.org/pdf/2505.14238v1.pdf","comment":"Raghav Singhal, Kaustubh Ponkshe, and Rohit Vartak contributed\n  equally to this work"},{"id":"http://arxiv.org/abs/2410.21673v2","updated":"2025-05-20T11:43:05Z","published":"2024-10-29T02:48:41Z","title":"Knowledge-Guided Prompt Learning for Request Quality Assurance in Public\n  Code Review","summary":"  Public Code Review (PCR) is developed in the Software Question Answering\n(SQA) community, assisting developers in exploring high-quality and efficient\nreview services. Current methods on PCR mainly focus on the reviewer's\nperspective, including finding a capable reviewer, predicting comment quality,\nand recommending/generating review comments. However, it is not well studied\nthat how to satisfy the review necessity requests posted by developers which\ncan increase their visibility, which in turn acts as a prerequisite for better\nreview responses. To this end, we propose Knowledge-guided Prompt learning for\nPublic Code Review (KP-PCR) to achieve developer-based code review request\nquality assurance (i.e., predicting request necessity and recommending tags\nsubtask). Specifically, we reformulate the two subtasks via 1) text prompt\ntuning which converts both of them into a Masked Language Model (MLM) by\nconstructing prompt templates using hard prompt; and 2) knowledge and code\nprefix tuning which introduces knowledge guidance from fine-tuned large\nlanguage models by soft prompt, and uses program dependence graph to\ncharacterize code snippets. Finally, both of the request necessity prediction\nand tag recommendation subtasks output predicted results through an answer\nengineering module. In addition, we further analysis the time complexity of our\nKP-PCR that has lightweight prefix based the operation of introducing knowledge\nguidance. Experimental results on the PCR dataset for the period 2011-2023\ndemonstrate that our KP-PCR outperforms baselines by 2.3%-8.4% in the request\nnecessity prediction and by 1.4%-6.9% in the tag recommendation. The code\nimplementation is released at https://github.com/WUT-IDEA/KP-PCR.\n","authors":["Lin Li","Xinchun Yu","Xinyu Chen","Peng Liang"],"pdf_url":"https://arxiv.org/pdf/2410.21673v2.pdf","comment":"27 pages, 5 images, 12 tables, Manuscript revision submitted to a\n  journal (2025)"},{"id":"http://arxiv.org/abs/2505.14235v1","updated":"2025-05-20T11:42:26Z","published":"2025-05-20T11:42:26Z","title":"Toward Embodied AGI: A Review of Embodied AI and the Road Ahead","summary":"  Artificial General Intelligence (AGI) is often envisioned as inherently\nembodied. With recent advances in robotics and foundational AI models, we stand\nat the threshold of a new era-one marked by increasingly generalized embodied\nAI systems. This paper contributes to the discourse by introducing a systematic\ntaxonomy of Embodied AGI spanning five levels (L1-L5). We review existing\nresearch and challenges at the foundational stages (L1-L2) and outline the key\ncomponents required to achieve higher-level capabilities (L3-L5). Building on\nthese insights and existing technologies, we propose a conceptual framework for\nan L3+ robotic brain, offering both a technical outlook and a foundation for\nfuture exploration.\n","authors":["Yequan Wang","Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2505.14235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14234v1","updated":"2025-05-20T11:41:26Z","published":"2025-05-20T11:41:26Z","title":"Fast and close Shannon entropy approximation","summary":"  Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy\nare key components in many tools used in physics, information theory, machine\nlearning (ML) and quantum computing. Besides of the significant amounts of SE\ncomputations required in these fields, the singularity of the SE gradient is\none of the central mathematical reason inducing the high cost, frequently low\nrobustness and slow convergence of such tools. Here we propose the Fast Entropy\nApproximation (FEA) - a non-singular rational approximation of Shannon entropy\nand its gradient that achieves a mean absolute error of $10^{-3}$, which is\napproximately $20$ times lower than comparable state-of-the-art methods. FEA\nallows around $50\\%$ faster computation, requiring only $5$ to $6$ elementary\ncomputational operations, as compared to tens of elementary operations behind\nthe fastest entropy computation algorithms with table look-ups, bitshifts, or\nseries approximations. On a set of common benchmarks for the feature selection\nproblem in machine learning, we show that the combined effect of fewer\nelementary operations, low approximation error, and a non-singular gradient\nallows significantly better model quality and enables ML feature extraction\nthat is two to three orders of magnitude faster and computationally cheaper\nwhen incorporating FEA into AI tools.\n","authors":["Illia Horenko","Davide Bassetti","Lukáš Pospíšil"],"pdf_url":"https://arxiv.org/pdf/2505.14234v1.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2505.14233v1","updated":"2025-05-20T11:41:21Z","published":"2025-05-20T11:41:21Z","title":"Mechanistic Fine-tuning for In-context Learning","summary":"  In-context Learning (ICL) utilizes structured demonstration-query inputs to\ninduce few-shot learning on Language Models (LMs), which are not originally\npre-trained on ICL-style data. To bridge the gap between ICL and pre-training,\nsome approaches fine-tune LMs on large ICL-style datasets by an end-to-end\nparadigm with massive computational costs. To reduce such costs, in this paper,\nwe propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous\nfindings on the inner mechanism of ICL, building training objectives on the\nattention scores instead of the final outputs, to force the attention scores to\nfocus on the correct label tokens presented in the context and mitigate\nattention scores from the wrong label tokens. Our experiments on 9 modern LMs\nand 8 datasets empirically find that ABFT outperforms in performance,\nrobustness, unbiasedness, and efficiency, with only around 0.01% data cost\ncompared to the previous methods. Moreover, our subsequent analysis finds that\nthe end-to-end training objective contains the ABFT objective, suggesting the\nimplicit bias of ICL-style data to the emergence of induction heads. Our work\ndemonstrates the possibility of controlling specific module sequences within\nLMs to improve their behavior, opening up the future application of mechanistic\ninterpretability.\n","authors":["Hakaze Cho","Peng Luo","Mariko Kato","Rin Kaenbyou","Naoya Inoue"],"pdf_url":"https://arxiv.org/pdf/2505.14233v1.pdf","comment":"28 pages, 31 figures, 6 tables"},{"id":"http://arxiv.org/abs/2505.14227v1","updated":"2025-05-20T11:37:49Z","published":"2025-05-20T11:37:49Z","title":"VoQA: Visual-only Question Answering","summary":"  We propose Visual-only Question Answering (VoQA), a novel multimodal task in\nwhich questions are visually embedded within images, without any accompanying\ntextual input. This requires models to locate, recognize, and reason over\nvisually embedded textual questions, posing challenges for existing large\nvision-language models (LVLMs), which show notable performance drops even with\ncarefully designed prompts. To bridge this gap, we introduce Guided Response\nTriggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy\nthat guides the model to perform step-by-step reasoning purely based on visual\ninput, significantly improving model performance. Our work enhances models'\ncapacity for human-like visual understanding in complex multimodal scenarios,\nwhere information, including language, is perceived visually.\n","authors":["Luyang Jiang","Jianing An","Jie Luo","Wenjun Wu","Lei Huang"],"pdf_url":"https://arxiv.org/pdf/2505.14227v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2502.07693v4","updated":"2025-05-20T11:35:47Z","published":"2025-02-11T16:46:56Z","title":"AI-driven Personalized Privacy Assistants: a Systematic Literature\n  Review","summary":"  In recent years, several personalized assistants based on AI have been\nresearched and developed to help users make privacy-related decisions. These\nAI-driven Personalized Privacy Assistants (AI-driven PPAs) can provide\nsignificant benefits for users, who might otherwise struggle with making\ndecisions about their personal data in online environments that often overload\nthem with different privacy decision requests. So far, no studies have\nsystematically investigated the emerging topic of AI-driven PPAs, classifying\ntheir underlying technologies, architecture and features, including decision\ntypes or the accuracy of their decisions. To fill this gap, we present a\nSystematic Literature Review (SLR) to map the existing solutions found in the\nscientific literature, which allows reasoning about existing approaches and\nopen challenges for this research field. We screened several hundred unique\nresearch papers over the recent years (2013-2025), constructing a\nclassification from 41 included papers. As a result, this SLR reviews several\naspects of existing research on AI-driven PPAs in terms of types of\npublications, contributions, methodological quality, and other quantitative\ninsights. Furthermore, we provide a comprehensive classification for AI-driven\nPPAs, delving into their architectural choices, system contexts, types of AI\nused, data sources, types of decisions, and control over decisions, among other\nfacets. Based on our SLR, we further underline the research gaps and challenges\nand formulate recommendations for the design and development of AI-driven PPAs\nas well as avenues for future research.\n","authors":["Victor Morel","Leonardo Iwaya","Simone Fischer-Hübner"],"pdf_url":"https://arxiv.org/pdf/2502.07693v4.pdf","comment":"Submitted to IEEE Access"},{"id":"http://arxiv.org/abs/2505.14226v1","updated":"2025-05-20T11:35:25Z","published":"2025-05-20T11:35:25Z","title":"\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed\n  Hinglish to Red-Team LLMs","summary":"  Large Language Models (LLMs) have become increasingly powerful, with\nmultilingual and multimodal capabilities improving by the day. These models are\nbeing evaluated through audits, alignment studies and red-teaming efforts to\nexpose model vulnerabilities towards generating harmful, biased and unfair\ncontent. Existing red-teaming efforts have previously focused on the English\nlanguage, using fixed template-based attacks; thus, models continue to be\nsusceptible to multilingual jailbreaking strategies, especially in the\nmultimodal context. In this study, we introduce a novel strategy that leverages\ncode-mixing and phonetic perturbations to jailbreak LLMs for both text and\nimage generation tasks. We also introduce two new jailbreak strategies that\nshow higher effectiveness than baseline strategies. Our work presents a method\nto effectively bypass safety filters in LLMs while maintaining interpretability\nby applying phonetic misspellings to sensitive words in code-mixed prompts. Our\nnovel prompts achieve a 99% Attack Success Rate for text generation and 78% for\nimage generation, with Attack Relevance Rate of 100% for text generation and\n95% for image generation when using the phonetically perturbed code-mixed\nprompts. Our interpretability experiments reveal that phonetic perturbations\nimpact word tokenization, leading to jailbreak success. Our study motivates\nincreasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words.\n","authors":["Darpan Aswal","Siddharth D Jaiswal"],"pdf_url":"https://arxiv.org/pdf/2505.14226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12767v5","updated":"2025-05-20T11:24:19Z","published":"2025-02-18T11:31:52Z","title":"R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on\n  Knowledge Graphs","summary":"  Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks still suffer two practical drawbacks: they must be re-tuned whenever\nthe KG or reasoning task changes, and they depend on a single, high-capacity\nLLM for reliable (i.e., trustworthy) reasoning. To address this, we introduce\nR2-KG, a plug-and-play, dual-agent framework that separates reasoning into two\nroles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor\n(a high-capacity LLM) that makes final judgments. This design is cost-efficient\nfor LLM inference while still maintaining strong reasoning accuracy.\nAdditionally, R2-KG employs an Abstention mechanism, generating answers only\nwhen sufficient evidence is collected from KG, which significantly enhances\nreliability. Experiments across five diverse benchmarks show that R2-KG\nconsistently outperforms baselines in both accuracy and reliability, regardless\nof the inherent capability of LLMs used as the Operator. Further experiments\nreveal that the single-agent version of R2-KG, equipped with a strict\nself-consistency strategy, achieves significantly higher-than-baseline\nreliability with reduced inference cost but increased abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning, reducing reliance on high-capacity LLMs while\nensuring trustworthy inference. The code is available at\nhttps://github.com/ekrxjwh2009/R2-KG/.\n","authors":["Sumin Jo","Junseong Choi","Jiho Kim","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2502.12767v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14217v1","updated":"2025-05-20T11:23:52Z","published":"2025-05-20T11:23:52Z","title":"Federated learning in low-resource settings: A chest imaging study in\n  Africa -- Challenges and lessons learned","summary":"  This study explores the use of Federated Learning (FL) for tuberculosis (TB)\ndiagnosis using chest X-rays in low-resource settings across Africa. FL allows\nhospitals to collaboratively train AI models without sharing raw patient data,\naddressing privacy concerns and data scarcity that hinder traditional\ncentralized models. The research involved hospitals and research centers in\neight African countries. Most sites used local datasets, while Ghana and The\nGambia used public ones. The study compared locally trained models with a\nfederated model built across all institutions to evaluate FL's real-world\nfeasibility. Despite its promise, implementing FL in sub-Saharan Africa faces\nchallenges such as poor infrastructure, unreliable internet, limited digital\nliteracy, and weak AI regulations. Some institutions were also reluctant to\nshare model updates due to data control concerns. In conclusion, FL shows\nstrong potential for enabling AI-driven healthcare in underserved regions, but\nbroader adoption will require improvements in infrastructure, education, and\nregulatory support.\n","authors":["Jorge Fabila","Lidia Garrucho","Víctor M. Campello","Carlos Martín-Isla","Karim Lekadir"],"pdf_url":"https://arxiv.org/pdf/2505.14217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14216v1","updated":"2025-05-20T11:22:34Z","published":"2025-05-20T11:22:34Z","title":"Reinforcement Learning vs. Distillation: Understanding Accuracy and\n  Capability in LLM Reasoning","summary":"  Recent studies have shown that reinforcement learning with verifiable rewards\n(RLVR) enhances overall accuracy but fails to improve capability, while\ndistillation can improve both. In this paper, we investigate the mechanisms\nbehind these phenomena. First, we demonstrate that RLVR does not improve\ncapability because it focuses on improving the accuracy of the less-difficult\nquestions to the detriment of the accuracy of the most difficult questions,\nthereby leading to no improvement in capability. Second, we find that RLVR does\nnot merely increase the success probability for the less difficult questions,\nbut in our small model settings produces quality responses that were absent in\nits output distribution before training. In addition, we show these responses\nare neither noticeably longer nor feature more reflection-related keywords,\nunderscoring the need for more reliable indicators of response quality. Third,\nwe show that while distillation reliably improves accuracy by learning strong\nreasoning patterns, it only improves capability when new knowledge is\nintroduced. Moreover, when distilling only with reasoning patterns and no new\nknowledge, the accuracy of the less-difficult questions improves to the\ndetriment of the most difficult questions, similar to RLVR. Together, these\nfindings offer a clearer understanding of how RLVR and distillation shape\nreasoning behavior in language models.\n","authors":["Minwu Kim","Anubhav Shrestha","Safal Shrestha","Aadim Nepal","Keith Ross"],"pdf_url":"https://arxiv.org/pdf/2505.14216v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2503.12908v3","updated":"2025-05-20T11:19:52Z","published":"2025-03-17T08:17:28Z","title":"HICD: Hallucination-Inducing via Attention Dispersion for Contrastive\n  Decoding to Mitigate Hallucinations in Large Language Models","summary":"  Large Language Models (LLMs) often generate hallucinations, producing outputs\nthat are contextually inaccurate or factually incorrect. We introduce HICD, a\nnovel method designed to induce hallucinations for contrastive decoding to\nmitigate hallucinations. Unlike existing contrastive decoding methods, HICD\nselects attention heads crucial to the model's prediction as inducing heads,\nthen induces hallucinations by dispersing attention of these inducing heads and\ncompares the hallucinated outputs with the original outputs to obtain the final\nresult. Our approach significantly improves performance on tasks requiring\ncontextual faithfulness, such as context completion, reading comprehension, and\nquestion answering. It also improves factuality in tasks requiring accurate\nknowledge recall. We demonstrate that our inducing heads selection and\nattention dispersion method leads to more \"contrast-effective\" hallucinations\nfor contrastive decoding, outperforming other hallucination-inducing methods.\nOur findings provide a promising strategy for reducing hallucinations by\ninducing hallucinations in a controlled manner, enhancing the performance of\nLLMs in a wide range of tasks.\n","authors":["Xinyan Jiang","Hang Ye","Yongxin Zhu","Xiaoying Zheng","Zikang Chen","Jun Gong"],"pdf_url":"https://arxiv.org/pdf/2503.12908v3.pdf","comment":"Accepted by ACL2025 findings"},{"id":"http://arxiv.org/abs/2505.14212v1","updated":"2025-05-20T11:16:29Z","published":"2025-05-20T11:16:29Z","title":"Automatic Dataset Generation for Knowledge Intensive Question Answering\n  Tasks","summary":"  A question-answering (QA) system is to search suitable answers within a\nknowledge base. Current QA systems struggle with queries requiring complex\nreasoning or real-time knowledge integration. They are often supplemented with\nretrieval techniques on a data source such as Retrieval-Augmented Generation\n(RAG). However, RAG continues to face challenges in handling complex reasoning\nand logical connections between multiple sources of information. A novel\napproach for enhancing Large Language Models (LLMs) in knowledge-intensive QA\ntasks is presented through the automated generation of context-based QA pairs.\nThis methodology leverages LLMs to create fine-tuning data, reducing reliance\non human labelling and improving model comprehension and reasoning\ncapabilities. The proposed system includes an automated QA generator and a\nmodel fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.\nComprehensive experiments demonstrate improvements in logical coherence and\nfactual accuracy, with implications for developing adaptable Artificial\nIntelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,\nBLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA\npairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA\npairs.\n","authors":["Sizhe Yuen","Ting Su","Ziyang Wang","Yali Du","Adam J. Sobey"],"pdf_url":"https://arxiv.org/pdf/2505.14212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02429v3","updated":"2025-05-20T11:12:15Z","published":"2024-10-03T12:24:18Z","title":"IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language\n  Models","summary":"  Large Language Models (LLMs) excel in textual and visual tasks but often\nproduce outputs that defy physical laws when dealing with physical-world\nreasoning tasks. Inspired by human cognition, where perception is fundamental\nto reasoning, we explore augmenting LLMs with enhanced perception abilities\nusing Internet of Things (IoT) sensor data and pertinent knowledge for\nIoT-sensory task reasoning in the physical world. In this work, we\nsystematically study LLMs' capability to address real-world IoT-sensory tasks\nby augmenting their perception and knowledge base, and then propose a unified\nframework, IoT-LLM, to enhance such capability. In IoT-LLM, we customize three\nsteps for LLMs: preprocessing IoT data into formats amenable to LLMs, expanding\ntheir understanding via IoT-oriented retrieval-augmented generation based on\nin-context learning and activating their commonsense knowledge through\nchain-of-thought prompting and specialized role definitions. We design a new\nbenchmark comprising five real-world tasks with varying data types and\nreasoning complexities to evaluate the performance of IoT-LLM. Experimental\nresults on six LLMs reveal that IoT-LLM significantly improves the performance\nof IoT-sensory task reasoning of LLMs, with models like GPT-4o-mini showing a\n49.4% average improvement over previous methods.\n","authors":["Tuo An","Yunjiao Zhou","Han Zou","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.02429v3.pdf","comment":"21 pages, 11 figures, under review"},{"id":"http://arxiv.org/abs/2505.14209v1","updated":"2025-05-20T11:11:46Z","published":"2025-05-20T11:11:46Z","title":"Embedded Mean Field Reinforcement Learning for Perimeter-defense Game","summary":"  With the rapid advancement of unmanned aerial vehicles (UAVs) and missile\ntechnologies, perimeter-defense game between attackers and defenders for the\nprotection of critical regions have become increasingly complex and\nstrategically significant across a wide range of domains. However, existing\nstudies predominantly focus on small-scale, simplified two-dimensional\nscenarios, often overlooking realistic environmental perturbations, motion\ndynamics, and inherent heterogeneity--factors that pose substantial challenges\nto real-world applicability. To bridge this gap, we investigate large-scale\nheterogeneous perimeter-defense game in a three-dimensional setting,\nincorporating realistic elements such as motion dynamics and wind fields. We\nderive the Nash equilibrium strategies for both attackers and defenders,\ncharacterize the victory regions, and validate our theoretical findings through\nextensive simulations. To tackle large-scale heterogeneous control challenges\nin defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)\nframework. EMFAC leverages representation learning to enable high-level action\naggregation in a mean-field manner, supporting scalable coordination among\ndefenders. Furthermore, we introduce a lightweight agent-level attention\nmechanism based on reward representation, which selectively filters\nobservations and mean-field information to enhance decision-making efficiency\nand accelerate convergence in large-scale tasks. Extensive simulations across\nvarying scales demonstrate the effectiveness and adaptability of EMFAC, which\noutperforms established baselines in both convergence speed and overall\nperformance. To further validate practicality, we test EMFAC in small-scale\nreal-world experiments and conduct detailed analyses, offering deeper insights\ninto the framework's effectiveness in complex scenarios.\n","authors":["Li Wang","Xin Yu","Xuxin Lv","Gangzheng Ai","Wenjun Wu"],"pdf_url":"https://arxiv.org/pdf/2505.14209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14206v1","updated":"2025-05-20T11:05:06Z","published":"2025-05-20T11:05:06Z","title":"Challenges and Limitations in the Synthetic Generation of mHealth Sensor\n  Data","summary":"  The widespread adoption of mobile sensors has the potential to provide\nmassive and heterogeneous time series data, driving Artificial Intelligence\napplications in mHealth. However, data collection remains limited due to\nstringent ethical regulations, privacy concerns, and other constraints,\nhindering progress in the field. Synthetic data generation, particularly\nthrough Generative Adversarial Networks and Diffusion Models, has emerged as a\npromising solution to address both data scarcity and privacy issues. Yet, these\nmodels are often limited to short-term, unimodal signal patterns. This paper\npresents a systematic evaluation of state-of-the-art generative models for time\nseries synthesis, with a focus on their ability to jointly handle\nmulti-modality, long-range dependencies, and conditional generation-key\nchallenges in the mHealth domain. To ensure a fair comparison, we introduce a\nnovel evaluation framework designed to measure both the intrinsic quality of\nsynthetic data and its utility in downstream predictive tasks. Our findings\nreveal critical limitations in the existing approaches, particularly in\nmaintaining cross-modal consistency, preserving temporal coherence, and\nensuring robust performance in train-on-synthetic, test-on-real, and data\naugmentation scenarios. Finally, we present our future research directions to\nenhance synthetic time series generation and improve the applicability of\ngenerative models in mHealth.\n","authors":["Flavio Di Martino","Franca Delmastro"],"pdf_url":"https://arxiv.org/pdf/2505.14206v1.pdf","comment":"Submitted to ACM Transactions on Computing for Healthcare (ACM\n  HEALTH)"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2505.14687v1","updated":"2025-05-20T17:59:59Z","published":"2025-05-20T17:59:59Z","title":"Grouping First, Attending Smartly: Training-Free Acceleration for\n  Diffusion Transformers","summary":"  Diffusion-based Transformers have demonstrated impressive generative\ncapabilities, but their high computational costs hinder practical deployment,\nfor example, generating an $8192\\times 8192$ image can take over an hour on an\nA100 GPU. In this work, we propose GRAT (\\textbf{GR}ouping first,\n\\textbf{AT}tending smartly), a training-free attention acceleration strategy\nfor fast image and video generation without compromising output quality. The\nkey insight is to exploit the inherent sparsity in learned attention maps\n(which tend to be locally focused) in pretrained Diffusion Transformers and\nleverage better GPU parallelism. Specifically, GRAT first partitions contiguous\ntokens into non-overlapping groups, aligning both with GPU execution patterns\nand the local attention structures learned in pretrained generative\nTransformers. It then accelerates attention by having all query tokens within\nthe same group share a common set of attendable key and value tokens. These key\nand value tokens are further restricted to structured regions, such as\nsurrounding blocks or criss-cross regions, significantly reducing computational\noverhead (e.g., attaining a \\textbf{35.8$\\times$} speedup over full attention\nwhen generating $8192\\times 8192$ images) while preserving essential attention\npatterns and long-range context. We validate GRAT on pretrained Flux and\nHunyuanVideo for image and video generation, respectively. In both cases, GRAT\nachieves substantially faster inference without any fine-tuning, while\nmaintaining the performance of full attention. We hope GRAT will inspire future\nresearch on accelerating Diffusion Transformers for scalable visual generation.\n","authors":["Sucheng Ren","Qihang Yu","Ju He","Alan Yuille","Liang-Chieh Chen"],"pdf_url":"https://arxiv.org/pdf/2505.14687v1.pdf","comment":"Project website at oliverrensu.github.io/project/GRAT"},{"id":"http://arxiv.org/abs/2505.14683v1","updated":"2025-05-20T17:59:30Z","published":"2025-05-20T17:59:30Z","title":"Emerging Properties in Unified Multimodal Pretraining","summary":"  Unifying multimodal understanding and generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0source foundational model that natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\non trillions of tokens curated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such as free-form image manipulation,\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/\n","authors":["Chaorui Deng","Deyao Zhu","Kunchang Li","Chenhui Gou","Feng Li","Zeyu Wang","Shu Zhong","Weihao Yu","Xiaonan Nie","Ziang Song","Guang Shi","Haoqi Fan"],"pdf_url":"https://arxiv.org/pdf/2505.14683v1.pdf","comment":"37 pages, 17 figures"},{"id":"http://arxiv.org/abs/2505.14682v1","updated":"2025-05-20T17:59:26Z","published":"2025-05-20T17:59:26Z","title":"UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal\n  Understanding and Generation","summary":"  We introduce UniGen, a unified multimodal large language model (MLLM) capable\nof image understanding and generation. We study the full training pipeline of\nUniGen from a data-centric perspective, including multi-stage pre-training,\nsupervised fine-tuning, and direct preference optimization. More importantly,\nwe propose a new Chain-of-Thought Verification (CoT-V) strategy for test-time\nscaling, which significantly boosts UniGen's image generation quality using a\nsimple Best-of-N test-time strategy. Specifically, CoT-V enables UniGen to act\nas both image generator and verifier at test time, assessing the semantic\nalignment between a text prompt and its generated image in a step-by-step CoT\nmanner. Trained entirely on open-source datasets across all stages, UniGen\nachieves state-of-the-art performance on a range of image understanding and\ngeneration benchmarks, with a final score of 0.78 on GenEval and 85.19 on\nDPG-Bench. Through extensive ablation studies, our work provides actionable\ninsights and addresses key challenges in the full life cycle of building\nunified MLLMs, contributing meaningful directions to the future research.\n","authors":["Rui Tian","Mingfei Gao","Mingze Xu","Jiaming Hu","Jiasen Lu","Zuxuan Wu","Yinfei Yang","Afshin Dehghan"],"pdf_url":"https://arxiv.org/pdf/2505.14682v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2505.14681v1","updated":"2025-05-20T17:59:16Z","published":"2025-05-20T17:59:16Z","title":"Two Experts Are All You Need for Steering Thinking: Reinforcing\n  Cognitive Effort in MoE Reasoning Models Without Additional Training","summary":"  Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models.\n","authors":["Mengru Wang","Xingyu Chen","Yue Wang","Zhiwei He","Jiahao Xu","Tian Liang","Qiuzhi Liu","Yunzhi Yao","Wenxuan Wang","Ruotian Ma","Haitao Mi","Ningyu Zhang","Zhaopeng Tu","Xiaolong Li","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2505.14681v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2505.14677v1","updated":"2025-05-20T17:58:35Z","published":"2025-05-20T17:58:35Z","title":"Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning","summary":"  Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.\n","authors":["Jiaer Xia","Yuhang Zang","Peng Gao","Yixuan Li","Kaiyang Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.14677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14673v1","updated":"2025-05-20T17:58:02Z","published":"2025-05-20T17:58:02Z","title":"Training-Free Watermarking for Autoregressive Image Generation","summary":"  Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.\n","authors":["Yu Tong","Zihao Pan","Shuai Yang","Kaiyang Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.14673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14671v1","updated":"2025-05-20T17:56:01Z","published":"2025-05-20T17:56:01Z","title":"UniCTokens: Boosting Personalized Understanding and Generation via\n  Unified Concept Tokens","summary":"  Personalized models have demonstrated remarkable success in understanding and\ngenerating concepts provided by users. However, existing methods use separate\nconcept tokens for understanding and generation, treating these tasks in\nisolation. This may result in limitations for generating images with complex\nprompts. For example, given the concept $\\langle bo\\rangle$, generating\n\"$\\langle bo\\rangle$ wearing its hat\" without additional textual descriptions\nof its hat. We call this kind of generation personalized knowledge-driven\ngeneration. To address the limitation, we present UniCTokens, a novel framework\nthat effectively integrates personalized information into a unified vision\nlanguage model (VLM) for understanding and generation. UniCTokens trains a set\nof unified concept tokens to leverage complementary semantics, boosting two\npersonalized tasks. Moreover, we propose a progressive training strategy with\nthree stages: understanding warm-up, bootstrapping generation from\nunderstanding, and deepening understanding from generation to enhance mutual\nbenefits between both tasks. To quantitatively evaluate the unified VLM\npersonalization, we present UnifyBench, the first benchmark for assessing\nconcept understanding, concept generation, and knowledge-driven generation.\nExperimental results on UnifyBench indicate that UniCTokens shows competitive\nperformance compared to leading methods in concept understanding, concept\ngeneration, and achieving state-of-the-art results in personalized\nknowledge-driven generation. Our research demonstrates that enhanced\nunderstanding improves generation, and the generation process can yield\nvaluable insights into understanding. Our code and dataset will be released at:\n\\href{https://github.com/arctanxarc/UniCTokens}{https://github.com/arctanxarc/UniCTokens}.\n","authors":["Ruichuan An","Sihan Yang","Renrui Zhang","Zijun Shen","Ming Lu","Gaole Dai","Hao Liang","Ziyu Guo","Shilin Yan","Yulin Luo","Bocheng Zou","Chaoqun Yang","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.14671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14664v1","updated":"2025-05-20T17:52:03Z","published":"2025-05-20T17:52:03Z","title":"AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of\n  Cross-Modal Embeddings","summary":"  Cross-modal embeddings form the foundation for multi-modal models. However,\nvisualization methods for interpreting cross-modal embeddings have been\nprimarily confined to traditional dimensionality reduction (DR) techniques like\nPCA and t-SNE. These DR methods primarily focus on feature distributions within\na single modality, whilst failing to incorporate metrics (e.g., CLIPScore)\nacross multiple modalities.This paper introduces AKRMap, a new DR technique\ndesigned to visualize cross-modal embeddings metric with enhanced accuracy by\nlearning kernel regression of the metric landscape in the projection space.\nSpecifically, AKRMap constructs a supervised projection network guided by a\npost-projection kernel regression loss, and employs adaptive generalized\nkernels that can be jointly optimized with the projection. This approach\nenables AKRMap to efficiently generate visualizations that capture complex\nmetric distributions, while also supporting interactive features such as zoom\nand overlay for deeper exploration. Quantitative experiments demonstrate that\nAKRMap outperforms existing DR methods in generating more accurate and\ntrustworthy visualizations. We further showcase the effectiveness of AKRMap in\nvisualizing and comparing cross-modal embeddings for text-to-image models. Code\nand demo are available at https://github.com/yilinye/AKRMap.\n","authors":["Yilin Ye","Junchao Huang","Xingchen Zeng","Jiazhi Xia","Wei Zeng"],"pdf_url":"https://arxiv.org/pdf/2505.14664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14660v1","updated":"2025-05-20T17:47:04Z","published":"2025-05-20T17:47:04Z","title":"EmoGist: Efficient In-Context Learning for Visual Emotion Understanding","summary":"  In this paper, we introduce EmoGist, a training-free, in-context learning\nmethod for performing visual emotion classification with LVLMs. The key\nintuition of our approach is that context-dependent definition of emotion\nlabels could allow more accurate predictions of emotions, as the ways in which\nemotions manifest within images are highly context dependent and nuanced.\nEmoGist pre-generates multiple explanations of emotion labels, by analyzing the\nclusters of example images belonging to each category. At test time, we\nretrieve a version of explanation based on embedding similarity, and feed it to\na fast VLM for classification. Through our experiments, we show that EmoGist\nallows up to 13 points improvement in micro F1 scores with the multi-label\nMemotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.\n","authors":["Ronald Seoh","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2505.14660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14654v1","updated":"2025-05-20T17:42:34Z","published":"2025-05-20T17:42:34Z","title":"Beyond Words: Multimodal LLM Knows When to Speak","summary":"  While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI.\n","authors":["Zikai Liao","Yi Ouyang","Yi-Lun Lee","Chen-Ping Yu","Yi-Hsuan Tsai","Zhaozheng Yin"],"pdf_url":"https://arxiv.org/pdf/2505.14654v1.pdf","comment":"Project page: https://github.com/lzk901372/MM-When2Speak"},{"id":"http://arxiv.org/abs/2505.14646v1","updated":"2025-05-20T17:34:44Z","published":"2025-05-20T17:34:44Z","title":"CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided\n  Design Code Generation","summary":"  Efficient creation of accurate and editable 3D CAD models is critical in\nengineering design, significantly impacting cost and time-to-market in product\ninnovation. Current manual workflows remain highly time-consuming and demand\nextensive user expertise. While recent developments in AI-driven CAD generation\nshow promise, existing models are limited by incomplete representations of CAD\noperations, inability to generalize to real-world images, and low output\naccuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model\n(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)\ndirectly from visual input. Leveraging a novel dataset that we\ncreated--GenCAD-Code, consisting of over 163k CAD-model image and code\npairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and\nQwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in\n3D solid similarity. Notably, our VLM demonstrates some signs of\ngeneralizability, successfully generating CAD code from real-world images and\nexecuting CAD operations unseen during fine-tuning. The performance and\nadaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code\nto streamline CAD workflows for engineers and designers. CAD-Coder is publicly\navailable at: https://github.com/anniedoris/CAD-Coder.\n","authors":["Anna C. Doris","Md Ferdous Alam","Amin Heyrani Nobari","Faez Ahmed"],"pdf_url":"https://arxiv.org/pdf/2505.14646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14640v1","updated":"2025-05-20T17:26:32Z","published":"2025-05-20T17:26:32Z","title":"VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation","summary":"  Large multimodal models (LMMs) have recently emerged as a powerful tool for\nlong video understanding (LVU), prompting the development of standardized LVU\nbenchmarks to evaluate their performance. However, our investigation reveals a\nrather sober lesson for existing LVU benchmarks. First, most existing\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\nresults are inflated due to the possibility of guessing the correct answer;\nSecond, a significant portion of questions in these benchmarks have strong\npriors to allow models to answer directly without even reading the input video.\nFor example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame\nfrom a long video on Video-MME. We also observe that increasing the number of\nframes does not necessarily lead to improvement on existing benchmarks, which\nis counterintuitive. As a result, the validity and robustness of current LVU\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\nrealistic LVU benchmark containing questions with open-ended short-answer,\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\nsegment-level and full-video understanding through perception and reasoning\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\nfollowing findings: (1) video LMMs show drastic performance ($>$25\\%) drops on\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\nframes. Our results show that VideoEval-Pro offers a more realistic and\nreliable measure of long video understanding, providing a clearer view of\nprogress in this domain.\n","authors":["Wentao Ma","Weiming Ren","Yiming Jia","Zhuofeng Li","Ping Nie","Ge Zhang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2505.14640v1.pdf","comment":"Dataset: https://huggingface.co/datasets/TIGER-Lab/VideoEval-Pro,\n  Project Webpage: https://tiger-ai-lab.github.io/VideoEval-Pro"},{"id":"http://arxiv.org/abs/2505.14638v1","updated":"2025-05-20T17:26:12Z","published":"2025-05-20T17:26:12Z","title":"Dual Precision Quantization for Efficient and Accurate Deep Neural\n  Networks Inference","summary":"  Deep neural networks have achieved state-of-the-art results in a wide range\nof applications, from natural language processing and computer vision to speech\nrecognition. However, as tasks become increasingly complex, model sizes\ncontinue to grow, posing challenges in latency and memory efficiency. To meet\nthese constraints, post-training quantization has emerged as a promising\nsolution. In this paper, we propose a novel hardware-efficient quantization and\ninference scheme that exploits hardware advantages with minimal accuracy\ndegradation. Specifically, we introduce a W4A8 scheme, where weights are\nquantized and stored using 4-bit integer precision, and inference computations\nare performed using 8-bit floating-point arithmetic, demonstrating significant\nspeedups and improved memory utilization compared to 16-bit operations,\napplicable on various modern accelerators. To mitigate accuracy loss, we\ndevelop a novel quantization algorithm, dubbed Dual Precision Quantization\n(DPQ), that leverages the unique structure of our scheme without introducing\nadditional inference overhead. Experimental results demonstrate improved\nperformance (i.e., increased throughput) while maintaining tolerable accuracy\ndegradation relative to the full-precision model.\n","authors":["Tomer Gafni","Asaf Karnieli","Yair Hanani"],"pdf_url":"https://arxiv.org/pdf/2505.14638v1.pdf","comment":"Accepted at eLVM Workshop, CVPR, 2025"},{"id":"http://arxiv.org/abs/2505.14634v1","updated":"2025-05-20T17:25:05Z","published":"2025-05-20T17:25:05Z","title":"A General Framework for Group Sparsity in Hyperspectral Unmixing Using\n  Endmember Bundles","summary":"  Due to low spatial resolution, hyperspectral data often consists of mixtures\nof contributions from multiple materials. This limitation motivates the task of\nhyperspectral unmixing (HU), a fundamental problem in hyperspectral imaging. HU\naims to identify the spectral signatures (\\textit{endmembers}) of the materials\npresent in an observed scene, along with their relative proportions\n(\\textit{fractional abundance}) in each pixel. A major challenge lies in the\nclass variability in materials, which hinders accurate representation by a\nsingle spectral signature, as assumed in the conventional linear mixing model.\nMoreover, To address this issue, we propose using group sparsity after\nrepresenting each material with a set of spectral signatures, known as\nendmember bundles, where each group corresponds to a specific material. In\nparticular, we develop a bundle-based framework that can enforce either\ninter-group sparsity or sparsity within and across groups (SWAG) on the\nabundance coefficients. Furthermore, our framework offers the flexibility to\nincorporate a variety of sparsity-promoting penalties, among which the\ntransformed $\\ell_1$ (TL1) penalty is a novel regularization in the HU\nliterature. Extensive experiments conducted on both synthetic and real\nhyperspectral data demonstrate the effectiveness and superiority of the\nproposed approaches.\n","authors":["Gokul Bhusal","Yifei Lou","Cristina Garcia-Cardona","Ekaterina Merkurjev"],"pdf_url":"https://arxiv.org/pdf/2505.14634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15250v3","updated":"2025-05-20T17:23:45Z","published":"2024-09-23T17:47:59Z","title":"ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models","summary":"  Recent progress in large language models and access to large-scale robotic\ndatasets has sparked a paradigm shift in robotics models transforming them into\ngeneralists able to adapt to various tasks, scenes, and robot modalities. A\nlarge step for the community are open Vision Language Action models which\nshowcase strong performance in a wide variety of tasks. In this work, we study\nthe visual generalization capabilities of three existing robotic foundation\nmodels, and propose a corresponding evaluation framework. Our study shows that\nthe existing models do not exhibit robustness to visual out-of-domain\nscenarios. This is potentially caused by limited variations in the training\ndata and/or catastrophic forgetting, leading to domain limitations in the\nvision foundation models. We further explore OpenVLA, which uses two\npre-trained vision foundation models and is, therefore, expected to generalize\nto out-of-domain experiments. However, we showcase catastrophic forgetting by\nDINO-v2 in OpenVLA through its failure to fulfill the task of depth regression.\nTo overcome the aforementioned issue of visual catastrophic forgetting, we\npropose a gradual backbone reversal approach founded on model merging. This\nenables OpenVLA -- which requires the adaptation of the visual backbones during\ninitial training -- to regain its visual generalization ability. Regaining this\ncapability enables our ReVLA model to improve over OpenVLA by a factor of 77\\%\nand 66\\% for grasping and lifting in visual OOD tasks. Comprehensive\nevaluations, episode rollouts and model weights are available on the ReVLA Page\n","authors":["Sombit Dey","Jan-Nico Zaech","Nikolay Nikolov","Luc Van Gool","Danda Pani Paudel"],"pdf_url":"https://arxiv.org/pdf/2409.15250v3.pdf","comment":"Accepted at ICRA-2025, Atlanta"},{"id":"http://arxiv.org/abs/2504.14783v2","updated":"2025-05-20T17:22:21Z","published":"2025-04-21T00:46:31Z","title":"How Effective Can Dropout Be in Multiple Instance Learning ?","summary":"  Multiple Instance Learning (MIL) is a popular weakly-supervised method for\nvarious applications, with a particular interest in histological whole slide\nimage (WSI) classification. Due to the gigapixel resolution of WSI,\napplications of MIL in WSI typically necessitate a two-stage training scheme:\nfirst, extract features from the pre-trained backbone and then perform MIL\naggregation. However, it is well-known that this suboptimal training scheme\nsuffers from \"noisy\" feature embeddings from the backbone and inherent weak\nsupervision, hindering MIL from learning rich and generalizable features.\nHowever, the most commonly used technique (i.e., dropout) for mitigating this\nissue has yet to be explored in MIL. In this paper, we empirically explore how\neffective the dropout can be in MIL. Interestingly, we observe that dropping\nthe top-k most important instances within a bag leads to better performance and\ngeneralization even under noise attack. Based on this key observation, we\npropose a novel MIL-specific dropout method, termed MIL-Dropout, which\nsystematically determines which instances to drop. Experiments on five MIL\nbenchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the\nperformance of current MIL methods with a negligible computational cost. The\ncode is available at https://github.com/ChongQingNoSubway/MILDropout.\n","authors":["Wenhui Zhu","Peijie Qiu","Xiwen Chen","Zhangsihao Yang","Aristeidis Sotiras","Abolfazl Razi","Yalin Wang"],"pdf_url":"https://arxiv.org/pdf/2504.14783v2.pdf","comment":"Accepted by ICML2025"},{"id":"http://arxiv.org/abs/2505.14629v1","updated":"2025-05-20T17:19:57Z","published":"2025-05-20T17:19:57Z","title":"KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large\n  Language Models","summary":"  Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL.\n","authors":["Fnu Mohbat","Mohammed J Zaki"],"pdf_url":"https://arxiv.org/pdf/2505.14629v1.pdf","comment":"Accepted at ACL 2025"},{"id":"http://arxiv.org/abs/2409.09451v4","updated":"2025-05-20T17:13:09Z","published":"2024-09-14T14:43:57Z","title":"On the Generalizability of Foundation Models for Crop Type Mapping","summary":"  Foundation models pre-trained using self-supervised learning have shown\npowerful transfer learning capabilities on various downstream tasks, including\nlanguage understanding, text generation, and image recognition. The Earth\nobservation (EO) field has produced several foundation models pre-trained\ndirectly on multispectral satellite imagery for applications like precision\nagriculture, wildfire and drought monitoring, and natural disaster response.\nHowever, few studies have investigated the ability of these models to\ngeneralize to new geographic locations, and potential concerns of geospatial\nbias -- models trained on data-rich developed nations not transferring well to\ndata-scarce developing nations -- remain. We evaluate three popular EO\nfoundation models, SSL4EO-S12, SatlasPretrain, and ImageNet, on five crop\nclassification datasets across five continents. Results show that pre-trained\nweights designed explicitly for Sentinel-2, such as SSL4EO-S12, outperform\ngeneral pre-trained weights like ImageNet. While only 100 labeled images are\nsufficient for achieving high overall accuracy, 900 images are required to\nmitigate class imbalance and improve average accuracy.\n","authors":["Yi-Chia Chang","Adam J. Stewart","Favyen Bastani","Piper Wolters","Shreya Kannan","George R. Huber","Jingtong Wang","Arindam Banerjee"],"pdf_url":"https://arxiv.org/pdf/2409.09451v4.pdf","comment":"Accepted to IEEE IGARSS 2025. The final version will appear in the\n  Proceedings of the IEEE International Geoscience and Remote Sensing Symposium\n  (IGARSS) 2025"},{"id":"http://arxiv.org/abs/2505.14621v1","updated":"2025-05-20T17:11:49Z","published":"2025-05-20T17:11:49Z","title":"3D Reconstruction from Sketches","summary":"  We consider the problem of reconstructing a 3D scene from multiple sketches.\nWe propose a pipeline which involves (1) stitching together multiple sketches\nthrough use of correspondence points, (2) converting the stitched sketch into a\nrealistic image using a CycleGAN, and (3) estimating that image's depth-map\nusing a pre-trained convolutional neural network based architecture called\nMegaDepth. Our contribution includes constructing a dataset of image-sketch\npairs, the images for which are from the Zurich Building Database, and sketches\nhave been generated by us. We use this dataset to train a CycleGAN for our\npipeline's second step. We end up with a stitching process that does not\ngeneralize well to real drawings, but the rest of the pipeline that creates a\n3D reconstruction from a single sketch performs quite well on a wide variety of\ndrawings.\n","authors":["Abhimanyu Talwar","Julien Laasri"],"pdf_url":"https://arxiv.org/pdf/2505.14621v1.pdf","comment":"6 pages, 8 figures, paper dated December 12, 2018"},{"id":"http://arxiv.org/abs/2408.07337v2","updated":"2025-05-20T17:08:45Z","published":"2024-08-14T07:22:28Z","title":"KIND: Knowledge Integration and Diversion for Training Decomposable\n  Models","summary":"  Pre-trained models have become the preferred backbone due to the increasing\ncomplexity of model parameters. However, traditional pre-trained models often\nface deployment challenges due to their fixed sizes, and are prone to negative\ntransfer when discrepancies arise between training tasks and target tasks. To\naddress this, we propose KIND, a novel pre-training method designed to\nconstruct decomposable models. KIND integrates knowledge by incorporating\nSingular Value Decomposition (SVD) as a structural constraint, with each basic\ncomponent represented as a combination of a column vector, singular value, and\nrow vector from U, \\Sigma, and V^\\top matrices. These components are\ncategorized into learngenes for encapsulating class-agnostic knowledge and\ntailors for capturing class-specific knowledge, with knowledge diversion\nfacilitated by a class gate mechanism during training. Extensive experiments\ndemonstrate that models pre-trained with KIND can be decomposed into learngenes\nand tailors, which can be adaptively recombined for diverse\nresource-constrained deployments. Moreover, for tasks with large domain shifts,\ntransferring only learngenes with task-agnostic knowledge, when combined with\nrandomly initialized tailors, effectively mitigates domain shifts. Code will be\nmade available at https://github.com/Te4P0t/KIND.\n","authors":["Yucheng Xie","Fu Feng","Ruixiao Shi","Jing Wang","Yong Rui","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2408.07337v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08200v2","updated":"2025-05-20T17:03:50Z","published":"2025-02-12T08:24:36Z","title":"ActiveSSF: An Active-Learning-Guided Self-Supervised Framework for\n  Long-Tailed Megakaryocyte Classification","summary":"  Precise classification of megakaryocytes is crucial for diagnosing\nmyelodysplastic syndromes. Although self-supervised learning has shown promise\nin medical image analysis, its application to classifying megakaryocytes in\nstained slides faces three main challenges: (1) pervasive background noise that\nobscures cellular details, (2) a long-tailed distribution that limits data for\nrare subtypes, and (3) complex morphological variations leading to high\nintra-class variability. To address these issues, we propose the ActiveSSF\nframework, which integrates active learning with self-supervised pretraining.\nSpecifically, our approach employs Gaussian filtering combined with K-means\nclustering and HSV analysis (augmented by clinical prior knowledge) for\naccurate region-of-interest extraction; an adaptive sample selection mechanism\nthat dynamically adjusts similarity thresholds to mitigate class imbalance; and\nprototype clustering on labeled samples to overcome morphological complexity.\nExperimental results on clinical megakaryocyte datasets demonstrate that\nActiveSSF not only achieves state-of-the-art performance but also significantly\nimproves recognition accuracy for rare subtypes. Moreover, the integration of\nthese advanced techniques further underscores the practical potential of\nActiveSSF in clinical settings.\n","authors":["Linghao Zhuang","Ying Zhang","Gege Yuan","Xingyue Zhao","Zhiping Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.08200v2.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2505.14583v1","updated":"2025-05-20T16:40:01Z","published":"2025-05-20T16:40:01Z","title":"Instance Segmentation for Point Sets","summary":"  Recently proposed neural network architectures like PointNet [QSMG16] and\nPointNet++ [QYSG17] have made it possible to apply Deep Learning to 3D point\nsets. The feature representations of shapes learned by these two networks\nenabled training classifiers for Semantic Segmentation, and more recently for\nInstance Segmentation via the Similarity Group Proposal Network (SGPN)\n[WYHN17]. One area of improvement which has been highlighted by SGPN's authors,\npertains to use of memory intensive similarity matrices which occupy memory\nquadratic in the number of points. In this report, we attempt to tackle this\nissue through use of two sampling based methods, which compute Instance\nSegmentation on a sub-sampled Point Set, and then extrapolate labels to the\ncomplete set using the nearest neigbhour approach. While both approaches\nperform equally well on large sub-samples, the random-based strategy gives the\nmost improvements in terms of speed and memory usage.\n","authors":["Abhimanyu Talwar","Julien Laasri"],"pdf_url":"https://arxiv.org/pdf/2505.14583v1.pdf","comment":"6 pages, 11 figures, paper dated 2019"},{"id":"http://arxiv.org/abs/2403.11340v2","updated":"2025-05-20T16:36:17Z","published":"2024-03-17T20:47:52Z","title":"StainDiffuser: MultiTask Dual Diffusion Model for Virtual Staining","summary":"  Hematoxylin and Eosin (H&E) staining is widely regarded as the standard in\npathology for diagnosing diseases and tracking tumor recurrence. While H&E\nstaining shows tissue structures, it lacks the ability to reveal specific\nproteins that are associated with disease severity and treatment response.\nImmunohistochemical (IHC) stains use antibodies to highlight the expression of\nthese proteins on their respective cell types, improving diagnostic accuracy,\nand assisting with drug selection for treatment. Despite their value, IHC\nstains require additional time and resources, limiting their utilization in\nsome clinical settings. Recent advances in deep learning have positioned\nImage-to-Image (I2I) translation as a computational, cost-effective alternative\nfor IHC. I2I generates high fidelity stain transformations digitally,\npotentially replacing manual staining in IHC. Diffusion models, the current\nstate of the art in image generation and conditional tasks, are particularly\nwell suited for virtual IHC due to their ability to produce high quality images\nand resilience to mode collapse. However, these models require extensive and\ndiverse datasets (often millions of samples) to achieve a robust performance, a\nchallenge in virtual staining applications where only thousands of samples are\ntypically available. Inspired by the success of multitask deep learning models\nin scenarios with limited data, we introduce STAINDIFFUSER, a novel multitask\ndiffusion architecture tailored to virtual staining that achieves convergence\nwith smaller datasets. STAINDIFFUSER simultaneously trains two diffusion\nprocesses: (a) generating cell specific IHC stains from H&E images and (b)\nperforming H&E based cell segmentation, utilizing coarse segmentation labels\nexclusively during training. STAINDIFFUSER generates high-quality virtual\nstains for two markers, outperforming over twenty I2I baselines.\n","authors":["Tushar Kataria","Beatrice Knudsen","Shireen Y. Elhabian"],"pdf_url":"https://arxiv.org/pdf/2403.11340v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14572v1","updated":"2025-05-20T16:31:09Z","published":"2025-05-20T16:31:09Z","title":"Automated Fetal Biometry Assessment with Deep Ensembles using\n  Sparse-Sampling of 2D Intrapartum Ultrasound Images","summary":"  The International Society of Ultrasound advocates Intrapartum Ultrasound (US)\nImaging in Obstetrics and Gynecology (ISUOG) to monitor labour progression\nthrough changes in fetal head position. Two reliable ultrasound-derived\nparameters that are used to predict outcomes of instrumental vaginal delivery\nare the angle of progression (AoP) and head-symphysis distance (HSD). In this\nwork, as part of the Intrapartum Ultrasounds Grand Challenge (IUGC) 2024, we\npropose an automated fetal biometry measurement pipeline to reduce intra- and\ninter-observer variability and improve measurement reliability. Our pipeline\nconsists of three key tasks: (i) classification of standard planes (SP) from US\nvideos, (ii) segmentation of fetal head and pubic symphysis from the detected\nSPs, and (iii) computation of the AoP and HSD from the segmented regions. We\nperform sparse sampling to mitigate class imbalances and reduce spurious\ncorrelations in task (i), and utilize ensemble-based deep learning methods for\ntask (i) and (ii) to enhance generalizability under different US acquisition\nsettings. Finally, to promote robustness in task iii) with respect to the\nstructural fidelity of measurements, we retain the largest connected components\nand apply ellipse fitting to the segmentations. Our solution achieved ACC:\n0.9452, F1: 0.9225, AUC: 0.983, MCC: 0.8361, DSC: 0.918, HD: 19.73, ASD: 5.71,\n$\\Delta_{AoP}$: 8.90 and $\\Delta_{HSD}$: 14.35 across an unseen hold-out set of\n4 patients and 224 US frames. The results from the proposed automated pipeline\ncan improve the understanding of labour arrest causes and guide the development\nof clinical risk stratification tools for efficient and effective prenatal\ncare.\n","authors":["Jayroop Ramesh","Valentin Bacher","Mark C. Eid","Hoda Kalabizadeh","Christian Rupprecht","Ana IL Namburete","Pak-Hei Yeung","Madeleine K. Wyburd","Nicola K. Dinsdale"],"pdf_url":"https://arxiv.org/pdf/2505.14572v1.pdf","comment":"Top 5 in MICCAI IUGC 2024: Intrapartum Ultrasound Grand Challenge &\n  Runners up in Classification!"},{"id":"http://arxiv.org/abs/2311.16515v4","updated":"2025-05-20T16:29:22Z","published":"2023-11-25T14:24:49Z","title":"Automatic Synthetic Data and Fine-grained Adaptive Feature Alignment for\n  Composed Person Retrieval","summary":"  Person retrieval has attracted rising attention. Existing methods are mainly\ndivided into two retrieval modes, namely image-only and text-only. However,\nthey are unable to make full use of the available information and are difficult\nto meet diverse application requirements. To address the above limitations, we\npropose a new Composed Person Retrieval (CPR) task, which combines visual and\ntextual queries to identify individuals of interest from large-scale person\nimage databases. Nevertheless, the foremost difficulty of the CPR task is the\nlack of available annotated datasets. Therefore, we first introduce a scalable\nautomatic data synthesis pipeline, which decomposes complex multimodal data\ngeneration into the creation of textual quadruples followed by\nidentity-consistent image synthesis using fine-tuned generative models.\nMeanwhile, a multimodal filtering method is designed to ensure the resulting\nSynCPR dataset retains 1.15 million high-quality and fully synthetic triplets.\nAdditionally, to improve the representation of composed person queries, we\npropose a novel Fine-grained Adaptive Feature Alignment (FAFA) framework\nthrough fine-grained dynamic alignment and masked feature reasoning. Moreover,\nfor objective evaluation, we manually annotate the Image-Text Composed Person\nRetrieval (ITCPR) test set. The extensive experiments demonstrate the\neffectiveness of the SynCPR dataset and the superiority of the proposed FAFA\nframework when compared with the state-of-the-art methods. All code and data\nwill be provided at\nhttps://github.com/Delong-liu-bupt/Composed_Person_Retrieval.\n","authors":["Delong Liu","Haiwen Li","Zhaohui Hou","Zhicheng Zhao","Fei Su","Yuan Dong"],"pdf_url":"https://arxiv.org/pdf/2311.16515v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14560v1","updated":"2025-05-20T16:19:16Z","published":"2025-05-20T16:19:16Z","title":"Neural Inverse Scattering with Score-based Regularization","summary":"  Inverse scattering is a fundamental challenge in many imaging applications,\nranging from microscopy to remote sensing. Solving this problem often requires\njointly estimating two unknowns -- the image and the scattering field inside\nthe object -- necessitating effective image prior to regularize the inference.\nIn this paper, we propose a regularized neural field (NF) approach which\nintegrates the denoising score function used in score-based generative models.\nThe neural field formulation offers convenient flexibility to performing joint\nestimation, while the denoising score function imposes the rich structural\nprior of images. Our results on three high-contrast simulated objects show that\nthe proposed approach yields a better imaging quality compared to the\nstate-of-the-art NF approach, where regularization is based on total variation.\n","authors":["Yuan Gao","Wenhan Guo","Yu Sun"],"pdf_url":"https://arxiv.org/pdf/2505.14560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14556v1","updated":"2025-05-20T16:14:37Z","published":"2025-05-20T16:14:37Z","title":"Dynadiff: Single-stage Decoding of Images from Continuously Evolving\n  fMRI","summary":"  Brain-to-image decoding has been recently propelled by the progress in\ngenerative AI models and the availability of large ultra-high field functional\nMagnetic Resonance Imaging (fMRI). However, current approaches depend on\ncomplicated multi-stage pipelines and preprocessing steps that typically\ncollapse the temporal dimension of brain recordings, thereby limiting\ntime-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural\nActivity Diffusion for Image Reconstruction), a new single-stage diffusion\nmodel designed for reconstructing images from dynamically evolving fMRI\nrecordings. Our approach offers three main contributions. First, Dynadiff\nsimplifies training as compared to existing approaches. Second, our model\noutperforms state-of-the-art models on time-resolved fMRI signals, especially\non high-level semantic image reconstruction metrics, while remaining\ncompetitive on preprocessed fMRI data that collapse time. Third, this approach\nallows a precise characterization of the evolution of image representations in\nbrain activity. Overall, this work lays the foundation for time-resolved\nbrain-to-image decoding.\n","authors":["Marlène Careil","Yohann Benchetrit","Jean-Rémi King"],"pdf_url":"https://arxiv.org/pdf/2505.14556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07035v2","updated":"2025-05-20T16:04:17Z","published":"2025-03-10T08:20:55Z","title":"Universal Incremental Learning: Mitigating Confusion from Inter- and\n  Intra-task Distribution Randomness","summary":"  Incremental learning (IL) aims to overcome catastrophic forgetting of\nprevious tasks while learning new ones. Existing IL methods make strong\nassumptions that the incoming task type will either only increases new classes\nor domains (i.e. Class IL, Domain IL), or increase by a static scale in a\nclass- and domain-agnostic manner (i.e. Versatile IL (VIL)), which greatly\nlimit their applicability in the unpredictable and dynamic wild. In this work,\nwe investigate $\\textbf{Universal Incremental Learning (UIL)}$, where a model\nneither knows which new classes or domains will increase along sequential\ntasks, nor the scale of the increments within each task. This uncertainty\nprevents the model from confidently learning knowledge from all task\ndistributions and symmetrically focusing on the diverse knowledge within each\ntask distribution. Consequently, UIL presents a more general and realistic IL\nscenario, making the model face confusion arising from inter-task and\nintra-task distribution randomness. To $\\textbf{Mi}$tigate both\n$\\textbf{Co}$nfusion, we propose a simple yet effective framework for UIL,\nnamed $\\textbf{MiCo}$. At the inter-task distribution level, we employ a\nmulti-objective learning scheme to enforce accurate and deterministic\npredictions, and its effectiveness is further enhanced by a direction\nrecalibration module that reduces conflicting gradients. Moreover, at the\nintra-task distribution level, we introduce a magnitude recalibration module to\nalleviate asymmetrical optimization towards imbalanced class distribution.\nExtensive experiments on three benchmarks demonstrate the effectiveness of our\nmethod, outperforming existing state-of-the-art methods in both the UIL\nscenario and the VIL scenario. Our code will be available at\n$\\href{https://github.com/rolsheng/UIL}{here}$.\n","authors":["Sheng Luo","Yi Zhou","Tao Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.07035v2.pdf","comment":"10 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2505.14541v1","updated":"2025-05-20T15:57:09Z","published":"2025-05-20T15:57:09Z","title":"Neural Video Compression with Context Modulation","summary":"  Efficient video coding is highly dependent on exploiting the temporal\nredundancy, which is usually achieved by extracting and leveraging the temporal\ncontext in the emerging conditional coding-based neural video codec (NVC).\nAlthough the latest NVC has achieved remarkable progress in improving the\ncompression performance, the inherent temporal context propagation mechanism\nlacks the ability to sufficiently leverage the reference information, limiting\nfurther improvement. In this paper, we address the limitation by modulating the\ntemporal context with the reference frame in two steps. Specifically, we first\npropose the flow orientation to mine the inter-correlation between the\nreference frame and prediction frame for generating the additional oriented\ntemporal context. Moreover, we introduce the context compensation to leverage\nthe oriented context to modulate the propagated temporal context generated from\nthe propagated reference feature. Through the synergy mechanism and decoupling\nloss supervision, the irrelevant propagated information can be effectively\neliminated to ensure better context modeling. Experimental results demonstrate\nthat our codec achieves on average 22.7% bitrate reduction over the advanced\ntraditional video codec H.266/VVC, and offers an average 10.1% bitrate saving\nover the previous state-of-the-art NVC DCVC-FM. The code is available at\nhttps://github.com/Austin4USTC/DCMVC.\n","authors":["Chuanbo Tang","Zhuoyuan Li","Yifan Bian","Li Li","Dong Liu"],"pdf_url":"https://arxiv.org/pdf/2505.14541v1.pdf","comment":"11 pages, 8 figures, accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2505.14537v1","updated":"2025-05-20T15:55:53Z","published":"2025-05-20T15:55:53Z","title":"Personalize Your Gaussian: Consistent 3D Scene Personalization from a\n  Single Image","summary":"  Personalizing 3D scenes from a single reference image enables intuitive\nuser-guided editing, which requires achieving both multi-view consistency\nacross perspectives and referential consistency with the input image. However,\nthese goals are particularly challenging due to the viewpoint bias caused by\nthe limited perspective provided in a single image. Lacking the mechanisms to\neffectively expand reference information beyond the original view, existing\nmethods of image-conditioned 3DGS personalization often suffer from this\nviewpoint bias and struggle to produce consistent results. Therefore, in this\npaper, we present Consistent Personalization for 3D Gaussian Splatting (CP-GS),\na framework that progressively propagates the single-view reference appearance\nto novel perspectives. In particular, CP-GS integrates pre-trained image-to-3D\ngeneration and iterative LoRA fine-tuning to extract and extend the reference\nappearance, and finally produces faithful multi-view guidance images and the\npersonalized 3DGS outputs through a view-consistent generation process guided\nby geometric cues. Extensive experiments on real-world scenes show that our\nCP-GS effectively mitigates the viewpoint bias, achieving high-quality\npersonalization that significantly outperforms existing methods. The code will\nbe released at https://github.com/Yuxuan-W/CP-GS.\n","authors":["Yuxuan Wang","Xuanyu Yi","Qingshan Xu","Yuan Zhou","Long Chen","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.14537v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2505.14527v1","updated":"2025-05-20T15:48:27Z","published":"2025-05-20T15:48:27Z","title":"diffDemorph: Extending Reference-Free Demorphing to Unseen Faces","summary":"  A face morph is created by combining two (or more) face images corresponding\nto two (or more) identities to produce a composite that successfully matches\nthe constituent identities. Reference-free (RF) demorphing reverses this\nprocess using only the morph image, without the need for additional reference\nimages. Previous RF demorphing methods were overly constrained, as they rely on\nassumptions about the distributions of training and testing morphs such as the\nmorphing technique used, face style, and images used to create the morph. In\nthis paper, we introduce a novel diffusion-based approach that effectively\ndisentangles component images from a composite morph image with high visual\nfidelity. Our method is the first to generalize across morph techniques and\nface styles, beating the current state of the art by $\\geq 59.46\\%$ under a\ncommon training protocol across all datasets tested. We train our method on\nmorphs created using synthetically generated face images and test on real\nmorphs, thereby enhancing the practicality of the technique. Experiments on six\ndatasets and two face matchers establish the utility and efficacy of our\nmethod.\n","authors":["Nitish Shukla","Arun Ross"],"pdf_url":"https://arxiv.org/pdf/2505.14527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14521v1","updated":"2025-05-20T15:44:54Z","published":"2025-05-20T15:44:54Z","title":"SparC: Sparse Representation and Construction for High-Resolution 3D\n  Shapes Modeling","summary":"  High-fidelity 3D object synthesis remains significantly more challenging than\n2D image generation due to the unstructured nature of mesh data and the cubic\ncomplexity of dense volumetric grids. Existing two-stage pipelines-compressing\nmeshes with a VAE (using either 2D or 3D supervision), followed by latent\ndiffusion sampling-often suffer from severe detail loss caused by inefficient\nrepresentations and modality mismatches introduced in VAE. We introduce SparC,\na unified framework that combines a sparse deformable marching cubes\nrepresentation SparseCubes with a novel encoder SparConv-VAE. SparseCubes\nconverts raw meshes into high-resolution ($1024^3$) surfaces with arbitrary\ntopology by scattering signed distance and deformation fields onto a sparse\ncube, allowing differentiable optimization. SparConv-VAE is the first\nmodality-consistent variational autoencoder built entirely upon sparse\nconvolutional networks, enabling efficient and near-lossless 3D reconstruction\nsuitable for high-resolution generative modeling through latent diffusion.\nSparC achieves state-of-the-art reconstruction fidelity on challenging inputs,\nincluding open surfaces, disconnected components, and intricate geometry. It\npreserves fine-grained shape details, reduces training and inference cost, and\nintegrates naturally with latent diffusion models for scalable, high-resolution\n3D generation.\n","authors":["Zhihao Li","Yufei Wang","Heliang Zheng","Yihao Luo","Bihan Wen"],"pdf_url":"https://arxiv.org/pdf/2505.14521v1.pdf","comment":"Homepage: https://lizhihao6.github.io/SparC"},{"id":"http://arxiv.org/abs/2505.14511v1","updated":"2025-05-20T15:39:20Z","published":"2025-05-20T15:39:20Z","title":"ReservoirTTA: Prolonged Test-time Adaptation for Evolving and Recurring\n  Domains","summary":"  This paper introduces ReservoirTTA, a novel plug-in framework designed for\nprolonged test-time adaptation (TTA) in scenarios where the test domain\ncontinuously shifts over time, including cases where domains recur or evolve\ngradually. At its core, ReservoirTTA maintains a reservoir of\ndomain-specialized models -- an adaptive test-time model ensemble -- that both\ndetects new domains via online clustering over style features of incoming\nsamples and routes each sample to the appropriate specialized model, and\nthereby enables domain-specific adaptation. This multi-model strategy overcomes\nkey limitations of single model adaptation, such as catastrophic forgetting,\ninter-domain interference, and error accumulation, ensuring robust and stable\nperformance on sustained non-stationary test distributions. Our theoretical\nanalysis reveals key components that bound parameter variance and prevent model\ncollapse, while our plug-in TTA module mitigates catastrophic forgetting of\npreviously encountered domains. Extensive experiments on the classification\ncorruption benchmarks, including ImageNet-C and CIFAR-10/100-C, as well as the\nCityscapes$\\rightarrow$ACDC semantic segmentation task, covering recurring and\ncontinuously evolving domain shifts, demonstrate that ReservoirTTA\nsignificantly improves adaptation accuracy and maintains stable performance\nacross prolonged, recurring shifts, outperforming state-of-the-art methods.\n","authors":["Guillaume Vray","Devavrat Tomar","Xufeng Gao","Jean-Philippe Thiran","Evan Shelhamer","Behzad Bozorgtabar"],"pdf_url":"https://arxiv.org/pdf/2505.14511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07266v2","updated":"2025-05-20T15:37:05Z","published":"2025-03-10T12:48:29Z","title":"Customized SAM 2 for Referring Remote Sensing Image Segmentation","summary":"  Referring Remote Sensing Image Segmentation (RRSIS) aims to segment target\nobjects in remote sensing (RS) images based on textual descriptions. Although\nSegment Anything Model 2 (SAM 2) has shown remarkable performance in various\nsegmentation tasks, its application to RRSIS presents several challenges,\nincluding understanding the text-described RS scenes and generating effective\nprompts from text descriptions. To address these issues, we propose RS2-SAM 2,\na novel framework that adapts SAM 2 to RRSIS by aligning the adapted RS\nfeatures and textual features, providing pseudo-mask-based dense prompts, and\nenforcing boundary constraints. Specifically, we first employ a union encoder\nto jointly encode the visual and textual inputs, generating aligned visual and\ntext embeddings as well as multimodal class tokens. Then, we design a\nbidirectional hierarchical fusion module to adapt SAM 2 to RS scenes and align\nadapted visual features with the visually enhanced text embeddings, improving\nthe model's interpretation of text-described RS scenes. Additionally, a mask\nprompt generator is introduced to take the visual embeddings and class tokens\nas input and produce a pseudo-mask as the dense prompt of SAM 2. To further\nrefine segmentation, we introduce a text-guided boundary loss to optimize\nsegmentation boundaries by computing text-weighted gradient differences.\nExperimental results on several RRSIS benchmarks demonstrate that RS2-SAM 2\nachieves state-of-the-art performance.\n","authors":["Fu Rong","Meng Lan","Qian Zhang","Lefei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.07266v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09083v2","updated":"2025-05-20T15:29:37Z","published":"2024-10-06T08:33:39Z","title":"Evaluating the Correctness of Inference Patterns Used by LLMs for\n  Judgment","summary":"  This paper presents a method to analyze the inference patterns used by Large\nLanguage Models (LLMs) for judgment in a case study on legal LLMs, so as to\nidentify potential incorrect representations of the LLM, according to human\ndomain knowledge. Unlike traditional evaluations on language generation\nresults, we propose to evaluate the correctness of the detailed inference\npatterns of an LLM behind its seemingly correct outputs. To this end, we\nquantify the interactions between input phrases used by the LLM as primitive\ninference patterns, because recent theoretical achievements have proven several\nmathematical guarantees of the faithfulness of the interaction-based\nexplanation. We design a set of metrics to evaluate the detailed inference\npatterns of LLMs. Experiments show that even when the language generation\nresults appear correct, a significant portion of the inference patterns used by\nthe LLM for the legal judgment may represent misleading or irrelevant logic.\n","authors":["Lu Chen","Yuxuan Huang","Yixing Li","Dongrui Liu","Qihan Ren","Shuai Zhao","Kun Kuang","Zilong Zheng","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.09083v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12482v2","updated":"2025-05-20T15:28:35Z","published":"2025-05-18T15:56:35Z","title":"Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral\n  Image Classification","summary":"  Few-shot classification of hyperspectral images (HSI) faces the challenge of\nscarce labeled samples. Self-Supervised learning (SSL) and Few-Shot Learning\n(FSL) offer promising avenues to address this issue. However, existing methods\noften struggle to adapt to the spatial geometric diversity of HSIs and lack\nsufficient spectral prior knowledge. To tackle these challenges, we propose a\nmethod, Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral\nImage Classification (S4L-FSC), aimed at improving the performance of few-shot\nHSI classification. Specifically, we first leverage heterogeneous datasets to\npretrain a spatial feature extractor using a designed Rotation-Mirror\nSelf-Supervised Learning (RM-SSL) method, combined with FSL. This approach\nenables the model to learn the spatial geometric diversity of HSIs using\nrotation and mirroring labels as supervisory signals, while acquiring\ntransferable spatial meta-knowledge through few-shot learning. Subsequently,\nhomogeneous datasets are utilized to pretrain a spectral feature extractor via\na combination of FSL and Masked Reconstruction Self-Supervised Learning\n(MR-SSL). The model learns to reconstruct original spectral information from\nrandomly masked spectral vectors, inferring spectral dependencies. In parallel,\nFSL guides the model to extract pixel-level discriminative features, thereby\nembedding rich spectral priors into the model. This spectral-spatial\npretraining method, along with the integration of knowledge from heterogeneous\nand homogeneous sources, significantly enhances model performance. Extensive\nexperiments on four HSI datasets demonstrate the effectiveness and superiority\nof the proposed S4L-FSC approach for few-shot HSI classification.\n","authors":["Wenchen Chen","Yanmei Zhang","Zhongwei Xiao","Jianping Chu","Xingbo Wang"],"pdf_url":"https://arxiv.org/pdf/2505.12482v2.pdf","comment":"https://github.com/Wenchen-Chen/S4L-FSC"},{"id":"http://arxiv.org/abs/2505.09018v2","updated":"2025-05-20T15:25:23Z","published":"2025-05-13T23:12:54Z","title":"Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric\n  Content Prediction","summary":"  Effective dietary monitoring is critical for managing Type 2 diabetes, yet\naccurately estimating caloric intake remains a major challenge. While\ncontinuous glucose monitors (CGMs) offer valuable physiological data, they\noften fall short in capturing the full nutritional profile of meals due to\ninter-individual and meal-specific variability. In this work, we introduce a\nmultimodal deep learning framework that jointly leverages CGM time-series data,\nDemographic/Microbiome, and pre-meal food images to enhance caloric estimation.\nOur model utilizes attention based encoding and a convolutional feature\nextraction for meal imagery, multi-layer perceptrons for CGM and Microbiome\ndata followed by a late fusion strategy for joint reasoning. We evaluate our\napproach on a curated dataset of over 40 participants, incorporating\nsynchronized CGM, Demographic and Microbiome data and meal photographs with\nstandardized caloric labels. Our model achieves a Root Mean Squared Relative\nError (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These\nfindings demonstrate the potential of multimodal sensing to improve automated\ndietary assessment tools for chronic disease management.\n","authors":["Adarsh Kumar"],"pdf_url":"https://arxiv.org/pdf/2505.09018v2.pdf","comment":"The manuscript was submitted without proper consideration of\n  institutional policies. Upon review with professor, it was found that the\n  content is subject to licensing restrictions which prohibit public\n  dissemination in its current form. Therefore, I am withdrawing the paper to\n  comply with these requirements"},{"id":"http://arxiv.org/abs/2505.06993v2","updated":"2025-05-20T15:25:12Z","published":"2025-05-11T14:37:30Z","title":"Technical Report: Quantifying and Analyzing the Generalization Power of\n  a DNN","summary":"  This paper proposes a new perspective for analyzing the generalization power\nof deep neural networks (DNNs), i.e., directly disentangling and analyzing the\ndynamics of generalizable and non-generalizable interaction encoded by a DNN\nthrough the training process. Specifically, this work builds upon the recent\ntheoretical achievement in explainble AI, which proves that the detailed\ninference logic of DNNs can be can be strictly rewritten as a small number of\nAND-OR interaction patterns. Based on this, we propose an efficient method to\nquantify the generalization power of each interaction, and we discover a\ndistinct three-phase dynamics of the generalization power of interactions\nduring training. In particular, the early phase of training typically removes\nnoisy and non-generalizable interactions and learns simple and generalizable\nones. The second and the third phases tend to capture increasingly complex\ninteractions that are harder to generalize. Experimental results verify that\nthe learning of non-generalizable interactions is the the direct cause for the\ngap between the training and testing losses.\n","authors":["Yuxuan He","Junpeng Zhang","Lei Cheng","Hongyuan Zhang","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.06993v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09948v3","updated":"2025-05-20T15:19:09Z","published":"2024-03-15T01:18:08Z","title":"RadCLIP: Enhancing Radiologic Image Analysis through Contrastive\n  Language-Image Pre-training","summary":"  The integration of artificial intelligence (AI) with radiology marks a\ntransformative era in medicine. Vision foundation models have been adopted to\nenhance radiologic imaging analysis. However, the distinct complexities of\nradiologic 2D and 3D radiologic data pose unique challenges that existing\nmodels, pre-trained on general non-medical images, fail to address adequately.\nTo bridge this gap and capitalize on the diagnostic precision required in\nradiologic imaging, we introduce Radiologic Contrastive Language-Image\nPre-training (RadCLIP): a cross-modal vision-language foundational model that\nharnesses Vision Language Pre-training (VLP) framework to improve radiologic\nimage analysis. Building upon Contrastive Language-Image Pre-training (CLIP),\nRadCLIP incorporates a slice pooling mechanism tailored for volumetric image\nanalysis and is pre-trained using a large and diverse dataset of radiologic\nimage-text pairs. The RadCLIP was pre-trained to effectively align radiologic\nimages with their corresponding text annotations, creating a robust vision\nbackbone for radiologic images. Extensive experiments demonstrate RadCLIP's\nsuperior performance in both uni-modal radiologic image classification and\ncross-modal image-text matching, highlighting its significant promise for\nimproving diagnostic accuracy and efficiency in clinical settings. Our Key\ncontributions include curating a large dataset with diverse radiologic 2D/3D\nradiologic image-text pairs, a slice pooling adapter using an attention\nmechanism for integrating 2D images, and comprehensive evaluations of RadCLIP\non various radiologic downstream tasks.\n","authors":["Zhixiu Lu","Hailong Li","Nehal A. Parikh","Jonathan R. Dillman","Lili He"],"pdf_url":"https://arxiv.org/pdf/2403.09948v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.02508v2","updated":"2025-05-20T15:17:39Z","published":"2024-12-03T15:39:05Z","title":"Towards Rich Emotions in 3D Avatars: A Text-to-3D Avatar Generation\n  Benchmark","summary":"  Producing emotionally dynamic 3D facial avatars with text derived from spoken\nwords (Emo3D) has been a pivotal research topic in 3D avatar generation. While\nprogress has been made in general-purpose 3D avatar generation, the exploration\nof generating emotional 3D avatars remains scarce, primarily due to the\ncomplexities of identifying and rendering rich emotions from spoken words. This\npaper reexamines Emo3D generation and draws inspiration from human processes,\nbreaking down Emo3D into two cascading steps: Text-to-3D Expression Mapping\n(T3DEM) and 3D Avatar Rendering (3DAR). T3DEM is the most crucial step in\ndetermining the quality of Emo3D generation and encompasses three key\nchallenges: Expression Diversity, Emotion-Content Consistency, and Expression\nFluidity. To address these challenges, we introduce a novel benchmark to\nadvance research in Emo3D generation. First, we present EmoAva, a large-scale,\nhigh-quality dataset for T3DEM, comprising 15,000 text-to-3D expression\nmappings that characterize the aforementioned three challenges in Emo3D\ngeneration. Furthermore, we develop various metrics to effectively evaluate\nmodels against these identified challenges. Next, to effectively model the\nconsistency, diversity, and fluidity of human expressions in the T3DEM step, we\npropose the Continuous Text-to-Expression Generator, which employs an\nautoregressive Conditional Variational Autoencoder for expression code\ngeneration, enhanced with Latent Temporal Attention and Expression-wise\nAttention mechanisms. Finally, to further enhance the 3DAR step on rendering\nhigher-quality subtle expressions, we present the Globally-informed Gaussian\nAvatar (GiGA) model. GiGA incorporates a global information mechanism into 3D\nGaussian representations, enabling the capture of subtle micro-expressions and\nseamless transitions between emotional states.\n","authors":["Haidong Xu","Meishan Zhang","Hao Ju","Zhedong Zheng","Erik Cambria","Min Zhang","Hao Fei"],"pdf_url":"https://arxiv.org/pdf/2412.02508v2.pdf","comment":"19 pages. Project website: https://github.com/WalkerMitty/EmoAva"},{"id":"http://arxiv.org/abs/2504.14440v2","updated":"2025-05-20T15:16:55Z","published":"2025-04-20T01:22:40Z","title":"SG-Reg: Generalizable and Efficient Scene Graph Registration","summary":"  This paper addresses the challenges of registering two rigid semantic scene\ngraphs, an essential capability when an autonomous agent needs to register its\nmap against a remote agent, or against a prior map. The hand-crafted\ndescriptors in classical semantic-aided registration, or the ground-truth\nannotation reliance in learning-based scene graph registration, impede their\napplication in practical real-world environments. To address the challenges, we\ndesign a scene graph network to encode multiple modalities of semantic nodes:\nopen-set semantic feature, local topology with spatial awareness, and shape\nfeature. These modalities are fused to create compact semantic node features.\nThe matching layers then search for correspondences in a coarse-to-fine manner.\nIn the back-end, we employ a robust pose estimator to decide transformation\naccording to the correspondences. We manage to maintain a sparse and\nhierarchical scene representation. Our approach demands fewer GPU resources and\nfewer communication bandwidth in multi-agent tasks. Moreover, we design a new\ndata generation approach using vision foundation models and a semantic mapping\nmodule to reconstruct semantic scene graphs. It differs significantly from\nprevious works, which rely on ground-truth semantic annotations to generate\ndata. We validate our method in a two-agent SLAM benchmark. It significantly\noutperforms the hand-crafted baseline in terms of registration success rate.\nCompared to visual loop closure networks, our method achieves a slightly higher\nregistration recall while requiring only 52 KB of communication bandwidth for\neach query frame. Code available at:\n\\href{http://github.com/HKUST-Aerial-Robotics/SG-Reg}{http://github.com/HKUST-Aerial-Robotics/SG-Reg}.\n","authors":["Chuhao Liu","Zhijian Qiao","Jieqi Shi","Ke Wang","Peize Liu","Shaojie Shen"],"pdf_url":"https://arxiv.org/pdf/2504.14440v2.pdf","comment":"IEEE Transactions Robotics Regular Paper"},{"id":"http://arxiv.org/abs/2505.14476v1","updated":"2025-05-20T15:10:01Z","published":"2025-05-20T15:10:01Z","title":"Enhancing Interpretability of Sparse Latent Representations with Class\n  Information","summary":"  Variational Autoencoders (VAEs) are powerful generative models for learning\nlatent representations. Standard VAEs generate dispersed and unstructured\nlatent spaces by utilizing all dimensions, which limits their interpretability,\nespecially in high-dimensional spaces. To address this challenge, Variational\nSparse Coding (VSC) introduces a spike-and-slab prior distribution, resulting\nin sparse latent representations for each input. These sparse representations,\ncharacterized by a limited number of active dimensions, are inherently more\ninterpretable. Despite this advantage, VSC falls short in providing structured\ninterpretations across samples within the same class. Intuitively, samples from\nthe same class are expected to share similar attributes while allowing for\nvariations in those attributes. This expectation should manifest as consistent\npatterns of active dimensions in their latent representations, but VSC does not\nenforce such consistency.\n  In this paper, we propose a novel approach to enhance the latent space\ninterpretability by ensuring that the active dimensions in the latent space are\nconsistent across samples within the same class. To achieve this, we introduce\na new loss function that encourages samples from the same class to share\nsimilar active dimensions. This alignment creates a more structured and\ninterpretable latent space, where each shared dimension corresponds to a\nhigh-level concept, or \"factor.\" Unlike existing disentanglement-based methods\nthat primarily focus on global factors shared across all classes, our method\ncaptures both global and class-specific factors, thereby enhancing the utility\nand interpretability of latent representations.\n","authors":["Farshad Sangari Abiz","Reshad Hosseini","Babak N. Araabi"],"pdf_url":"https://arxiv.org/pdf/2505.14476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14462v1","updated":"2025-05-20T14:57:16Z","published":"2025-05-20T14:57:16Z","title":"RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture\n  Understanding","summary":"  As vision-language models (VLMs) become increasingly integrated into daily\nlife, the need for accurate visual culture understanding is becoming critical.\nYet, these models frequently fall short in interpreting cultural nuances\neffectively. Prior work has demonstrated the effectiveness of\nretrieval-augmented generation (RAG) in enhancing cultural understanding in\ntext-only settings, while its application in multimodal scenarios remains\nunderexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented\nVisual culturE uNdErstAnding), a new benchmark designed to advance visual\nculture understanding through retrieval, focusing on two tasks: culture-focused\nvisual question answering (cVQA) and culture-informed image captioning (cIC).\nRAVENEA extends existing datasets by integrating over 10,000 Wikipedia\ndocuments curated and ranked by human annotators. With RAVENEA, we train and\nevaluate seven multimodal retrievers for each image query, and measure the\ndownstream impact of retrieval-augmented inputs across fourteen\nstate-of-the-art VLMs. Our results show that lightweight VLMs, when augmented\nwith culture-aware retrieval, outperform their non-augmented counterparts (by\nat least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the\nvalue of retrieval-augmented methods and culturally inclusive benchmarks for\nmultimodal understanding.\n","authors":["Jiaang Li","Yifei Yuan","Wenyan Li","Mohammad Aliannejadi","Daniel Hershcovich","Anders Søgaard","Ivan Vulić","Wenxuan Zhang","Paul Pu Liang","Yang Deng","Serge Belongie"],"pdf_url":"https://arxiv.org/pdf/2505.14462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14460v1","updated":"2025-05-20T14:56:50Z","published":"2025-05-20T14:56:50Z","title":"VisualQuality-R1: Reasoning-Induced Image Quality Assessment via\n  Reinforcement Learning to Rank","summary":"  DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing\nreasoning and generalization capabilities of large language models (LLMs)\nthrough reinforcement learning. Nevertheless, the potential of\nreasoning-induced computational modeling has not been thoroughly explored in\nthe context of image quality assessment (IQA), a task critically dependent on\nvisual reasoning. In this paper, we introduce VisualQuality-R1, a\nreasoning-induced no-reference IQA (NR-IQA) model, and we train it with\nreinforcement learning to rank, a learning algorithm tailored to the\nintrinsically relative nature of visual quality. Specifically, for a pair of\nimages, we employ group relative policy optimization to generate multiple\nquality scores for each image. These estimates are then used to compute\ncomparative probabilities of one image having higher quality than the other\nunder the Thurstone model. Rewards for each quality estimate are defined using\ncontinuous fidelity measures rather than discretized binary labels. Extensive\nexperiments show that the proposed VisualQuality-R1 consistently outperforms\ndiscriminative deep learning-based NR-IQA models as well as a recent\nreasoning-induced quality regression method. Moreover, VisualQuality-R1 is\ncapable of generating contextually rich, human-aligned quality descriptions,\nand supports multi-dataset training without requiring perceptual scale\nrealignment. These features make VisualQuality-R1 especially well-suited for\nreliably measuring progress in a wide range of image processing tasks like\nsuper-resolution and image generation.\n","authors":["Tianhe Wu","Jian Zou","Jie Liang","Lei Zhang","Kede Ma"],"pdf_url":"https://arxiv.org/pdf/2505.14460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14454v1","updated":"2025-05-20T14:52:31Z","published":"2025-05-20T14:52:31Z","title":"Video Compression Commander: Plug-and-Play Inference Acceleration for\n  Video Large Language Models","summary":"  Video large language models (VideoLLM) excel at video understanding, but face\nefficiency challenges due to the quadratic complexity of abundant visual\ntokens. Our systematic analysis of token compression methods for VideoLLMs\nreveals two critical issues: (i) overlooking distinctive visual signals across\nframes, leading to information loss; (ii) suffering from implementation\nconstraints, causing incompatibility with modern architectures or efficient\noperators. To address these challenges, we distill three design principles for\nVideoLLM token compression and propose a plug-and-play inference acceleration\nframework \"Video Compression Commander\" (VidCom2). By quantifying each frame's\nuniqueness, VidCom2 adaptively adjusts compression intensity across frames,\neffectively preserving essential information while reducing redundancy in video\nsequences. Extensive experiments across various VideoLLMs and benchmarks\ndemonstrate the superior performance and efficiency of our VidCom2. With only\n25% visual tokens, VidCom2 achieves 99.6% of the original performance on\nLLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame\nCompression Adjustment strategy is compatible with other token compression\nmethods to further improve their performance. Our code is available at\nhttps://github.com/xuyang-liu16/VidCom2.\n","authors":["Xuyang Liu","Yiyu Wang","Junpeng Ma","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.14454v1.pdf","comment":"Our code is available at https://github.com/xuyang-liu16/VidCom2"},{"id":"http://arxiv.org/abs/2501.08828v2","updated":"2025-05-20T14:49:55Z","published":"2025-01-15T14:30:13Z","title":"MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents","summary":"  Multimodal document retrieval aims to identify and retrieve various forms of\nmultimodal content, such as figures, tables, charts, and layout information\nfrom extensive documents. Despite its increasing popularity, there is a notable\nlack of a comprehensive and robust benchmark to effectively evaluate the\nperformance of systems in such tasks. To address this gap, this work introduces\na new benchmark, named MMDocIR, that encompasses two distinct tasks: page-level\nand layout-level retrieval. The former evaluates the performance of identifying\nthe most relevant pages within a long document, while the later assesses the\nability of detecting specific layouts, providing a more fine-grained measure\nthan whole-page analysis. A layout refers to a variety of elements, including\ntextual paragraphs, equations, figures, tables, or charts. The MMDocIR\nbenchmark comprises a rich dataset featuring 1,685 questions annotated by\nexperts and 173,843 questions with bootstrapped labels, making it a valuable\nresource in multimodal document retrieval for both training and evaluation.\nThrough rigorous experiments, we demonstrate that (i) visual retrievers\nsignificantly outperform their text counterparts, (ii) MMDocIR training set\neffectively enhances the performance of multimodal document retrieval and (iii)\ntext retrievers leveraging VLM-text significantly outperforms retrievers\nrelying on OCR-text. Our dataset is available at\nhttps://mmdocrag.github.io/MMDocIR/.\n","authors":["Kuicai Dong","Yujing Chang","Xin Deik Goh","Dexun Li","Ruiming Tang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08828v2.pdf","comment":"https://huggingface.co/MMDocIR"},{"id":"http://arxiv.org/abs/2505.11983v2","updated":"2025-05-20T14:49:41Z","published":"2025-05-17T12:31:12Z","title":"Online Iterative Self-Alignment for Radiology Report Generation","summary":"  Radiology Report Generation (RRG) is an important research topic for\nrelieving radiologist' heavy workload. Existing RRG models mainly rely on\nsupervised fine-tuning (SFT) based on different model architectures using data\npairs of radiological images and corresponding radiologist-annotated reports.\nRecent research has shifted focus to post-training improvements, aligning RRG\nmodel outputs with human preferences using reinforcement learning (RL).\nHowever, the limited data coverage of high-quality annotated data poses risks\nof overfitting and generalization. This paper proposes a novel Online Iterative\nSelf-Alignment (OISA) method for RRG that consists of four stages:\nself-generation of diverse data, self-evaluation for multi-objective preference\ndata,self-alignment for multi-objective optimization and self-iteration for\nfurther improvement. Our approach allows for generating varied reports tailored\nto specific clinical objectives, enhancing the overall performance of the RRG\nmodel iteratively. Unlike existing methods, our frame-work significantly\nincreases data quality and optimizes performance through iterative\nmulti-objective optimization. Experimental results demonstrate that our method\nsurpasses previous approaches, achieving state-of-the-art performance across\nmultiple evaluation metrics.\n","authors":["Ting Xiao","Lei Shi","Yang Zhang","HaoFeng Yang","Zhe Wang","Chenjia Bai"],"pdf_url":"https://arxiv.org/pdf/2505.11983v2.pdf","comment":"Accepted by ACL 2025 Main"},{"id":"http://arxiv.org/abs/2503.16282v2","updated":"2025-05-20T14:45:03Z","published":"2025-03-20T16:10:33Z","title":"Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language\n  Model","summary":"  Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to\nnew classes with few support samples while retaining base class segmentation.\nExisting GFS-PCS methods enhance prototypes via interacting with support or\nquery features but remain limited by sparse knowledge from few-shot samples.\nMeanwhile, 3D vision-language models (3D VLMs), generalizing across open-world\nnovel classes, contain rich but noisy novel class knowledge. In this work, we\nintroduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels\nfrom 3D VLMs with precise yet sparse few-shot samples to maximize the strengths\nof both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label\nselection to filter low-quality regions, followed by an adaptive infilling\nstrategy that combines knowledge from pseudo-label contexts and few-shot\nsamples to adaptively label the filtered, unlabeled areas. Additionally, we\ndesign a novel-base mix strategy to embed few-shot samples into training\nscenes, preserving essential context for improved novel class learning.\nMoreover, recognizing the limited diversity in current GFS-PCS benchmarks, we\nintroduce two challenging benchmarks with diverse novel classes for\ncomprehensive generalization evaluation. Experiments validate the effectiveness\nof our framework across models and datasets. Our approach and benchmarks\nprovide a solid foundation for advancing GFS-PCS in the real world. The code is\nat https://github.com/ZhaochongAn/GFS-VL\n","authors":["Zhaochong An","Guolei Sun","Yun Liu","Runjia Li","Junlin Han","Ender Konukoglu","Serge Belongie"],"pdf_url":"https://arxiv.org/pdf/2503.16282v2.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2501.15641v2","updated":"2025-05-20T14:39:58Z","published":"2025-01-26T19:01:19Z","title":"IP-Prompter: Training-Free Theme-Specific Image Generation via Dynamic\n  Visual Prompting","summary":"  The stories and characters that captivate us as we grow up shape unique\nfantasy worlds, with images serving as the primary medium for visually\nexperiencing these realms. Personalizing generative models through fine-tuning\nwith theme-specific data has become a prevalent approach in text-to-image\ngeneration. However, unlike object customization, which focuses on learning\nspecific objects, theme-specific generation encompasses diverse elements such\nas characters, scenes, and objects. Such diversity also introduces a key\nchallenge: how to adaptively generate multi-character, multi-concept, and\ncontinuous theme-specific images (TSI). Moreover, fine-tuning approaches often\ncome with significant computational overhead, time costs, and risks of\noverfitting. This paper explores a fundamental question: Can image generation\nmodels directly leverage images as contextual input, similarly to how large\nlanguage models use text as context? To address this, we present IP-Prompter, a\nnovel training-free TSI generation method. IP-Prompter introduces visual\nprompting, a mechanism that integrates reference images into generative models,\nallowing users to seamlessly specify the target theme without requiring\nadditional training. To further enhance this process, we propose a Dynamic\nVisual Prompting (DVP) mechanism, which iteratively optimizes visual prompts to\nimprove the accuracy and quality of generated images. Our approach enables\ndiverse applications, including consistent story generation, character design,\nrealistic character generation, and style-guided image generation. Comparative\nevaluations against state-of-the-art personalization methods demonstrate that\nIP-Prompter achieves significantly better results and excels in maintaining\ncharacter identity preserving, style consistency and text alignment, offering a\nrobust and flexible solution for theme-specific image generation.\n","authors":["Yuxin Zhang","Minyan Luo","Weiming Dong","Xiao Yang","Haibin Huang","Chongyang Ma","Oliver Deussen","Tong-Yee Lee","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2501.15641v2.pdf","comment":"Accepted by ACM SIGGRAPH 2025. Project page:\n  https://ip-prompter.github.io/"},{"id":"http://arxiv.org/abs/2503.06794v3","updated":"2025-05-20T14:31:45Z","published":"2025-03-09T22:16:48Z","title":"Does Acceleration Cause Hidden Instability in Vision Language Models?\n  Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study","summary":"  Vision-Language Models (VLMs) are powerful yet computationally intensive for\nwidespread practical deployments. To address such challenge without costly\nre-training, post-training acceleration techniques like quantization and token\nreduction are extensively explored. However, current acceleration evaluations\nprimarily target minimal overall performance degradation, overlooking a crucial\nquestion: does the accelerated model still give the same answers to the same\nquestions as it did before acceleration? This is vital for stability-centered\nindustrial applications where consistently correct answers for specific, known\nsituations are paramount, such as in AI-based disease diagnosis. We\nsystematically investigate this for accelerated VLMs, testing four leading\nmodels (LLaVA-1.5, LLaVA-Next, Qwen2-VL, Qwen2.5-VL) with eight acceleration\nmethods on ten multi-modal benchmarks. Our findings are stark: despite minimal\naggregate performance drops, accelerated models changed original answers up to\n20% of the time. Critically, up to 6.5% of these changes converted correct\nanswers to incorrect. Input perturbations magnified these inconsistencies, and\nthe trend is confirmed by case studies with the medical VLM LLaVA-Med. This\nresearch reveals a significant oversight in VLM acceleration, stressing an\nurgent need for instance-level stability checks to ensure trustworthy\nreal-world deployment.\n","authors":["Yizheng Sun","Hao Li","Chang Xu","Hongpeng Zhou","Chenghua Lin","Riza Batista-Navarro","Jingyuan Sun"],"pdf_url":"https://arxiv.org/pdf/2503.06794v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14414v1","updated":"2025-05-20T14:27:45Z","published":"2025-05-20T14:27:45Z","title":"Diving into the Fusion of Monocular Priors for Generalized Stereo\n  Matching","summary":"  The matching formulation makes it naturally hard for the stereo matching to\nhandle ill-posed regions like occlusions and non-Lambertian surfaces. Fusing\nmonocular priors has been proven helpful for ill-posed matching, but the biased\nmonocular prior learned from small stereo datasets constrains the\ngeneralization. Recently, stereo matching has progressed by leveraging the\nunbiased monocular prior from the vision foundation model (VFM) to improve the\ngeneralization in ill-posed regions. We dive into the fusion process and\nobserve three main problems limiting the fusion of the VFM monocular prior. The\nfirst problem is the misalignment between affine-invariant relative monocular\ndepth and absolute depth of disparity. Besides, when we use the monocular\nfeature in an iterative update structure, the over-confidence in the disparity\nupdate leads to local optima results. A direct fusion of a monocular depth map\ncould alleviate the local optima problem, but noisy disparity results computed\nat the first several iterations will misguide the fusion. In this paper, we\npropose a binary local ordering map to guide the fusion, which converts the\ndepth map into a binary relative format, unifying the relative and absolute\ndepth representation. The computed local ordering map is also used to re-weight\nthe initial disparity update, resolving the local optima and noisy problem. In\naddition, we formulate the final direct fusion of monocular depth to the\ndisparity as a registration problem, where a pixel-wise linear regression\nmodule can globally and adaptively align them. Our method fully exploits the\nmonocular prior to support stereo matching results effectively and efficiently.\nWe significantly improve the performance from the experiments when generalizing\nfrom SceneFlow to Middlebury and Booster datasets while barely reducing the\nefficiency.\n","authors":["Chengtang Yao","Lidong Yu","Zhidan Liu","Jiaxi Zeng","Yuwei Wu","Yunde Jia"],"pdf_url":"https://arxiv.org/pdf/2505.14414v1.pdf","comment":"Code:\n  https://github.com/YaoChengTang/Diving-into-the-Fusion-of-Monocular-Priors-for-Generalized-Stereo-Matching"},{"id":"http://arxiv.org/abs/2505.02043v2","updated":"2025-05-20T14:23:48Z","published":"2025-05-04T09:42:03Z","title":"Point2Primitive: CAD Reconstruction from Point Cloud by Direct Primitive\n  Prediction","summary":"  Recovering CAD models from point clouds, especially the sketch-extrusion\nprocess, can be seen as the process of rebuilding the topology and extrusion\nprimitives. Previous methods utilize implicit fields for sketch representation,\nleading to shape reconstruction of curved edges. In this paper, we proposed a\nCAD reconstruction network that produces editable CAD models from input point\nclouds (Point2Primitive) by directly predicting every element of the extrusion\nprimitives. Point2Primitive can directly detect and predict sketch curves (type\nand parameter) from point clouds based on an improved transformer. The sketch\ncurve parameters are formulated as position queries and optimized in an\nautoregressive way, leading to high parameter accuracy. The topology is rebuilt\nby extrusion segmentation, and each extrusion parameter (sketch and extrusion\noperation) is recovered by combining the predicted curves and the computed\nextrusion operation. Extensive experiments demonstrate that our method is\nsuperior in primitive prediction accuracy and CAD reconstruction. The\nreconstructed shapes are of high geometrical fidelity.\n","authors":["Cheng Wang","Xinzhu Ma","Bin Wang","Shixiang Tang","Yuan Meng","Ping Jiang"],"pdf_url":"https://arxiv.org/pdf/2505.02043v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13794v2","updated":"2025-05-20T14:22:45Z","published":"2025-03-18T00:50:40Z","title":"LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation","summary":"  Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design.\n","authors":["Yang Zhou","Shiyu Zhao","Yuxiao Chen","Zhenting Wang","Can Jin","Dimitris N. Metaxas"],"pdf_url":"https://arxiv.org/pdf/2503.13794v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14405v1","updated":"2025-05-20T14:18:56Z","published":"2025-05-20T14:18:56Z","title":"Investigating and Enhancing the Robustness of Large Multimodal Models\n  Against Temporal Inconsistency","summary":"  Large Multimodal Models (LMMs) have recently demonstrated impressive\nperformance on general video comprehension benchmarks. Nevertheless, for\nbroader applications, the robustness of their temporal analysis capability\nneeds to be thoroughly investigated yet predominantly ignored. Motivated by\nthis, we propose a novel temporal robustness benchmark (TemRobBench), which\nintroduces temporal inconsistency perturbations separately at the visual and\ntextual modalities to assess the robustness of models. We evaluate 16\nmainstream LMMs and find that they exhibit over-reliance on prior knowledge and\ntextual context in adversarial environments, while ignoring the actual temporal\ndynamics in the video. To mitigate this issue, we design panoramic direct\npreference optimization (PanoDPO), which encourages LMMs to incorporate both\nvisual and linguistic feature preferences simultaneously. Experimental results\nshow that PanoDPO can effectively enhance the model's robustness and\nreliability in temporal analysis.\n","authors":["Jiafeng Liang","Shixin Jiang","Xuan Dong","Ning Wang","Zheng Chu","Hui Su","Jinlan Fu","Ming Liu","See-Kiong Ng","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2505.14405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14404v1","updated":"2025-05-20T14:18:54Z","published":"2025-05-20T14:18:54Z","title":"ViC-Bench: Benchmarking Visual-Interleaved Chain-of-Thought Capability\n  in MLLMs with Free-Style Intermediate State Representations","summary":"  Visual-Interleaved Chain-of-Thought (VI-CoT) enables MLLMs to continually\nupdate their understanding and decisions based on step-wise intermediate visual\nstates (IVS), much like a human would, which demonstrates impressive success in\nvarious tasks, thereby leading to emerged advancements in related benchmarks.\nDespite promising progress, current benchmarks provide models with relatively\nfixed IVS, rather than free-style IVS, whch might forcibly distort the original\nthinking trajectories, failing to evaluate their intrinsic reasoning\ncapabilities. More importantly, existing benchmarks neglect to systematically\nexplore the impact factors that IVS would impart to untamed reasoning\nperformance. To tackle above gaps, we introduce a specialized benchmark termed\nViC-Bench, consisting of four representive tasks: maze navigation, jigsaw\npuzzle, embodied long-horizon planning, and complex counting, where each task\nhas dedicated free-style IVS generation pipeline supporting function calls. To\nsystematically examine VI-CoT capability, we propose a thorough evaluation\nsuite incorporating a progressive three-stage strategy with targeted new\nmetrics. Besides, we establish Incremental Prompting Information Injection\n(IPII) strategy to ablatively explore the prompting factors for VI-CoT. We\nextensively conduct evaluations for 18 advanced MLLMs, revealing key insights\ninto their VI-CoT capability. Our proposed benchmark is publicly open at\nHuggingface.\n","authors":["Xuecheng Wu","Jiaxing Liu","Danlei Huang","Xiaoyu Li","Yifan Wang","Chen Chen","Liya Ma","Xuezhi Cao","Junxiao Xue"],"pdf_url":"https://arxiv.org/pdf/2505.14404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05191v3","updated":"2025-05-20T14:04:25Z","published":"2023-01-12T18:19:00Z","title":"A Unified Framework for Event-based Frame Interpolation with Ad-hoc\n  Deblurring in the Wild","summary":"  Effective video frame interpolation hinges on the adept handling of motion in\nthe input scene. Prior work acknowledges asynchronous event information for\nthis, but often overlooks whether motion induces blur in the video, limiting\nits scope to sharp frame interpolation. We instead propose a unified framework\nfor event-based frame interpolation that performs deblurring ad-hoc and thus\nworks both on sharp and blurry input videos. Our model consists in a\nbidirectional recurrent network that incorporates the temporal dimension of\ninterpolation and fuses information from the input frames and the events\nadaptively based on their temporal proximity. To enhance the generalization\nfrom synthetic data to real event cameras, we integrate self-supervised\nframework with the proposed model to enhance the generalization on real-world\ndatasets in the wild. At the dataset level, we introduce a novel real-world\nhigh-resolution dataset with events and color videos named HighREV, which\nprovides a challenging evaluation setting for the examined task. Extensive\nexperiments show that our network consistently outperforms previous\nstate-of-the-art methods on frame interpolation, single image deblurring, and\nthe joint task of both. Experiments on domain transfer reveal that\nself-supervised training effectively mitigates the performance degradation\nobserved when transitioning from synthetic data to real-world data. Code and\ndatasets are available at https://github.com/AHupuJR/REFID.\n","authors":["Lei Sun","Daniel Gehrig","Christos Sakaridis","Mathias Gehrig","Jingyun Liang","Peng Sun","Zhijie Xu","Kaiwei Wang","Luc Van Gool","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2301.05191v3.pdf","comment":"Accepted to T-PAMI"},{"id":"http://arxiv.org/abs/2502.01051v3","updated":"2025-05-20T13:59:40Z","published":"2025-02-03T04:51:28Z","title":"Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level\n  Preference Optimization","summary":"  Preference optimization for diffusion models aims to align them with human\npreferences for images. Previous methods typically use Vision-Language Models\n(VLMs) as pixel-level reward models to approximate human preferences. However,\nwhen used for step-level preference optimization, these models face challenges\nin handling noisy images of different timesteps and require complex\ntransformations into pixel space. In this work, we show that pre-trained\ndiffusion models are naturally suited for step-level reward modeling in the\nnoisy latent space, as they are explicitly designed to process latent images at\nvarious noise levels. Accordingly, we propose the Latent Reward Model (LRM),\nwhich repurposes components of the diffusion model to predict preferences of\nlatent images at arbitrary timesteps. Building on LRM, we introduce Latent\nPreference Optimization (LPO), a step-level preference optimization method\nconducted directly in the noisy latent space. Experimental results indicate\nthat LPO significantly improves the model's alignment with general, aesthetic,\nand text-image alignment preferences, while achieving a 2.5-28x training\nspeedup over existing preference optimization methods. Our code and models are\navailable at https://github.com/Kwai-Kolors/LPO.\n","authors":["Tao Zhang","Cheng Da","Kun Ding","Huan Yang","Kun Jin","Yan Li","Tingting Gao","Di Zhang","Shiming Xiang","Chunhong Pan"],"pdf_url":"https://arxiv.org/pdf/2502.01051v3.pdf","comment":"25 pages, 26 tables, 15 figures"},{"id":"http://arxiv.org/abs/2505.14362v1","updated":"2025-05-20T13:48:11Z","published":"2025-05-20T13:48:11Z","title":"DeepEyes: Incentivizing \"Thinking with Images\" via Reinforcement\n  Learning","summary":"  Large Vision-Language Models (VLMs) have shown strong capabilities in\nmultimodal understanding and reasoning, yet they are primarily constrained by\ntext-based reasoning processes. However, achieving seamless integration of\nvisual and textual reasoning which mirrors human cognitive processes remains a\nsignificant challenge. In particular, effectively incorporating advanced visual\ninput processing into reasoning mechanisms is still an open question. Thus, in\nthis paper, we explore the interleaved multimodal reasoning paradigm and\nintroduce DeepEyes, a model with \"thinking with images\" capabilities\nincentivized through end-to-end reinforcement learning without the need for\ncold-start SFT. Notably, this ability emerges natively within the model itself,\nleveraging its inherent grounding ability as a tool instead of depending on\nseparate specialized models. Specifically, we propose a tool-use-oriented data\nselection mechanism and a reward strategy to encourage successful tool-assisted\nreasoning trajectories. DeepEyes achieves significant performance gains on\nfine-grained perception and reasoning benchmarks and also demonstrates\nimprovement in grounding, hallucination, and mathematical reasoning tasks.\nInterestingly, we observe the distinct evolution of tool-calling behavior from\ninitial exploration to efficient and accurate exploitation, and diverse\nthinking patterns that closely mirror human visual reasoning processes. Code is\navailable at https://github.com/Visual-Agent/DeepEyes.\n","authors":["Ziwei Zheng","Michael Yang","Jack Hong","Chenxiao Zhao","Guohai Xu","Le Yang","Chao Shen","Xing Yu"],"pdf_url":"https://arxiv.org/pdf/2505.14362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14361v1","updated":"2025-05-20T13:47:40Z","published":"2025-05-20T13:47:40Z","title":"Vision-Language Modeling Meets Remote Sensing: Models, Datasets and\n  Perspectives","summary":"  Vision-language modeling (VLM) aims to bridge the information gap between\nimages and natural language. Under the new paradigm of first pre-training on\nmassive image-text pairs and then fine-tuning on task-specific data, VLM in the\nremote sensing domain has made significant progress. The resulting models\nbenefit from the absorption of extensive general knowledge and demonstrate\nstrong performance across a variety of remote sensing data analysis tasks.\nMoreover, they are capable of interacting with users in a conversational\nmanner. In this paper, we aim to provide the remote sensing community with a\ntimely and comprehensive review of the developments in VLM using the two-stage\nparadigm. Specifically, we first cover a taxonomy of VLM in remote sensing:\ncontrastive learning, visual instruction tuning, and text-conditioned image\ngeneration. For each category, we detail the commonly used network architecture\nand pre-training objectives. Second, we conduct a thorough review of existing\nworks, examining foundation models and task-specific adaptation methods in\ncontrastive-based VLM, architectural upgrades, training strategies and model\ncapabilities in instruction-based VLM, as well as generative foundation models\nwith their representative downstream applications. Third, we summarize datasets\nused for VLM pre-training, fine-tuning, and evaluation, with an analysis of\ntheir construction methodologies (including image sources and caption\ngeneration) and key properties, such as scale and task adaptability. Finally,\nwe conclude this survey with insights and discussions on future research\ndirections: cross-modal representation alignment, vague requirement\ncomprehension, explanation-driven model reliability, continually scalable model\ncapabilities, and large-scale datasets featuring richer modalities and greater\nchallenges.\n","authors":["Xingxing Weng","Chao Pang","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2505.14361v1.pdf","comment":"Accepted by IEEE Geoscience and Remote Sensing Magazine"},{"id":"http://arxiv.org/abs/2505.14359v1","updated":"2025-05-20T13:42:38Z","published":"2025-05-20T13:42:38Z","title":"Dual Data Alignment Makes AI-Generated Image Detector Easier\n  Generalizable","summary":"  Existing detectors are often trained on biased datasets, leading to the\npossibility of overfitting on non-causal image attributes that are spuriously\ncorrelated with real/synthetic labels. While these biased features enhance\nperformance on the training data, they result in substantial performance\ndegradation when applied to unbiased datasets. One common solution is to\nperform dataset alignment through generative reconstruction, matching the\nsemantic content between real and synthetic images. However, we revisit this\napproach and show that pixel-level alignment alone is insufficient. The\nreconstructed images still suffer from frequency-level misalignment, which can\nperpetuate spurious correlations. To illustrate, we observe that reconstruction\nmodels tend to restore the high-frequency details lost in real images (possibly\ndue to JPEG compression), inadvertently creating a frequency-level\nmisalignment, where synthetic images appear to have richer high-frequency\ncontent than real ones. This misalignment leads to models associating\nhigh-frequency features with synthetic labels, further reinforcing biased cues.\nTo resolve this, we propose Dual Data Alignment (DDA), which aligns both the\npixel and frequency domains. Moreover, we introduce two new test sets:\nDDA-COCO, containing DDA-aligned synthetic images for testing detector\nperformance on the most aligned dataset, and EvalGEN, featuring the latest\ngenerative models for assessing detectors under new generative architectures\nsuch as visual auto-regressive generators. Finally, our extensive evaluations\ndemonstrate that a detector trained exclusively on DDA-aligned MSCOCO could\nimprove across 8 diverse benchmarks by a non-trivial margin, showing a +7.2% on\nin-the-wild benchmarks, highlighting the improved generalizability of unbiased\ndetectors.\n","authors":["Ruoxin Chen","Junwei Xi","Zhiyuan Yan","Ke-Yue Zhang","Shuang Wu","Jingyi Xie","Xu Chen","Lei Xu","Isabel Guan","Taiping Yao","Shouhong Ding"],"pdf_url":"https://arxiv.org/pdf/2505.14359v1.pdf","comment":"12 Pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.14357v1","updated":"2025-05-20T13:41:45Z","published":"2025-05-20T13:41:45Z","title":"Vid2World: Crafting Video Diffusion Models to Interactive World Models","summary":"  World models, which predict transitions based on history observation and\naction sequences, have shown great promise in improving data efficiency for\nsequential decision making. However, existing world models often require\nextensive domain-specific training and still produce low-fidelity, coarse\npredictions, limiting their applicability in complex environments. In contrast,\nvideo diffusion models trained on large, internet-scale datasets have\ndemonstrated impressive capabilities in generating high-quality videos that\ncapture diverse real-world dynamics. In this work, we present Vid2World, a\ngeneral approach for leveraging and transferring pre-trained video diffusion\nmodels into interactive world models. To bridge the gap, Vid2World performs\ncasualization of a pre-trained video diffusion model by crafting its\narchitecture and training objective to enable autoregressive generation.\nFurthermore, it introduces a causal action guidance mechanism to enhance action\ncontrollability in the resulting interactive world model. Extensive experiments\nin robot manipulation and game simulation domains show that our method offers a\nscalable and effective approach for repurposing highly capable video diffusion\nmodels to interactive world models.\n","authors":["Siqiao Huang","Jialong Wu","Qixing Zhou","Shangchen Miao","Mingsheng Long"],"pdf_url":"https://arxiv.org/pdf/2505.14357v1.pdf","comment":"Project page: http://knightnemo.github.io/vid2world/"},{"id":"http://arxiv.org/abs/2410.00580v2","updated":"2025-05-20T13:38:03Z","published":"2024-10-01T11:02:34Z","title":"Deep activity propagation via weight initialization in spiking neural\n  networks","summary":"  Spiking Neural Networks (SNNs) and neuromorphic computing offer bio-inspired\nadvantages such as sparsity and ultra-low power consumption, providing a\npromising alternative to conventional networks. However, training deep SNNs\nfrom scratch remains a challenge, as SNNs process and transmit information by\nquantizing the real-valued membrane potentials into binary spikes. This can\nlead to information loss and vanishing spikes in deeper layers, impeding\neffective training. While weight initialization is known to be critical for\ntraining deep neural networks, what constitutes an effective initial state for\na deep SNN is not well-understood. Existing weight initialization methods\ndesigned for conventional networks (ANNs) are often applied to SNNs without\naccounting for their distinct computational properties. In this work we derive\nan optimal weight initialization method specifically tailored for SNNs, taking\ninto account the quantization operation. We show theoretically that, unlike\nstandard approaches, this method enables the propagation of activity in deep\nSNNs without loss of spikes. We demonstrate this behavior in numerical\nsimulations of SNNs with up to 100 layers across multiple time steps. We\npresent an in-depth analysis of the numerical conditions, regarding layer width\nand neuron hyperparameters, which are necessary to accurately apply our\ntheoretical findings. Furthermore, our experiments on MNIST demonstrate higher\naccuracy and faster convergence when using the proposed weight initialization\nscheme. Finally, we show that the newly introduced weight initialization is\nrobust against variations in several network and neuron hyperparameters.\n","authors":["Aurora Micheli","Olaf Booij","Jan van Gemert","Nergis Tömen"],"pdf_url":"https://arxiv.org/pdf/2410.00580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14346v1","updated":"2025-05-20T13:29:33Z","published":"2025-05-20T13:29:33Z","title":"Egocentric Action-aware Inertial Localization in Point Clouds","summary":"  This paper presents a novel inertial localization framework named Egocentric\nAction-aware Inertial Localization (EAIL), which leverages egocentric action\ncues from head-mounted IMU signals to localize the target individual within a\n3D point cloud. Human inertial localization is challenging due to IMU sensor\nnoise that causes trajectory drift over time. The diversity of human actions\nfurther complicates IMU signal processing by introducing various motion\npatterns. Nevertheless, we observe that some actions observed through the\nhead-mounted IMU correlate with spatial environmental structures (e.g., bending\ndown to look inside an oven, washing dishes next to a sink), thereby serving as\nspatial anchors to compensate for the localization drift. The proposed EAIL\nframework learns such correlations via hierarchical multi-modal alignment. By\nassuming that the 3D point cloud of the environment is available, it\ncontrastively learns modality encoders that align short-term egocentric action\ncues in IMU signals with local environmental features in the point cloud. These\nencoders are then used in reasoning the IMU data and the point cloud over time\nand space to perform inertial localization. Interestingly, these encoders can\nfurther be utilized to recognize the corresponding sequence of actions as a\nby-product. Extensive experiments demonstrate the effectiveness of the proposed\nframework over state-of-the-art inertial localization and inertial action\nrecognition baselines.\n","authors":["Mingfang Zhang","Ryo Yonetani","Yifei Huang","Liangyang Ouyang","Ruicong Liu","Yoichi Sato"],"pdf_url":"https://arxiv.org/pdf/2505.14346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14341v1","updated":"2025-05-20T13:27:52Z","published":"2025-05-20T13:27:52Z","title":"Replace in Translation: Boost Concept Alignment in Counterfactual\n  Text-to-Image","summary":"  Text-to-Image (T2I) has been prevalent in recent years, with most common\ncondition tasks having been optimized nicely. Besides, counterfactual\nText-to-Image is obstructing us from a more versatile AIGC experience. For\nthose scenes that are impossible to happen in real world and anti-physics, we\nshould spare no efforts in increasing the factual feel, which means\nsynthesizing images that people think very likely to be happening, and concept\nalignment, which means all the required objects should be in the same frame. In\nthis paper, we focus on concept alignment. As controllable T2I models have\nachieved satisfactory performance for real applications, we utilize this\ntechnology to replace the objects in a synthesized image in latent space\nstep-by-step to change the image from a common scene to a counterfactual scene\nto meet the prompt. We propose a strategy to instruct this replacing process,\nwhich is called as Explicit Logical Narrative Prompt (ELNP), by using the newly\nSoTA language model DeepSeek to generate the instructions. Furthermore, to\nevaluate models' performance in counterfactual T2I, we design a metric to\ncalculate how many required concepts in the prompt can be covered averagely in\nthe synthesized images. The extensive experiments and qualitative comparisons\ndemonstrate that our strategy can boost the concept alignment in counterfactual\nT2I.\n","authors":["Sifan Li","Ming Tao","Hao Zhao","Ling Shao","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2505.14341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14340v1","updated":"2025-05-20T13:27:17Z","published":"2025-05-20T13:27:17Z","title":"Plane Geometry Problem Solving with Multi-modal Reasoning: A Survey","summary":"  Plane geometry problem solving (PGPS) has recently gained significant\nattention as a benchmark to assess the multi-modal reasoning capabilities of\nlarge vision-language models. Despite the growing interest in PGPS, the\nresearch community still lacks a comprehensive overview that systematically\nsynthesizes recent work in PGPS. To fill this gap, we present a survey of\nexisting PGPS studies. We first categorize PGPS methods into an encoder-decoder\nframework and summarize the corresponding output formats used by their encoders\nand decoders. Subsequently, we classify and analyze these encoders and decoders\naccording to their architectural designs. Finally, we outline major challenges\nand promising directions for future research. In particular, we discuss the\nhallucination issues arising during the encoding phase within encoder-decoder\narchitectures, as well as the problem of data leakage in current PGPS\nbenchmarks.\n","authors":["Seunghyuk Cho","Zhenyue Qin","Yang Liu","Youngbin Choi","Seungbeom Lee","Dongwoo Kim"],"pdf_url":"https://arxiv.org/pdf/2505.14340v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2505.14336v1","updated":"2025-05-20T13:20:55Z","published":"2025-05-20T13:20:55Z","title":"Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors\n  Approach","summary":"  Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy\nenvironments by integrating visual cues. While recent advances integrate Large\nLanguage Models (LLMs) into AVSR, their high computational cost hinders\ndeployment in resource-constrained settings. To address this, we propose\nLlama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of\nProjectors (SMoP) module to scale model capacity without increasing inference\ncosts. By incorporating sparsely-gated mixture-of-experts (MoE) projectors,\nLlama-SMoP enables the use of smaller LLMs while maintaining strong\nperformance. We explore three SMoP configurations and show that Llama-SMoP DEDR\n(Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and\nexperts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation\nstudies confirm its effectiveness in expert activation, scalability, and noise\nrobustness.\n","authors":["Umberto Cappellazzo","Minsu Kim","Stavros Petridis","Daniele Falavigna","Alessio Brutti"],"pdf_url":"https://arxiv.org/pdf/2505.14336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14333v1","updated":"2025-05-20T13:18:20Z","published":"2025-05-20T13:18:20Z","title":"Domain Adaptation for Multi-label Image Classification: a\n  Discriminator-free Approach","summary":"  This paper introduces a discriminator-free adversarial-based approach termed\nDDA-MLIC for Unsupervised Domain Adaptation (UDA) in the context of Multi-Label\nImage Classification (MLIC). While recent efforts have explored\nadversarial-based UDA methods for MLIC, they typically include an additional\ndiscriminator subnet. Nevertheless, decoupling the classification and the\ndiscrimination tasks may harm their task-specific discriminative power. Herein,\nwe address this challenge by presenting a novel adversarial critic directly\nderived from the task-specific classifier. Specifically, we employ a\ntwo-component Gaussian Mixture Model (GMM) to model both source and target\npredictions, distinguishing between two distinct clusters. Instead of using the\ntraditional Expectation Maximization (EM) algorithm, our approach utilizes a\nDeep Neural Network (DNN) to estimate the parameters of each GMM component.\nSubsequently, the source and target GMM parameters are leveraged to formulate\nan adversarial loss using the Fr\\'echet distance. The proposed framework is\ntherefore not only fully differentiable but is also cost-effective as it avoids\nthe expensive iterative process usually induced by the standard EM method. The\nproposed method is evaluated on several multi-label image datasets covering\nthree different types of domain shift. The obtained results demonstrate that\nDDA-MLIC outperforms existing state-of-the-art methods in terms of precision\nwhile requiring a lower number of parameters. The code is made publicly\navailable at github.com/cvi2snt/DDA-MLIC.\n","authors":["Inder Pal Singh","Enjie Ghorbel","Anis Kacem","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2505.14333v1.pdf","comment":"The paper is under consideration at Computer Vision and Image\n  Understanding. arXiv admin note: text overlap with arXiv:2301.10611"},{"id":"http://arxiv.org/abs/2505.14330v1","updated":"2025-05-20T13:16:55Z","published":"2025-05-20T13:16:55Z","title":"Handloom Design Generation Using Generative Networks","summary":"  This paper proposes deep learning techniques of generating designs for\nclothing, focused on handloom fabric and discusses the associated challenges\nalong with its application. The capability of generative neural network models\nin understanding artistic designs and synthesizing those is not yet explored\nwell. In this work, multiple methods are employed incorporating the current\nstate of the art generative models and style transfer algorithms to study and\nobserve their performance for the task. The results are then evaluated through\nuser score. This work also provides a new dataset NeuralLoom for the task of\nthe design generation.\n","authors":["Rajat Kanti Bhattacharjee","Meghali Nandi","Amrit Jha","Gunajit Kalita","Ferdous Ahmed Barbhuiya"],"pdf_url":"https://arxiv.org/pdf/2505.14330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00379v4","updated":"2025-05-20T13:10:54Z","published":"2025-02-01T09:35:51Z","title":"Latent Action Learning Requires Supervision in the Presence of\n  Distractors","summary":"  Recently, latent action learning, pioneered by Latent Action Policies (LAPO),\nhave shown remarkable pre-training efficiency on observation-only data,\noffering potential for leveraging vast amounts of video available on the web\nfor embodied AI. However, prior work has focused on distractor-free data, where\nchanges between observations are primarily explained by ground-truth actions.\nUnfortunately, real-world videos contain action-correlated distractors that may\nhinder latent action learning. Using Distracting Control Suite (DCS) we\nempirically investigate the effect of distractors on latent action learning and\ndemonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO\nmodification that improves the quality of latent actions by 8x, as measured by\nlinear probing. Importantly, we show that providing supervision with\nground-truth actions, as few as 2.5% of the full dataset, during latent action\nlearning improves downstream performance by 4.2x on average. Our findings\nsuggest that integrating supervision during Latent Action Models (LAM) training\nis critical in the presence of distractors, challenging the conventional\npipeline of first learning LAM and only then decoding from latent to\nground-truth actions.\n","authors":["Alexander Nikulin","Ilya Zisman","Denis Tarasov","Nikita Lyubaykin","Andrei Polubarov","Igor Kiselev","Vladislav Kurenkov"],"pdf_url":"https://arxiv.org/pdf/2502.00379v4.pdf","comment":"ICML 2025, Poster, Source code: https://github.com/dunnolab/laom"},{"id":"http://arxiv.org/abs/2505.14321v1","updated":"2025-05-20T13:07:55Z","published":"2025-05-20T13:07:55Z","title":"Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or\n  True Temporal Understanding?","summary":"  Existing video understanding benchmarks often conflate knowledge-based and\npurely image-based questions, rather than clearly isolating a model's temporal\nreasoning ability, which is the key aspect that distinguishes video\nunderstanding from other modalities. We identify two major limitations that\nobscure whether higher scores truly indicate stronger understanding of the\ndynamic content in videos: (1) strong language priors, where models can answer\nquestions without watching the video; and (2) shuffling invariance, where\nmodels maintain similar performance on certain questions even when video frames\nare temporally shuffled. To alleviate these issues, we propose VBenchComp, an\nautomated pipeline that categorizes questions into different domains:\nLLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions\ncan be answered without viewing the video; Semantic questions remain answerable\neven when the video frames are shuffled; and Temporal questions require\nunderstanding the correct temporal order of frames. The rest of the questions\nare labeled as Others. This can enable fine-grained evaluation of different\ncapabilities of a video LLM. Our analysis reveals nuanced model weaknesses that\nare hidden by traditional overall scores, and we offer insights and\nrecommendations for designing future benchmarks that more accurately assess\nvideo LLMs.\n","authors":["Bo Feng","Zhengfeng Lai","Shiyu Li","Zizhen Wang","Simon Wang","Ping Huang","Meng Cao"],"pdf_url":"https://arxiv.org/pdf/2505.14321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14320v1","updated":"2025-05-20T13:07:16Z","published":"2025-05-20T13:07:16Z","title":"Accuracy and Fairness of Facial Recognition Technology in Low-Quality\n  Police Images: An Experiment With Synthetic Faces","summary":"  Facial recognition technology (FRT) is increasingly used in criminal\ninvestigations, yet most evaluations of its accuracy rely on high-quality\nimages, unlike those often encountered by law enforcement. This study examines\nhow five common forms of image degradation--contrast, brightness, motion blur,\npose shift, and resolution--affect FRT accuracy and fairness across demographic\ngroups. Using synthetic faces generated by StyleGAN3 and labeled with FairFace,\nwe simulate degraded images and evaluate performance using Deepface with\nArcFace loss in 1:n identification tasks. We perform an experiment and find\nthat false positive rates peak near baseline image quality, while false\nnegatives increase as degradation intensifies--especially with blur and low\nresolution. Error rates are consistently higher for women and Black\nindividuals, with Black females most affected. These disparities raise concerns\nabout fairness and reliability when FRT is used in real-world investigative\ncontexts. Nevertheless, even under the most challenging conditions and for the\nmost affected subgroups, FRT accuracy remains substantially higher than that of\nmany traditional forensic methods. This suggests that, if appropriately\nvalidated and regulated, FRT should be considered a valuable investigative\ntool. However, algorithmic accuracy alone is not sufficient: we must also\nevaluate how FRT is used in practice, including user-driven data manipulation.\nSuch cases underscore the need for transparency and oversight in FRT deployment\nto ensure both fairness and forensic validity.\n","authors":["Maria Cuellar","Hon Kiu"," To","Arush Mehrotra"],"pdf_url":"https://arxiv.org/pdf/2505.14320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14319v1","updated":"2025-05-20T13:06:19Z","published":"2025-05-20T13:06:19Z","title":"RETRO: REthinking Tactile Representation Learning with Material PriOrs","summary":"  Tactile perception is profoundly influenced by the surface properties of\nobjects in contact. However, despite their crucial role in shaping tactile\nexperiences, these material characteristics have been largely neglected in\nexisting tactile representation learning methods. Most approaches primarily\nfocus on aligning tactile data with visual or textual information, overlooking\nthe richness of tactile feedback that comes from understanding the materials'\ninherent properties. In this work, we address this gap by revisiting the\ntactile representation learning framework and incorporating material-aware\npriors into the learning process. These priors, which represent pre-learned\ncharacteristics specific to different materials, allow tactile models to better\ncapture and generalize the nuances of surface texture. Our method enables more\naccurate, contextually rich tactile feedback across diverse materials and\ntextures, improving performance in real-world applications such as robotics,\nhaptic feedback systems, and material editing.\n","authors":["Weihao Xia","Chenliang Zhou","Cengiz Oztireli"],"pdf_url":"https://arxiv.org/pdf/2505.14319v1.pdf","comment":"Code: https://github.com/weihaox/RETRO"},{"id":"http://arxiv.org/abs/2505.14318v1","updated":"2025-05-20T13:05:41Z","published":"2025-05-20T13:05:41Z","title":"RADAR: Enhancing Radiology Report Generation with Supplementary\n  Knowledge Injection","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, including radiology report generation. Previous approaches\nhave attempted to utilize multimodal LLMs for this task, enhancing their\nperformance through the integration of domain-specific knowledge retrieval.\nHowever, these approaches often overlook the knowledge already embedded within\nthe LLMs, leading to redundant information integration and inefficient\nutilization of learned representations. To address this limitation, we propose\nRADAR, a framework for enhancing radiology report generation with supplementary\nknowledge injection. RADAR improves report generation by systematically\nleveraging both the internal knowledge of an LLM and externally retrieved\ninformation. Specifically, it first extracts the model's acquired knowledge\nthat aligns with expert image-based classification outputs. It then retrieves\nrelevant supplementary knowledge to further enrich this information. Finally,\nby aggregating both sources, RADAR generates more accurate and informative\nradiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU\nX-ray demonstrate that our model outperforms state-of-the-art LLMs in both\nlanguage quality and clinical accuracy\n","authors":["Wenjun Hou","Yi Cheng","Kaishuai Xu","Heng Li","Yan Hu","Wenjie Li","Jiang Liu"],"pdf_url":"https://arxiv.org/pdf/2505.14318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12711v2","updated":"2025-05-20T12:57:58Z","published":"2025-05-19T05:07:34Z","title":"Any-to-Any Learning in Computational Pathology via Triplet Multimodal\n  Pretraining","summary":"  Recent advances in computational pathology and artificial intelligence have\nsignificantly enhanced the utilization of gigapixel whole-slide images and and\nadditional modalities (e.g., genomics) for pathological diagnosis. Although\ndeep learning has demonstrated strong potential in pathology, several key\nchallenges persist: (1) fusing heterogeneous data types requires sophisticated\nstrategies beyond simple concatenation due to high computational costs; (2)\ncommon scenarios of missing modalities necessitate flexible strategies that\nallow the model to learn robustly in the absence of certain modalities; (3) the\ndownstream tasks in CPath are diverse, ranging from unimodal to multimodal,\ncnecessitating a unified model capable of handling all modalities. To address\nthese challenges, we propose ALTER, an any-to-any tri-modal pretraining\nframework that integrates WSIs, genomics, and pathology reports. The term \"any\"\nemphasizes ALTER's modality-adaptive design, enabling flexible pretraining with\nany subset of modalities, and its capacity to learn robust, cross-modal\nrepresentations beyond WSI-centric approaches. We evaluate ALTER across\nextensive clinical tasks including survival prediction, cancer subtyping, gene\nmutation prediction, and report generation, achieving superior or comparable\nperformance to state-of-the-art baselines.\n","authors":["Qichen Sun","Zhengrui Guo","Rui Peng","Hao Chen","Jinzhuo Wang"],"pdf_url":"https://arxiv.org/pdf/2505.12711v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11850v2","updated":"2025-05-20T12:50:53Z","published":"2024-07-16T15:32:39Z","title":"SpaceJAM: a Lightweight and Regularization-free Method for Fast Joint\n  Alignment of Images","summary":"  The unsupervised task of Joint Alignment (JA) of images is beset by\nchallenges such as high complexity, geometric distortions, and convergence to\npoor local or even global optima. Although Vision Transformers (ViT) have\nrecently provided valuable features for JA, they fall short of fully addressing\nthese issues. Consequently, researchers frequently depend on expensive models\nand numerous regularization terms, resulting in long training times and\nchallenging hyperparameter tuning. We introduce the Spatial Joint Alignment\nModel (SpaceJAM), a novel approach that addresses the JA task with efficiency\nand simplicity. SpaceJAM leverages a compact architecture with only 16K\ntrainable parameters and uniquely operates without the need for regularization\nor atlas maintenance. Evaluations on SPair-71K and CUB datasets demonstrate\nthat SpaceJAM matches the alignment capabilities of existing methods while\nsignificantly reducing computational demands and achieving at least a 10x\nspeedup. SpaceJAM sets a new standard for rapid and effective image alignment,\nmaking the process more accessible and efficient. Our code is available at:\nhttps://bgu-cs-vil.github.io/SpaceJAM/.\n","authors":["Nir Barel","Ron Shapira Weber","Nir Mualem","Shahaf E. Finder","Oren Freifeld"],"pdf_url":"https://arxiv.org/pdf/2407.11850v2.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2505.14298v1","updated":"2025-05-20T12:47:07Z","published":"2025-05-20T12:47:07Z","title":"A Review of Vision-Based Assistive Systems for Visually Impaired People:\n  Technologies, Applications, and Future Directions","summary":"  Visually impaired individuals rely heavily on accurate and timely information\nabout obstacles and their surrounding environments to achieve independent\nliving. In recent years, significant progress has been made in the development\nof assistive technologies, particularly vision-based systems, that enhance\nmobility and facilitate interaction with the external world in both indoor and\noutdoor settings. This paper presents a comprehensive review of recent advances\nin assistive systems designed for the visually impaired, with a focus on\nstate-of-the-art technologies in obstacle detection, navigation, and user\ninteraction. In addition, emerging trends and future directions in visual\nguidance systems are discussed.\n","authors":["Fulong Yao","Wenju Zhou","Huosheng Hu"],"pdf_url":"https://arxiv.org/pdf/2505.14298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14232v2","updated":"2025-05-20T12:46:42Z","published":"2025-03-18T13:09:01Z","title":"CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion\n  Models","summary":"  Text-to-Image diffusion models can produce undesirable content that\nnecessitates concept erasure. However, existing methods struggle with\nunder-erasure, leaving residual traces of targeted concepts, or over-erasure,\nmistakenly eliminating unrelated but visually similar concepts. To address\nthese limitations, we introduce CRCE, a novel concept erasure framework that\nleverages Large Language Models to identify both semantically related concepts\nthat should be erased alongside the target and distinct concepts that should be\npreserved. By explicitly modelling coreferential and retained concepts\nsemantically, CRCE enables more precise concept removal, without unintended\nerasure. Experiments demonstrate that CRCE outperforms existing methods on\ndiverse erasure tasks, including real-world object, person identities, and\nabstract intellectual property characteristics. The constructed dataset\nCorefConcept and the source code will be release upon acceptance.\n","authors":["Yuyang Xue","Edward Moroshko","Feng Chen","Jingyu Sun","Steven McDonagh","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2503.14232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14296v1","updated":"2025-05-20T12:44:19Z","published":"2025-05-20T12:44:19Z","title":"Towards Generating Realistic Underwater Images","summary":"  This paper explores the use of contrastive learning and generative\nadversarial networks for generating realistic underwater images from synthetic\nimages with uniform lighting. We investigate the performance of image\ntranslation models for generating realistic underwater images using the VAROS\ndataset. Two key evaluation metrics, Fr\\'echet Inception Distance (FID) and\nStructural Similarity Index Measure (SSIM), provide insights into the\ntrade-offs between perceptual quality and structural preservation. For paired\nimage translation, pix2pix achieves the best FID scores due to its paired\nsupervision and PatchGAN discriminator, while the autoencoder model attains the\nhighest SSIM, suggesting better structural fidelity despite producing blurrier\noutputs. Among unpaired methods, CycleGAN achieves a competitive FID score by\nleveraging cycle-consistency loss, whereas CUT, which replaces\ncycle-consistency with contrastive learning, attains higher SSIM, indicating\nimproved spatial similarity retention. Notably, incorporating depth information\ninto CUT results in the lowest overall FID score, demonstrating that depth cues\nenhance realism. However, the slight decrease in SSIM suggests that depth-aware\nlearning may introduce structural variations.\n","authors":["Abdul-Kazeem Shamba"],"pdf_url":"https://arxiv.org/pdf/2505.14296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12061v2","updated":"2025-05-20T12:37:32Z","published":"2025-05-17T15:56:17Z","title":"Bayesian Deep Learning Approaches for Uncertainty-Aware Retinal OCT\n  Image Segmentation for Multiple Sclerosis","summary":"  Optical Coherence Tomography (OCT) provides valuable insights in\nophthalmology, cardiology, and neurology due to high-resolution,\ncross-sectional images of the retina. One critical task for ophthalmologists\nusing OCT is delineation of retinal layers within scans. This process is\ntime-consuming and prone to human bias, affecting the accuracy and reliability\nof diagnoses. Previous efforts to automate delineation using deep learning face\nchallenges in uptake from clinicians and statisticians due to the absence of\nuncertainty estimation, leading to \"confidently wrong\" models via\nhallucinations. In this study, we address these challenges by applying Bayesian\nconvolutional neural networks (BCNNs) to segment an openly available OCT\nimaging dataset containing 35 human retina OCTs split between healthy controls\nand patients with multiple sclerosis. Our findings demonstrate that Bayesian\nmodels can be used to provide uncertainty maps of the segmentation, which can\nfurther be used to identify highly uncertain samples that exhibit recording\nartefacts such as noise or miscalibration at inference time. Our method also\nallows for uncertainty-estimation for important secondary measurements such as\nlayer thicknesses, that are medically relevant for patients. We show that these\nfeatures come in addition to greater performance compared to similar work over\nall delineations; with an overall Dice score of 95.65%. Our work brings greater\nclinical applicability, statistical robustness, and performance to retinal OCT\nsegmentation.\n","authors":["Samuel T. M. Ball"],"pdf_url":"https://arxiv.org/pdf/2505.12061v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07447v2","updated":"2025-05-20T12:27:53Z","published":"2025-05-12T11:15:39Z","title":"Unified Continuous Generative Models","summary":"  Recent advances in continuous generative models, including multi-step\napproaches like diffusion and flow-matching (typically requiring 8-1000\nsampling steps) and few-step methods such as consistency models (typically 1-8\nsteps), have demonstrated impressive generative performance. However, existing\nwork often treats these approaches as distinct paradigms, resulting in separate\ntraining and sampling methodologies. We introduce a unified framework for\ntraining, sampling, and analyzing these models. Our implementation, the Unified\nContinuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves\nstate-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a\n675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID\nin 20 steps and a few-step model reaching 1.42 FID in just 2 steps.\nAdditionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at\n250 steps) improves performance to 1.06 FID in only 40 steps. Code is available\nat: https://github.com/LINs-lab/UCGM.\n","authors":["Peng Sun","Yi Jiang","Tao Lin"],"pdf_url":"https://arxiv.org/pdf/2505.07447v2.pdf","comment":"https://github.com/LINs-lab/UCGM"},{"id":"http://arxiv.org/abs/2505.13232v2","updated":"2025-05-20T12:27:33Z","published":"2025-05-19T15:15:35Z","title":"StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment","summary":"  Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions. We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features. Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance.\n","authors":["Younghyun Kim","Jongheon Jeong","Sangkyung Kwak","Kyungmin Lee","Juho Lee","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2505.13232v2.pdf","comment":"IJCAI 2025; Code is available at https://github.com/alinlab/StarFT"},{"id":"http://arxiv.org/abs/2505.14270v1","updated":"2025-05-20T12:23:21Z","published":"2025-05-20T12:23:21Z","title":"RA-Touch: Retrieval-Augmented Touch Understanding with Enriched Visual\n  Data","summary":"  Visuo-tactile perception aims to understand an object's tactile properties,\nsuch as texture, softness, and rigidity. However, the field remains\nunderexplored because collecting tactile data is costly and labor-intensive. We\nobserve that visually distinct objects can exhibit similar surface textures or\nmaterial properties. For example, a leather sofa and a leather jacket have\ndifferent appearances but share similar tactile properties. This implies that\ntactile understanding can be guided by material cues in visual data, even\nwithout direct tactile supervision. In this paper, we introduce RA-Touch, a\nretrieval-augmented framework that improves visuo-tactile perception by\nleveraging visual data enriched with tactile semantics. We carefully recaption\na large-scale visual dataset with tactile-focused descriptions, enabling the\nmodel to access tactile semantics typically absent from conventional visual\ndatasets. A key challenge remains in effectively utilizing these tactile-aware\nexternal descriptions. RA-Touch addresses this by retrieving visual-textual\nrepresentations aligned with tactile inputs and integrating them to focus on\nrelevant textural and material properties. By outperforming prior methods on\nthe TVL benchmark, our method demonstrates the potential of retrieval-based\nvisual reuse for tactile understanding. Code is available at\nhttps://aim-skku.github.io/RA-Touch\n","authors":["Yoorhim Cho","Hongyeob Kim","Semin Kim","Youjia Zhang","Yunseok Choi","Sungeun Hong"],"pdf_url":"https://arxiv.org/pdf/2505.14270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14260v1","updated":"2025-05-20T12:12:17Z","published":"2025-05-20T12:12:17Z","title":"Speculative Decoding Reimagined for Multimodal Large Language Models","summary":"  This paper introduces Multimodal Speculative Decoding (MSD) to accelerate\nMultimodal Large Language Models (MLLMs) inference. Speculative decoding has\nbeen shown to accelerate Large Language Models (LLMs) without sacrificing\naccuracy. However, current speculative decoding methods for MLLMs fail to\nachieve the same speedup as they do for LLMs. To address this, we reimagine\nspeculative decoding specifically for MLLMs. Our analysis of MLLM\ncharacteristics reveals two key design principles for MSD: (1) Text and visual\ntokens have fundamentally different characteristics and need to be processed\nseparately during drafting. (2) Both language modeling ability and visual\nperception capability are crucial for the draft model. For the first principle,\nMSD decouples text and visual tokens in the draft model, allowing each to be\nhandled based on its own characteristics. For the second principle, MSD uses a\ntwo-stage training strategy: In stage one, the draft model is trained on\ntext-only instruction-tuning datasets to improve its language modeling ability.\nIn stage two, MSD gradually introduces multimodal data to enhance the visual\nperception capability of the draft model. Experiments show that MSD boosts\ninference speed by up to $2.29\\times$ for LLaVA-1.5-7B and up to $2.46\\times$\nfor LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.\nOur code is available at https://github.com/Lyn-Lucy/MSD.\n","authors":["Luxi Lin","Zhihang Lin","Zhanpeng Zeng","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2505.14260v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2505.14257v1","updated":"2025-05-20T12:10:13Z","published":"2025-05-20T12:10:13Z","title":"Aligning Attention Distribution to Information Flow for Hallucination\n  Mitigation in Large Vision-Language Models","summary":"  Due to the unidirectional masking mechanism, Decoder-Only models propagate\ninformation from left to right. LVLMs (Large Vision-Language Models) follow the\nsame architecture, with visual information gradually integrated into semantic\nrepresentations during forward propagation. Through systematic analysis, we\nobserve that over 80\\% of the visual information is absorbed into the semantic\nrepresentations. However, the model's attention still predominantly focuses on\nthe visual representations. This misalignment between the attention\ndistribution and the actual information flow undermines the model's visual\nunderstanding ability and contributes to hallucinations. To address this issue,\nwe enhance the model's visual understanding by leveraging the core information\nembedded in semantic representations. Specifically, we identify attention heads\nthat focus on core semantic representations based on their attention\ndistributions. Then, through a two-stage optimization paradigm, we propagate\nthe advantages of these attention heads across the entire model, aligning the\nattention distribution with the actual information flow. We evaluate our method\non three image captioning benchmarks using five different LVLMs, demonstrating\nits effectiveness in significantly reducing hallucinations. Further experiments\nreveal a trade-off between reduced hallucinations and richer details. Notably,\nour method allows for manual adjustment of the model's conservativeness,\nenabling flexible control to meet diverse real-world requirements. Code will be\nreleased once accepted.\n","authors":["Jianfei Zhao","Feng Zhang","Xin Sun","Chong Feng"],"pdf_url":"https://arxiv.org/pdf/2505.14257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14254v1","updated":"2025-05-20T12:07:01Z","published":"2025-05-20T12:07:01Z","title":"Instructing Text-to-Image Diffusion Models via Classifier-Guided\n  Semantic Optimization","summary":"  Text-to-image diffusion models have emerged as powerful tools for\nhigh-quality image generation and editing. Many existing approaches rely on\ntext prompts as editing guidance. However, these methods are constrained by the\nneed for manual prompt crafting, which can be time-consuming, introduce\nirrelevant details, and significantly limit editing performance. In this work,\nwe propose optimizing semantic embeddings guided by attribute classifiers to\nsteer text-to-image models toward desired edits, without relying on text\nprompts or requiring any training or fine-tuning of the diffusion model. We\nutilize classifiers to learn precise semantic embeddings at the dataset level.\nThe learned embeddings are theoretically justified as the optimal\nrepresentation of attribute semantics, enabling disentangled and accurate\nedits. Experiments further demonstrate that our method achieves high levels of\ndisentanglement and strong generalization across different domains of data.\n","authors":["Yuanyuan Chang","Yinghua Yao","Tao Qin","Mengmeng Wang","Ivor Tsang","Guang Dai"],"pdf_url":"https://arxiv.org/pdf/2505.14254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15633v4","updated":"2025-05-20T12:06:56Z","published":"2024-11-23T19:10:32Z","title":"Orthogonal Subspace Decomposition for Generalizable AI-Generated Image\n  Detection","summary":"  AI-generated images (AIGIs), such as natural or face images, have become\nincreasingly important yet challenging. In this paper, we start from a new\nperspective to excavate the reason behind the failure generalization in AIGI\ndetection, named the \\textit{asymmetry phenomenon}, where a naively trained\ndetector tends to favor overfitting to the limited and monotonous fake\npatterns, causing the feature space to become highly constrained and\nlow-ranked, which is proved seriously limiting the expressivity and\ngeneralization. One potential remedy is incorporating the pre-trained knowledge\nwithin the vision foundation models (higher-ranked) to expand the feature\nspace, alleviating the model's overfitting to fake. To this end, we employ\nSingular Value Decomposition (SVD) to decompose the original feature space into\n\\textit{two orthogonal subspaces}. By freezing the principal components and\nadapting only the remained components, we preserve the pre-trained knowledge\nwhile learning fake patterns. Compared to existing full-parameters and\nLoRA-based tuning methods, we explicitly ensure orthogonality, enabling the\nhigher rank of the whole feature space, effectively minimizing overfitting and\nenhancing generalization. We finally identify a crucial insight: our method\nimplicitly learns \\textit{a vital prior that fakes are actually derived from\nthe real}, indicating a hierarchical relationship rather than independence.\nModeling this prior, we believe, is essential for achieving superior\ngeneralization. Our codes are publicly available at\n\\href{https://github.com/YZY-stack/Effort-AIGI-Detection}{GitHub}.\n","authors":["Zhiyuan Yan","Jiangming Wang","Peng Jin","Ke-Yue Zhang","Chengchun Liu","Shen Chen","Taiping Yao","Shouhong Ding","Baoyuan Wu","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2411.15633v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02171v2","updated":"2025-05-20T12:05:04Z","published":"2025-02-04T09:45:49Z","title":"DeepForest: Sensing Into Self-Occluding Volumes of Vegetation With\n  Aerial Imaging","summary":"  Access to below-canopy volumetric vegetation data is crucial for\nunderstanding ecosystem dynamics. We address the long-standing limitation of\nremote sensing to penetrate deep into dense canopy layers. LiDAR and radar are\ncurrently considered the primary options for measuring 3D vegetation\nstructures, while cameras can only extract the reflectance and depth of top\nlayers. Using conventional, high-resolution aerial images, our approach allows\nsensing deep into self-occluding vegetation volumes, such as forests. It is\nsimilar in spirit to the imaging process of wide-field microscopy, but can\nhandle much larger scales and strong occlusion. We scan focal stacks by\nsynthetic-aperture imaging with drones and reduce outof-focus signal\ncontributions using pre-trained 3D convolutional neural networks with mean\nsquared error (MSE) as the loss function. The resulting volumetric reflectance\nstacks contain low-frequency representations of the vegetation volume.\nCombining multiple reflectance stacks from various spectral channels provides\ninsights into plant health, growth, and environmental conditions throughout the\nentire vegetation volume. Compared with simulated ground truth, our correction\nleads to ~x7 average improvements (min: ~x2, max: ~x12) for forest densities of\n200 trees/ha - 1680 trees/ha. In our field experiment, we achieved an MSE of\n0.05 when comparing with the top-vegetation layer that was measured with\nclassical multispectral aerial imaging.\n","authors":["Mohamed Youssef","Jian Peng","Oliver Bimber"],"pdf_url":"https://arxiv.org/pdf/2502.02171v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14246v1","updated":"2025-05-20T11:59:25Z","published":"2025-05-20T11:59:25Z","title":"Visual Agentic Reinforcement Fine-Tuning","summary":"  A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\nagentic ability to use external tools such as web browsers for searching and\nwriting/executing code for image manipulation to think with images. In the\nopen-source research community, while significant progress has been made in\nlanguage-only agentic abilities such as function calling and tool integration,\nthe development of multi-modal agentic capabilities that involve truly thinking\nwith images, and their corresponding benchmarks, are still less explored. This\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\nability to browse websites for real-time information updates and write code to\nmanipulate and analyze input images through cropping, rotation, and other image\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\nagentic search and coding abilities. Our experimental results demonstrate that\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\nOur findings suggest that Visual-ARFT offers a promising path toward building\nrobust and generalizable multimodal agents.\n","authors":["Ziyu Liu","Yuhang Zang","Yushan Zou","Zijian Liang","Xiaoyi Dong","Yuhang Cao","Haodong Duan","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2505.14246v1.pdf","comment":"project url:\n  https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT"},{"id":"http://arxiv.org/abs/2404.14807v2","updated":"2025-05-20T11:52:35Z","published":"2024-04-23T07:37:43Z","title":"BigReg: An Efficient Registration Pipeline for High-Resolution X-Ray and\n  Light-Sheet Fluorescence Microscopy","summary":"  Recently, X-ray microscopy (XRM) and light-sheet fluorescence microscopy\n(LSFM) have emerged as pivotal tools in preclinical research, particularly for\nstudying bone remodeling diseases such as osteoporosis. These modalities offer\nmicrometer-level resolution, and their integration allows for a complementary\nexamination of bone microstructures which is essential for analyzing functional\nchanges. However, registering high-resolution volumes from these independently\nscanned modalities poses substantial challenges, especially in real-world and\nreference-free scenarios. This paper presents BigReg, a fast, two-stage\npipeline designed for large-volume registration of XRM and LSFM data. The first\nstage involves extracting surface features and applying two successive point\ncloud-based methods for coarse alignment. The subsequent stage refines this\nalignment using a modified cross-correlation technique, achieving precise\nvolumetric registration. Evaluations using expert-annotated landmarks and\naugmented test data demonstrate that BigReg approaches the accuracy of\nlandmark-based registration with a landmark distance (LMD) of 8.36\\,\\textmu\nm\\,$\\pm$\\,0.12\\,\\textmu m and a landmark fitness (LM fitness) of\n85.71\\%\\,$\\pm$\\,1.02\\%. Moreover, BigReg can provide an optimal initialization\nfor mutual information-based methods which otherwise fail independently,\nfurther reducing LMD to 7.24\\,\\textmu m\\,$\\pm$\\,0.11\\,\\textmu m and increasing\nLM fitness to 93.90\\%\\,$\\pm$\\,0.77\\%. Ultimately, key microstructures, notably\nlacunae in XRM and bone cells in LSFM, are accurately aligned, enabling\nunprecedented insights into the pathology of osteoporosis.\n","authors":["Siyuan Mei","Fuxin Fan","Mareike Thies","Mingxuan Gu","Fabian Wagner","Oliver Aust","Ina Erceg","Zeynab Mirzaei","Georgiana Neag","Yipeng Sun","Yixing Huang","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2404.14807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14239v1","updated":"2025-05-20T11:47:34Z","published":"2025-05-20T11:47:34Z","title":"Decoupling Classifier for Boosting Few-shot Object Detection and\n  Instance Segmentation","summary":"  This paper focus on few-shot object detection~(FSOD) and instance\nsegmentation~(FSIS), which requires a model to quickly adapt to novel classes\nwith a few labeled instances. The existing methods severely suffer from bias\nclassification because of the missing label issue which naturally exists in an\ninstance-level few-shot scenario and is first formally proposed by us. Our\nanalysis suggests that the standard classification head of most FSOD or FSIS\nmodels needs to be decoupled to mitigate the bias classification. Therefore, we\npropose an embarrassingly simple but effective method that decouples the\nstandard classifier into two heads. Then, these two individual heads are\ncapable of independently addressing clear positive samples and noisy negative\nsamples which are caused by the missing label. In this way, the model can\neffectively learn novel classes while mitigating the effects of noisy negative\nsamples. Without bells and whistles, our model without any additional\ncomputation cost and parameters consistently outperforms its baseline and\nstate-of-the-art by a large margin on PASCAL VOC and MS-COCO benchmarks for\nFSOD and FSIS tasks. The Code is available at\nhttps://csgaobb.github.io/Projects/DCFS.\n","authors":["Bin-Bin Gao","Xiaochen Chen","Zhongyi Huang","Congchong Nie","Jun Liu","Jinxiang Lai","Guannan Jiang","Xi Wang","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2505.14239v1.pdf","comment":"Accepted by NeurIPS 2022"},{"id":"http://arxiv.org/abs/2505.10634v3","updated":"2025-05-20T11:46:11Z","published":"2025-05-15T18:16:56Z","title":"Cross-Image Contrastive Decoding: Precise, Lossless Suppression of\n  Language Priors in Large Vision-Language Models","summary":"  Language priors are a major cause of hallucinations in Large Vision-Language\nModels (LVLMs), often leading to text that is linguistically plausible but\nvisually inconsistent. Recent work explores contrastive decoding as a\ntraining-free solution, but these methods typically construct negative contexts\nfrom the original image, resulting in visual information loss and distorted\ndistribution. Motivated by the observation that language priors stem from the\nLLM backbone and remain consistent across images, we propose Cross-Images\nContrastive Decoding (CICD), a simple yet effective training-free method that\nuses different images to construct negative contexts. We further analyze the\ncross-image behavior of language priors and introduce a distinction between\nessential priors (supporting fluency) and detrimental priors (causing\nhallucinations). By selectively preserving essential priors and suppressing\ndetrimental ones, our method reduces hallucinations while maintaining coherent\nand fluent language generation. Experiments on 4 benchmarks and 6 LVLMs across\nthree model families confirm the effectiveness and generalizability of CICD,\nespecially in image captioning, where language priors are particularly\npronounced. Code will be released once accepted.\n","authors":["Jianfei Zhao","Feng Zhang","Xin Sun","Chong Feng"],"pdf_url":"https://arxiv.org/pdf/2505.10634v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14231v1","updated":"2025-05-20T11:40:43Z","published":"2025-05-20T11:40:43Z","title":"UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement\n  Learning","summary":"  Traditional visual grounding methods primarily focus on single-image\nscenarios with simple textual references. However, extending these methods to\nreal-world scenarios that involve implicit and complex instructions,\nparticularly in conjunction with multiple images, poses significant challenges,\nwhich is mainly due to the lack of advanced reasoning ability across diverse\nmulti-modal contexts. In this work, we aim to address the more practical\nuniversal grounding task, and propose UniVG-R1, a reasoning guided multimodal\nlarge language model (MLLM) for universal visual grounding, which enhances\nreasoning capabilities through reinforcement learning (RL) combined with\ncold-start data. Specifically, we first construct a high-quality\nChain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning\nchains, to guide the model towards correct reasoning paths via supervised\nfine-tuning. Subsequently, we perform rule-based reinforcement learning to\nencourage the model to identify correct reasoning chains, thereby incentivizing\nits reasoning capabilities. In addition, we identify a difficulty bias arising\nfrom the prevalence of easy samples as RL training progresses, and we propose a\ndifficulty-aware weight adjustment strategy to further strengthen the\nperformance. Experimental results demonstrate the effectiveness of UniVG-R1,\nwhich achieves state-of-the-art performance on MIG-Bench with a 9.1%\nimprovement over the previous method. Furthermore, our model exhibits strong\ngeneralizability, achieving an average improvement of 23.4% in zero-shot\nperformance across four image and video reasoning grounding benchmarks. The\nproject page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.\n","authors":["Sule Bai","Mingxing Li","Yong Liu","Jing Tang","Haoji Zhang","Lei Sun","Xiangxiang Chu","Yansong Tang"],"pdf_url":"https://arxiv.org/pdf/2505.14231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14227v1","updated":"2025-05-20T11:37:49Z","published":"2025-05-20T11:37:49Z","title":"VoQA: Visual-only Question Answering","summary":"  We propose Visual-only Question Answering (VoQA), a novel multimodal task in\nwhich questions are visually embedded within images, without any accompanying\ntextual input. This requires models to locate, recognize, and reason over\nvisually embedded textual questions, posing challenges for existing large\nvision-language models (LVLMs), which show notable performance drops even with\ncarefully designed prompts. To bridge this gap, we introduce Guided Response\nTriggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy\nthat guides the model to perform step-by-step reasoning purely based on visual\ninput, significantly improving model performance. Our work enhances models'\ncapacity for human-like visual understanding in complex multimodal scenarios,\nwhere information, including language, is perceived visually.\n","authors":["Luyang Jiang","Jianing An","Jie Luo","Wenjun Wu","Lei Huang"],"pdf_url":"https://arxiv.org/pdf/2505.14227v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2501.12368v2","updated":"2025-05-20T11:36:34Z","published":"2025-01-21T18:47:32Z","title":"InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward\n  Model","summary":"  Despite the promising performance of Large Vision Language Models (LVLMs) in\nvisual understanding, they occasionally generate incorrect outputs. While\nreward models (RMs) with reinforcement learning or test-time scaling offer the\npotential for improving generation quality, a critical gap remains: publicly\navailable multi-modal RMs for LVLMs are scarce, and the implementation details\nof proprietary models are often unclear. We bridge this gap with\nInternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective\nmulti-modal reward model that aligns LVLMs with human preferences. To ensure\nthe robustness and versatility of IXC-2.5-Reward, we set up a high-quality\nmulti-modal preference corpus spanning text, image, and video inputs across\ndiverse domains, such as instruction following, general understanding,\ntext-rich documents, mathematical reasoning, and video understanding.\nIXC-2.5-Reward achieves excellent results on the latest multi-modal reward\nmodel benchmark and shows competitive performance on text-only reward model\nbenchmarks. We further demonstrate three key applications of IXC-2.5-Reward:\n(1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward\nwith Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows\nconsistent improvements in instruction following and multi-modal open-ended\ndialogue; (2) Selecting the best response from candidate responses for\ntest-time scaling; and (3) Filtering outlier or noisy samples from existing\nimage and video instruction tuning training data. To ensure reproducibility and\nfacilitate further research, we have open-sourced all model weights and\ntraining recipes at\nhttps://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-Reward\n","authors":["Yuhang Zang","Xiaoyi Dong","Pan Zhang","Yuhang Cao","Ziyu Liu","Shengyuan Ding","Shenxi Wu","Yubo Ma","Haodong Duan","Wenwei Zhang","Kai Chen","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2501.12368v2.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2505.14218v1","updated":"2025-05-20T11:26:05Z","published":"2025-05-20T11:26:05Z","title":"Flexible-weighted Chamfer Distance: Enhanced Objective Function for\n  Point Cloud Completion","summary":"  Chamfer Distance (CD) comprises two components that can evaluate the global\ndistribution and local performance of generated point clouds, making it widely\nutilized as a similarity measure between generated and target point clouds in\npoint cloud completion tasks. Additionally, CD's computational efficiency has\nled to its frequent application as an objective function for guiding point\ncloud generation. However, using CD directly as an objective function with\nfixed equal weights for its two components can often result in seemingly high\noverall performance (i.e., low CD score), while failing to achieve a good\nglobal distribution. This is typically reflected in high Earth Mover's Distance\n(EMD) and Decomposed Chamfer Distance (DCD) scores, alongside poor human\nassessments. To address this issue, we propose a Flexible-Weighted Chamfer\nDistance (FCD) to guide point cloud generation. FCD assigns a higher weight to\nthe global distribution component of CD and incorporates a flexible weighting\nstrategy to adjust the balance between the two components, aiming to improve\nglobal distribution while maintaining robust overall performance. Experimental\nresults on two state-of-the-art networks demonstrate that our method achieves\nsuperior results across multiple evaluation metrics, including CD, EMD, DCD,\nand F-Score, as well as in human evaluations.\n","authors":["Jie Li","Shengwei Tian","Long Yu","Xin Ning"],"pdf_url":"https://arxiv.org/pdf/2505.14218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12200v2","updated":"2025-05-20T11:17:17Z","published":"2025-05-18T02:30:52Z","title":"CompBench: Benchmarking Complex Instruction-guided Image Editing","summary":"  While real-world applications increasingly demand intricate scene\nmanipulation, existing instruction-guided image editing benchmarks often\noversimplify task complexity and lack comprehensive, fine-grained instructions.\nTo bridge this gap, we introduce, a large-scale benchmark specifically designed\nfor complex instruction-guided image editing. CompBench features challenging\nediting scenarios that incorporate fine-grained instruction following, spatial\nand contextual reasoning, thereby enabling comprehensive evaluation of image\nediting models' precise manipulation capabilities. To construct CompBench, We\npropose an MLLM-human collaborative framework with tailored task pipelines.\nFurthermore, we propose an instruction decoupling strategy that disentangles\nediting intents into four key dimensions: location, appearance, dynamics, and\nobjects, ensuring closer alignment between instructions and complex editing\nrequirements. Extensive evaluations reveal that CompBench exposes fundamental\nlimitations of current image editing models and provides critical insights for\nthe development of next-generation instruction-guided image editing systems.\nThe dataset, code, and models are available in https://comp-bench.github.io/.\n","authors":["Bohan Jia","Wenxuan Huang","Yuntian Tang","Junbo Qiao","Jincheng Liao","Shaosheng Cao","Fei Zhao","Zhaopeng Feng","Zhouhong Gu","Zhenfei Yin","Lei Bai","Wanli Ouyang","Lin Chen","Fei Zhao","Zihan Wang","Yuan Xie","Shaohui Lin"],"pdf_url":"https://arxiv.org/pdf/2505.12200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11997v2","updated":"2025-05-20T11:04:32Z","published":"2025-05-17T13:16:54Z","title":"Multimodal Cancer Survival Analysis via Hypergraph Learning with\n  Cross-Modality Rebalance","summary":"  Multimodal pathology-genomic analysis has become increasingly prominent in\ncancer survival prediction. However, existing studies mainly utilize\nmulti-instance learning to aggregate patch-level features, neglecting the\ninformation loss of contextual and hierarchical details within pathology\nimages. Furthermore, the disparity in data granularity and dimensionality\nbetween pathology and genomics leads to a significant modality imbalance. The\nhigh spatial resolution inherent in pathology data renders it a dominant role\nwhile overshadowing genomics in multimodal integration. In this paper, we\npropose a multimodal survival prediction framework that incorporates hypergraph\nlearning to effectively capture both contextual and hierarchical details from\npathology images. Moreover, it employs a modality rebalance mechanism and an\ninteractive alignment fusion strategy to dynamically reweight the contributions\nof the two modalities, thereby mitigating the pathology-genomics imbalance.\nQuantitative and qualitative experiments are conducted on five TCGA datasets,\ndemonstrating that our model outperforms advanced methods by over 3.4\\% in\nC-Index performance.\n","authors":["Mingcheng Qu","Guang Yang","Donglin Di","Tonghua Su","Yue Gao","Yang Song","Lei Fan"],"pdf_url":"https://arxiv.org/pdf/2505.11997v2.pdf","comment":"accepted by IJCAI2025 Code: https://github.com/MCPathology/MRePath"},{"id":"http://arxiv.org/abs/2505.14204v1","updated":"2025-05-20T11:04:14Z","published":"2025-05-20T11:04:14Z","title":"Beginning with You: Perceptual-Initialization Improves Vision-Language\n  Representation and Alignment","summary":"  We introduce Perceptual-Initialization (PI), a paradigm shift in visual\nrepresentation learning that incorporates human perceptual structure during the\ninitialization phase rather than as a downstream fine-tuning step. By\nintegrating human-derived triplet embeddings from the NIGHTS dataset to\ninitialize a CLIP vision encoder, followed by self-supervised learning on\nYFCC15M, our approach demonstrates significant zero-shot performance\nimprovements, without any task-specific fine-tuning, across 29 zero shot\nclassification and 2 retrieval benchmarks. On ImageNet-1K, zero-shot gains\nemerge after approximately 15 epochs of pretraining. Benefits are observed\nacross datasets of various scales, with improvements manifesting at different\nstages of the pretraining process depending on dataset characteristics. Our\napproach consistently enhances zero-shot top-1 accuracy, top-5 accuracy, and\nretrieval recall (e.g., R@1, R@5) across these diverse evaluation tasks,\nwithout requiring any adaptation to target domains. These findings challenge\nthe conventional wisdom of using human-perceptual data primarily for\nfine-tuning and demonstrate that embedding human perceptual structure during\nearly representation learning yields more capable and vision-language aligned\nsystems that generalize immediately to unseen tasks. Our work shows that\n\"beginning with you\", starting with human perception, provides a stronger\nfoundation for general-purpose vision-language intelligence.\n","authors":["Yang Hu","Runchen Wang","Stephen Chong Zhao","Xuhui Zhan","Do Hun Kim","Mark Wallace","David A. Tovar"],"pdf_url":"https://arxiv.org/pdf/2505.14204v1.pdf","comment":"10 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.00275v3","updated":"2025-05-20T11:03:37Z","published":"2024-09-30T23:04:55Z","title":"Exploring Social Media Image Categorization Using Large Models with\n  Different Adaptation Methods: A Case Study on Cultural Nature's Contributions\n  to People","summary":"  Social media images provide valuable insights for modeling, mapping, and\nunderstanding human interactions with natural and cultural heritage. However,\ncategorizing these images into semantically meaningful groups remains highly\ncomplex due to the vast diversity and heterogeneity of their visual content as\nthey contain an open-world human and nature elements. This challenge becomes\ngreater when categories involve abstract concepts and lack consistent visual\npatterns. Related studies involve human supervision in the categorization\nprocess and the lack of public benchmark datasets make comparisons between\nthese works unfeasible. On the other hand, the continuous advances in large\nmodels, including Large Language Models (LLMs), Large Visual Models (LVMs), and\nLarge Visual Language Models (LVLMs), provide a large space of unexplored\nsolutions. In this work 1) we introduce FLIPS a dataset of Flickr images that\ncapture the interaction between human and nature, and 2) evaluate various\nsolutions based on different types and combinations of large models using\nvarious adaptation methods. We assess and report their performance in terms of\ncost, productivity, scalability, and result quality to address the challenges\nof social media image categorization.\n","authors":["Rohaifa Khaldi","Domingo Alcaraz-Segura","Ignacio Sánchez-Herrera","Javier Martinez-Lopez","Carlos Javier Navarro","Siham Tabik"],"pdf_url":"https://arxiv.org/pdf/2410.00275v3.pdf","comment":"23 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.14197v1","updated":"2025-05-20T10:55:26Z","published":"2025-05-20T10:55:26Z","title":"Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and\n  GRPO-based Method","summary":"  Omnidirectional images (ODIs), with their 360{\\deg} field of view, provide\nunparalleled spatial awareness for immersive applications like augmented\nreality and embodied AI. However, the capability of existing multi-modal large\nlanguage models (MLLMs) to comprehend and reason about such panoramic scenes\nremains underexplored. This paper addresses this gap by introducing OmniVQA,\nthe first dataset and conducting the first benchmark for omnidirectional visual\nquestion answering. Our evaluation of state-of-the-art MLLMs reveals\nsignificant limitations in handling omnidirectional visual question answering,\nhighlighting persistent challenges in object localization, feature extraction,\nand hallucination suppression within panoramic contexts. These results\nunderscore the disconnect between current MLLM capabilities and the demands of\nomnidirectional visual understanding, which calls for dedicated architectural\nor training innovations tailored to 360{\\deg} imagery. Building on the OmniVQA\ndataset and benchmark, we further introduce a rule-based reinforcement learning\nmethod, 360-R1, based on Qwen2.5-VL-Instruct. Concretely, we modify the group\nrelative policy optimization (GRPO) by proposing three novel reward functions:\n(1) reasoning process similarity reward, (2) answer semantic accuracy reward,\nand (3) structured format compliance reward. Extensive experiments on our\nOmniVQA demonstrate the superiority of our proposed method in omnidirectional\nspace (+6% improvement).\n","authors":["Xinshen Zhang","Zhen Ye","Xu Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.14197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14180v1","updated":"2025-05-20T10:36:25Z","published":"2025-05-20T10:36:25Z","title":"Bridge the Gap between Past and Future: Siamese Model Optimization for\n  Context-Aware Document Ranking","summary":"  In the realm of information retrieval, users often engage in multi-turn\ninteractions with search engines to acquire information, leading to the\nformation of sequences of user feedback behaviors. Leveraging the session\ncontext has proven to be beneficial for inferring user search intent and\ndocument ranking. A multitude of approaches have been proposed to exploit\nin-session context for improved document ranking. Despite these advances, the\nlimitation of historical session data for capturing evolving user intent\nremains a challenge. In this work, we explore the integration of future\ncontextual information into the session context to enhance document ranking. We\npresent the siamese model optimization framework, comprising a\nhistory-conditioned model and a future-aware model. The former processes only\nthe historical behavior sequence, while the latter integrates both historical\nand anticipated future behaviors. Both models are trained collaboratively using\nthe supervised labels and pseudo labels predicted by the other. The\nhistory-conditioned model, referred to as ForeRanker, progressively learns\nfuture-relevant information to enhance ranking, while it singly uses historical\nsession at inference time. To mitigate inconsistencies during training, we\nintroduce the peer knowledge distillation method with a dynamic gating\nmechanism, allowing models to selectively incorporate contextual information.\nExperimental results on benchmark datasets demonstrate the effectiveness of our\nForeRanker, showcasing its superior performance compared to existing methods.\n","authors":["Songhao Wu","Quan Tu","Mingjie Zhong","Hong Liu","Jia Xu","Jinjie Gu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2505.14180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14718v6","updated":"2025-05-20T10:34:20Z","published":"2024-01-26T08:59:38Z","title":"A Survey on Future Frame Synthesis: Bridging Deterministic and\n  Generative Approaches","summary":"  Future Frame Synthesis (FFS, aka Video Frame Prediction) focuses on\ngenerating future frame sequences conditioned on existing content. This survey\nprovides a comprehensive review of existing research on FFS, covering commonly\nused datasets and representative algorithms. We discuss key challenges and\ntrace the evolution of FFS in computer vision, particularly the shift from\ndeterministic to generative approaches. Our taxonomy outlines major advances\nand methodological shifts, emphasizing the rising significance of generative\nmodels in producing realistic and diverse predictions. This survey offers a\ncomprehensive analysis of current research and, moreover, suggests promising\navenues for future exploration in this ever-changing domain.\n","authors":["Ruibo Ming","Zhewei Huang","Jingwei Wu","Zhuoxuan Ju","Jianming Hu","Lihui Peng","Shuchang Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.14718v6.pdf","comment":"under review, 22 pages"},{"id":"http://arxiv.org/abs/2505.14177v1","updated":"2025-05-20T10:29:57Z","published":"2025-05-20T10:29:57Z","title":"From stability of Langevin diffusion to convergence of proximal MCMC for\n  non-log-concave sampling","summary":"  We consider the problem of sampling distributions stemming from non-convex\npotentials with Unadjusted Langevin Algorithm (ULA). We prove the stability of\nthe discrete-time ULA to drift approximations under the assumption that the\npotential is strongly convex at infinity. In many context, e.g. imaging inverse\nproblems, potentials are non-convex and non-smooth. Proximal Stochastic\nGradient Langevin Algorithm (PSGLA) is a popular algorithm to handle such\npotentials. It combines the forward-backward optimization algorithm with a ULA\nstep. Our main stability result combined with properties of the Moreau envelope\nallows us to derive the first proof of convergence of the PSGLA for non-convex\npotentials. We empirically validate our methodology on synthetic data and in\nthe context of imaging inverse problems. In particular, we observe that PSGLA\nexhibits faster convergence rates than Stochastic Gradient Langevin Algorithm\nfor posterior sampling while preserving its restoration properties.\n","authors":["Marien Renaud","Valentin De Bortoli","Arthur Leclaire","Nicolas Papadakis"],"pdf_url":"https://arxiv.org/pdf/2505.14177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22577v2","updated":"2025-05-20T10:29:41Z","published":"2025-03-28T16:26:52Z","title":"Breaking Language Barriers in Visual Language Models via Multilingual\n  Textual Regularization","summary":"  Rapid advancements in Visual Language Models (VLMs) have transformed\nmultimodal understanding but are often constrained by generating English\nresponses regardless of the input language. This phenomenon has been termed as\nImage-induced Fidelity Loss (IFL) and stems from limited multimodal\nmultilingual training data. To address this, we propose a continuous\nmultilingual integration strategy that injects text-only multilingual data\nduring visual instruction tuning, preserving the language model's original\nmultilingual capabilities. Extensive evaluations demonstrate that our approach\nsignificantly improves linguistic fidelity across languages without degradation\nin visual performance. We also explore model merging, which improves language\nfidelity but comes at the cost of visual performance. In contrast, our core\nmethod achieves robust multilingual alignment without trade-offs, offering a\nscalable and effective path to mitigating IFL for global VLM adoption.\n","authors":["Iñigo Pikabea","Iñaki Lacunza","Oriol Pareras","Carlos Escolano","Aitor Gonzalez-Agirre","Javier Hernando","Marta Villegas"],"pdf_url":"https://arxiv.org/pdf/2503.22577v2.pdf","comment":"v2: Expanded model merging experiments. Fix duplicated subsection on\n  limitations"},{"id":"http://arxiv.org/abs/2503.06897v2","updated":"2025-05-20T10:29:12Z","published":"2025-03-10T04:01:48Z","title":"Multi-granular body modeling with Redundancy-Free Spatiotemporal Fusion\n  for Text-Driven Motion Generation","summary":"  Text-to-motion generation sits at the intersection of multimodal learning and\ncomputer graphics and is gaining momentum because it can simplify content\ncreation for games, animation, robotics and virtual reality. Most current\nmethods stack spatial and temporal features in a straightforward way, which\nadds redundancy and still misses subtle joint-level cues. We introduce HiSTF\nMamba, a framework with three parts: Dual-Spatial Mamba, Bi-Temporal Mamba and\na Dynamic Spatiotemporal Fusion Module (DSFM). The Dual-Spatial module runs\npart-based and whole-body models in parallel, capturing both overall\ncoordination and fine-grained joint motion. The Bi-Temporal module scans\nsequences forward and backward to encode short-term details and long-term\ndependencies. DSFM removes redundant temporal information, extracts\ncomplementary cues and fuses them with spatial features to build a richer\nspatiotemporal representation. Experiments on the HumanML3D benchmark show that\nHiSTF Mamba performs well across several metrics, achieving high fidelity and\ntight semantic alignment between text and motion.\n","authors":["Xingzu Zhan","Chen Xie","Honghang Chen","Haoran Sun","Xiaochun Mai"],"pdf_url":"https://arxiv.org/pdf/2503.06897v2.pdf","comment":"15pages,5figures,"},{"id":"http://arxiv.org/abs/2505.14167v1","updated":"2025-05-20T10:18:29Z","published":"2025-05-20T10:18:29Z","title":"LMP: Leveraging Motion Prior in Zero-Shot Video Generation with\n  Diffusion Transformer","summary":"  In recent years, large-scale pre-trained diffusion transformer models have\nmade significant progress in video generation. While current DiT models can\nproduce high-definition, high-frame-rate, and highly diverse videos, there is a\nlack of fine-grained control over the video content. Controlling the motion of\nsubjects in videos using only prompts is challenging, especially when it comes\nto describing complex movements. Further, existing methods fail to control the\nmotion in image-to-video generation, as the subject in the reference image\noften differs from the subject in the reference video in terms of initial\nposition, size, and shape. To address this, we propose the Leveraging Motion\nPrior (LMP) framework for zero-shot video generation. Our framework harnesses\nthe powerful generative capabilities of pre-trained diffusion transformers to\nenable motion in the generated videos to reference user-provided motion videos\nin both text-to-video and image-to-video generation. To this end, we first\nintroduce a foreground-background disentangle module to distinguish between\nmoving subjects and backgrounds in the reference video, preventing interference\nin the target video generation. A reweighted motion transfer module is designed\nto allow the target video to reference the motion from the reference video. To\navoid interference from the subject in the reference video, we propose an\nappearance separation module to suppress the appearance of the reference\nsubject in the target video. We annotate the DAVIS dataset with detailed\nprompts for our experiments and design evaluation metrics to validate the\neffectiveness of our method. Extensive experiments demonstrate that our\napproach achieves state-of-the-art performance in generation quality,\nprompt-video consistency, and control capability. Our homepage is available at\nhttps://vpx-ecnu.github.io/LMP-Website/\n","authors":["Changgu Chen","Xiaoyan Yang","Junwei Shu","Changbo Wang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2505.14167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14159v1","updated":"2025-05-20T10:13:00Z","published":"2025-05-20T10:13:00Z","title":"M3Depth: Wavelet-Enhanced Depth Estimation on Mars via Mutual Boosting\n  of Dual-Modal Data","summary":"  Depth estimation plays a great potential role in obstacle avoidance and\nnavigation for further Mars exploration missions. Compared to traditional\nstereo matching, learning-based stereo depth estimation provides a data-driven\napproach to infer dense and precise depth maps from stereo image pairs.\nHowever, these methods always suffer performance degradation in environments\nwith sparse textures and lacking geometric constraints, such as the\nunstructured terrain of Mars. To address these challenges, we propose M3Depth,\na depth estimation model tailored for Mars rovers. Considering the sparse and\nsmooth texture of Martian terrain, which is primarily composed of low-frequency\nfeatures, our model incorporates a convolutional kernel based on wavelet\ntransform that effectively captures low-frequency response and expands the\nreceptive field. Additionally, we introduce a consistency loss that explicitly\nmodels the complementary relationship between depth map and surface normal map,\nutilizing the surface normal as a geometric constraint to enhance the accuracy\nof depth estimation. Besides, a pixel-wise refinement module with mutual\nboosting mechanism is designed to iteratively refine both depth and surface\nnormal predictions. Experimental results on synthetic Mars datasets with depth\nannotations show that M3Depth achieves a significant 16% improvement in depth\nestimation accuracy compared to other state-of-the-art methods in depth\nestimation. Furthermore, the model demonstrates strong applicability in\nreal-world Martian scenarios, offering a promising solution for future Mars\nexploration missions.\n","authors":["Junjie Li","Jiawei Wang","Miyu Li","Yu Liu","Yumei Wang","Haitao Xu"],"pdf_url":"https://arxiv.org/pdf/2505.14159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14156v1","updated":"2025-05-20T10:05:06Z","published":"2025-05-20T10:05:06Z","title":"Unify Graph Learning with Text: Unleashing LLM Potentials for Session\n  Search","summary":"  Session search involves a series of interactive queries and actions to\nfulfill user's complex information need. Current strategies typically\nprioritize sequential modeling for deep semantic understanding, overlooking the\ngraph structure in interactions. While some approaches focus on capturing\nstructural information, they use a generalized representation for documents,\nneglecting the word-level semantic modeling. In this paper, we propose Symbolic\nGraph Ranker (SGR), which aims to take advantage of both text-based and\ngraph-based approaches by leveraging the power of recent Large Language Models\n(LLMs). Concretely, we first introduce a set of symbolic grammar rules to\nconvert session graph into text. This allows integrating session history,\ninteraction process, and task instruction seamlessly as inputs for the LLM.\nMoreover, given the natural discrepancy between LLMs pre-trained on textual\ncorpora, and the symbolic language we produce using our graph-to-text grammar,\nour objective is to enhance LLMs' ability to capture graph structures within a\ntextual format. To achieve this, we introduce a set of self-supervised symbolic\nlearning tasks including link prediction, node content generation, and\ngenerative contrastive learning, to enable LLMs to capture the topological\ninformation from coarse-grained to fine-grained. Experiment results and\ncomprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm\nthe superiority of our approach. Our paradigm also offers a novel and effective\nmethodology that bridges the gap between traditional search strategies and\nmodern LLMs.\n","authors":["Songhao Wu","Quan Tu","Hong Liu","Jia Xu","Zhongyi Liu","Guannan Zhang","Ran Wang","Xiuying Chen","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2505.14156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14151v1","updated":"2025-05-20T10:01:37Z","published":"2025-05-20T10:01:37Z","title":"ReactDiff: Latent Diffusion for Facial Reaction Generation","summary":"  Given the audio-visual clip of the speaker, facial reaction generation aims\nto predict the listener's facial reactions. The challenge lies in capturing the\nrelevance between video and audio while balancing appropriateness, realism, and\ndiversity. While prior works have mostly focused on uni-modal inputs or\nsimplified reaction mappings, recent approaches such as PerFRDiff have explored\nmulti-modal inputs and the one-to-many nature of appropriate reaction mappings.\nIn this work, we propose the Facial Reaction Diffusion (ReactDiff) framework\nthat uniquely integrates a Multi-Modality Transformer with conditional\ndiffusion in the latent space for enhanced reaction generation. Unlike existing\nmethods, ReactDiff leverages intra- and inter-class attention for fine-grained\nmulti-modal interaction, while the latent diffusion process between the encoder\nand decoder enables diverse yet contextually appropriate outputs. Experimental\nresults demonstrate that ReactDiff significantly outperforms existing\napproaches, achieving a facial reaction correlation of 0.26 and diversity score\nof 0.094 while maintaining competitive realism. The code is open-sourced at\n\\href{https://github.com/Hunan-Tiger/ReactDiff}{github}.\n","authors":["Jiaming Li","Sheng Wang","Xin Wang","Yitao Zhu","Honglin Xiong","Zixu Zhuang","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2505.14151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.04095v2","updated":"2025-05-20T09:58:37Z","published":"2023-05-06T16:47:52Z","title":"Gradient Leakage Defense with Key-Lock Module for Federated Learning","summary":"  Federated Learning (FL) is a widely adopted privacy-preserving machine\nlearning approach where private data remains local, enabling secure\ncomputations and the exchange of local model gradients between local clients\nand third-party parameter servers. However, recent findings reveal that privacy\nmay be compromised and sensitive information potentially recovered from shared\ngradients. In this study, we offer detailed analysis and a novel perspective on\nunderstanding the gradient leakage problem. These theoretical works lead to a\nnew gradient leakage defense technique that secures arbitrary model\narchitectures using a private key-lock module. Only the locked gradient is\ntransmitted to the parameter server for global model aggregation. Our proposed\nlearning method is resistant to gradient leakage attacks, and the key-lock\nmodule is designed and trained to ensure that, without the private information\nof the key-lock module: a) reconstructing private training data from the shared\ngradient is infeasible; and b) the global model's inference performance is\nsignificantly compromised. We discuss the theoretical underpinnings of why\ngradients can leak private information and provide theoretical proof of our\nmethod's effectiveness. We conducted extensive empirical evaluations with many\nmodels on several popular benchmarks, demonstrating the robustness of our\nproposed approach in both maintaining model performance and defending against\ngradient leakage attacks.\n","authors":["Hanchi Ren","Jingjing Deng","Xianghua Xie","Xiaoke Ma","Jianfeng Ma"],"pdf_url":"https://arxiv.org/pdf/2305.04095v2.pdf","comment":"The source code can be found at https://github.com/Rand2AI/FedKL"},{"id":"http://arxiv.org/abs/2405.14979v2","updated":"2025-05-20T09:44:45Z","published":"2024-05-23T18:30:12Z","title":"CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and\n  Interactive Geometry Refiner","summary":"  We present a novel generative 3D modeling system, coined CraftsMan, which can\ngenerate high-fidelity 3D geometries with highly varied shapes, regular mesh\ntopologies, and detailed surfaces, and, notably, allows for refining the\ngeometry in an interactive manner. Despite the significant advancements in 3D\ngeneration, existing methods still struggle with lengthy optimization\nprocesses, irregular mesh topologies, noisy surfaces, and difficulties in\naccommodating user edits, consequently impeding their widespread adoption and\nimplementation in 3D modeling software. Our work is inspired by the craftsman,\nwho usually roughs out the holistic figure of the work first and elaborates the\nsurface details subsequently. Specifically, we employ a 3D native diffusion\nmodel, which operates on latent space learned from latent set-based 3D\nrepresentations, to generate coarse geometries with regular mesh topology in\nseconds. In particular, this process takes as input a text prompt or a\nreference image and leverages a powerful multi-view (MV) diffusion model to\ngenerate multiple views of the coarse geometry, which are fed into our\nMV-conditioned 3D diffusion model for generating the 3D geometry, significantly\nimproving robustness and generalizability. Following that, a normal-based\ngeometry refiner is used to significantly enhance the surface details. This\nrefinement can be performed automatically, or interactively with user-supplied\nedits. Extensive experiments demonstrate that our method achieves high efficacy\nin producing superior-quality 3D assets compared to existing methods. HomePage:\nhttps://craftsman3d.github.io/, Code: https://github.com/wyysf-98/CraftsMan\n","authors":["Weiyu Li","Jiarui Liu","Rui Chen","Yixun Liang","Xuelin Chen","Ping Tan","Xiaoxiao Long"],"pdf_url":"https://arxiv.org/pdf/2405.14979v2.pdf","comment":"HomePage: https://craftsman3d.github.io/, Code:\n  https://github.com/wyysf-98/CraftsMan3D"},{"id":"http://arxiv.org/abs/2505.12427v2","updated":"2025-05-20T09:42:53Z","published":"2025-05-18T13:52:19Z","title":"DragLoRA: Online Optimization of LoRA Adapters for Drag-based Image\n  Editing in Diffusion Model","summary":"  Drag-based editing within pretrained diffusion model provides a precise and\nflexible way to manipulate foreground objects. Traditional methods optimize the\ninput feature obtained from DDIM inversion directly, adjusting them iteratively\nto guide handle points towards target locations. However, these approaches\noften suffer from limited accuracy due to the low representation ability of the\nfeature in motion supervision, as well as inefficiencies caused by the large\nsearch space required for point tracking. To address these limitations, we\npresent DragLoRA, a novel framework that integrates LoRA (Low-Rank Adaptation)\nadapters into the drag-based editing pipeline. To enhance the training of LoRA\nadapters, we introduce an additional denoising score distillation loss which\nregularizes the online model by aligning its output with that of the original\nmodel. Additionally, we improve the consistency of motion supervision by\nadapting the input features using the updated LoRA, giving a more stable and\naccurate input feature for subsequent operations. Building on this, we design\nan adaptive optimization scheme that dynamically toggles between two modes,\nprioritizing efficiency without compromising precision. Extensive experiments\ndemonstrate that DragLoRA significantly enhances the control precision and\ncomputational efficiency for drag-based image editing. The Codes of DragLoRA\nare available at: https://github.com/Sylvie-X/DragLoRA.\n","authors":["Siwei Xia","Li Sun","Tiantian Sun","Qingli Li"],"pdf_url":"https://arxiv.org/pdf/2505.12427v2.pdf","comment":"Accepted by ICML2025"},{"id":"http://arxiv.org/abs/2505.14135v1","updated":"2025-05-20T09:39:48Z","published":"2025-05-20T09:39:48Z","title":"Hunyuan-Game: Industrial-grade Intelligent Game Creation Model","summary":"  Intelligent game creation represents a transformative advancement in game\ndevelopment, utilizing generative artificial intelligence to dynamically\ngenerate and enhance game content. Despite notable progress in generative\nmodels, the comprehensive synthesis of high-quality game assets, including both\nimages and videos, remains a challenging frontier. To create high-fidelity game\ncontent that simultaneously aligns with player preferences and significantly\nboosts designer efficiency, we present Hunyuan-Game, an innovative project\ndesigned to revolutionize intelligent game production. Hunyuan-Game encompasses\ntwo primary branches: image generation and video generation. The image\ngeneration component is built upon a vast dataset comprising billions of game\nimages, leading to the development of a group of customized image generation\nmodels tailored for game scenarios: (1) General Text-to-Image Generation. (2)\nGame Visual Effects Generation, involving text-to-effect and reference\nimage-based game visual effect generation. (3) Transparent Image Generation for\ncharacters, scenes, and game visual effects. (4) Game Character Generation\nbased on sketches, black-and-white images, and white models. The video\ngeneration component is built upon a comprehensive dataset of millions of game\nand anime videos, leading to the development of five core algorithmic models,\neach targeting critical pain points in game development and having robust\nadaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2)\n360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4)\nGenerative Video Super-Resolution. (5) Interactive Game Video Generation. These\nimage and video generation models not only exhibit high-level aesthetic\nexpression but also deeply integrate domain-specific knowledge, establishing a\nsystematic understanding of diverse game and anime art styles.\n","authors":["Ruihuang Li","Caijin Zhou","Shoujian Zheng","Jianxiang Lu","Jiabin Huang","Comi Chen","Junshu Tang","Guangzheng Xu","Jiale Tao","Hongmei Wang","Donghao Li","Wenqing Yu","Senbo Wang","Zhimin Li","Yetshuan Shi","Haoyu Yang","Yukun Wang","Wenxun Dai","Jiaqi Li","Linqing Wang","Qixun Wang","Zhiyong Xu","Yingfang Zhang","Jiangfeng Xiong","Weijie Kong","Chao Zhang","Hongxin Zhang","Qiaoling Zheng","Weiting Guo","Xinchi Deng","Yixuan Li","Renjia Wei","Yulin Jian","Duojun Huang","Xuhua Ren","Sihuan Lin","Yifu Sun","Yuan Zhou","Joey Wang","Qin Lin","Jingmiao Yu","Jihong Zhang","Caesar Zhong","Di Wang","Yuhong Liu"," Linus","Jie Jiang","Longhuang Wu","Shuai Shao","Qinglin Lu"],"pdf_url":"https://arxiv.org/pdf/2505.14135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14124v1","updated":"2025-05-20T09:30:19Z","published":"2025-05-20T09:30:19Z","title":"Intra-class Patch Swap for Self-Distillation","summary":"  Knowledge distillation (KD) is a valuable technique for compressing large\ndeep learning models into smaller, edge-suitable networks. However,\nconventional KD frameworks rely on pre-trained high-capacity teacher networks,\nwhich introduce significant challenges such as increased memory/storage\nrequirements, additional training costs, and ambiguity in selecting an\nappropriate teacher for a given student model. Although a teacher-free\ndistillation (self-distillation) has emerged as a promising alternative, many\nexisting approaches still rely on architectural modifications or complex\ntraining procedures, which limit their generality and efficiency.\n  To address these limitations, we propose a novel framework based on\nteacher-free distillation that operates using a single student network without\nany auxiliary components, architectural modifications, or additional learnable\nparameters. Our approach is built on a simple yet highly effective\naugmentation, called intra-class patch swap augmentation. This augmentation\nsimulates a teacher-student dynamic within a single model by generating pairs\nof intra-class samples with varying confidence levels, and then applying\ninstance-to-instance distillation to align their predictive distributions. Our\nmethod is conceptually simple, model-agnostic, and easy to implement, requiring\nonly a single augmentation function. Extensive experiments across image\nclassification, semantic segmentation, and object detection show that our\nmethod consistently outperforms both existing self-distillation baselines and\nconventional teacher-based KD approaches. These results suggest that the\nsuccess of self-distillation could hinge on the design of the augmentation\nitself. Our codes are available at\nhttps://github.com/hchoi71/Intra-class-Patch-Swap.\n","authors":["Hongjun Choi","Eun Som Jeon","Ankita Shukla","Pavan Turaga"],"pdf_url":"https://arxiv.org/pdf/2505.14124v1.pdf","comment":"Accepted for publication in Neurocomputing"},{"id":"http://arxiv.org/abs/2504.21561v3","updated":"2025-05-20T09:22:47Z","published":"2025-04-30T12:01:27Z","title":"Iterative Tool Usage Exploration for Multimodal Agents via Step-wise\n  Preference Tuning","summary":"  Multimodal agents, which integrate a controller e.g., a vision language\nmodel) with external tools, have demonstrated remarkable capabilities in\ntackling complex multimodal tasks. Existing approaches for training these\nagents, both supervised fine-tuning and reinforcement learning, depend on\nextensive human-annotated task-answer pairs and tool trajectories. However, for\ncomplex multimodal tasks, such annotations are prohibitively expensive or\nimpractical to obtain. In this paper, we propose an iterative tool usage\nexploration method for multimodal agents without any pre-collected data, namely\nSPORT, via step-wise preference optimization to refine the trajectories of tool\nusage. Our method enables multimodal agents to autonomously discover effective\ntool usage strategies through self-exploration and optimization, eliminating\nthe bottleneck of human annotation. SPORT has four iterative components: task\nsynthesis, step sampling, step verification, and preference tuning. We first\nsynthesize multimodal tasks using language models. Then, we introduce a novel\ntrajectory exploration scheme, where step sampling and step verification are\nexecuted alternately to solve synthesized tasks. In step sampling, the agent\ntries different tools and obtains corresponding results. In step verification,\nwe employ a verifier to provide AI feedback to construct step-wise preference\ndata. The data is subsequently used to update the controller for tool usage\nthrough preference tuning, producing a SPORT agent. By interacting with real\nenvironments, the SPORT agent gradually evolves into a more refined and capable\nsystem. Evaluation in the GTA and GAIA benchmarks shows that the SPORT agent\nachieves 6.41% and 3.64% improvements, underscoring the generalization and\neffectiveness introduced by our method. The project page is\nhttps://SPORT-Agents.github.io.\n","authors":["Pengxiang Li","Zhi Gao","Bofei Zhang","Yapeng Mi","Xiaojian Ma","Chenrui Shi","Tao Yuan","Yuwei Wu","Yunde Jia","Song-Chun Zhu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2504.21561v3.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2407.13911v4","updated":"2025-05-20T09:21:06Z","published":"2024-07-18T21:52:57Z","title":"Continual Distillation Learning: Knowledge Distillation in Prompt-based\n  Continual Learning","summary":"  We introduce the problem of continual distillation learning (CDL) in order to\nuse knowledge distillation (KD) to improve prompt-based continual learning (CL)\nmodels. The CDL problem is valuable to study since the use of a larger vision\ntransformer (ViT) leads to better performance in prompt-based continual\nlearning. The distillation of knowledge from a large ViT to a small ViT\nimproves the inference efficiency for prompt-based CL models. We empirically\nfound that existing KD methods such as logit distillation and feature\ndistillation cannot effectively improve the student model in the CDL setup. To\naddress this issue, we introduce a novel method named Knowledge Distillation\nbased on Prompts (KDP), in which globally accessible prompts specifically\ndesigned for knowledge distillation are inserted into the frozen ViT backbone\nof the student model. We demonstrate that our KDP method effectively enhances\nthe distillation performance in comparison to existing KD methods in the CDL\nsetup.\n","authors":["Qifan Zhang","Yunhui Guo","Yu Xiang"],"pdf_url":"https://arxiv.org/pdf/2407.13911v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04058v2","updated":"2025-05-20T09:19:53Z","published":"2025-05-07T02:02:15Z","title":"AS3D: 2D-Assisted Cross-Modal Understanding with Semantic-Spatial Scene\n  Graphs for 3D Visual Grounding","summary":"  3D visual grounding aims to localize the unique target described by natural\nlanguages in 3D scenes. The significant gap between 3D and language modalities\nmakes it a notable challenge to distinguish multiple similar objects through\nthe described spatial relationships. Current methods attempt to achieve\ncross-modal understanding in complex scenes via a target-centered learning\nmechanism, ignoring the perception of referred objects. We propose a novel\n2D-assisted 3D visual grounding framework that constructs semantic-spatial\nscene graphs with referred object discrimination for relationship perception.\nThe framework incorporates a dual-branch visual encoder that utilizes 2D\npre-trained attributes to guide the multi-modal object encoding. Furthermore,\nour cross-modal interaction module uses graph attention to facilitate\nrelationship-oriented information fusion. The enhanced object representation\nand iterative relational learning enable the model to establish effective\nalignment between 3D vision and referential descriptions. Experimental results\non the popular benchmarks demonstrate our superior performance compared to\nstate-of-the-art methods, especially in addressing the challenges of multiple\nsimilar distractors.\n","authors":["Feng Xiao","Hongbin Xu","Guocan Zhao","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2505.04058v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14113v1","updated":"2025-05-20T09:19:37Z","published":"2025-05-20T09:19:37Z","title":"CONSIGN: Conformal Segmentation Informed by Spatial Groupings via\n  Decomposition","summary":"  Most machine learning-based image segmentation models produce pixel-wise\nconfidence scores - typically derived from softmax outputs - that represent the\nmodel's predicted probability for each class label at every pixel. While this\ninformation can be particularly valuable in high-stakes domains such as medical\nimaging, these (uncalibrated) scores are heuristic in nature and do not\nconstitute rigorous quantitative uncertainty estimates. Conformal prediction\n(CP) provides a principled framework for transforming heuristic confidence\nscores into statistically valid uncertainty estimates. However, applying CP\ndirectly to image segmentation ignores the spatial correlations between pixels,\na fundamental characteristic of image data. This can result in overly\nconservative and less interpretable uncertainty estimates. To address this, we\npropose CONSIGN (Conformal Segmentation Informed by Spatial Groupings via\nDecomposition), a CP-based method that incorporates spatial correlations to\nimprove uncertainty quantification in image segmentation. Our method generates\nmeaningful prediction sets that come with user-specified, high-probability\nerror guarantees. It is compatible with any pre-trained segmentation model\ncapable of generating multiple sample outputs - such as those using dropout,\nBayesian modeling, or ensembles. We evaluate CONSIGN against a standard\npixel-wise CP approach across three medical imaging datasets and two COCO\ndataset subsets, using three different pre-trained segmentation models. Results\ndemonstrate that accounting for spatial structure significantly improves\nperformance across multiple metrics and enhances the quality of uncertainty\nestimates.\n","authors":["Bruno Viti","Elias Karabelas","Martin Holler"],"pdf_url":"https://arxiv.org/pdf/2505.14113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.17821v2","updated":"2025-05-20T09:15:16Z","published":"2025-04-23T13:47:30Z","title":"VideoVista-CulturalLingo: 360$^\\circ$ Horizons-Bridging Cultures,\n  Languages, and Domains in Video Comprehension","summary":"  Assessing the video comprehension capabilities of multimodal AI systems can\neffectively measure their understanding and reasoning abilities. Most video\nevaluation benchmarks are limited to a single language, typically English, and\npredominantly feature videos rooted in Western cultural contexts. In this\npaper, we present VideoVista-CulturalLingo, the first video evaluation\nbenchmark designed to bridge cultural, linguistic, and domain divide in video\ncomprehension. Our work differs from existing benchmarks in the following ways:\n1) Cultural diversity, incorporating cultures from China, North America, and\nEurope; 2) Multi-linguistics, with questions presented in Chinese and\nEnglish-two of the most widely spoken languages; and 3) Broad domain, featuring\nvideos sourced from hundreds of human-created domains. VideoVista-CulturalLingo\ncontains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent\nopen-source or proprietary video large models. From the experiment results, we\nobserve that: 1) Existing models perform worse on Chinese-centric questions\nthan Western-centric ones, particularly those related to Chinese history; 2)\nCurrent open-source models still exhibit limitations in temporal understanding,\nespecially in the Event Localization task, achieving a maximum score of only\n45.2%; 3) Mainstream models demonstrate strong performance in general\nscientific questions, while open-source models demonstrate weak performance in\nmathematics.\n","authors":["Xinyu Chen","Yunxin Li","Haoyuan Shi","Baotian Hu","Wenhan Luo","Yaowei Wang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.17821v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14105v1","updated":"2025-05-20T09:11:53Z","published":"2025-05-20T09:11:53Z","title":"Unintended Bias in 2D+ Image Segmentation and Its Effect on Attention\n  Asymmetry","summary":"  Supervised pretrained models have become widely used in deep learning,\nespecially for image segmentation tasks. However, when applied to specialized\ndatasets such as biomedical imaging, pretrained weights often introduce\nunintended biases. These biases cause models to assign different levels of\nimportance to different slices, leading to inconsistencies in feature\nutilization, which can be observed as asymmetries in saliency map\ndistributions. This transfer of color distributions from natural images to\nnon-natural datasets can compromise model performance and reduce the\nreliability of results. In this study, we investigate the effects of these\nbiases and propose strategies to mitigate them. Through a series of\nexperiments, we test both pretrained and randomly initialized models, comparing\ntheir performance and saliency map distributions. Our proposed methods, which\naim to neutralize the bias introduced by pretrained color channel weights,\ndemonstrate promising results, offering a practical approach to improving model\nexplainability while maintaining the benefits of pretrained models. This\npublication presents our findings, providing insights into addressing\npretrained weight biases across various deep learning tasks.\n","authors":["Zsófia Molnár","Gergely Szabó","András Horváth"],"pdf_url":"https://arxiv.org/pdf/2505.14105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14100v1","updated":"2025-05-20T09:02:53Z","published":"2025-05-20T09:02:53Z","title":"Unlocking the Power of SAM 2 for Few-Shot Segmentation","summary":"  Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few\nclasses to segment arbitrary classes, but at the risk of overfitting. To\naddress this, some methods use the well-learned knowledge of foundation models\n(e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM\nby supporting video segmentation, whose class-agnostic matching ability is\nuseful to FSS. A simple idea is to encode support foreground (FG) features as\nmemory, with which query FG features are matched and fused. Unfortunately, the\nFG objects in different frames of SAM 2's video data are always the same\nidentity, while those in FSS are different identities, i.e., the matching step\nis incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo\nquery memory, matching with query features in a compatible way. However, the\nmemories can never be as accurate as the real ones, i.e., they are likely to\ncontain incomplete query FG, and some unexpected query background (BG)\nfeatures, leading to wrong segmentation. Hence, we further design Iterative\nMemory Refinement to fuse more query FG features into the memory, and devise a\nSupport-Calibrated Memory Attention to suppress the unexpected query BG\nfeatures in memory. Extensive experiments have been conducted on PASCAL-5$^i$\nand COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot\nmIoU can be 4.2\\% better than the best baseline.\n","authors":["Qianxiong Xu","Lanyun Zhu","Xuanyi Liu","Guosheng Lin","Cheng Long","Ziyue Li","Rui Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.14100v1.pdf","comment":"This paper is accepted by ICML'25"},{"id":"http://arxiv.org/abs/2505.12772v2","updated":"2025-05-20T09:01:16Z","published":"2025-05-19T07:00:54Z","title":"Pyramid Sparse Transformer: Enhancing Multi-Scale Feature Fusion with\n  Dynamic Token Selection","summary":"  Feature fusion is critical for high-performance vision models but often\nincurs prohibitive complexity. However, prevailing attention-based fusion\nmethods often involve significant computational complexity and implementation\nchallenges, limiting their efficiency in resource-constrained environments. To\naddress these issues, we introduce the Pyramid Sparse Transformer (PST), a\nlightweight, plug-and-play module that integrates coarse-to-fine token\nselection and shared attention parameters to reduce computation while\npreserving spatial detail. PST can be trained using only coarse attention and\nseamlessly activated at inference for further accuracy gains without\nretraining. When added to state-of-the-art real-time detection models, such as\nYOLOv11-N/S/M, PST yields mAP improvements of 0.9%, 0.5%, and 0.4% on MS COCO\nwith minimal latency impact. Likewise, embedding PST into ResNet-18/50/101 as\nbackbones, boosts ImageNet top-1 accuracy by 6.5%, 1.7%, and 1.0%,\nrespectively. These results demonstrate PST's effectiveness as a simple,\nhardware-friendly enhancement for both detection and classification tasks.\n","authors":["Junyi Hu","Tian Bai","Fengyi Wu","Zhenming Peng","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.12772v2.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.14088v1","updated":"2025-05-20T08:52:28Z","published":"2025-05-20T08:52:28Z","title":"Generalizable Multispectral Land Cover Classification via\n  Frequency-Aware Mixture of Low-Rank Token Experts","summary":"  We introduce Land-MoE, a novel approach for multispectral land cover\nclassification (MLCC). Spectral shift, which emerges from disparities in\nsensors and geospatial conditions, poses a significant challenge in this\ndomain. Existing methods predominantly rely on domain adaptation and\ngeneralization strategies, often utilizing small-scale models that exhibit\nlimited performance. In contrast, Land-MoE addresses these issues by\nhierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts,\nto fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner.\nSpecifically, Land-MoE comprises two key modules: the mixture of low-rank token\nexperts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages\nrank-differentiated tokens to generate diverse feature adjustments for\nindividual instances within multispectral images. By dynamically combining\nlearnable low-rank token experts of varying ranks, it enhances the robustness\nagainst spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on\nthe refined features. This process enables the model to effectively capture\nfrequency band information that is strongly correlated with semantic essence,\nwhile simultaneously suppressing frequency noise irrelevant to the task.\nComprehensive experiments on MLCC tasks involving cross-sensor and\ncross-geospatial setups demonstrate that Land-MoE outperforms existing methods\nby a large margin. Additionally, the proposed approach has also achieved\nstate-of-the-art performance in domain generalization semantic segmentation\ntasks of RGB remote sensing images.\n","authors":["Xi Chen","Shen Yan","Juelin Zhu","Chen Chen","Yu Liu","Maojun Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.14088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14087v1","updated":"2025-05-20T08:49:27Z","published":"2025-05-20T08:49:27Z","title":"Large-Scale Multi-Character Interaction Synthesis","summary":"  Generating large-scale multi-character interactions is a challenging and\nimportant task in character animation. Multi-character interactions involve not\nonly natural interactive motions but also characters coordinated with each\nother for transition. For example, a dance scenario involves characters dancing\nwith partners and also characters coordinated to new partners based on spatial\nand temporal observations. We term such transitions as coordinated interactions\nand decompose them into interaction synthesis and transition planning. Previous\nmethods of single-character animation do not consider interactions that are\ncritical for multiple characters. Deep-learning-based interaction synthesis\nusually focuses on two characters and does not consider transition planning.\nOptimization-based interaction synthesis relies on manually designing objective\nfunctions that may not generalize well. While crowd simulation involves more\ncharacters, their interactions are sparse and passive. We identify two\nchallenges to multi-character interaction synthesis, including the lack of data\nand the planning of transitions among close and dense interactions. Existing\ndatasets either do not have multiple characters or do not have close and dense\ninteractions. The planning of transitions for multi-character close and dense\ninteractions needs both spatial and temporal considerations. We propose a\nconditional generative pipeline comprising a coordinatable multi-character\ninteraction space for interaction synthesis and a transition planning network\nfor coordinations. Our experiments demonstrate the effectiveness of our\nproposed pipeline for multicharacter interaction synthesis and the applications\nfacilitated by our method show the scalability and transferability.\n","authors":["Ziyi Chang","He Wang","George Alex Koulieris","Hubert P. H. Shum"],"pdf_url":"https://arxiv.org/pdf/2505.14087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16301v2","updated":"2025-05-20T08:47:10Z","published":"2024-11-25T11:36:34Z","title":"DiffDesign: Controllable Diffusion with Meta Prior for Efficient\n  Interior Design Generation","summary":"  Interior design is a complex and creative discipline involving aesthetics,\nfunctionality, ergonomics, and materials science. Effective solutions must meet\ndiverse requirements, typically producing multiple deliverables such as\nrenderings and design drawings from various perspectives. Consequently,\ninterior design processes are often inefficient and demand significant\ncreativity. With advances in machine learning, generative models have emerged\nas a promising means of improving efficiency by creating designs from text\ndescriptions or sketches. However, few generative works focus on interior\ndesign, leading to substantial discrepancies between outputs and practical\nneeds, such as differences in size, spatial scope, and the lack of controllable\ngeneration quality. To address these challenges, we propose DiffDesign, a\ncontrollable diffusion model with meta priors for efficient interior design\ngeneration. Specifically, we utilize the generative priors of a 2D diffusion\nmodel pre-trained on a large image dataset as our rendering backbone. We\nfurther guide the denoising process by disentangling cross-attention control\nover design attributes, such as appearance, pose, and size, and introduce an\noptimal transfer-based alignment module to enforce view consistency.\nSimultaneously, we construct an interior design-specific dataset, DesignHelper,\nconsisting of over 400 solutions across more than 15 spatial types and 15\ndesign styles. This dataset helps fine-tune DiffDesign. Extensive experiments\nconducted on various benchmark datasets demonstrate the effectiveness and\nrobustness of DiffDesign.\n","authors":["Yuxuan Yang","Tao Geng","Jingyao Wang","Changwen Zheng","Fuchun Sun"],"pdf_url":"https://arxiv.org/pdf/2411.16301v2.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2403.11083v3","updated":"2025-05-20T08:32:39Z","published":"2024-03-17T04:30:57Z","title":"Customizing Visual-Language Foundation Models for Multi-modal Anomaly\n  Detection and Reasoning","summary":"  Anomaly detection is vital in various industrial scenarios, including the\nidentification of unusual patterns in production lines and the detection of\nmanufacturing defects for quality control. Existing techniques tend to be\nspecialized in individual scenarios and lack generalization capacities. In this\nstudy, our objective is to develop a generic anomaly detection model that can\nbe applied in multiple scenarios. To achieve this, we custom-build generic\nvisual language foundation models that possess extensive knowledge and robust\nreasoning abilities as anomaly detectors and reasoners. Specifically, we\nintroduce a multi-modal prompting strategy that incorporates domain knowledge\nfrom experts as conditions to guide the models. Our approach considers diverse\nprompt types, including task descriptions, class context, normality rules, and\nreference images. In addition, we unify the input representation of\nmulti-modality into a 2D image format, enabling multi-modal anomaly detection\nand reasoning. Our preliminary studies demonstrate that combining visual and\nlanguage prompts as conditions for customizing the models enhances anomaly\ndetection performance. The customized models showcase the ability to detect\nanomalies across different data modalities such as images, point clouds, and\nvideos. Qualitative case studies further highlight the anomaly detection and\nreasoning capabilities, particularly for multi-object scenes and temporal data.\nOur code is publicly available at\nhttps://github.com/Xiaohao-Xu/Customizable-VLM\n","authors":["Xiaohao Xu","Yunkang Cao","Huaxin Zhang","Nong Sang","Xiaonan Huang"],"pdf_url":"https://arxiv.org/pdf/2403.11083v3.pdf","comment":"Best Student Paper Award at IEEE International Conference on Computer\n  Supported Cooperative Work in Design, 2025"},{"id":"http://arxiv.org/abs/2503.18339v4","updated":"2025-05-20T08:27:14Z","published":"2025-03-24T04:44:21Z","title":"GranQ: Granular Zero-Shot Quantization with Channel-Wise Activation\n  Scaling in QAT","summary":"  Zero-shot quantization (ZSQ) enables neural network compression without\noriginal training data, making it a promising solution for restricted data\naccess scenarios. To compensate for the lack of data, recent ZSQ methods\ntypically rely on synthetic inputs generated from the full-precision model.\nHowever, these synthetic inputs often lead to activation distortion, especially\nunder low-bit settings. As a result, existing methods struggle to mitigate this\nissue due to coarse activation scaling. To address this issue, we propose\nGranQ, a novel activation quantization framework that efficiently applies\nper-channel scaling through vectorized computation. In contrast to conventional\nchannel-wise methods, which apply vectorization only to the quantization step,\nGranQ improves efficiency by vectorizing the scaling operation. This design\nallows GranQ to maintain fine-grained quantization granularity with minimal\ncomputational overhead, even in low-bit environments. Extensive experiments\nunder quantization-aware training (QAT) settings demonstrate that GranQ\nconsistently outperforms state-of-the-art ZSQ methods across CIFAR and\nImageNet. In particular, our method achieves up to 5.45% higher accuracy in the\n3-bit setting on CIFAR-100 and even surpasses the full-precision baseline on\nCIFAR-10. Furthermore, GranQ achieves significant speedup in quantization\nlatency over conventional per-channel methods, demonstrating improved\nefficiency. With these findings, we anticipate that GranQ will inspire future\nresearch beyond conventional ZSQ approaches centered on data generation and\nmodel fine-tuning.\n","authors":["Inpyo Hong","Youngwan Jo","Hyojeong Lee","Sunghyun Ahn","Sanghyun Park"],"pdf_url":"https://arxiv.org/pdf/2503.18339v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15060v3","updated":"2025-05-20T08:24:32Z","published":"2025-03-19T09:53:11Z","title":"Conjuring Positive Pairs for Efficient Unification of Representation\n  Learning and Image Synthesis","summary":"  While representation learning and generative modeling seek to understand\nvisual data, unifying both domains remains unexplored. Recent Unified\nSelf-Supervised Learning (SSL) methods have started to bridge the gap between\nboth paradigms. However, they rely solely on semantic token reconstruction,\nwhich requires an external tokenizer during training -- introducing a\nsignificant overhead. In this work, we introduce Sorcen, a novel unified SSL\nframework, incorporating a synergic Contrastive-Reconstruction objective. Our\nContrastive objective, \"Echo Contrast\", leverages the generative capabilities\nof Sorcen, eliminating the need for additional image crops or augmentations\nduring training. Sorcen \"generates\" an echo sample in the semantic token space,\nforming the contrastive positive pair. Sorcen operates exclusively on\nprecomputed tokens, eliminating the need for an online token transformation\nduring training, thereby significantly reducing computational overhead.\nExtensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the\nprevious Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear\nprobing, unconditional image generation, few-shot learning, and transfer\nlearning, respectively, while being 60.8% more efficient. Additionally, Sorcen\nsurpasses previous single-crop MIM SoTA in linear probing and achieves SoTA\nperformance in unconditional image generation, highlighting significant\nimprovements and breakthroughs in Unified SSL models.\n","authors":["Imanol G. Estepa","Jesús M. Rodríguez-de-Vera","Ignacio Sarasúa","Bhalaji Nagarajan","Petia Radeva"],"pdf_url":"https://arxiv.org/pdf/2503.15060v3.pdf","comment":"The source code is available in https://github.com/ImaGonEs/Sorcen"},{"id":"http://arxiv.org/abs/2505.14071v1","updated":"2025-05-20T08:23:08Z","published":"2025-05-20T08:23:08Z","title":"Textual Steering Vectors Can Improve Visual Understanding in Multimodal\n  Large Language Models","summary":"  Steering methods have emerged as effective and targeted tools for guiding\nlarge language models' (LLMs) behavior without modifying their parameters.\nMultimodal large language models (MLLMs), however, do not currently enjoy the\nsame suite of techniques, due in part to their recency and architectural\ndiversity. Inspired by this gap, we investigate whether MLLMs can be steered\nusing vectors derived from their text-only LLM backbone, via sparse\nautoencoders (SAEs), mean shift, and linear probing. We find that text-derived\nsteering consistently enhances multimodal accuracy across diverse MLLM\narchitectures and visual tasks. In particular, mean shift boosts spatial\nrelationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to\n+3.3%, outperforming prompting and exhibiting strong generalization to\nout-of-distribution datasets. These results highlight textual steering vectors\nas a powerful, efficient mechanism for enhancing grounding in MLLMs with\nminimal additional data collection and computational overhead.\n","authors":["Woody Haosheng Gan","Deqing Fu","Julian Asilis","Ollie Liu","Dani Yogatama","Vatsal Sharan","Robin Jia","Willie Neiswanger"],"pdf_url":"https://arxiv.org/pdf/2505.14071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10238v3","updated":"2025-05-20T08:20:41Z","published":"2025-05-15T12:50:29Z","title":"MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation","summary":"  Human image animation has gained increasing attention and developed rapidly\ndue to its broad applications in digital humans. However, existing methods rely\nlargely on 2D-rendered pose images for motion guidance, which limits\ngeneralization and discards essential 3D information for open-world animation.\nTo tackle this problem, we propose MTVCrafter (Motion Tokenization Video\nCrafter), the first framework that directly models raw 3D motion sequences\n(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT\n(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.\nCompared to 2D-rendered pose images, 4D motion tokens offer more robust\nspatio-temporal cues and avoid strict pixel-level alignment between pose image\nand character, enabling more flexible and disentangled control. Then, we\nintroduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention\nwith 4D positional encodings, MV-DiT can effectively leverage motion tokens as\n4D compact yet expressive context for human image animation in the complex 3D\nworld. Hence, it marks a significant step forward in this field and opens a new\ndirection for pose-guided human video generation. Experiments show that our\nMTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,\nsurpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter\nalso generalizes well to diverse open-world characters (single/multiple,\nfull/half-body) across various styles and scenarios. Our video demos and code\nare on: https://github.com/DINGYANB/MTVCrafter.\n","authors":["Yanbo Ding","Xirui Hu","Zhizhi Guo","Yali Wang"],"pdf_url":"https://arxiv.org/pdf/2505.10238v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14068v1","updated":"2025-05-20T08:16:37Z","published":"2025-05-20T08:16:37Z","title":"Place Recognition: A Comprehensive Review, Current Challenges and Future\n  Directions","summary":"  Place recognition is a cornerstone of vehicle navigation and mapping, which\nis pivotal in enabling systems to determine whether a location has been\npreviously visited. This capability is critical for tasks such as loop closure\nin Simultaneous Localization and Mapping (SLAM) and long-term navigation under\nvarying environmental conditions. In this survey, we comprehensively review\nrecent advancements in place recognition, emphasizing three representative\nmethodological paradigms: Convolutional Neural Network (CNN)-based approaches,\nTransformer-based frameworks, and cross-modal strategies. We begin by\nelucidating the significance of place recognition within the broader context of\nautonomous systems. Subsequently, we trace the evolution of CNN-based methods,\nhighlighting their contributions to robust visual descriptor learning and\nscalability in large-scale environments. We then examine the emerging class of\nTransformer-based models, which leverage self-attention mechanisms to capture\nglobal dependencies and offer improved generalization across diverse scenes.\nFurthermore, we discuss cross-modal approaches that integrate heterogeneous\ndata sources such as Lidar, vision, and text description, thereby enhancing\nresilience to viewpoint, illumination, and seasonal variations. We also\nsummarize standard datasets and evaluation metrics widely adopted in the\nliterature. Finally, we identify current research challenges and outline\nprospective directions, including domain adaptation, real-time performance, and\nlifelong learning, to inspire future advancements in this domain. The unified\nframework of leading-edge place recognition methods, i.e., code library, and\nthe results of their experimental evaluations are available at\nhttps://github.com/CV4RA/SOTA-Place-Recognitioner.\n","authors":["Zhenyu Li","Tianyi Shang","Pengjie Xu","Zhaojun Deng"],"pdf_url":"https://arxiv.org/pdf/2505.14068v1.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2505.14064v1","updated":"2025-05-20T08:10:57Z","published":"2025-05-20T08:10:57Z","title":"NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in\n  Brain MRI","summary":"  In many real-world applications, deployed models encounter inputs that differ\nfrom the data seen during training. Out-of-distribution detection identifies\nwhether an input stems from an unseen distribution, while open-world\nrecognition flags such inputs to ensure the system remains robust as\never-emerging, previously $unknown$ categories appear and must be addressed\nwithout retraining. Foundation and vision-language models are pre-trained on\nlarge and diverse datasets with the expectation of broad generalization across\ndomains, including medical imaging. However, benchmarking these models on test\nsets with only a few common outlier types silently collapses the evaluation\nback to a closed-set problem, masking failures on rare or truly novel\nconditions encountered in clinical use.\n  We therefore present $NOVA$, a challenging, real-life $evaluation-only$\nbenchmark of $\\sim$900 brain MRI scans that span 281 rare pathologies and\nheterogeneous acquisition protocols. Each case includes rich clinical\nnarratives and double-blinded expert bounding-box annotations. Together, these\nenable joint assessment of anomaly localisation, visual captioning, and\ndiagnostic reasoning. Because NOVA is never used for training, it serves as an\n$extreme$ stress-test of out-of-distribution generalisation: models must bridge\na distribution gap both in sample appearance and in semantic space. Baseline\nresults with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and\nQwen2.5-VL-72B) reveal substantial performance drops across all tasks,\nestablishing NOVA as a rigorous testbed for advancing models that can detect,\nlocalize, and reason about truly unknown anomalies.\n","authors":["Cosmin I. Bercea","Jun Li","Philipp Raffler","Evamaria O. Riedel","Lena Schmitzer","Angela Kurz","Felix Bitzer","Paula Roßmüller","Julian Canisius","Mirjam L. Beyrle","Che Liu","Wenjia Bai","Bernhard Kainz","Julia A. Schnabel","Benedikt Wiestler"],"pdf_url":"https://arxiv.org/pdf/2505.14064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14062v1","updated":"2025-05-20T08:08:28Z","published":"2025-05-20T08:08:28Z","title":"Scaling Vision Mamba Across Resolutions via Fractal Traversal","summary":"  Vision Mamba has recently emerged as a promising alternative to\nTransformer-based architectures, offering linear complexity in sequence length\nwhile maintaining strong modeling capacity. However, its adaptation to visual\ninputs is hindered by challenges in 2D-to-1D patch serialization and weak\nscalability across input resolutions. Existing serialization strategies such as\nraster scanning disrupt local spatial continuity and limit the model's ability\nto generalize across scales. In this paper, we propose FractalMamba++, a robust\nvision backbone that leverages fractal-based patch serialization via Hilbert\ncurves to preserve spatial locality and enable seamless resolution\nadaptability. To address long-range dependency fading in high-resolution\ninputs, we further introduce a Cross-State Routing (CSR) mechanism that\nenhances global context propagation through selective state reuse.\nAdditionally, we propose a Positional-Relation Capture (PRC) module to recover\nlocal adjacency disrupted by curve inflection points. Extensive experiments on\nimage classification, semantic segmentation, object detection, and change\ndetection demonstrate that FractalMamba++ consistently outperforms previous\nMamba-based backbones, particularly under high-resolution settings.\n","authors":["Bo Li","Haoke Xiao","Lv Tang"],"pdf_url":"https://arxiv.org/pdf/2505.14062v1.pdf","comment":"Work in progressing"},{"id":"http://arxiv.org/abs/2505.14059v1","updated":"2025-05-20T08:03:59Z","published":"2025-05-20T08:03:59Z","title":"Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting","summary":"  Document image parsing is challenging due to its complexly intertwined\nelements such as text paragraphs, figures, formulas, and tables. Current\napproaches either assemble specialized expert models or directly generate\npage-level content autoregressively, facing integration overhead, efficiency\nbottlenecks, and layout structure degradation despite their decent performance.\nTo address these limitations, we present \\textit{Dolphin}\n(\\textit{\\textbf{Do}cument Image \\textbf{P}arsing via \\textbf{H}eterogeneous\nAnchor Prompt\\textbf{in}g}), a novel multimodal document image parsing model\nfollowing an analyze-then-parse paradigm. In the first stage, Dolphin generates\na sequence of layout elements in reading order. These heterogeneous elements,\nserving as anchors and coupled with task-specific prompts, are fed back to\nDolphin for parallel content parsing in the second stage. To train Dolphin, we\nconstruct a large-scale dataset of over 30 million samples, covering\nmulti-granularity parsing tasks. Through comprehensive evaluations on both\nprevalent benchmarks and self-constructed ones, Dolphin achieves\nstate-of-the-art performance across diverse page-level and element-level\nsettings, while ensuring superior efficiency through its lightweight\narchitecture and parallel parsing mechanism. The code and pre-trained models\nare publicly available at https://github.com/ByteDance/Dolphin\n","authors":["Hao Feng","Shu Wei","Xiang Fei","Wei Shi","Yingdong Han","Lei Liao","Jinghui Lu","Binghong Wu","Qi Liu","Chunhui Lin","Jingqun Tang","Hao Liu","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2505.14059v1.pdf","comment":"Accepted to ACL 2025"},{"id":"http://arxiv.org/abs/2505.12007v2","updated":"2025-05-20T08:03:45Z","published":"2025-05-17T13:48:40Z","title":"Multi-modal Collaborative Optimization and Expansion Network for\n  Event-assisted Single-eye Expression Recognition","summary":"  In this paper, we proposed a Multi-modal Collaborative Optimization and\nExpansion Network (MCO-E Net), to use event modalities to resist challenges\nsuch as low light, high exposure, and high dynamic range in single-eye\nexpression recognition tasks. The MCO-E Net introduces two innovative designs:\nMulti-modal Collaborative Optimization Mamba (MCO-Mamba) and Heterogeneous\nCollaborative and Expansion Mixture-of-Experts (HCE-MoE). MCO-Mamba, building\nupon Mamba, leverages dual-modal information to jointly optimize the model,\nfacilitating collaborative interaction and fusion of modal semantics. This\napproach encourages the model to balance the learning of both modalities and\nharness their respective strengths. HCE-MoE, on the other hand, employs a\ndynamic routing mechanism to distribute structurally varied experts (deep,\nattention, and focal), fostering collaborative learning of complementary\nsemantics. This heterogeneous architecture systematically integrates diverse\nfeature extraction paradigms to comprehensively capture expression semantics.\nExtensive experiments demonstrate that our proposed network achieves\ncompetitive performance in the task of single-eye expression recognition,\nespecially under poor lighting conditions.\n","authors":["Runduo Han","Xiuping Liu","Shangxuan Yi","Yi Zhang","Hongchen Tan"],"pdf_url":"https://arxiv.org/pdf/2505.12007v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14049v1","updated":"2025-05-20T07:48:33Z","published":"2025-05-20T07:48:33Z","title":"Learning Concept-Driven Logical Rules for Interpretable and\n  Generalizable Medical Image Classification","summary":"  The pursuit of decision safety in clinical applications highlights the\npotential of concept-based methods in medical imaging. While these models offer\nactive interpretability, they often suffer from concept leakages, where\nunintended information within soft concept representations undermines both\ninterpretability and generalizability. Moreover, most concept-based models\nfocus solely on local explanations (instance-level), neglecting the global\ndecision logic (dataset-level). To address these limitations, we propose\nConcept Rule Learner (CRL), a novel framework to learn Boolean logical rules\nfrom binarized visual concepts. CRL employs logical layers to capture concept\ncorrelations and extract clinically meaningful rules, thereby providing both\nlocal and global interpretability. Experiments on two medical image\nclassification tasks show that CRL achieves competitive performance with\nexisting methods while significantly improving generalizability to\nout-of-distribution data. The code of our work is available at\nhttps://github.com/obiyoag/crl.\n","authors":["Yibo Gao","Hangqi Zhou","Zheyao Gao","Bomin Wang","Shangqi Gao","Sihan Wang","Xiahai Zhuang"],"pdf_url":"https://arxiv.org/pdf/2505.14049v1.pdf","comment":"early accepted by MICCAI 2025"},{"id":"http://arxiv.org/abs/2504.04837v2","updated":"2025-05-20T07:47:12Z","published":"2025-04-07T08:47:36Z","title":"Uni4D: A Unified Self-Supervised Learning Framework for Point Cloud\n  Videos","summary":"  Self-supervised representation learning for point cloud videos remains a\nchallenging problem with two key limitations: (1) existing methods rely on\nexplicit knowledge to learn motion, resulting in suboptimal representations;\n(2) prior Masked AutoEncoder (MAE) frameworks struggle to bridge the gap\nbetween low-level geometry and high-level dynamics in 4D data. In this work, we\npropose a novel self-disentangled MAE for learning expressive, discriminative,\nand transferable 4D representations. To overcome the first limitation, we learn\nmotion by aligning high-level semantics in the latent space \\textit{without any\nexplicit knowledge}. To tackle the second, we introduce a\n\\textit{self-disentangled learning} strategy that incorporates the latent token\nwith the geometry token within a shared decoder, effectively disentangling\nlow-level geometry and high-level semantics. In addition to the reconstruction\nobjective, we employ three alignment objectives to enhance temporal\nunderstanding, including frame-level motion and video-level global information.\nWe show that our pre-trained encoder surprisingly discriminates spatio-temporal\nrepresentation without further fine-tuning. Extensive experiments on\nMSR-Action3D, NTU-RGBD, HOI4D, NvGesture, and SHREC'17 demonstrate the\nsuperiority of our approach in both coarse-grained and fine-grained 4D\ndownstream tasks. Notably, Uni4D improves action segmentation accuracy on HOI4D\nby $+3.8\\%$.\n","authors":["Zhi Zuo","Chenyi Zhuang","Pan Gao","Jie Qin","Hao Feng","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2504.04837v2.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.13279v2","updated":"2025-05-20T07:45:25Z","published":"2025-05-19T16:02:37Z","title":"Event-Driven Dynamic Scene Depth Completion","summary":"  Depth completion in dynamic scenes poses significant challenges due to rapid\nego-motion and object motion, which can severely degrade the quality of input\nmodalities such as RGB images and LiDAR measurements. Conventional RGB-D\nsensors often struggle to align precisely and capture reliable depth under such\nconditions. In contrast, event cameras with their high temporal resolution and\nsensitivity to motion at the pixel level provide complementary cues that are\n%particularly beneficial in dynamic environments.To this end, we propose\nEventDC, the first event-driven depth completion framework. It consists of two\nkey components: Event-Modulated Alignment (EMA) and Local Depth Filtering\n(LDF). Both modules adaptively learn the two fundamental components of\nconvolution operations: offsets and weights conditioned on motion-sensitive\nevent streams. In the encoder, EMA leverages events to modulate the sampling\npositions of RGB-D features to achieve pixel redistribution for improved\nalignment and fusion. In the decoder, LDF refines depth estimations around\nmoving objects by learning motion-aware masks from events. Additionally,\nEventDC incorporates two loss terms to further benefit global alignment and\nenhance local depth recovery. Moreover, we establish the first benchmark for\nevent-based depth completion comprising one real-world and two synthetic\ndatasets to facilitate future research. Extensive experiments on this benchmark\ndemonstrate the superiority of our EventDC.\n","authors":["Zhiqiang Yan","Jianhao Jiao","Zhengxue Wang","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2505.13279v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2505.14043v1","updated":"2025-05-20T07:39:27Z","published":"2025-05-20T07:39:27Z","title":"Selective Structured State Space for Multispectral-fused Small Target\n  Detection","summary":"  Target detection in high-resolution remote sensing imagery faces challenges\ndue to the low recognition accuracy of small targets and high computational\ncosts. The computational complexity of the Transformer architecture increases\nquadratically with image resolution, while Convolutional Neural Networks (CNN)\narchitectures are forced to stack deeper convolutional layers to expand their\nreceptive fields, leading to an explosive growth in computational demands. To\naddress these computational constraints, we leverage Mamba's linear complexity\nfor efficiency. However, Mamba's performance declines for small targets,\nprimarily because small targets occupy a limited area in the image and have\nlimited semantic information. Accurate identification of these small targets\nnecessitates not only Mamba's global attention capabilities but also the\nprecise capture of fine local details. To this end, we enhance Mamba by\ndeveloping the Enhanced Small Target Detection (ESTD) module and the\nConvolutional Attention Residual Gate (CARG) module. The ESTD module bolsters\nlocal attention to capture fine-grained details, while the CARG module, built\nupon Mamba, emphasizes spatial and channel-wise information, collectively\nimproving the model's ability to capture distinctive representations of small\ntargets. Additionally, to highlight the semantic representation of small\ntargets, we design a Mask Enhanced Pixel-level Fusion (MEPF) module for\nmultispectral fusion, which enhances target features by effectively fusing\nvisible and infrared multimodal information.\n","authors":["Qianqian Zhang","WeiJun Wang","Yunxing Liu","Li Zhou","Hao Zhao","Junshe An","Zihan Wang"],"pdf_url":"https://arxiv.org/pdf/2505.14043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14042v1","updated":"2025-05-20T07:39:22Z","published":"2025-05-20T07:39:22Z","title":"Adversarially Pretrained Transformers may be Universally Robust\n  In-Context Learners","summary":"  Adversarial training is one of the most effective adversarial defenses, but\nit incurs a high computational cost. In this study, we show that transformers\nadversarially pretrained on diverse tasks can serve as robust foundation models\nand eliminate the need for adversarial training in downstream tasks.\nSpecifically, we theoretically demonstrate that through in-context learning, a\nsingle adversarially pretrained transformer can robustly generalize to multiple\nunseen tasks without any additional training, i.e., without any parameter\nupdates. This robustness stems from the model's focus on robust features and\nits resistance to attacks that exploit non-predictive features. Besides these\npositive findings, we also identify several limitations. Under certain\nconditions (though unrealistic), no universally robust single-layer\ntransformers exist. Moreover, robust transformers exhibit an\naccuracy--robustness trade-off and require a large number of in-context\ndemonstrations. The code is available at\nhttps://github.com/s-kumano/universally-robust-in-context-learner.\n","authors":["Soichiro Kumano","Hiroshi Kera","Toshihiko Yamasaki"],"pdf_url":"https://arxiv.org/pdf/2505.14042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14029v1","updated":"2025-05-20T07:29:22Z","published":"2025-05-20T07:29:22Z","title":"AppleGrowthVision: A large-scale stereo dataset for phenological\n  analysis, fruit detection, and 3D reconstruction in apple orchards","summary":"  Deep learning has transformed computer vision for precision agriculture, yet\napple orchard monitoring remains limited by dataset constraints. The lack of\ndiverse, realistic datasets and the difficulty of annotating dense,\nheterogeneous scenes. Existing datasets overlook different growth stages and\nstereo imagery, both essential for realistic 3D modeling of orchards and tasks\nlike fruit localization, yield estimation, and structural analysis. To address\nthese gaps, we present AppleGrowthVision, a large-scale dataset comprising two\nsubsets. The first includes 9,317 high resolution stereo images collected from\na farm in Brandenburg (Germany), covering six agriculturally validated growth\nstages over a full growth cycle. The second subset consists of 1,125 densely\nannotated images from the same farm in Brandenburg and one in Pillnitz\n(Germany), containing a total of 31,084 apple labels. AppleGrowthVision\nprovides stereo-image data with agriculturally validated growth stages,\nenabling precise phenological analysis and 3D reconstructions. Extending\nMinneApple with our data improves YOLOv8 performance by 7.69 % in terms of\nF1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by\n31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy\nusing VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges\nthe gap between agricultural science and computer vision, by enabling the\ndevelopment of robust models for fruit detection, growth modeling, and 3D\nanalysis in precision agriculture. Future work includes improving annotation,\nenhancing 3D reconstruction, and extending multimodal analysis across all\ngrowth stages.\n","authors":["Laura-Sophia von Hirschhausen","Jannes S. Magnusson","Mykyta Kovalenko","Fredrik Boye","Tanay Rawat","Peter Eisert","Anna Hilsmann","Sebastian Pretzsch","Sebastian Bosse"],"pdf_url":"https://arxiv.org/pdf/2505.14029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14028v1","updated":"2025-05-20T07:29:21Z","published":"2025-05-20T07:29:21Z","title":"OmniStyle: Filtering High Quality Style Transfer Data at Scale","summary":"  In this paper, we introduce OmniStyle-1M, a large-scale paired style transfer\ndataset comprising over one million content-style-stylized image triplets\nacross 1,000 diverse style categories, each enhanced with textual descriptions\nand instruction prompts. We show that OmniStyle-1M can not only enable\nefficient and scalable of style transfer models through supervised training but\nalso facilitate precise control over target stylization. Especially, to ensure\nthe quality of the dataset, we introduce OmniFilter, a comprehensive style\ntransfer quality assessment framework, which filters high-quality triplets\nbased on content preservation, style consistency, and aesthetic appeal.\nBuilding upon this foundation, we propose OmniStyle, a framework based on the\nDiffusion Transformer (DiT) architecture designed for high-quality and\nefficient style transfer. This framework supports both instruction-guided and\nimage-guided style transfer, generating high resolution outputs with\nexceptional detail. Extensive qualitative and quantitative evaluations\ndemonstrate OmniStyle's superior performance compared to existing approaches,\nhighlighting its efficiency and versatility. OmniStyle-1M and its accompanying\nmethodologies provide a significant contribution to advancing high-quality\nstyle transfer, offering a valuable resource for the research community.\n","authors":["Ye Wang","Ruiqi Liu","Jiang Lin","Fei Liu","Zili Yi","Yilin Wang","Rui Ma"],"pdf_url":"https://arxiv.org/pdf/2505.14028v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2411.12199v3","updated":"2025-05-20T07:26:56Z","published":"2024-11-19T03:30:44Z","title":"Rethinking Text-Promptable Surgical Instrument Segmentation with Robust\n  Framework","summary":"  Surgical instrument segmentation is an essential component of\ncomputer-assisted and robotic surgery systems. Vision-based segmentation models\ntypically produce outputs limited to a predefined set of instrument categories,\nwhich restricts their applicability in interactive systems and robotic task\nautomation. Promptable segmentation methods allow selective predictions based\non textual prompts. However, they often rely on the assumption that the\ninstruments present in the scene are already known, and prompts are generated\naccordingly, limiting their ability to generalize to unseen or dynamically\nemerging instruments. In practical surgical environments, where instrument\nexistence information is not provided, this assumption does not hold\nconsistently, resulting in false-positive segmentation. To address these\nlimitations, we formulate a new task called Robust text-promptable Surgical\nInstrument Segmentation (R-SIS). Under this setting, prompts are issued for all\ncandidate categories without access to instrument presence information. R-SIS\nrequires distinguishing which prompts refer to visible instruments and\ngenerating masks only when such instruments are explicitly present in the\nscene. This setting reflects practical conditions where uncertainty in\ninstrument presence is inherent. We evaluate existing segmentation methods\nunder the R-SIS protocol using surgical video datasets and observe substantial\nfalse-positive predictions in the absence of ground-truth instruments. These\nfindings demonstrate a mismatch between current evaluation protocols and\nreal-world use cases, and support the need for benchmarks that explicitly\naccount for prompt uncertainty and instrument absence.\n","authors":["Tae-Min Choi","Juyoun Park"],"pdf_url":"https://arxiv.org/pdf/2411.12199v3.pdf","comment":"15 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2505.12499v2","updated":"2025-05-20T07:25:42Z","published":"2025-05-18T17:18:06Z","title":"Contrastive Alignment with Semantic Gap-Aware Corrections in Text-Video\n  Retrieval","summary":"  Recent advances in text-video retrieval have been largely driven by\ncontrastive learning frameworks. However, existing methods overlook a key\nsource of optimization tension: the separation between text and video\ndistributions in the representation space (referred to as the modality gap),\nand the prevalence of false negatives in batch sampling. These factors lead to\nconflicting gradients under the InfoNCE loss, impeding stable alignment. To\nmitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces\na learnable, pair-specific increment Delta_ij between text t_i and video v_j to\noffload the tension from the global anchor representation. We first derive the\nideal form of Delta_ij via a coupled multivariate first-order Taylor\napproximation of the InfoNCE loss under a trust-region constraint, revealing it\nas a mechanism for resolving gradient conflicts by guiding updates along a\nlocally optimal descent direction. Due to the high cost of directly computing\nDelta_ij, we introduce a lightweight neural module conditioned on the semantic\ngap between each video-text pair, enabling structure-aware correction guided by\ngradient supervision. To further stabilize learning and promote\ninterpretability, we regularize Delta using three components: a trust-region\nconstraint to prevent oscillation, a directional diversity term to promote\nsemantic coverage, and an information bottleneck to limit redundancy.\nExperiments across four retrieval benchmarks show that GARE consistently\nimproves alignment accuracy and robustness to noisy supervision, confirming the\neffectiveness of gap-aware tension mitigation.\n","authors":["Jian Xiao","Zijie Song","Jialong Hu","Hao Cheng","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2505.12499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14022v1","updated":"2025-05-20T07:25:23Z","published":"2025-05-20T07:25:23Z","title":"Towards Efficient Multi-Scale Deformable Attention on NPU","summary":"  Multi-scale deformable attention (MSDA) is a flexible and powerful feature\nextraction mechanism for visual tasks, but its random-access grid sampling\nstrategy poses significant optimization challenges, especially on\ndomain-specific accelerators such as NPUs. In this work, we present a co-design\napproach that systematically rethinks memory access and computation strategies\nfor MSDA on the Ascend NPU architecture. With this co-design approach, our\nimplementation supports both efficient forward and backward computation, is\nfully adapted for training workloads, and incorporates a suite of\nhardware-aware optimizations. Extensive experiments show that our solution\nachieves up to $5.9\\times$ (forward), $8.9\\times$ (backward), and $7.3\\times$\n(end-to-end training) speedup over the grid sample-based baseline, and\n$1.9\\times$, $2.4\\times$, and $2.0\\times$ acceleration over the latest vendor\nlibrary, respectively.\n","authors":["Chenghuan Huang","Zhigeng Xu","Chong Sun","Chen Li","Ziyang Ma"],"pdf_url":"https://arxiv.org/pdf/2505.14022v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.14021v1","updated":"2025-05-20T07:22:21Z","published":"2025-05-20T07:22:21Z","title":"Adversarial Training from Mean Field Perspective","summary":"  Although adversarial training is known to be effective against adversarial\nexamples, training dynamics are not well understood. In this study, we present\nthe first theoretical analysis of adversarial training in random deep neural\nnetworks without any assumptions on data distributions. We introduce a new\ntheoretical framework based on mean field theory, which addresses the\nlimitations of existing mean field-based approaches. Based on this framework,\nwe derive (empirically tight) upper bounds of $\\ell_q$ norm-based adversarial\nloss with $\\ell_p$ norm-based adversarial examples for various values of $p$\nand $q$. Moreover, we prove that networks without shortcuts are generally not\nadversarially trainable and that adversarial training reduces network capacity.\nWe also show that network width alleviates these issues. Furthermore, we\npresent the various impacts of the input and output dimensions on the upper\nbounds and time evolution of the weight variance.\n","authors":["Soichiro Kumano","Hiroshi Kera","Toshihiko Yamasaki"],"pdf_url":"https://arxiv.org/pdf/2505.14021v1.pdf","comment":"NeurIPS23"},{"id":"http://arxiv.org/abs/2407.10707v2","updated":"2025-05-20T07:19:36Z","published":"2024-07-15T13:25:07Z","title":"Interactive Rendering of Relightable and Animatable Gaussian Avatars","summary":"  Creating relightable and animatable avatars from multi-view or monocular\nvideos is a challenging task for digital human creation and virtual reality\napplications. Previous methods rely on neural radiance fields or ray tracing,\nresulting in slow training and rendering processes. By utilizing Gaussian\nSplatting, we propose a simple and efficient method to decouple body materials\nand lighting from sparse-view or monocular avatar videos, so that the avatar\ncan be rendered simultaneously under novel viewpoints, poses, and lightings at\ninteractive frame rates (6.9 fps). Specifically, we first obtain the canonical\nbody mesh using a signed distance function and assign attributes to each mesh\nvertex. The Gaussians in the canonical space then interpolate from nearby body\nmesh vertices to obtain the attributes. We subsequently deform the Gaussians to\nthe posed space using forward skinning, and combine the learnable environment\nlight with the Gaussian attributes for shading computation. To achieve fast\nshadow modeling, we rasterize the posed body mesh from dense viewpoints to\nobtain the visibility. Our approach is not only simple but also fast enough to\nallow interactive rendering of avatar animation under environmental light\nchanges. Experiments demonstrate that, compared to previous works, our method\ncan render higher quality results at a faster speed on both synthetic and real\ndatasets.\n","authors":["Youyi Zhan","Tianjia Shao","He Wang","Yin Yang","Kun Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.10707v2.pdf","comment":"IEEE Transactions on Visualization and Computer Graphics. Project\n  page https://gapszju.github.io/InteractRAGA . Code\n  https://github.com/1231234zhan/InteractRAGA"},{"id":"http://arxiv.org/abs/2505.14017v1","updated":"2025-05-20T07:18:58Z","published":"2025-05-20T07:18:58Z","title":"End-to-end Cortical Surface Reconstruction from Clinical Magnetic\n  Resonance Images","summary":"  Surface-based cortical analysis is valuable for a variety of neuroimaging\ntasks, such as spatial normalization, parcellation, and gray matter (GM)\nthickness estimation. However, most tools for estimating cortical surfaces work\nexclusively on scans with at least 1 mm isotropic resolution and are tuned to a\nspecific magnetic resonance (MR) contrast, often T1-weighted (T1w). This\nprecludes application using most clinical MR scans, which are very\nheterogeneous in terms of contrast and resolution. Here, we use synthetic\ndomain-randomized data to train the first neural network for explicit\nestimation of cortical surfaces from scans of any contrast and resolution,\nwithout retraining. Our method deforms a template mesh to the white matter (WM)\nsurface, which guarantees topological correctness. This mesh is further\ndeformed to estimate the GM surface. We compare our method to\nrecon-all-clinical (RAC), an implicit surface reconstruction method which is\ncurrently the only other tool capable of processing heterogeneous clinical MR\nscans, on ADNI and a large clinical dataset (n=1,332). We show a approximately\n50 % reduction in cortical thickness error (from 0.50 to 0.24 mm) with respect\nto RAC and better recovery of the aging-related cortical thinning patterns\ndetected by FreeSurfer on high-resolution T1w scans. Our method enables fast\nand accurate surface reconstruction of clinical scans, allowing studies (1)\nwith sample sizes far beyond what is feasible in a research setting, and (2) of\nclinical populations that are difficult to enroll in research studies. The code\nis publicly available at https://github.com/simnibs/brainnet.\n","authors":["Jesper Duemose Nielsen","Karthik Gopinath","Andrew Hoopes","Adrian Dalca","Colin Magdamo","Steven Arnold","Sudeshna Das","Axel Thielscher","Juan Eugenio Iglesias","Oula Puonti"],"pdf_url":"https://arxiv.org/pdf/2505.14017v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.13238v2","updated":"2025-05-20T07:15:43Z","published":"2024-03-20T01:59:43Z","title":"Learning Coherent Matrixized Representation in Latent Space for\n  Volumetric 4D Generation","summary":"  Directly learning to model 4D content, including shape, color, and motion, is\nchallenging. Existing methods rely on pose priors for motion control, resulting\nin limited motion diversity and continuity in details. To address this, we\npropose a framework that generates volumetric 4D sequences, where 3D shapes are\nanimated under given conditions (text-image guidance) with dynamic evolution in\nshape and color across spatial and temporal dimensions, allowing for free\nnavigation and rendering from any direction. We first use a coherent 3D shape\nand color modeling to encode the shape and color of each detailed 3D geometry\nframe into a latent space. Then we propose a matrixized 4D sequence\nrepresentation allowing efficient diffusion model operation. Finally, we\nintroduce spatio-temporal diffusion for 4D volumetric generation under given\nimages and text prompts. Extensive experiments on the ShapeNet, 3DBiCar,\nDeformingThings4D and Objaverse datasets for several tasks demonstrate that our\nmethod effectively learns to generate high quality 3D shapes with consistent\ncolor and coherent mesh animations, improving over the current methods. Our\ncode will be publicly available.\n","authors":["Qitong Yang","Mingtao Feng","Zijie Wu","Shijie Sun","Weisheng Dong","Yaonan Wang","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2403.13238v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14014v1","updated":"2025-05-20T07:08:49Z","published":"2025-05-20T07:08:49Z","title":"EGFormer: Towards Efficient and Generalizable Multimodal Semantic\n  Segmentation","summary":"  Recent efforts have explored multimodal semantic segmentation using various\nbackbone architectures. However, while most methods aim to improve accuracy,\ntheir computational efficiency remains underexplored. To address this, we\npropose EGFormer, an efficient multimodal semantic segmentation framework that\nflexibly integrates an arbitrary number of modalities while significantly\nreducing model parameters and inference time without sacrificing performance.\nOur framework introduces two novel modules. First, the Any-modal Scoring Module\n(ASM) assigns importance scores to each modality independently, enabling\ndynamic ranking based on their feature maps. Second, the Modal Dropping Module\n(MDM) filters out less informative modalities at each stage, selectively\npreserving and aggregating only the most valuable features. This design allows\nthe model to leverage useful information from all available modalities while\ndiscarding redundancy, thus ensuring high segmentation quality. In addition to\nefficiency, we evaluate EGFormer on a synthetic-to-real transfer task to\ndemonstrate its generalizability. Extensive experiments show that EGFormer\nachieves competitive performance with up to 88 percent reduction in parameters\nand 50 percent fewer GFLOPs. Under unsupervised domain adaptation settings, it\nfurther achieves state-of-the-art transfer performance compared to existing\nmethods.\n","authors":["Zelin Zhang","Tao Zhang"," KediLI","Xu Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.14014v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2505.14681v1","updated":"2025-05-20T17:59:16Z","published":"2025-05-20T17:59:16Z","title":"Two Experts Are All You Need for Steering Thinking: Reinforcing\n  Cognitive Effort in MoE Reasoning Models Without Additional Training","summary":"  Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models.\n","authors":["Mengru Wang","Xingyu Chen","Yue Wang","Zhiwei He","Jiahao Xu","Tian Liang","Qiuzhi Liu","Yunzhi Yao","Wenxuan Wang","Ruotian Ma","Haitao Mi","Ningyu Zhang","Zhaopeng Tu","Xiaolong Li","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2505.14681v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2505.14680v1","updated":"2025-05-20T17:59:13Z","published":"2025-05-20T17:59:13Z","title":"NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search","summary":"  Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.\n","authors":["Sunhao Dai","Wenjie Wang","Liang Pang","Jun Xu","See-Kiong Ng","Ji-Rong Wen","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2505.14680v1.pdf","comment":"SIGIR 2025 Perspective Paper"},{"id":"http://arxiv.org/abs/2311.16515v4","updated":"2025-05-20T16:29:22Z","published":"2023-11-25T14:24:49Z","title":"Automatic Synthetic Data and Fine-grained Adaptive Feature Alignment for\n  Composed Person Retrieval","summary":"  Person retrieval has attracted rising attention. Existing methods are mainly\ndivided into two retrieval modes, namely image-only and text-only. However,\nthey are unable to make full use of the available information and are difficult\nto meet diverse application requirements. To address the above limitations, we\npropose a new Composed Person Retrieval (CPR) task, which combines visual and\ntextual queries to identify individuals of interest from large-scale person\nimage databases. Nevertheless, the foremost difficulty of the CPR task is the\nlack of available annotated datasets. Therefore, we first introduce a scalable\nautomatic data synthesis pipeline, which decomposes complex multimodal data\ngeneration into the creation of textual quadruples followed by\nidentity-consistent image synthesis using fine-tuned generative models.\nMeanwhile, a multimodal filtering method is designed to ensure the resulting\nSynCPR dataset retains 1.15 million high-quality and fully synthetic triplets.\nAdditionally, to improve the representation of composed person queries, we\npropose a novel Fine-grained Adaptive Feature Alignment (FAFA) framework\nthrough fine-grained dynamic alignment and masked feature reasoning. Moreover,\nfor objective evaluation, we manually annotate the Image-Text Composed Person\nRetrieval (ITCPR) test set. The extensive experiments demonstrate the\neffectiveness of the SynCPR dataset and the superiority of the proposed FAFA\nframework when compared with the state-of-the-art methods. All code and data\nwill be provided at\nhttps://github.com/Delong-liu-bupt/Composed_Person_Retrieval.\n","authors":["Delong Liu","Haiwen Li","Zhaohui Hou","Zhicheng Zhao","Fei Su","Yuan Dong"],"pdf_url":"https://arxiv.org/pdf/2311.16515v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14558v1","updated":"2025-05-20T16:15:30Z","published":"2025-05-20T16:15:30Z","title":"R2MED: A Benchmark for Reasoning-Driven Medical Retrieval","summary":"  Current medical retrieval benchmarks primarily emphasize lexical or shallow\nsemantic similarity, overlooking the reasoning-intensive demands that are\ncentral to clinical decision-making. In practice, physicians often retrieve\nauthoritative medical evidence to support diagnostic hypotheses. Such evidence\ntypically aligns with an inferred diagnosis rather than the surface form of a\npatient's symptoms, leading to low lexical or semantic overlap between queries\nand relevant documents. To address this gap, we introduce R2MED, the first\nbenchmark explicitly designed for reasoning-driven medical retrieval. It\ncomprises 876 queries spanning three tasks: Q&A reference retrieval, clinical\nevidence retrieval, and clinical case retrieval. These tasks are drawn from\nfive representative medical scenarios and twelve body systems, capturing the\ncomplexity and diversity of real-world medical information needs. We evaluate\n15 widely-used retrieval systems on R2MED and find that even the best model\nachieves only 31.4 nDCG@10, demonstrating the benchmark's difficulty. Classical\nre-ranking and generation-augmented retrieval methods offer only modest\nimprovements. Although large reasoning models improve performance via\nintermediate inference generation, the best results still peak at 41.4 nDCG@10.\nThese findings underscore a substantial gap between current retrieval\ntechniques and the reasoning demands of real clinical tasks. We release R2MED\nas a challenging benchmark to foster the development of next-generation medical\nretrieval systems with enhanced reasoning capabilities. Data and code are\navailable at https://github.com/R2MED/R2MED\n","authors":["Lei Li","Xiao Zhou","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2505.14558v1.pdf","comment":"38 pages, 16 figures"},{"id":"http://arxiv.org/abs/2501.08828v2","updated":"2025-05-20T14:49:55Z","published":"2025-01-15T14:30:13Z","title":"MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents","summary":"  Multimodal document retrieval aims to identify and retrieve various forms of\nmultimodal content, such as figures, tables, charts, and layout information\nfrom extensive documents. Despite its increasing popularity, there is a notable\nlack of a comprehensive and robust benchmark to effectively evaluate the\nperformance of systems in such tasks. To address this gap, this work introduces\na new benchmark, named MMDocIR, that encompasses two distinct tasks: page-level\nand layout-level retrieval. The former evaluates the performance of identifying\nthe most relevant pages within a long document, while the later assesses the\nability of detecting specific layouts, providing a more fine-grained measure\nthan whole-page analysis. A layout refers to a variety of elements, including\ntextual paragraphs, equations, figures, tables, or charts. The MMDocIR\nbenchmark comprises a rich dataset featuring 1,685 questions annotated by\nexperts and 173,843 questions with bootstrapped labels, making it a valuable\nresource in multimodal document retrieval for both training and evaluation.\nThrough rigorous experiments, we demonstrate that (i) visual retrievers\nsignificantly outperform their text counterparts, (ii) MMDocIR training set\neffectively enhances the performance of multimodal document retrieval and (iii)\ntext retrievers leveraging VLM-text significantly outperforms retrievers\nrelying on OCR-text. Our dataset is available at\nhttps://mmdocrag.github.io/MMDocIR/.\n","authors":["Kuicai Dong","Yujing Chang","Xin Deik Goh","Dexun Li","Ruiming Tang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08828v2.pdf","comment":"https://huggingface.co/MMDocIR"},{"id":"http://arxiv.org/abs/2505.14432v1","updated":"2025-05-20T14:39:34Z","published":"2025-05-20T14:39:34Z","title":"Rank-K: Test-Time Reasoning for Listwise Reranking","summary":"  Retrieve-and-rerank is a popular retrieval pipeline because of its ability to\nmake slow but effective rerankers efficient enough at query time by reducing\nthe number of comparisons. Recent works in neural rerankers take advantage of\nlarge language models for their capability in reasoning between queries and\npassages and have achieved state-of-the-art retrieval effectiveness. However,\nsuch rerankers are resource-intensive, even after heavy optimization. In this\nwork, we introduce Rank-K, a listwise passage reranking model that leverages\nthe reasoning capability of the reasoning language model at query time that\nprovides test time scalability to serve hard queries. We show that Rank-K\nimproves retrieval effectiveness by 23\\% over the RankZephyr, the\nstate-of-the-art listwise reranker, when reranking a BM25 initial ranked list\nand 19\\% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is\ninherently a multilingual model, we found that it ranks passages based on\nqueries in different languages as effectively as it does in monolingual\nretrieval.\n","authors":["Eugene Yang","Andrew Yates","Kathryn Ricci","Orion Weller","Vivek Chari","Benjamin Van Durme","Dawn Lawrie"],"pdf_url":"https://arxiv.org/pdf/2505.14432v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.14310v1","updated":"2025-05-20T12:59:16Z","published":"2025-05-20T12:59:16Z","title":"Taming Recommendation Bias with Causal Intervention on Evolving Personal\n  Popularity","summary":"  Popularity bias occurs when popular items are recommended far more frequently\nthan they should be, negatively impacting both user experience and\nrecommendation accuracy. Existing debiasing methods mitigate popularity bias\noften uniformly across all users and only partially consider the time evolution\nof users or items. However, users have different levels of preference for item\npopularity, and this preference is evolving over time. To address these issues,\nwe propose a novel method called CausalEPP (Causal Intervention on Evolving\nPersonal Popularity) for taming recommendation bias, which accounts for the\nevolving personal popularity of users. Specifically, we first introduce a\nmetric called {Evolving Personal Popularity} to quantify each user's preference\nfor popular items. Then, we design a causal graph that integrates evolving\npersonal popularity into the conformity effect, and apply deconfounded training\nto mitigate the popularity bias of the causal graph. During inference, we\nconsider the evolution consistency between users and items to achieve a better\nrecommendation. Empirical studies demonstrate that CausalEPP outperforms\nbaseline methods in reducing popularity bias while improving recommendation\naccuracy.\n","authors":["Shiyin Tan","Dongyuan Li","Renhe Jiang","Zhen Wang","Xingtong Yu","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2505.14310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05863v2","updated":"2025-05-20T12:37:05Z","published":"2025-02-09T11:46:05Z","title":"Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education","summary":"  In AI-facilitated teaching, leveraging various query styles to interpret\nabstract text descriptions is crucial for ensuring high-quality teaching.\nHowever, current retrieval models primarily focus on natural text-image\nretrieval, making them insufficiently tailored to educational scenarios due to\nthe ambiguities in the retrieval process. In this paper, we propose a diverse\nexpression retrieval task tailored to educational scenarios, supporting\nretrieval based on multiple query styles and expressions. We introduce the STEM\nEducation Retrieval Dataset (SER), which contains over 24,000 query pairs of\ndifferent styles, and the Uni-Retrieval, an efficient and style-diversified\nretrieval vision-language model based on prompt tuning. Uni-Retrieval extracts\nquery style features as prototypes and builds a continuously updated Prompt\nBank containing prompt tokens for diverse queries. This bank can updated during\ntest time to represent domain-specific knowledge for different subject\nretrieval scenarios. Our framework demonstrates scalability and robustness by\ndynamically retrieving prompt tokens based on prototype similarity, effectively\nfacilitating learning for unknown queries. Experimental results indicate that\nUni-Retrieval outperforms existing retrieval models in most retrieval tasks.\nThis advancement provides a scalable and precise solution for diverse\neducational needs.\n","authors":["Yanhao Jia","Xinyi Wu","Hao Li","Qinglin Zhang","Yuxiao Hu","Shuai Zhao","Wenqi Fan"],"pdf_url":"https://arxiv.org/pdf/2502.05863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14242v1","updated":"2025-05-20T11:52:17Z","published":"2025-05-20T11:52:17Z","title":"Technical Report on classification of literature related to children\n  speech disorder","summary":"  This technical report presents a natural language processing (NLP)-based\napproach for systematically classifying scientific literature on childhood\nspeech disorders. We retrieved and filtered 4,804 relevant articles published\nafter 2015 from the PubMed database using domain-specific keywords. After\ncleaning and pre-processing the abstracts, we applied two topic modeling\ntechniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify\nlatent thematic structures in the corpus. Our models uncovered 14 clinically\nmeaningful clusters, such as infantile hyperactivity and abnormal epileptic\nbehavior. To improve relevance and precision, we incorporated a custom stop\nword list tailored to speech pathology. Evaluation results showed that the LDA\nmodel achieved a coherence score of 0.42 and a perplexity of -7.5, indicating\nstrong topic coherence and predictive performance. The BERTopic model exhibited\na low proportion of outlier topics (less than 20%), demonstrating its capacity\nto classify heterogeneous literature effectively. These results provide a\nfoundation for automating literature reviews in speech-language pathology.\n","authors":["Ziang Wang","Amir Aryani"],"pdf_url":"https://arxiv.org/pdf/2505.14242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14241v1","updated":"2025-05-20T11:47:56Z","published":"2025-05-20T11:47:56Z","title":"The Limits of Graph Samplers for Training Inductive Recommender Systems:\n  Extended results","summary":"  Inductive Recommender Systems are capable of recommending for new users and\nwith new items thus avoiding the need to retrain after new data reaches the\nsystem. However, these methods are still trained on all the data available,\nrequiring multiple days to train a single model, without counting\nhyperparameter tuning. In this work we focus on graph-based recommender\nsystems, i.e., systems that model the data as a heterogeneous network. In other\napplications, graph sampling allows to study a subgraph and generalize the\nfindings to the original graph. Thus, we investigate the applicability of\nsampling techniques for this task. We test on three real world datasets, with\nthree state-of-the-art inductive methods, and using six different sampling\nmethods. We find that its possible to maintain performance using only 50% of\nthe training data with up to 86% percent decrease in training time; however,\nusing less training data leads to far worse performance. Further, we find that\nwhen it comes to data for recommendations, graph sampling should also account\nfor the temporal dimension. Therefore, we find that if higher data reduction is\nneeded, new graph based sampling techniques should be studied and new inductive\nmethods should be designed.\n","authors":["Theis E. Jendal","Matteo Lissandrini","Peter Dolog","Katja Hose"],"pdf_url":"https://arxiv.org/pdf/2505.14241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02589v3","updated":"2025-05-20T10:58:29Z","published":"2025-03-04T13:12:39Z","title":"MCiteBench: A Multimodal Benchmark for Generating Text with Citations","summary":"  Multimodal Large Language Models (MLLMs) have advanced in integrating diverse\nmodalities but frequently suffer from hallucination. A promising solution to\nmitigate this issue is to generate text with citations, providing a transparent\nchain for verification. However, existing work primarily focuses on generating\ncitations for text-only content, leaving the challenges of multimodal scenarios\nlargely unexplored. In this paper, we introduce MCiteBench, the first benchmark\ndesigned to assess the ability of MLLMs to generate text with citations in\nmultimodal contexts. Our benchmark comprises data derived from academic papers\nand review-rebuttal interactions, featuring diverse information sources and\nmultimodal content. Experimental results reveal that MLLMs struggle to ground\ntheir outputs reliably when handling multimodal input. Further analysis\nuncovers a systematic modality bias and reveals how models internally rely on\ndifferent sources when generating citations, offering insights into model\nbehavior and guiding future directions for multimodal citation tasks.\n","authors":["Caiyu Hu","Yikai Zhang","Tinghui Zhu","Yiwei Ye","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2503.02589v3.pdf","comment":"https://caiyuhu.github.io/MCiteBench/"},{"id":"http://arxiv.org/abs/2505.14180v1","updated":"2025-05-20T10:36:25Z","published":"2025-05-20T10:36:25Z","title":"Bridge the Gap between Past and Future: Siamese Model Optimization for\n  Context-Aware Document Ranking","summary":"  In the realm of information retrieval, users often engage in multi-turn\ninteractions with search engines to acquire information, leading to the\nformation of sequences of user feedback behaviors. Leveraging the session\ncontext has proven to be beneficial for inferring user search intent and\ndocument ranking. A multitude of approaches have been proposed to exploit\nin-session context for improved document ranking. Despite these advances, the\nlimitation of historical session data for capturing evolving user intent\nremains a challenge. In this work, we explore the integration of future\ncontextual information into the session context to enhance document ranking. We\npresent the siamese model optimization framework, comprising a\nhistory-conditioned model and a future-aware model. The former processes only\nthe historical behavior sequence, while the latter integrates both historical\nand anticipated future behaviors. Both models are trained collaboratively using\nthe supervised labels and pseudo labels predicted by the other. The\nhistory-conditioned model, referred to as ForeRanker, progressively learns\nfuture-relevant information to enhance ranking, while it singly uses historical\nsession at inference time. To mitigate inconsistencies during training, we\nintroduce the peer knowledge distillation method with a dynamic gating\nmechanism, allowing models to selectively incorporate contextual information.\nExperimental results on benchmark datasets demonstrate the effectiveness of our\nForeRanker, showcasing its superior performance compared to existing methods.\n","authors":["Songhao Wu","Quan Tu","Mingjie Zhong","Hong Liu","Jia Xu","Jinjie Gu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2505.14180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14179v1","updated":"2025-05-20T10:34:45Z","published":"2025-05-20T10:34:45Z","title":"Enhancing Abstractive Summarization of Scientific Papers Using Structure\n  Information","summary":"  Abstractive summarization of scientific papers has always been a research\nfocus, yet existing methods face two main challenges. First, most summarization\nmodels rely on Encoder-Decoder architectures that treat papers as sequences of\nwords, thus fail to fully capture the structured information inherent in\nscientific papers. Second, existing research often use keyword mapping or\nfeature engineering to identify the structural information, but these methods\nstruggle with the structural flexibility of scientific papers and lack\nrobustness across different disciplines. To address these challenges, we\npropose a two-stage abstractive summarization framework that leverages\nautomatic recognition of structural functions within scientific papers. In the\nfirst stage, we standardize chapter titles from numerous scientific papers and\nconstruct a large-scale dataset for structural function recognition. A\nclassifier is then trained to automatically identify the key structural\ncomponents (e.g., Background, Methods, Results, Discussion), which provides a\nfoundation for generating more balanced summaries. In the second stage, we\nemploy Longformer to capture rich contextual relationships across sections and\ngenerating context-aware summaries. Experiments conducted on two\ndomain-specific scientific paper summarization datasets demonstrate that our\nmethod outperforms advanced baselines, and generates more comprehensive\nsummaries. The code and dataset can be accessed at\nhttps://github.com/tongbao96/code-for-SFR-AS.\n","authors":["Tong Bao","Heng Zhang","Chengzhi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.14179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14156v1","updated":"2025-05-20T10:05:06Z","published":"2025-05-20T10:05:06Z","title":"Unify Graph Learning with Text: Unleashing LLM Potentials for Session\n  Search","summary":"  Session search involves a series of interactive queries and actions to\nfulfill user's complex information need. Current strategies typically\nprioritize sequential modeling for deep semantic understanding, overlooking the\ngraph structure in interactions. While some approaches focus on capturing\nstructural information, they use a generalized representation for documents,\nneglecting the word-level semantic modeling. In this paper, we propose Symbolic\nGraph Ranker (SGR), which aims to take advantage of both text-based and\ngraph-based approaches by leveraging the power of recent Large Language Models\n(LLMs). Concretely, we first introduce a set of symbolic grammar rules to\nconvert session graph into text. This allows integrating session history,\ninteraction process, and task instruction seamlessly as inputs for the LLM.\nMoreover, given the natural discrepancy between LLMs pre-trained on textual\ncorpora, and the symbolic language we produce using our graph-to-text grammar,\nour objective is to enhance LLMs' ability to capture graph structures within a\ntextual format. To achieve this, we introduce a set of self-supervised symbolic\nlearning tasks including link prediction, node content generation, and\ngenerative contrastive learning, to enable LLMs to capture the topological\ninformation from coarse-grained to fine-grained. Experiment results and\ncomprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm\nthe superiority of our approach. Our paradigm also offers a novel and effective\nmethodology that bridges the gap between traditional search strategies and\nmodern LLMs.\n","authors":["Songhao Wu","Quan Tu","Hong Liu","Jia Xu","Zhongyi Liu","Guannan Zhang","Ran Wang","Xiuying Chen","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2505.14156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14149v1","updated":"2025-05-20T09:57:34Z","published":"2025-05-20T09:57:34Z","title":"Enhancing Keyphrase Extraction from Academic Articles Using Section\n  Structure Information","summary":"  The exponential increase in academic papers has significantly increased the\ntime required for researchers to access relevant literature. Keyphrase\nExtraction (KPE) offers a solution to this situation by enabling researchers to\nefficiently retrieve relevant literature. The current study on KPE from\nacademic articles aims to improve the performance of extraction models through\ninnovative approaches using Title and Abstract as input corpora. However, the\nsemantic richness of keywords is significantly constrained by the length of the\nabstract. While full-text-based KPE can address this issue, it simultaneously\nintroduces noise, which significantly diminishes KPE performance. To address\nthis issue, this paper utilized the structural features and section texts\nobtained from the section structure information of academic articles to extract\nkeyphrase from academic papers. The approach consists of two main parts: (1)\nexploring the effect of seven structural features on KPE models, and (2)\nintegrating the extraction results from all section texts used as input corpora\nfor KPE models via a keyphrase integration algorithm to obtain the keyphrase\nintegration result. Furthermore, this paper also examined the effect of the\nclassification quality of section structure on the KPE performance. The results\nshow that incorporating structural features improves KPE performance, though\ndifferent features have varying effects on model efficacy. The keyphrase\nintegration approach yields the best performance, and the classification\nquality of section structure can affect KPE performance. These findings\nindicate that using the section structure information of academic articles\ncontributes to effective KPE from academic articles. The code and dataset\nsupporting this study are available at https://github.com/yan-xinyi/SSB_KPE.\n","authors":["Chengzhi Zhang","Xinyi Yan","Lei Zhao","Yingyi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.14149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14099v1","updated":"2025-05-20T09:01:52Z","published":"2025-05-20T09:01:52Z","title":"Beyond Chains: Bridging Large Language Models and Knowledge Bases in\n  Complex Question Answering","summary":"  Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions using structured knowledge from KBs. While LLM-only approaches offer\ngeneralization, they suffer from outdated knowledge, hallucinations, and lack\nof transparency. Chain-based KG-RAG methods address these issues by\nincorporating external KBs, but are limited to simple chain-structured\nquestions due to the absence of planning and logical structuring. Inspired by\nsemantic parsing methods, we propose PDRR: a four-stage framework consisting of\nPredict, Decompose, Retrieve, and Reason. Our method first predicts the\nquestion type and decomposes the question into structured triples. Then\nretrieves relevant information from KBs and guides the LLM as an agent to\nreason over and complete the decomposed triples. Experimental results\ndemonstrate that PDRR consistently outperforms existing methods across various\nLLM backbones and achieves superior performance on both chain-structured and\nnon-chain complex questions.\n","authors":["Yihua Zhu","Qianying Liu","Akiko Aizawa","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2505.14099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14069v1","updated":"2025-05-20T08:21:00Z","published":"2025-05-20T08:21:00Z","title":"Process vs. Outcome Reward: Which is Better for Agentic RAG\n  Reinforcement Learning","summary":"  Retrieval-augmented generation (RAG) enhances the text generation\ncapabilities of large language models (LLMs) by integrating external knowledge\nand up-to-date information. However, traditional RAG systems are limited by\nstatic workflows and lack the adaptability required for multistep reasoning and\ncomplex task management. To address these limitations, agentic RAG systems\n(e.g., DeepResearch) have been proposed, enabling dynamic retrieval strategies,\niterative context refinement, and adaptive workflows for handling complex\nsearch queries beyond the capabilities of conventional RAG. Recent advances,\nsuch as Search-R1, have demonstrated promising gains using outcome-based\nreinforcement learning, where the correctness of the final answer serves as the\nreward signal. Nevertheless, such outcome-supervised agentic RAG methods face\nchallenges including low exploration efficiency, gradient conflict, and sparse\nreward signals. To overcome these challenges, we propose to utilize\nfine-grained, process-level rewards to improve training stability, reduce\ncomputational costs, and enhance efficiency. Specifically, we introduce a novel\nmethod ReasonRAG that automatically constructs RAG-ProGuide, a high-quality\ndataset providing process-level rewards for (i) query generation, (ii) evidence\nextraction, and (iii) answer generation, thereby enhancing model inherent\ncapabilities via process-supervised reinforcement learning. With the\nprocess-level policy optimization, the proposed framework empowers LLMs to\nautonomously invoke search, generate queries, extract relevant evidence, and\nproduce final answers. Compared to existing approaches such as Search-R1 and\ntraditional RAG systems, ReasonRAG, leveraging RAG-ProGuide, achieves superior\nperformance on five benchmark datasets using only 5k training instances,\nsignificantly fewer than the 90k training instances required by Search-R1.\n","authors":["Wenlin Zhang","Xiangyang Li","Kuicai Dong","Yichao Wang","Pengyue Jia","Xiaopeng Li","Yingyi Zhang","Derong Xu","Zhaocheng Du","Huifeng Guo","Ruiming Tang","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.14069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14057v1","updated":"2025-05-20T08:02:41Z","published":"2025-05-20T08:02:41Z","title":"Field Matters: A lightweight LLM-enhanced Method for CTR Prediction","summary":"  Click-through rate (CTR) prediction is a fundamental task in modern\nrecommender systems. In recent years, the integration of large language models\n(LLMs) has been shown to effectively enhance the performance of traditional CTR\nmethods. However, existing LLM-enhanced methods often require extensive\nprocessing of detailed textual descriptions for large-scale instances or\nuser/item entities, leading to substantial computational overhead. To address\nthis challenge, this work introduces LLaCTR, a novel and lightweight\nLLM-enhanced CTR method that employs a field-level enhancement paradigm.\nSpecifically, LLaCTR first utilizes LLMs to distill crucial and lightweight\nsemantic knowledge from small-scale feature fields through self-supervised\nfield-feature fine-tuning. Subsequently, it leverages this field-level semantic\nknowledge to enhance both feature representation and feature interactions. In\nour experiments, we integrate LLaCTR with six representative CTR models across\nfour datasets, demonstrating its superior performance in terms of both\neffectiveness and efficiency compared to existing LLM-enhanced methods. Our\ncode is available at https://anonymous.4open.science/r/LLaCTR-EC46.\n","authors":["Yu Cui","Feng Liu","Jiawei Chen","Xingyu Lou","Changwang Zhang","Jun Wang","Yuegang Sun","Xiaohu Yang","Can Wang"],"pdf_url":"https://arxiv.org/pdf/2505.14057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12499v2","updated":"2025-05-20T07:25:42Z","published":"2025-05-18T17:18:06Z","title":"Contrastive Alignment with Semantic Gap-Aware Corrections in Text-Video\n  Retrieval","summary":"  Recent advances in text-video retrieval have been largely driven by\ncontrastive learning frameworks. However, existing methods overlook a key\nsource of optimization tension: the separation between text and video\ndistributions in the representation space (referred to as the modality gap),\nand the prevalence of false negatives in batch sampling. These factors lead to\nconflicting gradients under the InfoNCE loss, impeding stable alignment. To\nmitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces\na learnable, pair-specific increment Delta_ij between text t_i and video v_j to\noffload the tension from the global anchor representation. We first derive the\nideal form of Delta_ij via a coupled multivariate first-order Taylor\napproximation of the InfoNCE loss under a trust-region constraint, revealing it\nas a mechanism for resolving gradient conflicts by guiding updates along a\nlocally optimal descent direction. Due to the high cost of directly computing\nDelta_ij, we introduce a lightweight neural module conditioned on the semantic\ngap between each video-text pair, enabling structure-aware correction guided by\ngradient supervision. To further stabilize learning and promote\ninterpretability, we regularize Delta using three components: a trust-region\nconstraint to prevent oscillation, a directional diversity term to promote\nsemantic coverage, and an information bottleneck to limit redundancy.\nExperiments across four retrieval benchmarks show that GARE consistently\nimproves alignment accuracy and robustness to noisy supervision, confirming the\neffectiveness of gap-aware tension mitigation.\n","authors":["Jian Xiao","Zijie Song","Jialong Hu","Hao Cheng","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2505.12499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14020v1","updated":"2025-05-20T07:22:03Z","published":"2025-05-20T07:22:03Z","title":"Disentangled Multi-span Evolutionary Network against Temporal Knowledge\n  Graph Reasoning","summary":"  Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs\n(KGs), incorporate the temporal feature to express the transience of knowledge\nby describing when facts occur. TKG extrapolation aims to infer possible future\nfacts based on known history, which has garnered significant attention in\nrecent years. Some existing methods treat TKG as a sequence of independent\nsubgraphs to model temporal evolution patterns, demonstrating impressive\nreasoning performance. However, they still have limitations: 1) In modeling\nsubgraph semantic evolution, they usually neglect the internal structural\ninteractions between subgraphs, which are actually crucial for encoding TKGs.\n2) They overlook the potential smooth features that do not lead to semantic\nchanges, which should be distinguished from the semantic evolution process.\nTherefore, we propose a novel Disentangled Multi-span Evolutionary Network\n(DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution\nstrategy that captures local neighbor features while perceiving historical\nneighbor semantic information, thus enabling internal interactions between\nsubgraphs during the evolution process. To maximize the capture of semantic\nchange patterns, we design a disentangle component that adaptively separates\nnodes' active and stable features, used to dynamically control the influence of\nhistorical semantics on future evolution. Extensive experiments conducted on\nfour real-world TKG datasets show that DiMNet demonstrates substantial\nperformance in TKG reasoning, and outperforms the state-of-the-art up to 22.7%\nin MRR.\n","authors":["Hao Dong","Ziyue Qiao","Zhiyuan Ning","Qi Hao","Yi Du","Pengyang Wang","Yuanchun Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.14020v1.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2505.10940v2","updated":"2025-05-20T06:58:19Z","published":"2025-05-16T07:26:41Z","title":"Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced\n  Logical Recommendation","summary":"  Recommender systems filter contents/items valuable to users by inferring\npreferences from user features and historical behaviors. Mainstream approaches\nfollow the learning-to-rank paradigm, which focus on discovering and modeling\nitem topics (e.g., categories), and capturing user preferences on these topics\nbased on historical interactions. However, this paradigm often neglects the\nmodeling of user characteristics and their social roles, which are logical\nconfounders influencing the correlated interest and user preference transition.\nTo bridge this gap, we introduce the user role identification task and the\nbehavioral logic modeling task that aim to explicitly model user roles and\nlearn the logical relations between item topics and user social roles. We show\nthat it is possible to explicitly solve these tasks through an efficient\nintegration framework of Large Language Model (LLM) and recommendation systems,\nfor which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal)\nLLM's world knowledge and logic inference ability to extract realistic\ntag-based virtual logic graphs that reveal dynamic and expressive knowledge of\nusers, refining our understanding of user behaviors. On the other hand, TagCF\npresents empirically effective integration modules that take advantage of the\nextracted tag-logic information, augmenting the recommendation performance. We\nconduct both online experiments and offline experiments with industrial and\npublic datasets as verification of TagCF's effectiveness, and we empirically\nshow that the user role modeling strategy is potentially a better choice than\nthe modeling of item topics. Additionally, we provide evidence that the\nextracted logic graphs are empirically a general and transferable knowledge\nthat can benefit a wide range of recommendation tasks.\n","authors":["Qing Yu","Xiaobei Wang","Shuchang Liu","Yandong Bai","Xiaoyu Yang","Xueliang Wang","Chang Meng","Shanshan Wu","Hailan Yang","Huihui Xiao","Xiang Li","Fan Yang","Xiaoqiang Feng","Lantao Hu","Han Li","Kun Gai","Lixin Zou"],"pdf_url":"https://arxiv.org/pdf/2505.10940v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13994v1","updated":"2025-05-20T06:44:34Z","published":"2025-05-20T06:44:34Z","title":"Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven\n  Graph Partitioning","summary":"  Retrieval-Augmented Generation (RAG) systems empower large language models\n(LLMs) with external knowledge, yet struggle with efficiency-accuracy\ntrade-offs when scaling to large knowledge graphs. Existing approaches often\nrely on monolithic graph retrieval, incurring unnecessary latency for simple\nqueries and fragmented reasoning for complex multi-hop questions. To address\nthese challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework\nthat addresses these limitations with question-driven semantic graph\npartitioning and collaborative subgraph retrieval. The innovative framework\nfirst create Semantic Partitioning of Linked Information, then use the\nType-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware\ngraph segmentation manages to divide knowledge graphs into semantically\ncoherent subgraphs, ensuring subgraphs align with different query types, while\nlightweight LLM agents are assigned to partitioned subgraphs, and only relevant\npartitions are activated during retrieval, thus reduce search space while\nenhancing efficiency. Finally, a hierarchical merging module resolves\ninconsistencies across subgraph-derived answers through logical verifications.\nExtensive experimental validation demonstrates considerable improvements\ncompared to existing approaches.\n","authors":["Ruiyi Yang","Hao Xue","Imran Razzak","Hakim Hacid","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2505.13994v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.13974v1","updated":"2025-05-20T06:13:53Z","published":"2025-05-20T06:13:53Z","title":"DIFF: Dual Side-Information Filtering and Fusion for Sequential\n  Recommendation","summary":"  Side-information Integrated Sequential Recommendation (SISR) benefits from\nauxiliary item information to infer hidden user preferences, which is\nparticularly effective for sparse interactions and cold-start scenarios.\nHowever, existing studies face two main challenges. (i) They fail to remove\nnoisy signals in item sequence and (ii) they underutilize the potential of\nside-information integration. To tackle these issues, we propose a novel SISR\nmodel, Dual Side-Information Filtering and Fusion (DIFF), which employs\nfrequency-based noise filtering and dual multi-sequence fusion. Specifically,\nwe convert the item sequence to the frequency domain to filter out noisy\nshort-term fluctuations in user interests. We then combine early and\nintermediate fusion to capture diverse relationships across item IDs and\nattributes. Thanks to our innovative filtering and fusion strategy, DIFF is\nmore robust in learning subtle and complex item correlations in the sequence.\nDIFF outperforms state-of-the-art SISR models, achieving improvements of up to\n14.1% and 12.5% in Recall@20 and NDCG@20 across four benchmark datasets.\n","authors":["Hye-young Kim","Minjin Choi","Sunkyung Lee","Ilwoong Baek","Jongwuk Lee"],"pdf_url":"https://arxiv.org/pdf/2505.13974v1.pdf","comment":"Accepted by SIGIR 2025. 10 pages"},{"id":"http://arxiv.org/abs/2411.13892v2","updated":"2025-05-20T05:38:53Z","published":"2024-11-21T07:12:47Z","title":"How Does Topology Bias Distort Message Passing? A Dirichlet Energy\n  Perspective","summary":"  Graph-based recommender systems have achieved remarkable effectiveness by\nmodeling high-order interactions between users and items. However, such\napproaches are significantly undermined by popularity bias, which distorts the\ninteraction graph's structure, referred to as topology bias. This leads to\noverrepresentation of popular items, thereby reinforcing biases and fairness\nissues through the user-system feedback loop. Despite attempts to study this\neffect, most prior work focuses on the embedding or gradient level bias,\noverlooking how topology bias fundamentally distorts the message passing\nprocess itself. We bridge this gap by providing an empirical and theoretical\nanalysis from a Dirichlet energy perspective, revealing that graph message\npassing inherently amplifies topology bias and consistently benefits highly\nconnected nodes. To address these limitations, we propose Test-time Simplicial\nPropagation (TSP), which extends message passing to higher-order simplicial\ncomplexes. By incorporating richer structures beyond pairwise connections, TSP\nmitigates harmful topology bias and substantially improves the representation\nand recommendation of long-tail items during inference. Extensive experiments\nacross five real-world datasets demonstrate the superiority of our approach in\nmitigating topology bias and enhancing recommendation quality.\n","authors":["Yanbiao Ji","Yue Ding","Dan Luo","Chang Liu","Yuxiang Lu","Xin Xin","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2411.13892v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13950v1","updated":"2025-05-20T05:29:01Z","published":"2025-05-20T05:29:01Z","title":"Benchmarking the Myopic Trap: Positional Bias in Information Retrieval","summary":"  This study investigates a specific form of positional bias, termed the Myopic\nTrap, where retrieval models disproportionately attend to the early parts of\ndocuments while overlooking relevant information that appears later. To\nsystematically quantify this phenomenon, we propose a semantics-preserving\nevaluation framework that repurposes the existing NLP datasets into\nposition-aware retrieval benchmarks. By evaluating the SOTA models of full\nretrieval pipeline, including BM25, embedding models, ColBERT-style\nlate-interaction models, and reranker models, we offer a broader empirical\nperspective on positional bias than prior work. Experimental results show that\nembedding models and ColBERT-style models exhibit significant performance\ndegradation when query-related content is shifted toward later positions,\nindicating a pronounced head bias. Notably, under the same training\nconfiguration, ColBERT-style approach show greater potential for mitigating\npositional bias compared to the traditional single-vector approach. In\ncontrast, BM25 and reranker models remain largely unaffected by such\nperturbations, underscoring their robustness to positional bias. Code and data\nare publicly available at: www.github.com/NovaSearch-Team/RAG-Retrieval.\n","authors":["Ziyang Zeng","Dun Zhang","Jiacheng Li","Panxiang Zou","Yuqing Yang"],"pdf_url":"https://arxiv.org/pdf/2505.13950v1.pdf","comment":"10 pages, 3 figures, 4 tables. Under review"},{"id":"http://arxiv.org/abs/2504.01281v3","updated":"2025-05-20T04:52:21Z","published":"2025-04-02T01:16:10Z","title":"Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding","summary":"  We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.\n","authors":["Sakhinana Sagar Srinivas","Akash Das","Shivam Gupta","Venkataramana Runkana"],"pdf_url":"https://arxiv.org/pdf/2504.01281v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13928v1","updated":"2025-05-20T04:49:09Z","published":"2025-05-20T04:49:09Z","title":"LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts","summary":"  Long videos contain a vast amount of information, making video-text retrieval\nan essential and challenging task in multimodal learning. However, existing\nbenchmarks suffer from limited video duration, low-quality captions, and coarse\nannotation granularity, which hinder the evaluation of advanced video-text\nretrieval methods. To address these limitations, we introduce LoVR, a benchmark\nspecifically designed for long video-text retrieval. LoVR contains 467 long\nvideos and over 40,804 fine-grained clips with high-quality captions. To\novercome the issue of poor machine-generated annotations, we propose an\nefficient caption generation framework that integrates VLM automatic\ngeneration, caption quality scoring, and dynamic refinement. This pipeline\nimproves annotation accuracy while maintaining scalability. Furthermore, we\nintroduce a semantic fusion method to generate coherent full-video captions\nwithout losing important contextual information. Our benchmark introduces\nlonger videos, more detailed captions, and a larger-scale dataset, presenting\nnew challenges for video understanding and retrieval. Extensive experiments on\nvarious advanced embedding models demonstrate that LoVR is a challenging\nbenchmark, revealing the limitations of current approaches and providing\nvaluable insights for future research. We release the code and dataset link at\nhttps://github.com/TechNomad-ds/LoVR-benchmark\n","authors":["Qifeng Cai","Hao Liang","Hejun Dong","Meiyi Qiang","Ruichuan An","Zhaoyang Han","Zhengzhou Zhu","Bin Cui","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.13928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13895v1","updated":"2025-05-20T03:59:26Z","published":"2025-05-20T03:59:26Z","title":"VulCPE: Context-Aware Cybersecurity Vulnerability Retrieval and\n  Management","summary":"  The dynamic landscape of cybersecurity demands precise and scalable solutions\nfor vulnerability management in heterogeneous systems, where\nconfiguration-specific vulnerabilities are often misidentified due to\ninconsistent data in databases like the National Vulnerability Database (NVD).\nInaccurate Common Platform Enumeration (CPE) data in NVD further leads to false\npositives and incomplete vulnerability retrieval. Informed by our systematic\nanalysis of CPE and CVEdeails data, revealing more than 50% vendor name\ninconsistencies, we propose VulCPE, a framework that standardizes data and\nmodels configuration dependencies using a unified CPE schema (uCPE), entity\nrecognition, relation extraction, and graph-based modeling. VulCPE achieves\nsuperior retrieval precision (0.766) and coverage (0.926) over existing tools.\nVulCPE ensures precise, context-aware vulnerability management, enhancing cyber\nresilience.\n","authors":["Yuning Jiang","Feiyang Shang","Freedy Tan Wei You","Huilin Wang","Chia Ren Cong","Qiaoran Meng","Nay Oo","Hoon Wei Lim","Biplab Sikdar"],"pdf_url":"https://arxiv.org/pdf/2505.13895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13881v1","updated":"2025-05-20T03:36:54Z","published":"2025-05-20T03:36:54Z","title":"TranSUN: A Preemptive Paradigm to Eradicate Retransformation Bias\n  Intrinsically from Regression Models in Recommender Systems","summary":"  Regression models are crucial in recommender systems. However,\nretransformation bias problem has been conspicuously neglected within the\ncommunity. While many works in other fields have devised effective bias\ncorrection methods, all of them are post-hoc cures externally to the model,\nfacing practical challenges when applied to real-world recommender systems.\nHence, we propose a preemptive paradigm to eradicate the bias intrinsically\nfrom the models via minor model refinement. Specifically, a novel TranSUN\nmethod is proposed with a joint bias learning manner to offer theoretically\nguaranteed unbiasedness under empirical superior convergence. It is further\ngeneralized into a novel generic regression model family, termed Generalized\nTranSUN (GTS), which not only offers more theoretical insights but also serves\nas a generic framework for flexibly developing various bias-free models.\nComprehensive experimental results demonstrate the superiority of our methods\nacross data from various domains, which have been successfully deployed in two\nreal-world industrial recommendation scenarios, i.e. product and short video\nrecommendation scenarios in Guess What You Like business domain in the homepage\nof Taobao App (a leading e-commerce platform), to serve the major online\ntraffic. Codes will be released after this paper is published.\n","authors":["Jiahao Yu","Haozhuang Liu","Yeqiu Yang","Lu Chen","Wu Jian","Yuning Jiang","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.13881v1.pdf","comment":"22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.01776v5","updated":"2025-05-20T02:32:33Z","published":"2025-03-03T17:59:48Z","title":"Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation","summary":"  Many large-scale systems rely on high-quality deep representations\n(embeddings) to facilitate tasks like retrieval, search, and generative\nmodeling. Matryoshka Representation Learning (MRL) recently emerged as a\nsolution for adaptive embedding lengths, but it requires full model retraining\nand suffers from noticeable performance degradations at short lengths. In this\npaper, we show that sparse coding offers a compelling alternative for achieving\nadaptive representation with minimal overhead and higher fidelity. We propose\nContrastive Sparse Representation (CSR), a method that sparsifies pre-trained\nembeddings into a high-dimensional but selectively activated feature space. By\nleveraging lightweight autoencoding and task-aware contrastive objectives, CSR\npreserves semantic quality while allowing flexible, cost-effective inference at\ndifferent sparsity levels. Extensive experiments on image, text, and multimodal\nbenchmarks demonstrate that CSR consistently outperforms MRL in terms of both\naccuracy and retrieval speed-often by large margins-while also cutting training\ntime to a fraction of that required by MRL. Our results establish sparse coding\nas a powerful paradigm for adaptive representation learning in real-world\napplications where efficiency and fidelity are both paramount. Code is\navailable at https://github.com/neilwen987/CSR_Adaptive_Rep\n","authors":["Tiansheng Wen","Yifei Wang","Zequn Zeng","Zhong Peng","Yudi Su","Xinyang Liu","Bo Chen","Hongwei Liu","Stefanie Jegelka","Chenyu You"],"pdf_url":"https://arxiv.org/pdf/2503.01776v5.pdf","comment":"Accepted by ICML2025"}]},"2025-05-19T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2502.14706v3","updated":"2025-05-19T23:24:52Z","published":"2025-02-20T16:30:45Z","title":"Building reliable sim driving agents by scaling self-play","summary":"  Simulation agents are essential for designing and testing systems that\ninteract with humans, such as autonomous vehicles (AVs). These agents serve\nvarious purposes, from benchmarking AV performance to stress-testing system\nlimits, but all applications share one key requirement: reliability. To enable\nsound experimentation, a simulation agent must behave as intended. It should\nminimize actions that may lead to undesired outcomes, such as collisions, which\ncan distort the signal-to-noise ratio in analyses. As a foundation for reliable\nsim agents, we propose scaling self-play to thousands of scenarios on the Waymo\nOpen Motion Dataset under semi-realistic limits on human perception and\ncontrol. Training from scratch on a single GPU, our agents solve almost the\nfull training set within a day. They generalize to unseen test scenes,\nachieving a 99.8% goal completion rate with less than 0.8% combined collision\nand off-road incidents across 10,000 held-out scenarios. Beyond in-distribution\ngeneralization, our agents show partial robustness to out-of-distribution\nscenes and can be fine-tuned in minutes to reach near-perfect performance in\nsuch cases. We open-source the pre-trained agents and integrate them with a\nbatched multi-agent simulator. Demonstrations of agent behaviors can be viewed\nat https://sites.google.com/view/reliable-sim-agents, and we open-source our\nagents at https://github.com/Emerge-Lab/gpudrive.\n","authors":["Daphne Cornelisse","Aarav Pandya","Kevin Joseph","Joseph Suárez","Eugene Vinitsky"],"pdf_url":"https://arxiv.org/pdf/2502.14706v3.pdf","comment":"v3"},{"id":"http://arxiv.org/abs/2505.13762v1","updated":"2025-05-19T22:28:05Z","published":"2025-05-19T22:28:05Z","title":"From Structural Design to Dynamics Modeling: Control-Oriented\n  Development of a 3-RRR Parallel Ankle Rehabilitation Robot","summary":"  This paper presents the development of a wearable ankle rehabilitation robot\nbased on a 3-RRR spherical parallel mechanism (SPM) to support multi-DOF\nrecovery through pitch, roll, and yaw motions. The system features a compact,\nergonomic structure designed for comfort, safety, and compatibility with ankle\nbiomechanics. A complete design-to-dynamics pipeline has been implemented,\nincluding structural design, kinematic modeling for motion planning, and\nLagrangian-based dynamic modeling for torque estimation and simulation\nanalysis. Preliminary simulations verify stable joint coordination and smooth\nmotion tracking under representative rehabilitation trajectories. The control\nframework is currently being developed to enhance responsiveness across the\nworkspace. Future work will focus on integrating personalized modeling and\nadaptive strategies to address kinematic singularities through model based\ncontrol. This work establishes a foundational platform for intelligent,\npersonalized ankle rehabilitation, enabling both static training and potential\nextension to gait-phase-timed assistance.\n","authors":["Siyuan Zhang","Yufei Zhang","Junlin Lyu","Sunil K. Agrawal"],"pdf_url":"https://arxiv.org/pdf/2505.13762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02145v2","updated":"2025-05-19T21:23:20Z","published":"2025-02-04T09:19:13Z","title":"From Words to Collisions: LLM-Guided Evaluation and Adversarial\n  Generation of Safety-Critical Driving Scenarios","summary":"  Ensuring the safety of autonomous vehicles requires virtual scenario-based\ntesting, which depends on the robust evaluation and generation of\nsafety-critical scenarios. So far, researchers have used scenario-based testing\nframeworks that rely heavily on handcrafted scenarios as safety metrics. To\nreduce the effort of human interpretation and overcome the limited scalability\nof these approaches, we combine Large Language Models (LLMs) with structured\nscenario parsing and prompt engineering to automatically evaluate and generate\nsafety-critical driving scenarios. We introduce Cartesian and Ego-centric\nprompt strategies for scenario evaluation, and an adversarial generation module\nthat modifies trajectories of risk-inducing vehicles (ego-attackers) to create\ncritical scenarios. We validate our approach using a 2D simulation framework\nand multiple pre-trained LLMs. The results show that the evaluation module\neffectively detects collision scenarios and infers scenario safety. Meanwhile,\nthe new generation module identifies high-risk agents and synthesizes\nrealistic, safety-critical scenarios. We conclude that an LLM equipped with\ndomain-informed prompting techniques can effectively evaluate and generate\nsafety-critical driving scenarios, reducing dependence on handcrafted metrics.\nWe release our open-source code and scenarios at:\nhttps://github.com/TUM-AVS/From-Words-to-Collisions.\n","authors":["Yuan Gao","Mattia Piccinini","Korbinian Moller","Johannes Betz"],"pdf_url":"https://arxiv.org/pdf/2502.02145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13729v1","updated":"2025-05-19T20:58:06Z","published":"2025-05-19T20:58:06Z","title":"SayCoNav: Utilizing Large Language Models for Adaptive Collaboration in\n  Decentralized Multi-Robot Navigation","summary":"  Adaptive collaboration is critical to a team of autonomous robots to perform\ncomplicated navigation tasks in large-scale unknown environments. An effective\ncollaboration strategy should be determined and adapted according to each\nrobot's skills and current status to successfully achieve the shared goal. We\npresent SayCoNav, a new approach that leverages large language models (LLMs)\nfor automatically generating this collaboration strategy among a team of\nrobots. Building on the collaboration strategy, each robot uses the LLM to\ngenerate its plans and actions in a decentralized way. By sharing information\nto each other during navigation, each robot also continuously updates its\nstep-by-step plans accordingly. We evaluate SayCoNav on Multi-Object Navigation\n(MultiON) tasks, that require the team of the robots to utilize their\ncomplementary strengths to efficiently search multiple different objects in\nunknown environments. By validating SayCoNav with varied team compositions and\nconditions against baseline methods, our experimental results show that\nSayCoNav can improve search efficiency by at most 44.28% through effective\ncollaboration among heterogeneous robots. It can also dynamically adapt to the\nchanging conditions during task execution.\n","authors":["Abhinav Rajvanshi","Pritish Sahu","Tixiao Shan","Karan Sikka","Han-Pang Chiu"],"pdf_url":"https://arxiv.org/pdf/2505.13729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15525v2","updated":"2025-05-19T20:46:35Z","published":"2025-02-21T15:27:29Z","title":"Enhanced Probabilistic Collision Detection for Motion Planning Under\n  Sensing Uncertainty","summary":"  Probabilistic collision detection (PCD) is essential in motion planning for\nrobots operating in unstructured environments, where considering sensing\nuncertainty helps prevent damage. Existing PCD methods mainly used simplified\ngeometric models and addressed only position estimation errors. This paper\npresents an enhanced PCD method with two key advancements: (a) using\nsuperquadrics for more accurate shape approximation and (b) accounting for both\nposition and orientation estimation errors to improve robustness under sensing\nuncertainty. Our method first computes an enlarged surface for each object that\nencapsulates its observed rotated copies, thereby addressing the orientation\nestimation errors. Then, the collision probability under the position\nestimation errors is formulated as a chance-constraint problem that is solved\nwith a tight upper bound. Both the two steps leverage the recently developed\nnormal parameterization of superquadric surfaces. Results show that our PCD\nmethod is twice as close to the Monte-Carlo sampled baseline as the best\nexisting PCD method and reduces path length by 30% and planning time by 37%,\nrespectively. A Real2Sim2Real pipeline further validates the importance of\nconsidering orientation estimation errors, showing that the collision\nprobability of executing the planned path in simulation is only 2%, compared to\n9% and 29% when considering only position estimation errors or none at all.\n","authors":["Xiaoli Wang","Sipu Ruan","Xin Meng","Gregory Chirikjian"],"pdf_url":"https://arxiv.org/pdf/2502.15525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13722v1","updated":"2025-05-19T20:40:52Z","published":"2025-05-19T20:40:52Z","title":"Practice Makes Perfect: A Study of Digital Twin Technology for Assembly\n  and Problem-solving using Lunar Surface Telerobotics","summary":"  Robotic systems that can traverse planetary or lunar surfaces to collect\nenvironmental data and perform physical manipulation tasks, such as assembling\nequipment or conducting mining operations, are envisioned to form the backbone\nof future human activities in space. However, the environmental conditions in\nwhich these robots, or \"rovers,\" operate present challenges toward achieving\nfully autonomous solutions, meaning that rover missions will require some\ndegree of human teleoperation or supervision for the foreseeable future. As a\nresult, human operators require training to successfully direct rovers and\navoid costly errors or mission failures, as well as the ability to recover from\nany issues that arise on the fly during mission activities. While analog\nenvironments, such as JPL's Mars Yard, can help with such training by\nsimulating surface environments in the real world, access to such resources may\nbe rare and expensive. As an alternative or supplement to such physical\nanalogs, we explore the design and evaluation of a virtual reality digital twin\nsystem to train human teleoperation of robotic rovers with mechanical arms for\nspace mission activities. We conducted an experiment with 24 human operators to\ninvestigate how our digital twin system can support human teleoperation of\nrovers in both pre-mission training and in real-time problem solving in a mock\nlunar mission in which users directed a physical rover in the context of\ndeploying dipole radio antennas. We found that operators who first trained with\nthe digital twin showed a 28% decrease in mission completion time, an 85%\ndecrease in unrecoverable errors, as well as improved mental markers, including\ndecreased cognitive load and increased situation awareness.\n","authors":["Xavier O'Keefe","Katy McCutchan","Alexis Muniz","Jack Burns","Daniel Szafir"],"pdf_url":"https://arxiv.org/pdf/2505.13722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09561v2","updated":"2025-05-19T20:37:41Z","published":"2025-05-14T17:00:47Z","title":"Learning Long-Context Diffusion Policies via Past-Token Prediction","summary":"  Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x.\n","authors":["Marcel Torne","Andy Tang","Yuejiang Liu","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2505.09561v2.pdf","comment":"Videos are available at https://long-context-dp.github.io"},{"id":"http://arxiv.org/abs/2505.13715v1","updated":"2025-05-19T20:24:43Z","published":"2025-05-19T20:24:43Z","title":"Dynamic Bipedal MPC with Foot-level Obstacle Avoidance and Adjustable\n  Step Timing","summary":"  Collision-free planning is essential for bipedal robots operating within\nunstructured environments. This paper presents a real-time Model Predictive\nControl (MPC) framework that addresses both body and foot avoidance for dynamic\nbipedal robots. Our contribution is two-fold: we introduce (1) a novel\nformulation for adjusting step timing to facilitate faster body avoidance and\n(2) a novel 3D foot-avoidance formulation that implicitly selects swing\ntrajectories and footholds that either steps over or navigate around obstacles\nwith awareness of Center of Mass (COM) dynamics. We achieve body avoidance by\napplying a half-space relaxation of the safe region but introduce a switching\nheuristic based on tracking error to detect a need to change foot-timing\nschedules. To enable foot avoidance and viable landing footholds on all sides\nof foot-level obstacles, we decompose the non-convex safe region on the ground\ninto several convex polygons and use Mixed-Integer Quadratic Programming to\ndetermine the optimal candidate. We found that introducing a soft\nminimum-travel-distance constraint is effective in preventing the MPC from\nbeing trapped in local minima that can stall half-space relaxation methods\nbehind obstacles. We demonstrated the proposed algorithms on multibody\nsimulations on the bipedal robot platforms, Cassie and Digit, as well as\nhardware experiments on Digit.\n","authors":["Tianze Wang","Christian Hubicki"],"pdf_url":"https://arxiv.org/pdf/2505.13715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05210v3","updated":"2025-05-19T19:43:40Z","published":"2024-05-08T16:58:22Z","title":"TCAFF: Temporal Consistency for Robot Frame Alignment","summary":"  In the field of collaborative robotics, the ability to communicate spatial\ninformation like planned trajectories and shared environment information is\ncrucial. When no global position information is available (e.g., indoor or\nGPS-denied environments), agents must align their coordinate frames before\nshared spatial information can be properly expressed and interpreted.\nCoordinate frame alignment is particularly difficult when robots have no\ninitial alignment and are affected by odometry drift. To this end, we develop a\nnovel multiple hypothesis algorithm, called TCAFF, for aligning the coordinate\nframes of neighboring robots. TCAFF considers potential alignments from\nassociating sparse open-set object maps and leverages temporal consistency to\ndetermine an initial alignment and correct for drift, all without any initial\nknowledge of neighboring robot poses. We demonstrate TCAFF being used for frame\nalignment in a collaborative object tracking application on a team of four\nrobots tracking six pedestrians and show that TCAFF enables robots to achieve a\ntracking accuracy similar to that of a system with ground truth localization.\nThe code and hardware dataset are available at\nhttps://github.com/mit-acl/tcaff.\n","authors":["Mason B. Peterson","Parker C. Lusk","Antonio Avila","Jonathan P. How"],"pdf_url":"https://arxiv.org/pdf/2405.05210v3.pdf","comment":"7 pages, 6 figures, accepted to ICRA 2025"},{"id":"http://arxiv.org/abs/2505.13674v1","updated":"2025-05-19T19:23:47Z","published":"2025-05-19T19:23:47Z","title":"Risk-Averse Traversal of Graphs with Stochastic and Correlated Edge\n  Costs for Safe Global Planetary Mobility","summary":"  In robotic planetary surface exploration, strategic mobility planning is an\nimportant task that involves finding candidate long-distance routes on orbital\nmaps and identifying segments with uncertain traversability. Then, expert human\noperators establish safe, adaptive traverse plans based on the actual\nnavigation difficulties encountered in these uncertain areas. In this paper, we\nformalize this challenge as a new, risk-averse variant of the Canadian\nTraveller Problem (CTP) tailored to global planetary mobility. The objective is\nto find a traverse policy minimizing a conditional value-at-risk (CVaR)\ncriterion, which is a risk measure with an intuitive interpretation. We propose\na novel search algorithm that finds exact CVaR-optimal policies. Our approach\nleverages well-established optimal AND-OR search techniques intended for\n(risk-agnostic) expectation minimization and extends these methods to the\nrisk-averse domain. We validate our approach through simulated long-distance\nplanetary surface traverses; we employ real orbital maps of the Martian surface\nto construct problem instances and use terrain maps to express traversal\nprobabilities in uncertain regions. Our results illustrate different adaptive\ndecision-making schemes depending on the level of risk aversion. Additionally,\nour problem setup allows accounting for traversability correlations between\nsimilar areas of the environment. In such a case, we empirically demonstrate\nhow information-seeking detours can mitigate risk.\n","authors":["Olivier Lamarre","Jonathan Kelly"],"pdf_url":"https://arxiv.org/pdf/2505.13674v1.pdf","comment":"Submitted to the Autonomous Robots journal"},{"id":"http://arxiv.org/abs/2505.13669v1","updated":"2025-05-19T19:17:06Z","published":"2025-05-19T19:17:06Z","title":"GeoVLM: Improving Automated Vehicle Geolocalisation Using\n  Vision-Language Matching","summary":"  Cross-view geo-localisation identifies coarse geographical position of an\nautomated vehicle by matching a ground-level image to a geo-tagged satellite\nimage from a database. Despite the advancements in Cross-view geo-localisation,\nsignificant challenges still persist such as similar looking scenes which makes\nit challenging to find the correct match as the top match. Existing approaches\nreach high recall rates but they still fail to rank the correct image as the\ntop match. To address this challenge, this paper proposes GeoVLM, a novel\napproach which uses the zero-shot capabilities of vision language models to\nenable cross-view geo-localisation using interpretable cross-view language\ndescriptions. GeoVLM is a trainable reranking approach which improves the best\nmatch accuracy of cross-view geo-localisation. GeoVLM is evaluated on standard\nbenchmark VIGOR and University-1652 and also through real-life driving\nenvironments using Cross-View United Kingdom, a new benchmark dataset\nintroduced in this paper. The results of the paper show that GeoVLM improves\nretrieval performance of cross-view geo-localisation compared to the\nstate-of-the-art methods with the help of explainable natural language\ndescriptions. The code is available at\nhttps://github.com/CAV-Research-Lab/GeoVLM\n","authors":["Barkin Dagda","Muhammad Awais","Saber Fallah"],"pdf_url":"https://arxiv.org/pdf/2505.13669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13667v1","updated":"2025-05-19T19:12:29Z","published":"2025-05-19T19:12:29Z","title":"Adaptive Diffusion Constrained Sampling for Bimanual Robot Manipulation","summary":"  Coordinated multi-arm manipulation requires satisfying multiple simultaneous\ngeometric constraints across high-dimensional configuration spaces, which poses\na significant challenge for traditional planning and control methods. In this\nwork, we propose Adaptive Diffusion Constrained Sampling (ADCS), a generative\nframework that flexibly integrates both equality (e.g., relative and absolute\npose constraints) and structured inequality constraints (e.g., proximity to\nobject surfaces) into an energy-based diffusion model. Equality constraints are\nmodeled using dedicated energy networks trained on pose differences in Lie\nalgebra space, while inequality constraints are represented via Signed Distance\nFunctions (SDFs) and encoded into learned constraint embeddings, allowing the\nmodel to reason about complex spatial regions. A key innovation of our method\nis a Transformer-based architecture that learns to weight constraint-specific\nenergy functions at inference time, enabling flexible and context-aware\nconstraint integration. Moreover, we adopt a two-phase sampling strategy that\nimproves precision and sample diversity by combining Langevin dynamics with\nresampling and density-aware re-weighting. Experimental results on dual-arm\nmanipulation tasks show that ADCS significantly improves sample diversity and\ngeneralization across settings demanding precise coordination and adaptive\nconstraint handling.\n","authors":["Haolei Tong","Yuezhe Zhang","Sophie Lueth","Georgia Chalvatzaki"],"pdf_url":"https://arxiv.org/pdf/2505.13667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03839v2","updated":"2025-05-19T18:27:47Z","published":"2023-07-07T21:17:20Z","title":"Proximity and Visuotactile Point Cloud Fusion for Contact Patches in\n  Extreme Deformation","summary":"  Visuotactile sensors are a popular tactile sensing strategy due to\nhigh-fidelity estimates of local object geometry. However, existing algorithms\nfor processing raw sensor inputs to useful intermediate signals such as contact\npatches struggle in high-deformation regimes. This is due to physical\nconstraints imposed by sensor hardware and small-deformation assumptions used\nby mechanics-based models. In this work, we propose a fusion algorithm for\nproximity and visuotactile point clouds for contact patch segmentation,\nentirely independent from membrane mechanics. This algorithm exploits the\nsynchronous, high spatial resolution proximity and visuotactile modalities\nenabled by an extremely deformable, selectively transmissive soft membrane,\nwhich uses visible light for visuotactile sensing and infrared light for\nproximity depth. We evaluate our contact patch algorithm in low (10%), medium\n(60%), and high (100%+) strain states. We compare our method against three\nbaselines: proximity-only, tactile-only, and a first principles mechanics\nmodel. Our approach outperforms all baselines with an average RMSE under 2.8 mm\nof the contact patch geometry across all strain ranges. We demonstrate our\ncontact patch algorithm in four applications: varied stiffness membranes,\ntorque and shear-induced wrinkling, closed loop control, and pose estimation.\n","authors":["Jessica Yin","Paarth Shah","Naveen Kuppuswamy","Andrew Beaulieu","Avinash Uttamchandani","Alejandro Castro","James Pikul","Russ Tedrake"],"pdf_url":"https://arxiv.org/pdf/2307.03839v2.pdf","comment":"Accepted to ICRA 2025"},{"id":"http://arxiv.org/abs/2407.07885v2","updated":"2025-05-19T18:18:39Z","published":"2024-07-10T17:52:30Z","title":"Learning In-Hand Translation Using Tactile Skin With Shear and Normal\n  Force Sensing","summary":"  Recent progress in reinforcement learning (RL) and tactile sensing has\nsignificantly advanced dexterous manipulation. However, these methods often\nutilize simplified tactile signals due to the gap between tactile simulation\nand the real world. We introduce a sensor model for tactile skin that enables\nzero-shot sim-to-real transfer of ternary shear and binary normal forces. Using\nthis model, we develop an RL policy that leverages sliding contact for\ndexterous in-hand translation. We conduct extensive real-world experiments to\nassess how tactile sensing facilitates policy adaptation to various unseen\nobject properties and robot hand orientations. We demonstrate that our 3-axis\ntactile policies consistently outperform baselines that use only shear forces,\nonly normal forces, or only proprioception. Website:\nhttps://jessicayin.github.io/tactile-skin-rl/\n","authors":["Jessica Yin","Haozhi Qi","Jitendra Malik","James Pikul","Mark Yim","Tess Hellebrekers"],"pdf_url":"https://arxiv.org/pdf/2407.07885v2.pdf","comment":"Website: https://jessicayin.github.io/tactile-skin-rl/. Accepted to\n  ICRA 2025"},{"id":"http://arxiv.org/abs/2503.15558v3","updated":"2025-05-19T17:59:19Z","published":"2025-03-18T22:06:58Z","title":"Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning","summary":"  Physical AI systems need to perceive, understand, and perform complex actions\nin the physical world. In this paper, we present the Cosmos-Reason1 models that\ncan understand the physical world and generate appropriate embodied decisions\n(e.g., next step action) in natural language through long chain-of-thought\nreasoning processes. We begin by defining key capabilities for Physical AI\nreasoning, with a focus on physical common sense and embodied reasoning. To\nrepresent physical common sense, we use a hierarchical ontology that captures\nfundamental knowledge about space, time, and physics. For embodied reasoning,\nwe rely on a two-dimensional ontology that generalizes across different\nphysical embodiments. Building on these capabilities, we develop two multimodal\nlarge language models, Cosmos-Reason1-7B and Cosmos-Reason1-56B. We curate data\nand train our models in two stages: Physical AI supervised fine-tuning (SFT)\nand Physical AI reinforcement learning (RL). To evaluate our models, we build\ncomprehensive benchmarks for physical common sense and embodied reasoning\naccording to our ontologies. Evaluation results show that Physical AI SFT and\nRL bring significant improvements. To facilitate the development of Physical\nAI, we make our code and pre-trained models available under the NVIDIA Open\nModel License at https://github.com/nvidia-cosmos/cosmos-reason1.\n","authors":[" NVIDIA"," :","Alisson Azzolini","Junjie Bai","Hannah Brandon","Jiaxin Cao","Prithvijit Chattopadhyay","Huayu Chen","Jinju Chu","Yin Cui","Jenna Diamond","Yifan Ding","Liang Feng","Francesco Ferroni","Rama Govindaraju","Jinwei Gu","Siddharth Gururani","Imad El Hanafi","Zekun Hao","Jacob Huffman","Jingyi Jin","Brendan Johnson","Rizwan Khan","George Kurian","Elena Lantz","Nayeon Lee","Zhaoshuo Li","Xuan Li","Maosheng Liao","Tsung-Yi Lin","Yen-Chen Lin","Ming-Yu Liu","Xiangyu Lu","Alice Luo","Andrew Mathau","Yun Ni","Lindsey Pavao","Wei Ping","David W. Romero","Misha Smelyanskiy","Shuran Song","Lyne Tchapmi","Andrew Z. Wang","Boxin Wang","Haoxiang Wang","Fangyin Wei","Jiashu Xu","Yao Xu","Dinghao Yang","Xiaodong Yang","Zhuolin Yang","Jingxu Zhang","Xiaohui Zeng","Zhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.15558v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13441v1","updated":"2025-05-19T17:59:06Z","published":"2025-05-19T17:59:06Z","title":"GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale\n  Synthetic Data Generation","summary":"  We present GrasMolmo, a generalizable open-vocabulary task-oriented grasping\n(TOG) model. GraspMolmo predicts semantically appropriate, stable grasps\nconditioned on a natural language instruction and a single RGB-D frame. For\ninstance, given \"pour me some tea\", GraspMolmo selects a grasp on a teapot\nhandle rather than its body. Unlike prior TOG methods, which are limited by\nsmall datasets, simplistic language, and uncluttered scenes, GraspMolmo learns\nfrom PRISM, a novel large-scale synthetic dataset of 379k samples featuring\ncluttered environments and diverse, realistic task descriptions. We fine-tune\nthe Molmo visual-language model on this data, enabling GraspMolmo to generalize\nto novel open-vocabulary instructions and objects. In challenging real-world\nevaluations, GraspMolmo achieves state-of-the-art results, with a 70%\nprediction success on complex tasks, compared to the 35% achieved by the next\nbest alternative. GraspMolmo also successfully demonstrates the ability to\npredict semantically correct bimanual grasps zero-shot. We release our\nsynthetic dataset, code, model, and benchmarks to accelerate research in\ntask-semantic robotic manipulation, which, along with videos, are available at\nhttps://abhaybd.github.io/GraspMolmo/.\n","authors":["Abhay Deshpande","Yuquan Deng","Arijit Ray","Jordi Salvador","Winson Han","Jiafei Duan","Kuo-Hao Zeng","Yuke Zhu","Ranjay Krishna","Rose Hendrix"],"pdf_url":"https://arxiv.org/pdf/2505.13441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10872v2","updated":"2025-05-19T17:21:49Z","published":"2025-05-16T05:27:15Z","title":"REI-Bench: Can Embodied Agents Understand Vague Human Instructions in\n  Task Planning?","summary":"  Robot task planning decomposes human instructions into executable action\nsequences that enable robots to complete a series of complex tasks. Although\nrecent large language model (LLM)-based task planners achieve amazing\nperformance, they assume that human instructions are clear and straightforward.\nHowever, real-world users are not experts, and their instructions to robots\noften contain significant vagueness. Linguists suggest that such vagueness\nfrequently arises from referring expressions (REs), whose meanings depend\nheavily on dialogue context and environment. This vagueness is even more\nprevalent among the elderly and children, who robots should serve more. This\npaper studies how such vagueness in REs within human instructions affects\nLLM-based robot task planning and how to overcome this issue. To this end, we\npropose the first robot task planning benchmark with vague REs (REI-Bench),\nwhere we discover that the vagueness of REs can severely degrade robot planning\nperformance, leading to success rate drops of up to 77.9%. We also observe that\nmost failure cases stem from missing objects in planners. To mitigate the REs\nissue, we propose a simple yet effective approach: task-oriented context\ncognition, which generates clear instructions for robots, achieving\nstate-of-the-art performance compared to aware prompt and chains of thought.\nThis work contributes to the research community of human-robot interaction\n(HRI) by making robot task planning more practical, particularly for non-expert\nusers, e.g., the elderly and children.\n","authors":["Chenxi Jiang","Chuhao Zhou","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2505.10872v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2505.13376v1","updated":"2025-05-19T17:19:43Z","published":"2025-05-19T17:19:43Z","title":"Seeing, Saying, Solving: An LLM-to-TL Framework for Cooperative Robots","summary":"  Increased robot deployment, such as in warehousing, has revealed a need for\nseamless collaboration among heterogeneous robot teams to resolve unforeseen\nconflicts. To address this challenge, we propose a novel, decentralized\nframework for robots to request and provide help. The framework begins with\nrobots detecting conflicts using a Vision Language Model (VLM), then reasoning\nover whether help is needed. If so, it crafts and broadcasts a natural language\n(NL) help request using a Large Language Model (LLM). Potential helper robots\nreason over the request and offer help (if able), along with information about\nimpact to their current tasks. Helper reasoning is implemented via an LLM\ngrounded in Signal Temporal Logic (STL) using a Backus-Naur Form (BNF) grammar\nto guarantee syntactically valid NL-to-STL translations, which are then solved\nas a Mixed Integer Linear Program (MILP). Finally, the requester robot chooses\na helper by reasoning over impact on the overall system. We evaluate our system\nvia experiments considering different strategies for choosing a helper, and\nfind that a requester robot can minimize overall time impact on the system by\nconsidering multiple help offers versus simple heuristics (e.g., selecting the\nnearest robot to help).\n","authors":["Dan BW Choe","Sundhar Vinodh Sangeetha","Steven Emanuel","Chih-Yuan Chiu","Samuel Coogan","Shreyas Kousik"],"pdf_url":"https://arxiv.org/pdf/2505.13376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10947v2","updated":"2025-05-19T17:11:49Z","published":"2025-05-16T07:36:40Z","title":"Certifying Stability of Reinforcement Learning Policies using\n  Generalized Lyapunov Functions","summary":"  We study the problem of certifying the stability of closed-loop systems under\ncontrol policies derived from optimal control or reinforcement learning (RL).\nClassical Lyapunov methods require a strict step-wise decrease in the Lyapunov\nfunction but such a certificate is difficult to construct for a learned control\npolicy. The value function associated with an RL policy is a natural Lyapunov\nfunction candidate but it is not clear how it should be modified. To gain\nintuition, we first study the linear quadratic regulator (LQR) problem and make\ntwo key observations. First, a Lyapunov function can be obtained from the value\nfunction of an LQR policy by augmenting it with a residual term related to the\nsystem dynamics and stage cost. Second, the classical Lyapunov decrease\nrequirement can be relaxed to a generalized Lyapunov condition requiring only\ndecrease on average over multiple time steps. Using this intuition, we consider\nthe nonlinear setting and formulate an approach to learn generalized Lyapunov\nfunctions by augmenting RL value functions with neural network residual terms.\nOur approach successfully certifies the stability of RL policies trained on\nGymnasium and DeepMind Control benchmarks. We also extend our method to jointly\ntrain neural controllers and stability certificates using a multi-step Lyapunov\nloss, resulting in larger certified inner approximations of the region of\nattraction compared to the classical Lyapunov approach. Overall, our\nformulation enables stability certification for a broad class of systems with\nlearned policies by making certificates easier to construct, thereby bridging\nclassical control theory and modern learning-based methods.\n","authors":["Kehan Long","Jorge Cortés","Nikolay Atanasov"],"pdf_url":"https://arxiv.org/pdf/2505.10947v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13350v1","updated":"2025-05-19T16:52:53Z","published":"2025-05-19T16:52:53Z","title":"Approximating Global Contact-Implicit MPC via Sampling and Local\n  Complementarity","summary":"  To achieve general-purpose dexterous manipulation, robots must rapidly devise\nand execute contact-rich behaviors. Existing model-based controllers are\nincapable of globally optimizing in real-time over the exponential number of\npossible contact sequences. Instead, recent progress in contact-implicit\ncontrol has leveraged simpler models that, while still hybrid, make local\napproximations. However, the use of local models inherently limits the\ncontroller to only exploit nearby interactions, potentially requiring\nintervention to richly explore the space of possible contacts. We present a\nnovel approach which leverages the strengths of local complementarity-based\ncontrol in combination with low-dimensional, but global, sampling of possible\nend-effector locations. Our key insight is to consider a contact-free stage\npreceding a contact-rich stage at every control loop. Our algorithm, in\nparallel, samples end effector locations to which the contact-free stage can\nmove the robot, then considers the cost predicted by contact-rich MPC local to\neach sampled location. The result is a globally-informed, contact-implicit\ncontroller capable of real-time dexterous manipulation. We demonstrate our\ncontroller on precise, non-prehensile manipulation of non-convex objects using\na Franka Panda arm. Project page: https://approximating-global-ci-mpc.github.io\n","authors":["Sharanya Venkatesh","Bibit Bianchini","Alp Aydinoglu","William Yang","Michael Posa"],"pdf_url":"https://arxiv.org/pdf/2505.13350v1.pdf","comment":"S.V. and B.B. contributed equally to this work. Project page:\n  https://approximating-global-ci-mpc.github.io"},{"id":"http://arxiv.org/abs/2505.13339v1","updated":"2025-05-19T16:48:14Z","published":"2025-05-19T16:48:14Z","title":"OPA-Pack: Object-Property-Aware Robotic Bin Packing","summary":"  Robotic bin packing aids in a wide range of real-world scenarios such as\ne-commerce and warehouses. Yet, existing works focus mainly on considering the\nshape of objects to optimize packing compactness and neglect object properties\nsuch as fragility, edibility, and chemistry that humans typically consider when\npacking objects. This paper presents OPA-Pack (Object-Property-Aware Packing\nframework), the first framework that equips the robot with object property\nconsiderations in planning the object packing. Technical-wise, we develop a\nnovel object property recognition scheme with retrieval-augmented generation\nand chain-of-thought reasoning, and build a dataset with object property\nannotations for 1,032 everyday objects. Also, we formulate OPA-Net, aiming to\njointly separate incompatible object pairs and reduce pressure on fragile\nobjects, while compacting the packing. Further, OPA-Net consists of a property\nembedding layer to encode the property of candidate objects to be packed,\ntogether with a fragility heightmap and an avoidance heightmap to keep track of\nthe packed objects. Then, we design a reward function and adopt a deep\nQ-learning scheme to train OPA-Net. Experimental results manifest that OPA-Pack\ngreatly improves the accuracy of separating incompatible object pairs (from 52%\nto 95%) and largely reduces pressure on fragile objects (by 29.4%), while\nmaintaining good packing compactness. Besides, we demonstrate the effectiveness\nof OPA-Pack on a real packing platform, showcasing its practicality in\nreal-world scenarios.\n","authors":["Jia-Hui Pan","Yeok Tatt Cheah","Zhengzhe Liu","Ka-Hei Hui","Xiaojie Gao","Pheng-Ann Heng","Yun-Hui Liu","Chi-Wing Fu"],"pdf_url":"https://arxiv.org/pdf/2505.13339v1.pdf","comment":"Submitted to IEEE Transactions on Robotics (TRO) on Feb. 10, 2025"},{"id":"http://arxiv.org/abs/2505.13335v1","updated":"2025-05-19T16:44:48Z","published":"2025-05-19T16:44:48Z","title":"Scalable Importance Sampling in High Dimensions with Low-Rank Mixture\n  Proposals","summary":"  Importance sampling is a Monte Carlo technique for efficiently estimating the\nlikelihood of rare events by biasing the sampling distribution towards the rare\nevent of interest. By drawing weighted samples from a learned proposal\ndistribution, importance sampling allows for more sample-efficient estimation\nof rare events or tails of distributions. A common choice of proposal density\nis a Gaussian mixture model (GMM). However, estimating full-rank GMM covariance\nmatrices in high dimensions is a challenging task due to numerical\ninstabilities. In this work, we propose using mixtures of probabilistic\nprincipal component analyzers (MPPCA) as the parametric proposal density for\nimportance sampling methods. MPPCA models are a type of low-rank mixture model\nthat can be fit quickly using expectation-maximization, even in\nhigh-dimensional spaces. We validate our method on three simulated systems,\ndemonstrating consistent gains in sample efficiency and quality of failure\ndistribution characterization.\n","authors":["Liam A. Kruse","Marc R. Schlichting","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2505.13335v1.pdf","comment":"Accepted at CoDIT 2025"},{"id":"http://arxiv.org/abs/2505.13278v1","updated":"2025-05-19T16:01:36Z","published":"2025-05-19T16:01:36Z","title":"Hybrid Voting-Based Task Assignment in Modular Construction Scenarios","summary":"  Modular construction, involving off-site prefabrication and on-site assembly,\noffers significant advantages but presents complex coordination challenges for\nrobotic automation. Effective task allocation is critical for leveraging\nmulti-agent systems (MAS) in these structured environments. This paper\nintroduces the Hybrid Voting-Based Task Assignment (HVBTA) framework, a novel\napproach to optimizing collaboration between heterogeneous multi-agent\nconstruction teams. Inspired by human reasoning in task delegation, HVBTA\nuniquely integrates multiple voting mechanisms with the capabilities of a Large\nLanguage Model (LLM) for nuanced suitability assessment between agent\ncapabilities and task requirements. The framework operates by assigning\nCapability Profiles to agents and detailed requirement lists called Task\nDescriptions to construction tasks, subsequently generating a quantitative\nSuitability Matrix. Six distinct voting methods, augmented by a pre-trained\nLLM, analyze this matrix to robustly identify the optimal agent for each task.\nConflict-Based Search (CBS) is integrated for decentralized, collision-free\npath planning, ensuring efficient and safe spatio-temporal coordination of the\nrobotic team during assembly operations. HVBTA enables efficient, conflict-free\nassignment and coordination, facilitating potentially faster and more accurate\nmodular assembly. Current work is evaluating HVBTA's performance across various\nsimulated construction scenarios involving diverse robotic platforms and task\ncomplexities. While designed as a generalizable framework for any domain with\nclearly definable tasks and capabilities, HVBTA will be particularly effective\nfor addressing the demanding coordination requirements of multi-agent\ncollaborative robotics in modular construction due to the predetermined\nconstruction planning involved.\n","authors":["Daniel Weiner","Raj Korpan"],"pdf_url":"https://arxiv.org/pdf/2505.13278v1.pdf","comment":"Accepted to Block by Block workshop at ICRA 2025"},{"id":"http://arxiv.org/abs/2503.13137v2","updated":"2025-05-19T15:59:12Z","published":"2025-03-17T13:05:23Z","title":"Rapid and Inexpensive Inertia Tensor Estimation from a Single Object\n  Throw","summary":"  The inertia tensor is an important parameter in many engineering fields, but\nmeasuring it can be cumbersome and involve multiple experiments or accurate and\nexpensive equipment. We propose a method to measure the moment of inertia\ntensor of a rigid body from a single spinning throw, by attaching a small and\ninexpensive stand-alone measurement device consisting of a gyroscope,\naccelerometer and a reaction wheel. The method includes a compensation for the\nincrease of moment of inertia due to adding the measurement device to the body,\nand additionally obtains the location of the centre of gravity of the body as\nan intermediate result. Experiments performed with known rigid bodies show that\nthe mean accuracy is around 2%.\n","authors":["Till M. Blaha","Mike M. Kuijper","Radu Pop","Ewoud J. J. Smeur"],"pdf_url":"https://arxiv.org/pdf/2503.13137v2.pdf","comment":"\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2409.14679v2","updated":"2025-05-19T15:50:40Z","published":"2024-09-23T03:01:50Z","title":"Quantifying Context Bias in Domain Adaptation for Object Detection","summary":"  Domain adaptation for object detection (DAOD) seeks to transfer a trained\nmodel from a source to a target domain. Various DAOD methods exist, some of\nwhich aim to minimize context bias between foreground-background associations\nin various domains. However, no prior work has studied context bias in DAOD by\nanalyzing changes in background features during adaptation and how context bias\nis represented in different domains. Our research experiment highlights the\npotential usability of context bias in DAOD. We address the problem by varying\nactivation values over different layers of two different trained models,\nDetectron2 and YOLOv11, and by masking the background, both of which impact the\nnumber and quality of detections. We use two synthetic datasets, CARLA and\nVirtual KITTI, and two different versions of real open-source data, Cityscapes\nand KITTI semantic, as separate domains to represent and quantify context bias.\nWe utilize different metrics such as Maximum Mean Discrepancy (MMD) and Maximum\nVariance Discrepancy (MVD) to find the layer-specific conditional probability\nestimates of foreground given manipulated background regions for separate\ndomains. We further analyze foreground-background associations across various\ndataset combinations. We find that state-of-the-art domain adaptation methods\nexhibit some form of context bias and apply a potentially simple way to\nalleviate the context bias achieving improved accuracy (from 51.189 to 53.646\nmAP on Cityscapes foggy validation with 63.207 mAP and 64.233 mAP on Cityscapes\nvalidation respectively). We demonstrate through detailed analysis that\nunderstanding of the context bias can affect DAOD approach and focusing solely\non aligning foreground features is insufficient for effective DAOD.\n","authors":["Hojun Son","Asma Almutairi","Arpan Kusari"],"pdf_url":"https://arxiv.org/pdf/2409.14679v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2505.13255v1","updated":"2025-05-19T15:39:08Z","published":"2025-05-19T15:39:08Z","title":"Policy Contrastive Decoding for Robotic Foundation Models","summary":"  Robotic foundation models, or generalist robot policies, hold immense\npotential to enable flexible, general-purpose and dexterous robotic systems.\nDespite their advancements, our empirical experiments reveal that existing\nrobot policies are prone to learning spurious correlations from pre-training\ntrajectories, adversely affecting their generalization capabilities beyond the\ntraining data. To tackle this, we propose a novel Policy Contrastive Decoding\n(PCD) approach, which redirects the robot policy's focus toward object-relevant\nvisual clues by contrasting action probability distributions derived from\noriginal and object-masked visual inputs. As a training-free method, our PCD\ncan be used as a plugin to improve different types of robot policies without\nneeding to finetune or access model weights. We conduct extensive experiments\non top of three open-source robot policies, including the autoregressive policy\nOpenVLA and the diffusion-based policies Octo and $\\pi_0$. The obtained results\nin both simulation and real-world environments prove PCD's flexibility and\neffectiveness, e.g., PCD enhances the state-of-the-art policy $\\pi_0$ by 8% in\nthe simulation environment and by 108% in the real-world environment. Code and\ndemos are publicly available at: https://Koorye.github.io/proj/PCD.\n","authors":["Shihan Wu","Ji Zhang","Xu Luo","Junlin Xie","Jingkuan Song","Heng Tao Shen","Lianli Gao"],"pdf_url":"https://arxiv.org/pdf/2505.13255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13253v1","updated":"2025-05-19T15:36:34Z","published":"2025-05-19T15:36:34Z","title":"Composing Dextrous Grasping and In-hand Manipulation via Scoring with a\n  Reinforcement Learning Critic","summary":"  In-hand manipulation and grasping are fundamental yet often separately\naddressed tasks in robotics. For deriving in-hand manipulation policies,\nreinforcement learning has recently shown great success. However, the derived\ncontrollers are not yet useful in real-world scenarios because they often\nrequire a human operator to place the objects in suitable initial (grasping)\nstates. Finding stable grasps that also promote the desired in-hand\nmanipulation goal is an open problem. In this work, we propose a method for\nbridging this gap by leveraging the critic network of a reinforcement learning\nagent trained for in-hand manipulation to score and select initial grasps. Our\nexperiments show that this method significantly increases the success rate of\nin-hand manipulation without requiring additional training. We also present an\nimplementation of a full grasp manipulation pipeline on a real-world system,\nenabling autonomous grasping and reorientation even of unwieldy objects.\n","authors":["Lennart Röstel","Dominik Winkelbauer","Johannes Pitz","Leon Sievers","Berthold Bäuml"],"pdf_url":"https://arxiv.org/pdf/2505.13253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05687v3","updated":"2025-05-19T15:31:45Z","published":"2024-06-09T08:17:13Z","title":"FlightBench: Benchmarking Learning-based Methods for Ego-vision-based\n  Quadrotors Navigation","summary":"  Ego-vision-based navigation in cluttered environments is crucial for mobile\nsystems, particularly agile quadrotors. While learning-based methods have shown\npromise recently, head-to-head comparisons with cutting-edge optimization-based\napproaches are scarce, leaving open the question of where and to what extent\nthey truly excel. In this paper, we introduce FlightBench, the first\ncomprehensive benchmark that implements various learning-based methods for\nego-vision-based navigation and evaluates them against mainstream\noptimization-based baselines using a broad set of performance metrics. More\nimportantly, we develop a suite of criteria to assess scenario difficulty and\ndesign test cases that span different levels of difficulty based on these\ncriteria. Our results show that while learning-based methods excel in\nhigh-speed flight and faster inference, they struggle with challenging\nscenarios like sharp corners or view occlusion. Analytical experiments validate\nthe correlation between our difficulty criteria and flight performance.\nMoreover, we verify the trend in flight performance within real-world\nenvironments through full-pipeline and hardware-in-the-loop experiments. We\nhope this benchmark and these criteria will drive future advancements in\nlearning-based navigation for ego-vision quadrotors. Code and documentation are\navailable at https://github.com/thu-uav/FlightBench.\n","authors":["Shu-Ang Yu","Chao Yu","Feng Gao","Yi Wu","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2406.05687v3.pdf","comment":"The first three authors contribute equally"},{"id":"http://arxiv.org/abs/2505.13231v1","updated":"2025-05-19T15:15:27Z","published":"2025-05-19T15:15:27Z","title":"Investigating Active Sampling for Hardness Classification with\n  Vision-Based Tactile Sensors","summary":"  One of the most important object properties that humans and robots perceive\nthrough touch is hardness. This paper investigates information-theoretic active\nsampling strategies for sample-efficient hardness classification with\nvision-based tactile sensors. We evaluate three probabilistic classifier models\nand two model-uncertainty-based sampling strategies on a robotic setup as well\nas on a previously published dataset of samples collected by human testers. Our\nfindings indicate that the active sampling approaches, driven by uncertainty\nmetrics, surpass a random sampling baseline in terms of accuracy and stability.\nAdditionally, while in our human study, the participants achieve an average\naccuracy of 48.00%, our best approach achieves an average accuracy of 88.78% on\nthe same set of objects, demonstrating the effectiveness of vision-based\ntactile sensors for object hardness classification.\n","authors":["Junyi Chen","Alap Kshirsagar","Frederik Heller","Mario Gómez Andreu","Boris Belousov","Tim Schneider","Lisa P. Y. Lin","Katja Doerschner","Knut Drewing","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2505.13231v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2505.13186v1","updated":"2025-05-19T14:44:02Z","published":"2025-05-19T14:44:02Z","title":"Interpretable Robotic Friction Learning via Symbolic Regression","summary":"  Accurately modeling the friction torque in robotic joints has long been\nchallenging due to the request for a robust mathematical description.\nTraditional model-based approaches are often labor-intensive, requiring\nextensive experiments and expert knowledge, and they are difficult to adapt to\nnew scenarios and dependencies. On the other hand, data-driven methods based on\nneural networks are easier to implement but often lack robustness,\ninterpretability, and trustworthiness--key considerations for robotic hardware\nand safety-critical applications such as human-robot interaction. To address\nthe limitations of both approaches, we propose the use of symbolic regression\n(SR) to estimate the friction torque. SR generates interpretable symbolic\nformulas similar to those produced by model-based methods while being flexible\nto accommodate various dynamic effects and dependencies. In this work, we apply\nSR algorithms to approximate the friction torque using collected data from a\nKUKA LWR-IV+ robot. Our results show that SR not only yields formulas with\ncomparable complexity to model-based approaches but also achieves higher\naccuracy. Moreover, SR-derived formulas can be seamlessly extended to include\nload dependencies and other dynamic factors.\n","authors":["Philipp Scholl","Alexander Dietrich","Sebastian Wolf","Jinoh Lee","Alin-Albu Schäffer","Gitta Kutyniok","Maged Iskandar"],"pdf_url":"https://arxiv.org/pdf/2505.13186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14259v2","updated":"2025-05-19T14:12:20Z","published":"2025-03-18T13:50:35Z","title":"Quantization-Free Autoregressive Action Transformer","summary":"  Current transformer-based imitation learning approaches introduce discrete\naction representations and train an autoregressive transformer decoder on the\nresulting latent code. However, the initial quantization breaks the continuous\nstructure of the action space thereby limiting the capabilities of the\ngenerative model. We propose a quantization-free method instead that leverages\nGenerative Infinite-Vocabulary Transformers (GIVT) as a direct, continuous\npolicy parametrization for autoregressive transformers. This simplifies the\nimitation learning pipeline while achieving state-of-the-art performance on a\nvariety of popular simulated robotics tasks. We enhance our policy roll-outs by\ncarefully studying sampling algorithms, further improving the results.\n","authors":["Ziyad Sheebaelhamd","Michael Tschannen","Michael Muehlebach","Claire Vernade"],"pdf_url":"https://arxiv.org/pdf/2503.14259v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13144v1","updated":"2025-05-19T14:11:14Z","published":"2025-05-19T14:11:14Z","title":"Temporal Distance-aware Transition Augmentation for Offline Model-based\n  Reinforcement Learning","summary":"  The goal of offline reinforcement learning (RL) is to extract a\nhigh-performance policy from the fixed datasets, minimizing performance\ndegradation due to out-of-distribution (OOD) samples. Offline model-based RL\n(MBRL) is a promising approach that ameliorates OOD issues by enriching\nstate-action transitions with augmentations synthesized via a learned dynamics\nmodel. Unfortunately, seminal offline MBRL methods often struggle in\nsparse-reward, long-horizon tasks. In this work, we introduce a novel MBRL\nframework, dubbed Temporal Distance-Aware Transition Augmentation (TempDATA),\nthat generates augmented transitions in a temporally structured latent space\nrather than in raw state space. To model long-horizon behavior, TempDATA learns\na latent abstraction that captures a temporal distance from both trajectory and\ntransition levels of state space. Our experiments confirm that TempDATA\noutperforms previous offline MBRL methods and achieves matching or surpassing\nthe performance of diffusion-based trajectory augmentation and goal-conditioned\nRL on the D4RL AntMaze, FrankaKitchen, CALVIN, and pixel-based FrankaKitchen.\n","authors":["Dongsu Lee","Minhae Kwon"],"pdf_url":"https://arxiv.org/pdf/2505.13144v1.pdf","comment":"2025 ICML"},{"id":"http://arxiv.org/abs/2505.10755v2","updated":"2025-05-19T14:08:22Z","published":"2025-05-15T23:47:58Z","title":"Infinigen-Sim: Procedural Generation of Articulated Simulation Assets","summary":"  We introduce Infinigen-Sim, a toolkit which enables users to create diverse\nand realistic articulated object procedural generators. These tools are\ncomposed of high-level utilities for use creating articulated assets in\nBlender, as well as an export pipeline to integrate the resulting assets into\ncommon robotics simulators. We demonstrate our system by creating procedural\ngenerators for 5 common articulated object categories. Experiments show that\nassets sampled from these generators are useful for movable object\nsegmentation, training generalizable reinforcement learning policies, and\nsim-to-real transfer of imitation learning policies.\n","authors":["Abhishek Joshi","Beining Han","Jack Nugent","Yiming Zuo","Jonathan Liu","Hongyu Wen","Stamatis Alexandropoulos","Tao Sun","Alexander Raistrick","Gaowen Liu","Yi Shao","Jia Deng"],"pdf_url":"https://arxiv.org/pdf/2505.10755v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13131v1","updated":"2025-05-19T14:00:17Z","published":"2025-05-19T14:00:17Z","title":"Constraint-Aware Diffusion Guidance for Robotics: Real-Time Obstacle\n  Avoidance for Autonomous Racing","summary":"  Diffusion models hold great potential in robotics due to their ability to\ncapture complex, high-dimensional data distributions. However, their lack of\nconstraint-awareness limits their deployment in safety-critical applications.\nWe propose Constraint-Aware Diffusion Guidance (CoDiG), a data-efficient and\ngeneral-purpose framework that integrates barrier functions into the denoising\nprocess, guiding diffusion sampling toward constraint-satisfying outputs. CoDiG\nenables constraint satisfaction even with limited training data and generalizes\nacross tasks. We evaluate our framework in the challenging setting of miniature\nautonomous racing, where real-time obstacle avoidance is essential. Real-world\nexperiments show that CoDiG generates safe outputs efficiently under dynamic\nconditions, highlighting its potential for broader robotic applications. A\ndemonstration video is available at https://youtu.be/KNYsTdtdxOU.\n","authors":["Hao Ma","Sabrina Bodmer","Andrea Carron","Melanie Zeilinger","Michael Muehlebach"],"pdf_url":"https://arxiv.org/pdf/2505.13131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13064v1","updated":"2025-05-19T12:54:03Z","published":"2025-05-19T12:54:03Z","title":"When do Lyapunov Subcenter Manifolds become Eigenmanifolds?","summary":"  Multi-body mechanical systems have rich internal dynamics, which can be\nexploited to formulate efficient control targets. For periodic regulation tasks\nin robotics applications, this motivated the extension of the theory on\nnonlinear normal modes to Riemannian manifolds, and led to the definition of\nEigenmanifolds. This definition is geometric, which is advantageous for\ngenerality within robotics but also obscures the connection of Eigenmanifolds\nto a large body of results from the literature on nonlinear dynamics. We bridge\nthis gap, showing that Eigenmanifolds are instances of Lyapunov subcenter\nmanifolds (LSMs), and that their stronger geometric properties with respect to\nLSMs follow from a time-symmetry of conservative mechanical systems. This\ndirectly leads to local existence and uniqueness results for Eigenmanifolds.\nFurthermore, we show that an additional spatial symmetry provides\nEigenmanifolds with yet stronger properties of Rosenberg manifolds, which can\nbe favorable for control applications, and we present a sufficient condition\nfor their existence and uniqueness. These theoretical results are numerically\nconfirmed on two mechanical systems with a non-constant inertia tensor: a\ndouble pendulum and a 5-link pendulum.\n","authors":["Yannik P. Wotte","Arne Sachtler","Alin Albu-Schäffer","Stefano Stramigioli","Cosimo Della Santina"],"pdf_url":"https://arxiv.org/pdf/2505.13064v1.pdf","comment":"22 pages, 24 figures, submitted to Automatica"},{"id":"http://arxiv.org/abs/2505.13054v1","updated":"2025-05-19T12:43:30Z","published":"2025-05-19T12:43:30Z","title":"Disentangling Coordiante Frames for Task Specific Motion Retargeting in\n  Teleoperation using Shared Control and VR Controllers","summary":"  Task performance in terms of task completion time in teleoperation is still\nfar behind compared to humans conducting tasks directly. One large identified\nimpact on this is the human capability to perform transformations and\nalignments, which is directly influenced by the point of view and the motion\nretargeting strategy. In modern teleoperation systems, motion retargeting is\nusually implemented through a one time calibration or switching modes. Complex\ntasks, like concatenated screwing, might be difficult, because the operator has\nto align (e.g. mirror) rotational and translational input commands. Recent\nresearch has shown, that the separation of translation and rotation leads to\nincreased task performance. This work proposes a formal motion retargeting\nmethod, which separates translational and rotational input commands. This\nmethod is then included in a optimal control based trajectory planner and shown\nto work on a UR5e manipulator.\n","authors":["Max Grobbel","Daniel Flögel","Philipp Rigoll","Sören Hohmann"],"pdf_url":"https://arxiv.org/pdf/2505.13054v1.pdf","comment":"8 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2505.06883v2","updated":"2025-05-19T11:28:40Z","published":"2025-05-11T07:23:26Z","title":"FACET: Force-Adaptive Control via Impedance Reference Tracking for\n  Legged Robots","summary":"  Reinforcement learning (RL) has made significant strides in legged robot\ncontrol, enabling locomotion across diverse terrains and complex\nloco-manipulation capabilities. However, the commonly used position or velocity\ntracking-based objectives are agnostic to forces experienced by the robot,\nleading to stiff and potentially dangerous behaviors and poor control during\nforceful interactions. To address this limitation, we present\n\\emph{Force-Adaptive Control via Impedance Reference Tracking} (FACET).\nInspired by impedance control, we use RL to train a control policy to imitate a\nvirtual mass-spring-damper system, allowing fine-grained control under external\nforces by manipulating the virtual spring. In simulation, we demonstrate that\nour quadruped robot achieves improved robustness to large impulses (up to 200\nNs) and exhibits controllable compliance, achieving an 80% reduction in\ncollision impulse. The policy is deployed to a physical robot to showcase both\ncompliance and the ability to engage with large forces by kinesthetic control\nand pulling payloads up to 2/3 of its weight. Further extension to a legged\nloco-manipulator and a humanoid shows the applicability of our method to more\ncomplex settings to enable whole-body compliance control. Project Website:\nhttps://facet.pages.dev/\n","authors":["Botian Xu","Haoyang Weng","Qingzhou Lu","Yang Gao","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2505.06883v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12934v1","updated":"2025-05-19T10:17:03Z","published":"2025-05-19T10:17:03Z","title":"Granular Loco-Manipulation: Repositioning Rocks Through Strategic Sand\n  Avalanche","summary":"  Legged robots have the potential to leverage obstacles to climb steep sand\nslopes. However, efficiently repositioning these obstacles to desired locations\nis challenging. Here we present DiffusiveGRAIN, a learning-based method that\nenables a multi-legged robot to strategically induce localized sand avalanches\nduring locomotion and indirectly manipulate obstacles. We conducted 375 trials,\nsystematically varying obstacle spacing, robot orientation, and leg actions in\n75 of them. Results show that the movement of closely-spaced obstacles exhibits\nsignificant interference, requiring joint modeling. In addition, different\nmulti-leg excavation actions could cause distinct robot state changes,\nnecessitating integrated planning of manipulation and locomotion. To address\nthese challenges, DiffusiveGRAIN includes a diffusion-based environment\npredictor to capture multi-obstacle movements under granular flow interferences\nand a robot state predictor to estimate changes in robot state from multi-leg\naction patterns. Deployment experiments (90 trials) demonstrate that by\nintegrating the environment and robot state predictors, the robot can\nautonomously plan its movements based on loco-manipulation goals, successfully\nshifting closely located rocks to desired locations in over 65% of trials. Our\nstudy showcases the potential for a locomoting robot to strategically\nmanipulate obstacles to achieve improved mobility on challenging terrains.\n","authors":["Haodi Hu","Yue Wu","Feifei Qian","Daniel Seita"],"pdf_url":"https://arxiv.org/pdf/2505.12934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06160v2","updated":"2025-05-19T09:54:46Z","published":"2023-10-09T21:18:14Z","title":"Efficient Multi-robot Active SLAM","summary":"  Autonomous exploration in unknown environments remains a fundamental\nchallenge in robotics, particularly for applications such as search and rescue,\nindustrial inspection, and planetary exploration. Multi-robot active SLAM\npresents a promising solution by enabling collaborative mapping and exploration\nwhile actively reducing uncertainty. However, existing approaches often suffer\nfrom high computational costs and inefficient frontier management, making them\ncomputationally expensive for real-time applications. In this paper, we\nintroduce an efficient multi-robot active SLAM framework that incorporates a\nfrontier-sharing strategy to enhance robot distribution in unexplored\nenvironments. Our approach integrates a utility function that considers both\npose graph uncertainty and path entropy, achieving an optimal balance between\nexploration coverage and computational efficiency. By filtering and\nprioritizing goal frontiers, our method significantly reduces computational\noverhead while preserving high mapping accuracy. The proposed framework has\nbeen implemented in ROS and validated through simulations and real-world\nexperiments. Results demonstrate superior exploration performance and mapping\nquality compared to state-of-the-art approaches.\n","authors":["Muhammad Farhan Ahmed","Matteo Maragliano","Vincent Frémont","Carmine Tommaso Recchiuto"],"pdf_url":"https://arxiv.org/pdf/2310.06160v2.pdf","comment":"31 pages, 15 figures"},{"id":"http://arxiv.org/abs/2505.12844v1","updated":"2025-05-19T08:30:13Z","published":"2025-05-19T08:30:13Z","title":"AGI-Elo: How Far Are We From Mastering A Task?","summary":"  As the field progresses toward Artificial General Intelligence (AGI), there\nis a pressing need for more comprehensive and insightful evaluation frameworks\nthat go beyond aggregate performance metrics. This paper introduces a unified\nrating system that jointly models the difficulty of individual test cases and\nthe competency of AI models (or humans) across vision, language, and action\ndomains. Unlike existing metrics that focus solely on models, our approach\nallows for fine-grained, difficulty-aware evaluations through competitive\ninteractions between models and tasks, capturing both the long-tail\ndistribution of real-world challenges and the competency gap between current\nmodels and full task mastery. We validate the generalizability and robustness\nof our system through extensive experiments on multiple established datasets\nand models across distinct AGI domains. The resulting rating distributions\noffer novel perspectives and interpretable insights into task difficulty, model\nprogression, and the outstanding challenges that remain on the path to\nachieving full AGI task mastery.\n","authors":["Shuo Sun","Yimin Zhao","Christina Dao Wen Lee","Jiawei Sun","Chengran Yuan","Zefan Huang","Dongen Li","Justin KW Yeoh","Alok Prakash","Thomas W. Malone","Marcelo H. Ang Jr"],"pdf_url":"https://arxiv.org/pdf/2505.12844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08504v4","updated":"2025-05-19T08:07:29Z","published":"2024-03-13T13:12:42Z","title":"Offboard Occupancy Refinement with Hybrid Propagation for Autonomous\n  Driving","summary":"  Vision-based occupancy prediction, also known as 3D Semantic Scene Completion\n(SSC), presents a significant challenge in computer vision. Previous methods,\nconfined to onboard processing, struggle with simultaneous geometric and\nsemantic estimation, continuity across varying viewpoints, and single-view\nocclusion. Our paper introduces OccFiner, a novel offboard framework designed\nto enhance the accuracy of vision-based occupancy predictions. OccFiner\noperates in two hybrid phases: 1) a multi-to-multi local propagation network\nthat implicitly aligns and processes multiple local frames for correcting\nonboard model errors and consistently enhancing occupancy accuracy across all\ndistances. 2) the region-centric global propagation, focuses on refining labels\nusing explicit multi-view geometry and integrating sensor bias, particularly\nfor increasing the accuracy of distant occupied voxels. Extensive experiments\ndemonstrate that OccFiner improves both geometric and semantic accuracy across\nvarious types of coarse occupancy, setting a new state-of-the-art performance\non the SemanticKITTI dataset. Notably, OccFiner significantly boosts the\nperformance of vision-based SSC models, achieving accuracy levels competitive\nwith established LiDAR-based onboard SSC methods. Furthermore, OccFiner is the\nfirst to achieve automatic annotation of SSC in a purely vision-based approach.\nQuantitative experiments prove that OccFiner successfully facilitates occupancy\ndata loop-closure in autonomous driving. Additionally, we quantitatively and\nqualitatively validate the superiority of the offboard approach on city-level\nSSC static maps. The source code will be made publicly available at\nhttps://github.com/MasterHow/OccFiner.\n","authors":["Hao Shi","Song Wang","Jiaming Zhang","Xiaoting Yin","Guangming Wang","Jianke Zhu","Kailun Yang","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08504v4.pdf","comment":"Accepted to IEEE Transactions on Intelligent Transportation Systems\n  (T-ITS). The source code will be made publicly available at\n  https://github.com/MasterHow/OccFiner"},{"id":"http://arxiv.org/abs/2505.11032v2","updated":"2025-05-19T07:28:03Z","published":"2025-05-16T09:26:59Z","title":"DexGarmentLab: Dexterous Garment Manipulation Environment with\n  Generalizable Policy","summary":"  Garment manipulation is a critical challenge due to the diversity in garment\ncategories, geometries, and deformations. Despite this, humans can effortlessly\nhandle garments, thanks to the dexterity of our hands. However, existing\nresearch in the field has struggled to replicate this level of dexterity,\nprimarily hindered by the lack of realistic simulations of dexterous garment\nmanipulation. Therefore, we propose DexGarmentLab, the first environment\nspecifically designed for dexterous (especially bimanual) garment manipulation,\nwhich features large-scale high-quality 3D assets for 15 task scenarios, and\nrefines simulation techniques tailored for garment modeling to reduce the\nsim-to-real gap. Previous data collection typically relies on teleoperation or\ntraining expert reinforcement learning (RL) policies, which are labor-intensive\nand inefficient. In this paper, we leverage garment structural correspondence\nto automatically generate a dataset with diverse trajectories using only a\nsingle expert demonstration, significantly reducing manual intervention.\nHowever, even extensive demonstrations cannot cover the infinite states of\ngarments, which necessitates the exploration of new algorithms. To improve\ngeneralization across diverse garment shapes and deformations, we propose a\nHierarchical gArment-manipuLation pOlicy (HALO). It first identifies\ntransferable affordance points to accurately locate the manipulation area, then\ngenerates generalizable trajectories to complete the task. Through extensive\nexperiments and detailed analysis of our method and baseline, we demonstrate\nthat HALO consistently outperforms existing methods, successfully generalizing\nto previously unseen instances even with significant variations in shape and\ndeformation where others fail. Our project page is available at:\nhttps://wayrise.github.io/DexGarmentLab/.\n","authors":["Yuran Wang","Ruihai Wu","Yue Chen","Jiarui Wang","Jiaqi Liang","Ziyu Zhu","Haoran Geng","Jitendra Malik","Pieter Abbeel","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2505.11032v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13556v1","updated":"2025-05-19T07:22:32Z","published":"2025-05-19T07:22:32Z","title":"Learning Collision Risk from Naturalistic Driving with Generalised\n  Surrogate Safety Measures","summary":"  Accurate and timely alerts for drivers or automated systems to unfolding\ncollisions remains a challenge in road safety, particularly in highly\ninteractive urban traffic. Existing approaches require labour-intensive\nannotation of sparse risk, struggle to consider varying interaction context, or\nare useful only in the scenarios they are designed for. To address these\nlimits, this study introduces the generalised surrogate safety measure (GSSM),\na new approach that learns exclusively from naturalistic driving without crash\nor risk labels. GSSM captures the patterns of normal driving and estimates the\nextent to which a traffic interaction deviates from the norm towards unsafe\nextreme. Utilising neural networks, normal interactions are characterised by\ncontext-conditioned distributions of multi-directional spacing between road\nusers. In the same interaction context, a spacing closer than normal entails\nhigher risk of potential collision. Then a context-adaptive risk score and its\nassociated probability can be calculated based on the theory of extreme values.\nAny measurable factors, such as motion kinematics, weather, lighting, can serve\nas part of the context, allowing for diverse coverage of safety-critical\ninteractions. Multiple public driving datasets are used to train GSSMs, which\nare tested with 4,875 real-world crashes and near-crashes reconstructed from\nthe SHRP2 NDS. A vanilla GSSM using only instantaneous states achieves AUPRC of\n0.9 and secures a median time advance of 2.6 seconds to prevent potential\ncollisions. Additional data and contextual factors provide further performance\ngains. Across various interaction types such as rear-end, merging, and\ncrossing, the accuracy and timeliness of GSSM consistently outperforms existing\nbaselines. GSSM therefore establishes a scalable, context-aware, and\ngeneralisable foundation to proactively quantify collision risk in traffic\ninteractions.\n","authors":["Yiru Jiao","Simeon C. Calvert","Sander van Cranenburgh","Hans van Lint"],"pdf_url":"https://arxiv.org/pdf/2505.13556v1.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.12752v1","updated":"2025-05-19T06:20:37Z","published":"2025-05-19T06:20:37Z","title":"MOON: Multi-Objective Optimization-Driven Object-Goal Navigation Using a\n  Variable-Horizon Set-Orienteering Planner","summary":"  Object-goal navigation (ON) enables autonomous robots to locate and reach\nuser-specified objects in previously unknown environments, offering promising\napplications in domains such as assistive care and disaster response. Existing\nON methods -- including training-free approaches, reinforcement learning, and\nzero-shot planners -- generally depend on active exploration to identify\nlandmark objects (e.g., kitchens or desks), followed by navigation toward\nsemantically related targets (e.g., a specific mug). However, these methods\noften lack strategic planning and do not adequately address trade-offs among\nmultiple objectives. To overcome these challenges, we propose a novel framework\nthat formulates ON as a multi-objective optimization problem (MOO), balancing\nfrontier-based knowledge exploration with knowledge exploitation over\npreviously observed landmarks; we call this framework MOON (MOO-driven ON). We\nimplement a prototype MOON system that integrates three key components: (1)\nbuilding on QOM [IROS05], a classical ON system that compactly and\ndiscriminatively encodes landmarks based on their semantic relevance to the\ntarget; (2) integrating StructNav [RSS23], a recently proposed training-free\nplanner, to enhance the navigation pipeline; and (3) introducing a\nvariable-horizon set orienteering problem formulation to enable global\noptimization over both exploration and exploitation strategies. This work\nrepresents an important first step toward developing globally optimized,\nnext-generation object-goal navigation systems.\n","authors":["Daigo Nakajima","Kanji Tanaka","Daiki Iwata","Kouki Terashima"],"pdf_url":"https://arxiv.org/pdf/2505.12752v1.pdf","comment":"6 pages, technical report"},{"id":"http://arxiv.org/abs/2505.12748v1","updated":"2025-05-19T06:08:53Z","published":"2025-05-19T06:08:53Z","title":"TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous\n  Teleoperation","summary":"  Teleoperation is a cornerstone of embodied-robot learning, and bimanual\ndexterous teleoperation in particular provides rich demonstrations that are\ndifficult to obtain with fully autonomous systems. While recent studies have\nproposed diverse hardware pipelines-ranging from inertial motion-capture gloves\nto exoskeletons and vision-based interfaces-there is still no unified benchmark\nthat enables fair, reproducible comparison of these systems. In this paper, we\nintroduce TeleOpBench, a simulator-centric benchmark tailored to bimanual\ndexterous teleoperation. TeleOpBench contains 30 high-fidelity task\nenvironments that span pick-and-place, tool use, and collaborative\nmanipulation, covering a broad spectrum of kinematic and force-interaction\ndifficulty. Within this benchmark we implement four representative\nteleoperation modalities-(i) MoCap, (ii) VR device, (iii) arm-hand\nexoskeletons, and (iv) monocular vision tracking-and evaluate them with a\ncommon protocol and metric suite. To validate that performance in simulation is\npredictive of real-world behavior, we conduct mirrored experiments on a\nphysical dual-arm platform equipped with two 6-DoF dexterous hands. Across 10\nheld-out tasks we observe a strong correlation between simulator and hardware\nperformance, confirming the external validity of TeleOpBench. TeleOpBench\nestablishes a common yardstick for teleoperation research and provides an\nextensible platform for future algorithmic and hardware innovation.\n","authors":["Hangyu Li","Qin Zhao","Haoran Xu","Xinyu Jiang","Qingwei Ben","Feiyu Jia","Haoyu Zhao","Liang Xu","Jia Zeng","Hanqing Wang","Bo Dai","Junting Dong","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2505.12748v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2505.11175v2","updated":"2025-05-19T05:14:55Z","published":"2025-05-16T12:19:13Z","title":"Real-Time Verification of Embodied Reasoning for Generative Skill\n  Acquisition","summary":"  Generative skill acquisition enables embodied agents to actively learn a\nscalable and evolving repertoire of control skills, crucial for the advancement\nof large decision models. While prior approaches often rely on supervision\nsignals from generalist agents (e.g., LLMs), their effectiveness in complex 3D\nenvironments remains unclear; exhaustive evaluation incurs substantial\ncomputational costs, significantly hindering the efficiency of skill learning.\nInspired by recent successes in verification models for mathematical reasoning,\nwe propose VERGSA (Verifying Embodied Reasoning in Generative Skill\nAcquisition), a framework that systematically integrates real-time verification\nprinciples into embodied skill learning. VERGSA establishes 1) a seamless\nextension from verification of mathematical reasoning into embodied learning by\ndynamically incorporating contextually relevant tasks into prompts and defining\nsuccess metrics for both subtasks and overall tasks, and 2) an automated,\nscalable reward labeling scheme that synthesizes dense reward signals by\niteratively finalizing the contribution of scene configuration and subtask\nlearning to overall skill acquisition. To the best of our knowledge, this\napproach constitutes the first comprehensive training dataset for\nverification-driven generative skill acquisition, eliminating arduous manual\nreward engineering. Experiments validate the efficacy of our approach: 1) the\nexemplar task pool improves the average task success rates by 21%, 2) our\nverification model boosts success rates by 24% for novel tasks and 36% for\nencountered tasks, and 3) outperforms LLM-as-a-Judge baselines in verification\nquality.\n","authors":["Bo Yue","Shuqi Guo","Kaiyu Hu","Chujiao Wang","Benyou Wang","Kui Jia","Guiliang Liu"],"pdf_url":"https://arxiv.org/pdf/2505.11175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12705v1","updated":"2025-05-19T04:55:39Z","published":"2025-05-19T04:55:39Z","title":"DreamGen: Unlocking Generalization in Robot Learning through Neural\n  Trajectories","summary":"  We introduce DreamGen, a simple yet highly effective 4-stage pipeline for\ntraining robot policies that generalize across behaviors and environments\nthrough neural trajectories - synthetic robot data generated from video world\nmodels. DreamGen leverages state-of-the-art image-to-video generative models,\nadapting them to the target robot embodiment to produce photorealistic\nsynthetic videos of familiar or novel tasks in diverse environments. Since\nthese models generate only videos, we recover pseudo-action sequences using\neither a latent action model or an inverse-dynamics model (IDM). Despite its\nsimplicity, DreamGen unlocks strong behavior and environment generalization: a\nhumanoid robot can perform 22 new behaviors in both seen and unseen\nenvironments, while requiring teleoperation data from only a single\npick-and-place task in one environment. To evaluate the pipeline\nsystematically, we introduce DreamGen Bench, a video generation benchmark that\nshows a strong correlation between benchmark performance and downstream policy\nsuccess. Our work establishes a promising new axis for scaling robot learning\nwell beyond manual data collection.\n","authors":["Joel Jang","Seonghyeon Ye","Zongyu Lin","Jiannan Xiang","Johan Bjorck","Yu Fang","Fengyuan Hu","Spencer Huang","Kaushil Kundalia","Yen-Chen Lin","Loic Magne","Ajay Mandlekar","Avnish Narayan","You Liang Tan","Guanzhi Wang","Jing Wang","Qi Wang","Yinzhen Xu","Xiaohui Zeng","Kaiyuan Zheng","Ruijie Zheng","Ming-Yu Liu","Luke Zettlemoyer","Dieter Fox","Jan Kautz","Scott Reed","Yuke Zhu","Linxi Fan"],"pdf_url":"https://arxiv.org/pdf/2505.12705v1.pdf","comment":"See website for videos:\n  https://research.nvidia.com/labs/gear/dreamgen"},{"id":"http://arxiv.org/abs/2505.13549v1","updated":"2025-05-19T04:32:14Z","published":"2025-05-19T04:32:14Z","title":"TD-GRPC: Temporal Difference Learning with Group Relative Policy\n  Constraint for Humanoid Locomotion","summary":"  Robot learning in high-dimensional control settings, such as humanoid\nlocomotion, presents persistent challenges for reinforcement learning (RL)\nalgorithms due to unstable dynamics, complex contact interactions, and\nsensitivity to distributional shifts during training. Model-based methods,\n\\textit{e.g.}, Temporal-Difference Model Predictive Control (TD-MPC), have\ndemonstrated promising results by combining short-horizon planning with\nvalue-based learning, enabling efficient solutions for basic locomotion tasks.\nHowever, these approaches remain ineffective in addressing policy mismatch and\ninstability introduced by off-policy updates. Thus, in this work, we introduce\nTemporal-Difference Group Relative Policy Constraint (TD-GRPC), an extension of\nthe TD-MPC framework that unifies Group Relative Policy Optimization (GRPO)\nwith explicit Policy Constraints (PC). TD-GRPC applies a trust-region\nconstraint in the latent policy space to maintain consistency between the\nplanning priors and learned rollouts, while leveraging group-relative ranking\nto assess and preserve the physical feasibility of candidate trajectories.\nUnlike prior methods, TD-GRPC achieves robust motions without modifying the\nunderlying planner, enabling flexible planning and policy learning. We validate\nour method across a locomotion task suite ranging from basic walking to highly\ndynamic movements on the 26-DoF Unitree H1-2 humanoid robot. Through simulation\nresults, TD-GRPC demonstrates its improvements in stability and policy\nrobustness with sampling efficiency while training for complex humanoid control\ntasks.\n","authors":["Khang Nguyen","Khai Nguyen","An T. Le","Jan Peters","Manfred Huber","Ngo Anh Vien","Minh Nhat Vu"],"pdf_url":"https://arxiv.org/pdf/2505.13549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12679v1","updated":"2025-05-19T03:55:43Z","published":"2025-05-19T03:55:43Z","title":"Dribble Master: Learning Agile Humanoid Dribbling Through Legged\n  Locomotion","summary":"  Humanoid soccer dribbling is a highly challenging task that demands dexterous\nball manipulation while maintaining dynamic balance. Traditional rule-based\nmethods often struggle to achieve accurate ball control due to their reliance\non fixed walking patterns and limited adaptability to real-time ball dynamics.\nTo address these challenges, we propose a two-stage curriculum learning\nframework that enables a humanoid robot to acquire dribbling skills without\nexplicit dynamics or predefined trajectories. In the first stage, the robot\nlearns basic locomotion skills; in the second stage, we fine-tune the policy\nfor agile dribbling maneuvers. We further introduce a virtual camera model in\nsimulation and design heuristic rewards to encourage active sensing, promoting\na broader visual range for continuous ball perception. The policy is trained in\nsimulation and successfully transferred to a physical humanoid robot.\nExperimental results demonstrate that our method enables effective ball\nmanipulation, achieving flexible and visually appealing dribbling behaviors\nacross multiple environments. This work highlights the potential of\nreinforcement learning in developing agile humanoid soccer robots. Additional\ndetails, video demonstrations, and code are available at\nhttps://zhuoheng0910.github.io/dribble-master/.\n","authors":["Zhuoheng Wang","Jinyin Zhou","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2505.12679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12665v1","updated":"2025-05-19T03:28:02Z","published":"2025-05-19T03:28:02Z","title":"Audio-Visual Contact Classification for Tree Structures in Agriculture","summary":"  Contact-rich manipulation tasks in agriculture, such as pruning and\nharvesting, require robots to physically interact with tree structures to\nmaneuver through cluttered foliage. Identifying whether the robot is contacting\nrigid or soft materials is critical for the downstream manipulation policy to\nbe safe, yet vision alone is often insufficient due to occlusion and limited\nviewpoints in this unstructured environment. To address this, we propose a\nmulti-modal classification framework that fuses vibrotactile (audio) and visual\ninputs to identify the contact class: leaf, twig, trunk, or ambient. Our key\ninsight is that contact-induced vibrations carry material-specific signals,\nmaking audio effective for detecting contact events and distinguishing material\ntypes, while visual features add complementary semantic cues that support more\nfine-grained classification. We collect training data using a hand-held sensor\nprobe and demonstrate zero-shot generalization to a robot-mounted probe\nembodiment, achieving an F1 score of 0.82. These results underscore the\npotential of audio-visual learning for manipulation in unstructured,\ncontact-rich environments.\n","authors":["Ryan Spears","Moonyoung Lee","George Kantor","Oliver Kroemer"],"pdf_url":"https://arxiv.org/pdf/2505.12665v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2505.12661v1","updated":"2025-05-19T03:23:48Z","published":"2025-05-19T03:23:48Z","title":"Digital Twins in the Cloud: A Modular, Scalable and Interoperable\n  Framework for Accelerating Verification and Validation of Autonomous Driving\n  Solutions","summary":"  Verification and validation (V&V) of autonomous vehicles (AVs) typically\nrequires exhaustive testing across a variety of operating environments and\ndriving scenarios including rare, extreme, or hazardous situations that might\nbe difficult or impossible to capture in reality. Additionally, physical V&V\nmethods such as track-based evaluations or public-road testing are often\nconstrained by time, cost, and safety, which motivates the need for virtual\nproving grounds. However, the fidelity and scalability of simulation-based V&V\nmethods can quickly turn into a bottleneck. In such a milieu, this work\nproposes a virtual proving ground that flexibly scales digital twins within\nhigh-performance computing clusters (HPCCs) and automates the V&V process.\nHere, digital twins enable high-fidelity virtual representation of the AV and\nits operating environments, allowing extensive scenario-based testing.\nMeanwhile, HPCC infrastructure brings substantial advantages in terms of\ncomputational power and scalability, enabling rapid iterations of simulations,\nprocessing and storage of massive amounts of data, and deployment of\nlarge-scale test campaigns, thereby reducing the time and cost associated with\nthe V&V process. We demonstrate the efficacy of this approach through a case\nstudy that focuses on the variability analysis of a candidate autonomy\nalgorithm to identify potential vulnerabilities in its perception, planning,\nand control sub-systems. The modularity, scalability, and interoperability of\nthe proposed framework are demonstrated by deploying a test campaign comprising\n256 test cases on two different HPCC architectures to ensure continuous\noperation in a publicly shared resource setting. The findings highlight the\nability of the proposed framework to accelerate and streamline the V&V process,\nthereby significantly compressing (~30x) the timeline.\n","authors":["Tanmay Vilas Samak","Chinmay Vilas Samak","Giovanni Martino","Pranav Nair","Venkat Krovi"],"pdf_url":"https://arxiv.org/pdf/2505.12661v1.pdf","comment":"Accepted at ASME International Design Engineering Technical\n  Conferences & Computers and Information in Engineering Conference (IDETC-CIE)\n  2025"},{"id":"http://arxiv.org/abs/2505.12649v1","updated":"2025-05-19T03:01:54Z","published":"2025-05-19T03:01:54Z","title":"The Robot of Theseus: A modular robotic testbed for legged locomotion","summary":"  Robotic models are useful for independently varying specific features, but\nmost quadrupedal robots differ so greatly from animal morphologies that they\nhave minimal biomechanical relevance. Commercially available quadrupedal robots\nare also prohibitively expensive for biological research programs and difficult\nto customize. Here, we present a low-cost quadrupedal robot with modular legs\nthat can match a wide range of animal morphologies for biomechanical hypothesis\ntesting. The Robot Of Theseus (TROT) costs approximately $4000 to build out of\n3D printed parts and standard off-the-shelf supplies. Each limb consists of 2\nor 3 rigid links; the proximal joint can be rotated to become a knee or elbow.\nTelescoping mechanisms vary the length of each limb link. The open-source\nsoftware accommodates user-defined gaits and morphology changes. Effective leg\nlength, or crouch, is determined by the four-bar linkage actuating each joint.\nThe backdrivable motors can vary virtual spring stiffness and range of motion.\nFull descriptions of the TROT hardware and software are freely available\nonline. We demonstrate the use of TROT to compare locomotion among extant,\nextinct, and theoretical morphologies. In addition to biomechanical hypothesis\ntesting, we envision a variety of different applications for this low-cost,\nmodular, legged robotic platform, including developing novel control\nstrategies, clearing land mines, or remote exploration. All CAD and code is\navailable for download on the TROT project page.\n","authors":["Karthik Urs","Jessica Carlson","Aditya Srinivas Manohar","Michael Rakowiecki","Abdulhadi Alkayyali","John E. Saunders","Faris Tulbah","Talia Y. Moore"],"pdf_url":"https://arxiv.org/pdf/2505.12649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12648v1","updated":"2025-05-19T03:00:44Z","published":"2025-05-19T03:00:44Z","title":"SafeMove-RL: A Certifiable Reinforcement Learning Framework for Dynamic\n  Motion Constraints in Trajectory Planning","summary":"  This study presents a dynamic safety margin-based reinforcement learning\nframework for local motion planning in dynamic and uncertain environments. The\nproposed planner integrates real-time trajectory optimization with adaptive gap\nanalysis, enabling effective feasibility assessment under partial observability\nconstraints. To address safety-critical computations in unknown scenarios, an\nenhanced online learning mechanism is introduced, which dynamically corrects\nspatial trajectories by forming dynamic safety margins while maintaining\ncontrol invariance. Extensive evaluations, including ablation studies and\ncomparisons with state-of-the-art algorithms, demonstrate superior success\nrates and computational efficiency. The framework's effectiveness is further\nvalidated on both simulated and physical robotic platforms.\n","authors":["Tengfei Liu","Haoyang Zhong","Jiazheng Hu","Tan Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.12648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10552v2","updated":"2025-05-19T02:45:11Z","published":"2025-05-15T17:58:01Z","title":"Loop closure grasping: Topological transformations enable strong,\n  gentle, and versatile grasps","summary":"  Grasping mechanisms must both create and subsequently hold grasps that permit\nsafe and effective object manipulation. Existing mechanisms address the\ndifferent functional requirements of grasp creation and grasp holding using a\nsingle morphology, but have yet to achieve the simultaneous strength,\ngentleness, and versatility needed for many applications. We present \"loop\nclosure grasping\", a class of robotic grasping that addresses these different\nfunctional requirements through topological transformations between open-loop\nand closed-loop morphologies. We formalize these morphologies for grasping,\nformulate the loop closure grasping method, and present principles and a design\narchitecture that we implement using soft growing inflated beams, winches, and\nclamps. The mechanisms' initial open-loop topology enables versatile grasp\ncreation via unencumbered tip movement, and closing the loop enables strong and\ngentle holding with effectively infinite bending compliance. Loop closure\ngrasping circumvents the tradeoffs of single-morphology designs, enabling\ngrasps involving historically challenging objects, environments, and\nconfigurations.\n","authors":["Kentaro Barhydt","O. Godson Osele","Sreela Kodali","Cosima du Pasquier","Chase M. Hartquist","H. Harry Asada","Allison M. Okamura"],"pdf_url":"https://arxiv.org/pdf/2505.10552v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15830v5","updated":"2025-05-19T02:40:18Z","published":"2025-01-27T07:34:33Z","title":"SpatialVLA: Exploring Spatial Representations for Visual-Language-Action\n  Model","summary":"  In this paper, we claim that spatial understanding is the keypoint in robot\nmanipulation, and propose SpatialVLA to explore effective spatial\nrepresentations for the robot foundation model. Specifically, we introduce\nEgo3D Position Encoding to inject 3D information into the input observations of\nthe visual-language-action model, and propose Adaptive Action Grids to\nrepresent spatial robot movement actions with adaptive discretized action\ngrids, facilitating learning generalizable and transferrable spatial action\nknowledge for cross-robot control. SpatialVLA is first pre-trained on top of a\nvision-language model with 1.1 Million real-world robot episodes, to learn a\ngeneralist manipulation policy across multiple robot environments and tasks.\nAfter pre-training, SpatialVLA is directly applied to perform numerous tasks in\na zero-shot manner. The superior results in both simulation and real-world\nrobots demonstrate its advantage of inferring complex robot motion trajectories\nand its strong in-domain multi-task generalization ability. We further show the\nproposed Adaptive Action Grids offer a new and effective way to fine-tune the\npre-trained SpatialVLA model for new simulation and real-world setups, where\nthe pre-learned action grids are re-discretized to capture robot-specific\nspatial action movements of new setups. The superior results from extensive\nevaluations demonstrate the exceptional in-distribution generalization and\nout-of-distribution adaptation capability, highlighting the crucial benefit of\nthe proposed spatial-aware representations for generalist robot policy\nlearning. All the details and codes will be open-sourced.\n","authors":["Delin Qu","Haoming Song","Qizhi Chen","Yuanqi Yao","Xinyi Ye","Yan Ding","Zhigang Wang","JiaYuan Gu","Bin Zhao","Dong Wang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2501.15830v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12624v1","updated":"2025-05-19T02:15:14Z","published":"2025-05-19T02:15:14Z","title":"EndoForce: Development of an Intuitive Axial Force Measurement Device\n  for Endoscopic Robotic Systems","summary":"  Robotic endoscopic systems provide intuitive control and eliminate radiation\nexposure, making them a promising alternative to conventional methods. However,\nthe lack of axial force measurement from the robot remains a major challenge,\nas it can lead to excessive colonic elongation, perforation, or ureteral\ncomplications. Although various methods have been proposed in previous studies,\nlimitations such as model dependency, bulkiness, and environmental sensitivity\nremain challenges that should be addressed before clinical application. In this\nstudy, we propose EndoForce, a device designed for intuitive and accurate axial\nforce measurement in endoscopic robotic systems. Inspired by the insertion\nmotion performed by medical doctors during ureteroscopy and gastrointestinal\n(GI) endoscopy, EndoForce ensures precise force measuring while maintaining\ncompatibility with clinical environments. The device features a streamlined\ndesign, allowing for the easy attachment and detachment of a sterile cover, and\nincorporates a commercial load cell to enhance cost-effectiveness and\nfacilitate practical implementation in real medical applications. To validate\nthe effectiveness of the proposed EndoForce, physical experiments were\nperformed using a testbed that simulates the ureter. We show that the axial\nforce generated during insertion was measured with high accuracy, regardless of\nwhether the pathway was straight or curved, in a testbed simulating the human\nureter.\n","authors":["Hansoul Kim","Dong-Ho Lee","Dukyoo Kong","Dong-Soo Kwon","Byungsik Cheon"],"pdf_url":"https://arxiv.org/pdf/2505.12624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12583v1","updated":"2025-05-19T00:11:42Z","published":"2025-05-19T00:11:42Z","title":"A Comprehensive Survey on Physical Risk Control in the Era of Foundation\n  Model-enabled Robotics","summary":"  Recent Foundation Model-enabled robotics (FMRs) display greatly improved\ngeneral-purpose skills, enabling more adaptable automation than conventional\nrobotics. Their ability to handle diverse tasks thus creates new opportunities\nto replace human labor. However, unlike general foundation models, FMRs\ninteract with the physical world, where their actions directly affect the\nsafety of humans and surrounding objects, requiring careful deployment and\ncontrol. Based on this proposition, our survey comprehensively summarizes robot\ncontrol approaches to mitigate physical risks by covering all the lifespan of\nFMRs ranging from pre-deployment to post-accident stage. Specifically, we\nbroadly divide the timeline into the following three phases: (1) pre-deployment\nphase, (2) pre-incident phase, and (3) post-incident phase. Throughout this\nsurvey, we find that there is much room to study (i) pre-incident risk\nmitigation strategies, (ii) research that assumes physical interaction with\nhumans, and (iii) essential issues of foundation models themselves. We hope\nthat this survey will be a milestone in providing a high-resolution analysis of\nthe physical risks of FMRs and their control, contributing to the realization\nof a good human-robot relationship.\n","authors":["Takeshi Kojima","Yaonan Zhu","Yusuke Iwasawa","Toshinori Kitamura","Gang Yan","Shu Morikuni","Ryosuke Takanami","Alfredo Solano","Tatsuya Matsushima","Akiko Murakami","Yutaka Matsuo"],"pdf_url":"https://arxiv.org/pdf/2505.12583v1.pdf","comment":"Accepted to IJCAI 2025 Survey Track"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2505.13757v1","updated":"2025-05-19T22:10:27Z","published":"2025-05-19T22:10:27Z","title":"LLM-Based Compact Reranking with Document Features for Scientific\n  Retrieval","summary":"  Scientific retrieval is essential for advancing academic discovery. Within\nthis process, document reranking plays a critical role by refining first-stage\nretrieval results. However, large language model (LLM) listwise reranking faces\nunique challenges in the scientific domain. First-stage retrieval is often\nsuboptimal in the scientific domain, so relevant documents are ranked lower.\nMoreover, conventional listwise reranking uses the full text of candidate\ndocuments in the context window, limiting the number of candidates that can be\nconsidered. As a result, many relevant documents are excluded before reranking,\nwhich constrains overall retrieval performance. To address these challenges, we\nexplore compact document representations based on semantic features such as\ncategories, sections, and keywords, and propose a training-free, model-agnostic\nreranking framework for scientific retrieval called CoRank. The framework\ninvolves three stages: (i) offline extraction of document-level features, (ii)\ncoarse reranking using these compact representations, and (iii) fine-grained\nreranking on full texts of the top candidates from stage (ii). This hybrid\ndesign provides a high-level abstraction of document semantics, expands\ncandidate coverage, and retains critical details required for precise ranking.\nExperiments on LitSearch and CSFCube show that CoRank significantly improves\nreranking performance across different LLM backbones, increasing nDCG@10 from\n32.0 to 39.7. Overall, these results highlight the value of information\nextraction for reranking in scientific retrieval.\n","authors":["Runchu Tian","Xueqiang Xu","Bowen Jin","SeongKu Kang","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2505.13757v1.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.09795v2","updated":"2025-05-19T17:37:21Z","published":"2025-05-14T20:45:29Z","title":"Beyond Pairwise Learning-To-Rank At Airbnb","summary":"  There are three fundamental asks from a ranking algorithm: it should scale to\nhandle a large number of items, sort items accurately by their utility, and\nimpose a total order on the items for logical consistency. But here's the\ncatch-no algorithm can achieve all three at the same time. We call this\nlimitation the SAT theorem for ranking algorithms. Given the dilemma, how can\nwe design a practical system that meets user needs? Our current work at Airbnb\nprovides an answer, with a working solution deployed at scale. We start with\npairwise learning-to-rank (LTR) models-the bedrock of search ranking tech\nstacks today. They scale linearly with the number of items ranked and perform\nstrongly on metrics like NDCG by learning from pairwise comparisons. They are\nat a sweet spot of performance vs. cost, making them an ideal choice for\nseveral industrial applications. However, they have a drawback-by ignoring\ninteractions between items, they compromise on accuracy. To improve accuracy,\nwe create a \"true\" pairwise LTR model-one that captures interactions between\nitems during pairwise comparisons. But accuracy comes at the expense of\nscalability and total order, and we discuss strategies to counter these\nchallenges. For greater accuracy, we take each item in the search result, and\ncompare it against the rest of the items along two dimensions: (1) Superiority:\nHow strongly do searchers prefer the given item over the remaining ones? (2)\nSimilarity: How similar is the given item to all the other items? This forms\nthe basis of our \"all-pairwise\" LTR framework, which factors in interactions\nacross all items at once. Looking at items on the search result page all\ntogether-superiority and similarity combined-gives us a deeper understanding of\nwhat searchers truly want. We quantify the resulting improvements in searcher\nexperience through offline and online experiments at Airbnb.\n","authors":["Malay Haldar","Daochen Zha","Huiji Gao","Liwei He","Sanjeev Katariya"],"pdf_url":"https://arxiv.org/pdf/2505.09795v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13306v1","updated":"2025-05-19T16:25:55Z","published":"2025-05-19T16:25:55Z","title":"GMM-Based Comprehensive Feature Extraction and Relative Distance\n  Preservation For Few-Shot Cross-Modal Retrieval","summary":"  Few-shot cross-modal retrieval focuses on learning cross-modal\nrepresentations with limited training samples, enabling the model to handle\nunseen classes during inference. Unlike traditional cross-modal retrieval\ntasks, which assume that both training and testing data share the same class\ndistribution, few-shot retrieval involves data with sparse representations\nacross modalities. Existing methods often fail to adequately model the\nmulti-peak distribution of few-shot cross-modal data, resulting in two main\nbiases in the latent semantic space: intra-modal bias, where sparse samples\nfail to capture intra-class diversity, and inter-modal bias, where\nmisalignments between image and text distributions exacerbate the semantic gap.\nThese biases hinder retrieval accuracy. To address these issues, we propose a\nnovel method, GCRDP, for few-shot cross-modal retrieval. This approach\neffectively captures the complex multi-peak distribution of data using a\nGaussian Mixture Model (GMM) and incorporates a multi-positive sample\ncontrastive learning mechanism for comprehensive feature modeling.\nAdditionally, we introduce a new strategy for cross-modal semantic alignment,\nwhich constrains the relative distances between image and text feature\ndistributions, thereby improving the accuracy of cross-modal representations.\nWe validate our approach through extensive experiments on four benchmark\ndatasets, demonstrating superior performance over six state-of-the-art methods.\n","authors":["Chengsong Sun","Weiping Li","Xiang Li","Yuankun Liu","Lianlei Shan"],"pdf_url":"https://arxiv.org/pdf/2505.13306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13581v1","updated":"2025-05-19T15:41:39Z","published":"2025-05-19T15:41:39Z","title":"RAR: Setting Knowledge Tripwires for Retrieval Augmented Rejection","summary":"  Content moderation for large language models (LLMs) remains a significant\nchallenge, requiring flexible and adaptable solutions that can quickly respond\nto emerging threats. This paper introduces Retrieval Augmented Rejection (RAR),\na novel approach that leverages a retrieval-augmented generation (RAG)\narchitecture to dynamically reject unsafe user queries without model\nretraining. By strategically inserting and marking malicious documents into the\nvector database, the system can identify and reject harmful requests when these\ndocuments are retrieved. Our preliminary results show that RAR achieves\ncomparable performance to embedded moderation in LLMs like Claude 3.5 Sonnet,\nwhile offering superior flexibility and real-time customization capabilities, a\nfundamental feature to timely address critical vulnerabilities. This approach\nintroduces no architectural changes to existing RAG systems, requiring only the\naddition of specially crafted documents and a simple rejection mechanism based\non retrieval results.\n","authors":["Tommaso Mario Buonocore","Enea Parimbelli"],"pdf_url":"https://arxiv.org/pdf/2505.13581v1.pdf","comment":"7 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2407.18383v2","updated":"2025-05-19T14:37:44Z","published":"2024-07-25T20:36:20Z","title":"Supporting Evidence-Based Medicine by Finding Both Relevant and\n  Significant Works","summary":"  In this paper, we present a new approach to improving the relevance and\nreliability of medical IR, which builds upon the concept of Level of Evidence\n(LoE). LoE framework categorizes medical publications into 7 distinct levels\nbased on the underlying empirical evidence. Despite LoE framework's relevance\nin medical research and evidence-based practice, only few medical publications\nexplicitly state their LoE. Therefore, we develop a classification model for\nautomatically assigning LoE to medical publications, which successfully\nclassifies over 26 million documents in MEDLINE database into LoE classes. The\nsubsequent retrieval experiments on TREC PM datasets show substantial\nimprovements in retrieval relevance, when LoE is used as a search filter.\n","authors":["Sameh Frihat","Norbert Fuhr"],"pdf_url":"https://arxiv.org/pdf/2407.18383v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13129v1","updated":"2025-05-19T14:00:10Z","published":"2025-05-19T14:00:10Z","title":"Optimizing Retrieval Augmented Generation for Object Constraint Language","summary":"  The Object Constraint Language (OCL) is essential for defining precise\nconstraints within Model-Based Systems Engineering (MBSE). However, manually\nwriting OCL rules is complex and time-consuming. This study explores the\noptimization of Retrieval-Augmented Generation (RAG) for automating OCL rule\ngeneration, focusing on the impact of different retrieval strategies. We\nevaluate three retrieval approaches $\\unicode{x2013}$ BM25 (lexical-based),\nBERT-based (semantic retrieval), and SPLADE (sparse-vector retrieval)\n$\\unicode{x2013}$ analyzing their effectiveness in providing relevant context\nfor a large language model.\n  To further assess our approach, we compare and benchmark our\nretrieval-optimized generation results against PathOCL, a state-of-the-art\ngraph-based method. We directly compare BM25, BERT, and SPLADE retrieval\nmethods with PathOCL to understand how different retrieval methods perform for\na unified evaluation framework. Our experimental results, focusing on\nretrieval-augmented generation, indicate that while retrieval can enhance\ngeneration accuracy, its effectiveness depends on the retrieval method and the\nnumber of retrieved chunks (k). BM25 underperforms the baseline, whereas\nsemantic approaches (BERT and SPLADE) achieve better results, with SPLADE\nperforming best at lower k values. However, excessive retrieval with high k\nparameter can lead to retrieving irrelevant chunks which degrades model\nperformance. Our findings highlight the importance of optimizing retrieval\nconfigurations to balance context relevance and output consistency. This\nresearch provides insights into improving OCL rule generation using RAG and\nunderscores the need for tailoring retrieval.\n","authors":["Kevin Chenhao Li","Vahid Zolfaghari","Nenad Petrovic","Fengjunjie Pan","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2505.13129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13572v1","updated":"2025-05-19T13:26:51Z","published":"2025-05-19T13:26:51Z","title":"Q${}^2$Forge: Minting Competency Questions and SPARQL Queries for\n  Question-Answering Over Knowledge Graphs","summary":"  The SPARQL query language is the standard method to access knowledge graphs\n(KGs). However, formulating SPARQL queries is a significant challenge for\nnon-expert users, and remains time-consuming for the experienced ones. Best\npractices recommend to document KGs with competency questions and example\nqueries to contextualise the knowledge they contain and illustrate their\npotential applications. In practice, however, this is either not the case or\nthe examples are provided in limited numbers. Large Language Models (LLMs) are\nbeing used in conversational agents and are proving to be an attractive\nsolution with a wide range of applications, from simple question-answering\nabout common knowledge to generating code in a targeted programming language.\nHowever, training and testing these models to produce high quality SPARQL\nqueries from natural language questions requires substantial datasets of\nquestion-query pairs. In this paper, we present Q${}^2$Forge that addresses the\nchallenge of generating new competency questions for a KG and corresponding\nSPARQL queries. It iteratively validates those queries with human feedback and\nLLM as a judge. Q${}^2$Forge is open source, generic, extensible and modular,\nmeaning that the different modules of the application (CQ generation, query\ngeneration and query refinement) can be used separately, as an integrated\npipeline, or replaced by alternative services. The result is a complete\npipeline from competency question formulation to query evaluation, supporting\nthe creation of reference query sets for any target KG.\n","authors":["Yousouf Taghzouti","Franck Michel","Tao Jiang","Louis-Félix Nothias","Fabien Gandon"],"pdf_url":"https://arxiv.org/pdf/2505.13572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08071v2","updated":"2025-05-19T12:37:24Z","published":"2025-02-12T02:24:26Z","title":"Collaborative Filtering Meets Spectrum Shift: Connecting User-Item\n  Interaction with Graph-Structured Side Information","summary":"  Graph Neural Networks (GNNs) have demonstrated their superiority in\ncollaborative filtering, where the user-item (U-I) interaction bipartite graph\nserves as the fundamental data format. However, when graph-structured side\ninformation (e.g., multimodal similarity graphs or social networks) is\nintegrated into the U-I bipartite graph, existing graph collaborative filtering\nmethods fall short of achieving satisfactory performance. We quantitatively\nanalyze this problem from a spectral perspective. Recall that a bipartite graph\npossesses a full spectrum within the range of [-1, 1], with the highest\nfrequency exactly achievable at -1 and the lowest frequency at 1; however, we\nobserve as more side information is incorporated, the highest frequency of the\naugmented adjacency matrix progressively shifts rightward. This spectrum shift\nphenomenon has caused previous approaches built for the full spectrum [-1, 1]\nto assign mismatched importance to different frequencies. To this end, we\npropose Spectrum Shift Correction (dubbed SSC), incorporating shifting and\nscaling factors to enable spectral GNNs to adapt to the shifted spectrum.\nUnlike previous paradigms of leveraging side information, which necessitate\ntailored designs for diverse data types, SSC directly connects traditional\ngraph collaborative filtering with any graph-structured side information.\nExperiments on social and multimodal recommendation demonstrate the\neffectiveness of SSC, achieving relative improvements of up to 23% without\nincurring any additional computational overhead. Our code is available at\nhttps://github.com/yhhe2004/SSC-KDD.\n","authors":["Yunhang He","Cong Xu","Jun Wang","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08071v2.pdf","comment":"Accepted at KDD 2025"},{"id":"http://arxiv.org/abs/2504.20734v2","updated":"2025-05-19T11:09:12Z","published":"2025-04-29T13:18:58Z","title":"UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse\n  Modalities and Granularities","summary":"  Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single aggregated corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over various modality-specific and unified\nbaselines.\n","authors":["Woongyeong Yeo","Kangsan Kim","Soyeong Jeong","Jinheon Baek","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2504.20734v2.pdf","comment":"Project page : https://universalrag.github.io"},{"id":"http://arxiv.org/abs/2502.11571v2","updated":"2025-05-19T10:58:50Z","published":"2025-02-17T09:05:21Z","title":"FaMTEB: Massive Text Embedding Benchmark in Persian Language","summary":"  In this paper, we introduce a comprehensive benchmark for Persian (Farsi)\ntext embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our\nbenchmark includes 63 datasets spanning seven different tasks: classification,\nclustering, pair classification, reranking, retrieval, summary retrieval, and\nsemantic textual similarity. The datasets are formed as a combination of\nexisting, translated, and newly generated data, offering a diverse evaluation\nframework for Persian language models. Given the increasing use of text\nembedding models in chatbots, evaluation datasets are becoming inseparable\ningredients in chatbot challenges and Retrieval-Augmented Generation systems.\nAs a contribution, we include chatbot evaluation datasets in the MTEB benchmark\nfor the first time. In addition, in this paper, we introduce the new task of\nsummary retrieval which is not part of the tasks included in standard MTEB.\nAnother contribution of this paper is the introduction of a substantial number\nof new Persian language NLP datasets suitable for training and evaluation, some\nof which have no previous counterparts in Persian. We evaluate the performance\nof several Persian and multilingual embedding models in a range of tasks. This\nwork introduces an open-source benchmark with datasets, code and a public\nleaderboard.\n","authors":["Erfan Zinvandi","Morteza Alikhani","Mehran Sarmadi","Zahra Pourbahman","Sepehr Arvin","Reza Kazemi","Arash Amini"],"pdf_url":"https://arxiv.org/pdf/2502.11571v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12925v1","updated":"2025-05-19T10:07:51Z","published":"2025-05-19T10:07:51Z","title":"CPRet: A Dataset, Benchmark, and Model for Retrieval in Competitive\n  Programming","summary":"  Competitive programming benchmarks are widely used in scenarios such as\nprogramming contests and large language model assessments. However, the growing\npresence of duplicate or highly similar problems raises concerns not only about\ncompetition fairness, but also about the validity of competitive programming as\na benchmark for model evaluation. In this paper, we propose a new problem --\nsimilar question retrieval -- to address this issue. Due to the lack of both\ndata and models, solving this problem is challenging. To this end, we introduce\nCPRet, a retrieval-oriented benchmark suite for competitive programming,\ncovering four retrieval tasks: two code-centric (i.e., Text-to-Code and\nCode-to-Code) and two newly proposed problem-centric tasks (i.e.,\nProblem-to-Duplicate and Simplified-to-Full), built from a combination of\nautomatically crawled problem-solution data and manually curated annotations.\nOur contribution includes both high-quality training data and temporally\nseparated test sets for reliable evaluation. In addition, we develop two\ntask-specialized retrievers based on this dataset: CPRetriever-Code, trained\nwith a novel Group-InfoNCE loss for problem-code alignment, and\nCPRetriever-Prob, fine-tuned for identifying problem-level similarity. Both\nmodels achieve strong results and are open-sourced for local use. Finally, we\nanalyze LiveCodeBench and find that high-similarity problems inflate model pass\nrates and reduce differentiation, underscoring the need for similarity-aware\nevaluation in future benchmarks.\n  Code and data are available at: https://github.com/coldchair/CPRet\n","authors":["Han Deng","Yuan Meng","Shixiang Tang","Wanli Ouyang","Xinzhu Ma"],"pdf_url":"https://arxiv.org/pdf/2505.12925v1.pdf","comment":"main 9 pages"},{"id":"http://arxiv.org/abs/2502.03041v2","updated":"2025-05-19T09:48:53Z","published":"2025-02-05T09:56:52Z","title":"Large Language Model as Universal Retriever in Industrial-Scale\n  Recommender System","summary":"  In real-world recommender systems, different retrieval objectives are\ntypically addressed using task-specific datasets with carefully designed model\narchitectures. We demonstrate that Large Language Models (LLMs) can function as\nuniversal retrievers, capable of handling multiple objectives within a\ngenerative retrieval framework. To model complex user-item relationships within\ngenerative retrieval, we propose multi-query representation. To address the\nchallenge of extremely large candidate sets in industrial recommender systems,\nwe introduce matrix decomposition to boost model learnability,\ndiscriminability, and transferability, and we incorporate probabilistic\nsampling to reduce computation costs. Finally, our Universal Retrieval Model\n(URM) can adaptively generate a set from tens of millions of candidates based\non arbitrary given objective while keeping the latency within tens of\nmilliseconds. Applied to industrial-scale data, URM outperforms expert models\nelaborately designed for different retrieval objectives on offline experiments\nand significantly improves the core metric of online advertising platform by\n$3\\%$.\n","authors":["Junguang Jiang","Yanwen Huang","Bin Liu","Xiaoyu Kong","Xinhang Li","Ziru Xu","Han Zhu","Jian Xu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.03041v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13557v1","updated":"2025-05-19T08:59:08Z","published":"2025-05-19T08:59:08Z","title":"AMAQA: A Metadata-based QA Dataset for RAG Systems","summary":"  Retrieval-augmented generation (RAG) systems are widely used in\nquestion-answering (QA) tasks, but current benchmarks lack metadata\nintegration, hindering evaluation in scenarios requiring both textual data and\nexternal information. To address this, we present AMAQA, a new open-access QA\ndataset designed to evaluate tasks combining text and metadata. The integration\nof metadata is especially important in fields that require rapid analysis of\nlarge volumes of data, such as cybersecurity and intelligence, where timely\naccess to relevant information is critical. AMAQA includes about 1.1 million\nEnglish messages collected from 26 public Telegram groups, enriched with\nmetadata such as timestamps, topics, emotional tones, and toxicity indicators,\nwhich enable precise and contextualized queries by filtering documents based on\nspecific criteria. It also includes 450 high-quality QA pairs, making it a\nvaluable resource for advancing research on metadata-driven QA and RAG systems.\nTo the best of our knowledge, AMAQA is the first single-hop QA benchmark to\nincorporate metadata and labels such as topics covered in the messages. We\nconduct extensive tests on the benchmark, establishing a new standard for\nfuture research. We show that leveraging metadata boosts accuracy from 0.12 to\n0.61, highlighting the value of structured context. Building on this, we\nexplore several strategies to refine the LLM input by iterating over provided\ncontext and enriching it with noisy documents, achieving a further 3-point gain\nover the best baseline and a 14-point improvement over simple metadata\nfiltering. The dataset is available at\nhttps://anonymous.4open.science/r/AMAQA-5D0D/\n","authors":["Davide Bruni","Marco Avvenuti","Nicola Tonellotto","Maurizio Tesconi"],"pdf_url":"https://arxiv.org/pdf/2505.13557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12989v2","updated":"2025-05-19T07:26:25Z","published":"2024-11-20T02:34:21Z","title":"Data Watermarking for Sequential Recommender Systems","summary":"  In the era of large foundation models, data has become a crucial component in\nbuilding high-performance AI systems. As the demand for high-quality and\nlarge-scale data continues to rise, data copyright protection is attracting\nincreasing attention. In this work, we explore the problem of data watermarking\nfor sequential recommender systems, where a watermark is embedded into the\ntarget dataset and can be detected in models trained on that dataset. We focus\non two settings: dataset watermarking, which protects the ownership of the\nentire dataset, and user watermarking, which safeguards the data of individual\nusers. We present a method named Dataset Watermarking for Recommender Systems\n(DWRS) to address them. We define the watermark as a sequence of consecutive\nitems inserted into normal users' interaction sequences. We define a Receptive\nField (RF) to guide the inserting process to facilitate the memorization of the\nwatermark. Extensive experiments on five representative sequential\nrecommendation models and three benchmark datasets demonstrate the\neffectiveness of DWRS in protecting data copyright while preserving model\nutility.\n","authors":["Sixiao Zhang","Cheng Long","Wei Yuan","Hongxu Chen","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2411.12989v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12791v1","updated":"2025-05-19T07:23:46Z","published":"2025-05-19T07:23:46Z","title":"Unlearning for Federated Online Learning to Rank: A Reproducibility\n  Study","summary":"  This paper reports on findings from a comparative study on the effectiveness\nand efficiency of federated unlearning strategies within Federated Online\nLearning to Rank (FOLTR), with specific attention to systematically analysing\nthe unlearning capabilities of methods in a verifiable manner.\n  Federated approaches to ranking of search results have recently garnered\nattention to address users privacy concerns. In FOLTR, privacy is safeguarded\nby collaboratively training ranking models across decentralized data sources,\npreserving individual user data while optimizing search results based on\nimplicit feedback, such as clicks.\n  Recent legislation introduced across numerous countries is establishing the\nso called \"the right to be forgotten\", according to which services based on\nmachine learning models like those in FOLTR should provide capabilities that\nallow users to remove their own data from those used to train models. This has\nsparked the development of unlearning methods, along with evaluation practices\nto measure whether unlearning of a user data successfully occurred. Current\nevaluation practices are however often controversial, necessitating the use of\nmultiple metrics for a more comprehensive assessment -- but previous proposals\nof unlearning methods only used single evaluation metrics.\n  This paper addresses this limitation: our study rigorously assesses the\neffectiveness of unlearning strategies in managing both under-unlearning and\nover-unlearning scenarios using adapted, and newly proposed evaluation metrics.\nThanks to our detailed analysis, we uncover the strengths and limitations of\nfive unlearning strategies, offering valuable insights into optimizing\nfederated unlearning to balance data privacy and system performance within\nFOLTR. We publicly release our code and complete results at\nhttps://github.com/Iris1026/Unlearning-for-FOLTR.git.\n","authors":["Yiling Tao","Shuyi Wang","Jiaxi Yang","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2505.12791v1.pdf","comment":"Accepted at SIGIR2025"},{"id":"http://arxiv.org/abs/2505.12782v1","updated":"2025-05-19T07:11:07Z","published":"2025-05-19T07:11:07Z","title":"AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large\n  Multimodal-Models Reasoning","summary":"  Large Multimodal Models (LMMs) have become a pivotal research focus in deep\nlearning, demonstrating remarkable capabilities in 3D scene understanding.\nHowever, current 3D LMMs employing thousands of spatial tokens for multimodal\nreasoning suffer from critical inefficiencies: excessive computational overhead\nand redundant information flows. Unlike 2D VLMs processing single images, 3D\nLMMs exhibit inherent architectural redundancy due to the heterogeneous\nmechanisms between spatial tokens and visual tokens. To address this challenge,\nwe propose AdaToken-3D, an adaptive spatial token optimization framework that\ndynamically prunes redundant tokens through spatial contribution analysis. Our\nmethod automatically tailors pruning strategies to different 3D LMM\narchitectures by quantifying token-level information flows via attention\npattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM)\ndemonstrate that AdaToken-3D achieves 21\\% faster inference speed and 63\\%\nFLOPs reduction while maintaining original task accuracy. Beyond efficiency\ngains, this work systematically investigates redundancy patterns in multimodal\nspatial information flows through quantitative token interaction analysis. Our\nfindings reveal that over 60\\% of spatial tokens contribute minimally ($<$5\\%)\nto the final predictions, establishing theoretical foundations for efficient 3D\nmultimodal learning.\n","authors":["Kai Zhang","Xingyu Chen","Xiaofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.12782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09436v2","updated":"2025-05-19T06:27:01Z","published":"2025-05-14T14:44:30Z","title":"CXMArena: Unified Dataset to benchmark performance in realistic CXM\n  Scenarios","summary":"  Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques.\n","authors":["Raghav Garg","Kapil Sharma","Karan Gupta"],"pdf_url":"https://arxiv.org/pdf/2505.09436v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13550v1","updated":"2025-05-19T04:49:47Z","published":"2025-05-19T04:49:47Z","title":"JIR-Arena: The First Benchmark Dataset for Just-in-time Information\n  Recommendation","summary":"  Just-in-time Information Recommendation (JIR) is a service designed to\ndeliver the most relevant information precisely when users need it, ,\naddressing their knowledge gaps with minimal effort and boosting\ndecision-making and efficiency in daily life. Advances in device-efficient\ndeployment of foundation models and the growing use of intelligent wearable\ndevices have made always-on JIR assistants feasible. However, there has been no\nsystematic effort to formally define JIR tasks or establish evaluation\nframeworks. To bridge this gap, we present the first mathematical definition of\nJIR tasks and associated evaluation metrics. Additionally, we introduce\nJIR-Arena, a multimodal benchmark dataset featuring diverse,\ninformation-request-intensive scenarios to evaluate JIR systems across critical\ndimensions: i) accurately inferring user information needs, ii) delivering\ntimely and relevant recommendations, and iii) avoiding irrelevant content that\nmay distract users.\n  Developing a JIR benchmark dataset poses challenges due to subjectivity in\nestimating user information needs and uncontrollable system variables affecting\nreproducibility. To address these, JIR-Arena: i) combines input from multiple\nhumans and large AI models to approximate information need distributions; ii)\nassesses JIR quality through information retrieval outcomes using static\nknowledge base snapshots; and iii) employs a multi-turn, multi-entity\nvalidation framework to improve objectivity and generality. Furthermore, we\nimplement a baseline JIR system capable of processing real-time information\nstreams aligned with user inputs. Our evaluation of this baseline system on\nJIR-Arena indicates that while foundation model-based JIR systems simulate user\nneeds with reasonable precision, they face challenges in recall and effective\ncontent retrieval. To support future research in this new area, we fully\nrelease our code and data.\n","authors":["Ke Yang","Kevin Ros","Shankar Kumar Senthil Kumar","ChengXiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2505.13550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12697v1","updated":"2025-05-19T04:37:53Z","published":"2025-05-19T04:37:53Z","title":"Towards A Generalist Code Embedding Model Based On Massive Data\n  Synthesis","summary":"  Code embedding models attract increasing attention due to the widespread\npopularity of retrieval-augmented generation (RAG) in software development.\nThese models are expected to capture the rich semantic relationships inherent\nto code, which differ significantly from those found in text. However, existing\nmodels remain severely limited due to the scarcity of high-quality training\ndata. In this work, we introduce \\textbf{CodeR} (\\underline{Code}\n\\underline{R}etrieval), a state-of-the-art embedding model for general-purpose\ncode retrieval. The superior performance of CodeR is built upon CodeR-Pile, a\nlarge-scale synthetic dataset constructed under the DRU (Diversity,\nReliability, Usability) principle via a novel data synthesis pipeline. To\noptimize training effectiveness, we propose Annealing, a curriculum learning\nstrategy that enables effective knowledge transfer across heterogeneous sources\nof data. We evaluate CodeR based on 16 diverse code retrieval tasks, where it\nsignificantly outperforms existing baselines and exhibits strong out-of-domain\ngeneralization performance. We have publicly released our code and the\nwell-trained model to facilitate further research in this critical area.\nhttps://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_Coder.\n","authors":["Chaofan Li","Jianlyu Chen","Yingxia Shao","Defu Lian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2505.12697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12694v1","updated":"2025-05-19T04:33:09Z","published":"2025-05-19T04:33:09Z","title":"LLM-based Query Expansion Fails for Unfamiliar and Ambiguous Queries","summary":"  Query expansion (QE) enhances retrieval by incorporating relevant terms, with\nlarge language models (LLMs) offering an effective alternative to traditional\nrule-based and statistical methods. However, LLM-based QE suffers from a\nfundamental limitation: it often fails to generate relevant knowledge,\ndegrading search performance. Prior studies have focused on hallucination, yet\nits underlying cause--LLM knowledge deficiencies--remains underexplored. This\npaper systematically examines two failure cases in LLM-based QE: (1) when the\nLLM lacks query knowledge, leading to incorrect expansions, and (2) when the\nquery is ambiguous, causing biased refinements that narrow search coverage. We\nconduct controlled experiments across multiple datasets, evaluating the effects\nof knowledge and query ambiguity on retrieval performance using sparse and\ndense retrieval models. Our results reveal that LLM-based QE can significantly\ndegrade the retrieval effectiveness when knowledge in the LLM is insufficient\nor query ambiguity is high. We introduce a framework for evaluating QE under\nthese conditions, providing insights into the limitations of LLM-based\nretrieval augmentation.\n","authors":["Kenya Abe","Kunihiro Takeoka","Makoto P. Kato","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2505.12694v1.pdf","comment":"Accepted at SIGIR 2025 short paper track"},{"id":"http://arxiv.org/abs/2501.15087v2","updated":"2025-05-19T03:34:25Z","published":"2025-01-25T05:30:58Z","title":"Multi-Grained Patch Training for Efficient LLM-based Recommendation","summary":"  Large Language Models (LLMs) have emerged as a new paradigm for\nrecommendation by converting interacted item history into language modeling.\nHowever, constrained by the limited context length of LLMs, existing approaches\nhave to truncate item history in the prompt, focusing only on recent\ninteractions and sacrificing the ability to model long-term history. To enable\nLLMs to model long histories, we pursue a concise embedding representation for\nitems and sessions. In the LLM embedding space, we construct an item's\nembedding by aggregating its textual token embeddings; similarly, we construct\na session's embedding by aggregating its item embeddings. While efficient, this\nway poses two challenges since it ignores the temporal significance of user\ninteractions and LLMs do not natively interpret our custom embeddings. To\novercome these, we propose PatchRec, a multi-grained patch training method\nconsisting of two stages: (1) Patch Pre-training, which familiarizes LLMs with\naggregated embeddings -- patches, and (2) Patch Fine-tuning, which enables LLMs\nto capture time-aware significance in interaction history. Extensive\nexperiments show that PatchRec effectively models longer behavior histories\nwith improved efficiency. This work facilitates the practical use of LLMs for\nmodeling long behavior histories. Codes are available at\nhttps://github.com/ljy0ustc/PatchRec.\n","authors":["Jiayi Liao","Ruobing Xie","Sihang Li","Xiang Wang","Xingwu Sun","Zhanhui Kang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2501.15087v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13545v1","updated":"2025-05-19T03:17:41Z","published":"2025-05-19T03:17:41Z","title":"Know Or Not: a library for evaluating out-of-knowledge base robustness","summary":"  While the capabilities of large language models (LLMs) have progressed\nsignificantly, their use in high-stakes applications have been limited due to\nrisks of hallucination. One key approach in reducing hallucination is\nretrieval-augmented generation (RAG), but even in such setups, LLMs may still\nhallucinate when presented with questions outside of the knowledge base. Such\nbehavior is unacceptable in high-stake applications where LLMs are expected to\nabstain from answering queries it does not have sufficient context on. In this\nwork, we present a novel methodology for systematically evaluating\nout-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not\nknow) in the RAG setting, without the need for manual annotation of gold\nstandard answers. We implement our methodology in knowornot, an open-source\nlibrary that enables users to develop their own customized evaluation data and\npipelines for OOKB robustness. knowornot comprises four main features. Firstly,\nit provides a unified, high-level API that streamlines the process of setting\nup and running robustness benchmarks. Secondly, its modular architecture\nemphasizes extensibility and flexibility, allowing users to easily integrate\ntheir own LLM clients and RAG settings. Thirdly, its rigorous data modeling\ndesign ensures experiment reproducibility, reliability and traceability.\nLastly, it implements a comprehensive suite of tools for users to customize\ntheir pipelines. We demonstrate the utility of knowornot by developing a\nchallenging benchmark, PolicyBench, which spans four Question-Answer (QA)\nchatbots on government policies, and analyze its OOKB robustness. The source\ncode of knowornot is available\nhttps://github.com/govtech-responsibleai/KnowOrNot.\n","authors":["Jessica Foo","Pradyumna Shyama Prasad","Shaun Khoo"],"pdf_url":"https://arxiv.org/pdf/2505.13545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12621v1","updated":"2025-05-19T02:08:20Z","published":"2025-05-19T02:08:20Z","title":"Think Before You Attribute: Improving the Performance of LLMs\n  Attribution Systems","summary":"  Large Language Models (LLMs) are increasingly applied in various science\ndomains, yet their broader adoption remains constrained by a critical\nchallenge: the lack of trustworthy, verifiable outputs. Current LLMs often\ngenerate answers without reliable source attribution, or worse, with incorrect\nattributions, posing a barrier to their use in scientific and high-stakes\nsettings, where traceability and accountability are non-negotiable. To be\nreliable, attribution systems need high accuracy and retrieve data with short\nlengths, i.e., attribute to a sentence within a document rather than a whole\ndocument. We propose a sentence-level pre-attribution step for\nRetrieve-Augmented Generation (RAG) systems that classify sentences into three\ncategories: not attributable, attributable to a single quote, and attributable\nto multiple quotes. By separating sentences before attribution, a proper\nattribution method can be selected for the type of sentence, or the attribution\ncan be skipped altogether. Our results indicate that classifiers are\nwell-suited for this task. In this work, we propose a pre-attribution step to\nreduce the computational complexity of attribution, provide a clean version of\nthe HAGRID dataset, and provide an end-to-end attribution system that works out\nof the box.\n","authors":["João Eduardo Batista","Emil Vatai","Mohamed Wahib"],"pdf_url":"https://arxiv.org/pdf/2505.12621v1.pdf","comment":"22 pages (9 pages of content, 4 pages of references, 9 pages of\n  supplementary material), 7 figures, 10 tables"}]},"2025-05-18T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2504.08541v2","updated":"2025-05-18T21:33:54Z","published":"2025-04-11T13:54:19Z","title":"Digital Twin Catalog: A Large-Scale Photorealistic 3D Object Digital\n  Twin Dataset","summary":"  We introduce the Digital Twin Catalog (DTC), a new large-scale photorealistic\n3D object digital twin dataset. A digital twin of a 3D object is a highly\ndetailed, virtually indistinguishable representation of a physical object,\naccurately capturing its shape, appearance, physical properties, and other\nattributes. Recent advances in neural-based 3D reconstruction and inverse\nrendering have significantly improved the quality of 3D object reconstruction.\nDespite these advancements, there remains a lack of a large-scale, digital\ntwin-quality real-world dataset and benchmark that can quantitatively assess\nand compare the performance of different reconstruction methods, as well as\nimprove reconstruction quality through training or fine-tuning. Moreover, to\ndemocratize 3D digital twin creation, it is essential to integrate creation\ntechniques with next-generation egocentric computing platforms, such as AR\nglasses. Currently, there is no dataset available to evaluate 3D object\nreconstruction using egocentric captured images. To address these gaps, the DTC\ndataset features 2,000 scanned digital twin-quality 3D objects, along with\nimage sequences captured under different lighting conditions using DSLR cameras\nand egocentric AR glasses. This dataset establishes the first comprehensive\nreal-world evaluation benchmark for 3D digital twin creation tasks, offering a\nrobust foundation for comparing and improving existing reconstruction methods.\nThe DTC dataset is already released at\nhttps://www.projectaria.com/datasets/dtc/ and we will also make the baseline\nevaluations open-source.\n","authors":["Zhao Dong","Ka Chen","Zhaoyang Lv","Hong-Xing Yu","Yunzhi Zhang","Cheng Zhang","Yufeng Zhu","Stephen Tian","Zhengqin Li","Geordie Moffatt","Sean Christofferson","James Fort","Xiaqing Pan","Mingfei Yan","Jiajun Wu","Carl Yuheng Ren","Richard Newcombe"],"pdf_url":"https://arxiv.org/pdf/2504.08541v2.pdf","comment":"accepted to CVPR 2025 (Highlight). Dataset page:\n  https://www.projectaria.com/datasets/dtc/"},{"id":"http://arxiv.org/abs/2502.19544v2","updated":"2025-05-18T21:26:23Z","published":"2025-02-26T20:34:29Z","title":"Efficient Reinforcement Learning by Guiding Generalist World Models with\n  Non-Curated Data","summary":"  Leveraging offline data is a promising way to improve the sample efficiency\nof online reinforcement learning (RL). This paper expands the pool of usable\ndata for offline-to-online RL by leveraging abundant non-curated data that is\nreward-free, of mixed quality, and collected across multiple embodiments.\nAlthough learning a world model appears promising for utilizing such data, we\nfind that naive fine-tuning fails to accelerate RL training on many tasks.\nThrough careful investigation, we attribute this failure to the distributional\nshift between offline and online data during fine-tuning. To address this issue\nand effectively use the offline data, we propose two essential techniques:\n\\emph{i)} experience rehearsal and \\emph{ii)} execution guidance. With these\nmodifications, the non-curated offline data substantially improves RL's sample\nefficiency. Under limited sample budgets, our method achieves a 102.8\\%\nrelative improvement in aggregate score over learning-from-scratch baselines\nacross 72 visuomotor tasks spanning 6 embodiments. On challenging tasks such as\nlocomotion and robotic manipulation, it outperforms prior methods that utilize\noffline data by a decent margin.\n","authors":["Yi Zhao","Aidan Scannell","Wenshuai Zhao","Yuxin Hou","Tianyu Cui","Le Chen","Dieter Büchler","Arno Solin","Juho Kannala","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2502.19544v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12547v1","updated":"2025-05-18T21:08:05Z","published":"2025-05-18T21:08:05Z","title":"ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation\n  with Bounding-Box Annotations","summary":"  In robotics applications, few-shot segmentation is crucial because it allows\nrobots to perform complex tasks with minimal training data, facilitating their\nadaptation to diverse, real-world environments. However, pixel-level\nannotations of even small amount of images is highly time-consuming and costly.\nIn this paper, we present a novel few-shot binary segmentation method based on\nbounding-box annotations instead of pixel-level labels. We introduce, ProMi, an\nefficient prototype-mixture-based method that treats the background class as a\nmixture of distributions. Our approach is simple, training-free, and effective,\naccommodating coarse annotations with ease. Compared to existing baselines,\nProMi achieves the best results across different datasets with significant\ngains, demonstrating its effectiveness. Furthermore, we present qualitative\nexperiments tailored to real-world mobile robot tasks, demonstrating the\napplicability of our approach in such scenarios. Our code:\nhttps://github.com/ThalesGroup/promi.\n","authors":["Florent Chiaroni","Ali Ayub","Ola Ahmad"],"pdf_url":"https://arxiv.org/pdf/2505.12547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12537v1","updated":"2025-05-18T20:29:23Z","published":"2025-05-18T20:29:23Z","title":"Robust Reinforcement Learning-Based Locomotion for Resource-Constrained\n  Quadrupeds with Exteroceptive Sensing","summary":"  Compact quadrupedal robots are proving increasingly suitable for deployment\nin real-world scenarios. Their smaller size fosters easy integration into human\nenvironments. Nevertheless, real-time locomotion on uneven terrains remains\nchallenging, particularly due to the high computational demands of terrain\nperception. This paper presents a robust reinforcement learning-based\nexteroceptive locomotion controller for resource-constrained small-scale\nquadrupeds in challenging terrains, which exploits real-time elevation mapping,\nsupported by a careful depth sensor selection. We concurrently train both a\npolicy and a state estimator, which together provide an odometry source for\nelevation mapping, optionally fused with visual-inertial odometry (VIO). We\ndemonstrate the importance of positioning an additional time-of-flight sensor\nfor maintaining robustness even without VIO, thus having the potential to free\nup computational resources. We experimentally demonstrate that the proposed\ncontroller can flawlessly traverse steps up to 17.5 cm in height and achieve an\n80% success rate on 22.5 cm steps, both with and without VIO. The proposed\ncontroller also achieves accurate forward and yaw velocity tracking of up to\n1.0 m/s and 1.5 rad/s respectively. We open-source our training code at\ngithub.com/ETH-PBL/elmap-rl-controller.\n","authors":["Davide Plozza","Patricia Apostol","Paul Joseph","Simon Schläpfer","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2505.12537v1.pdf","comment":"This paper has been accepted for publication at the IEEE\n  International Conference on Robotics and Automation (ICRA), Atlanta 2025. The\n  code is available at github.com/ETH-PBL/elmap-rl-controller"},{"id":"http://arxiv.org/abs/2505.12525v1","updated":"2025-05-18T19:26:40Z","published":"2025-05-18T19:26:40Z","title":"Development of a non-wearable support robot capable of reproducing\n  natural standing-up movements","summary":"  To reproduce natural standing-up motion, recent studies have emphasized the\nimportance of coordination between the assisting robot and the human. However,\nmany non-wearable assistive devices have struggled to replicate natural motion\ntrajectories. While wearable devices offer better coordination with the human\nbody, they present challenges in completely isolating mechanical and electrical\nhazards. To address this, we developed a novel standing-assist robot that\nintegrates features of both wearable and non-wearable systems, aiming to\nachieve high coordination while maintaining safety. The device employs a\nfour-link mechanism aligned with the human joint structure, designed to\nreproduce the S-shaped trajectory of the hip and the arc trajectory of the knee\nduring natural standing-up motion. Subject-specific trajectory data were\nobtained using a gyroscope, and the link lengths were determined to drive the\nseat along the optimal path. A feedforward speed control using a stepping motor\nwas implemented, and the reproducibility of the trajectory was evaluated based\non the geometric constraints of the mechanism. A load-bearing experiment with\nweights fixed to the seat was conducted to assess the trajectory accuracy under\ndifferent conditions. Results showed that the reproduction errors for the hip\nand knee trajectories remained within approximately 4 percent of the seat's\ntotal displacement, demonstrating high fidelity to the target paths. In\naddition, durability testing, thermal safety evaluation, and risk assessment\nconfirmed the reliability and safety of the system for indoor use. These\nfindings suggest that the proposed design offers a promising approach for\ndeveloping assistive technologies that adapt to individual physical\ncharacteristics, with potential applications in elderly care and\nrehabilitation.\n","authors":["Atsuya Kusui","Susumu Hirai","Asuka Takai"],"pdf_url":"https://arxiv.org/pdf/2505.12525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18999v3","updated":"2025-05-18T19:10:06Z","published":"2024-05-29T11:25:53Z","title":"Continuously Optimizing Radar Placement with Model Predictive Path\n  Integrals","summary":"  Continuously optimizing sensor placement is essential for precise target\nlocalization in various military and civilian applications. While information\ntheory has shown promise in optimizing sensor placement, many studies\noversimplify sensor measurement models or neglect dynamic constraints of mobile\nsensors. To address these challenges, we employ a range measurement model that\nincorporates radar parameters and radar-target distance, coupled with Model\nPredictive Path Integral (MPPI) control to manage complex environmental\nobstacles and dynamic constraints. We compare the proposed approach against\nstationary radars or simplified range measurement models based on the root mean\nsquared error (RMSE) of the Cubature Kalman Filter (CKF) estimator for the\ntargets' state. Additionally, we visualize the evolving geometry of radars and\ntargets over time, highlighting areas of highest measurement information gain,\ndemonstrating the strengths of the approach. The proposed strategy outperforms\nstationary radars and simplified range measurement models in target\nlocalization, achieving a 38-74% reduction in mean RMSE and a 33-79% reduction\nin the upper tail of the 90% Highest Density Interval (HDI) over 500 Monte Carl\n(MC) trials across all time steps.\n  Code will be made publicly available upon acceptance.\n","authors":["Michael Potter","Shuo Tang","Paul Ghanem","Milica Stojanovic","Pau Closas","Murat Akcakaya","Ben Wright","Marius Necsoiu","Deniz Erdogmus","Michael Everett","Tales Imbiriba"],"pdf_url":"https://arxiv.org/pdf/2405.18999v3.pdf","comment":"Accepted to IEEE Aerospace and Electronic Systems"},{"id":"http://arxiv.org/abs/2504.16734v3","updated":"2025-05-18T17:58:12Z","published":"2025-04-23T14:05:04Z","title":"DYNUS: Uncertainty-aware Trajectory Planner in Dynamic Unknown\n  Environments","summary":"  This paper introduces DYNUS, an uncertainty-aware trajectory planner designed\nfor dynamic unknown environments. Operating in such settings presents many\nchallenges -- most notably, because the agent cannot predict the ground-truth\nfuture paths of obstacles, a previously planned trajectory can become unsafe at\nany moment, requiring rapid replanning to avoid collisions.\n  Recently developed planners have used soft-constraint approaches to achieve\nthe necessary fast computation times; however, these methods do not guarantee\ncollision-free paths even with static obstacles. In contrast, hard-constraint\nmethods ensure collision-free safety, but typically have longer computation\ntimes.\n  To address these issues, we propose three key contributions. First, the DYNUS\nGlobal Planner (DGP) and Temporal Safe Corridor Generation operate in\nspatio-temporal space and handle both static and dynamic obstacles in the 3D\nenvironment. Second, the Safe Planning Framework leverages a combination of\nexploratory, safe, and contingency trajectories to flexibly re-route when\npotential future collisions with dynamic obstacles are detected. Finally, the\nFast Hard-Constraint Local Trajectory Formulation uses a variable elimination\napproach to reduce the problem size and enable faster computation by\npre-computing dependencies between free and dependent variables while still\nensuring collision-free trajectories.\n  We evaluated DYNUS in a variety of simulations, including dense forests,\nconfined office spaces, cave systems, and dynamic environments. Our experiments\nshow that DYNUS achieves a success rate of 100% and travel times that are\napproximately 25.0% faster than state-of-the-art methods. We also evaluated\nDYNUS on multiple platforms -- a quadrotor, a wheeled robot, and a quadruped --\nin both simulation and hardware experiments.\n","authors":["Kota Kondo","Mason Peterson","Nicholas Rober","Juan Rached Viso","Lucas Jia","Jialin Chen","Harvey Merton","Jonathan P. How"],"pdf_url":"https://arxiv.org/pdf/2504.16734v3.pdf","comment":"20 pages, 30 figures, Under review at IEEE Transactions on Robotics"},{"id":"http://arxiv.org/abs/2505.12502v1","updated":"2025-05-18T17:32:40Z","published":"2025-05-18T17:32:40Z","title":"Event-Driven Simulation for Rapid Iterative Development of Distributed\n  Space Flight Software","summary":"  This paper presents the design, development, and application of a novel space\nsimulation environment for rapidly prototyping and testing flight software for\ndistributed space systems. The environment combines the flexibility,\ndeterminism, and observability of software-only simulation with the fidelity\nand depth normally attained only by real-time hardware-in-the-loop testing.\nUltimately, this work enables an engineering process in which flight software\nis continuously improved and delivered in its final, flight-ready form, and\nwhich reduces the cost of design changes and software revisions with respect to\na traditional linear development process. Three key methods not found in\nexisting tools enable this environment's novel capabilities: first, a hybrid\nevent-driven simulation architecture that combines continuous-time and\ndiscrete-event simulation paradigms; second, a lightweight application-layer\nsoftware virtualization design that allows executing compiled flight software\nbinaries while modeling process scheduling, input/output, and memory use; and\nthird, high-fidelity models for the multi-spacecraft space environment,\nincluding for wireless communication, relative sensing such as differential GPS\nand cameras, and flight computer health metrics like heap exhaustion and\nfragmentation. The simulation environment's capabilities are applied to the\niterative development and testing of two flight-ready software packages: the\nguidance, navigation, and control software for the VISORS mission, and the\nStanford Space Rendezvous Laboratory software kit for rendezvous and proximity\noperations. Results from 33 months of flight software development demonstrate\nthe use of this simulation environment to rapidly and reliably identify and\nresolve defects, characterize navigation and control performance, and\nscrutinize implementation details like memory allocation and inter-spacecraft\nnetwork protocols.\n","authors":["Toby Bell","Simone D'Amico"],"pdf_url":"https://arxiv.org/pdf/2505.12502v1.pdf","comment":"IEEE Aerospace Conference 2025"},{"id":"http://arxiv.org/abs/2409.04730v2","updated":"2025-05-18T17:28:02Z","published":"2024-09-07T06:25:41Z","title":"IR2: Implicit Rendezvous for Robotic Exploration Teams under Sparse\n  Intermittent Connectivity","summary":"  Information sharing is critical in time-sensitive and realistic multi-robot\nexploration, especially for smaller robotic teams in large-scale environments\nwhere connectivity may be sparse and intermittent. Existing methods often\noverlook such communication constraints by assuming unrealistic global\nconnectivity. Other works account for communication constraints (by maintaining\nclose proximity or line of sight during information exchange), but are often\ninefficient. For instance, preplanned rendezvous approaches typically involve\nunnecessary detours resulting from poorly timed rendezvous, while pursuit-based\napproaches often result in short-sighted decisions due to their greedy nature.\nWe present IR2, a deep reinforcement learning approach to information sharing\nfor multi-robot exploration. Leveraging attention-based neural networks trained\nvia reinforcement and curriculum learning, IR2 allows robots to effectively\nreason about the longer-term trade-offs between disconnecting for solo\nexploration and reconnecting for information sharing. In addition, we propose a\nhierarchical graph formulation to maintain a sparse yet informative graph,\nenabling our approach to scale to large-scale environments. We present\nsimulation results in three large-scale Gazebo environments, which show that\nour approach yields 6.6-34.1% shorter exploration paths when compared to\nstate-of-the-art baselines, and lastly deploy our learned policy on hardware.\nOur simulation training and testing code is available at\nhttps://ir2-explore.github.io.\n","authors":["Derek Ming Siang Tan","Yixiao Ma","Jingsong Liang","Yi Cheng Chng","Yuhong Cao","Guillaume Sartoretti"],"pdf_url":"https://arxiv.org/pdf/2409.04730v2.pdf","comment":"\\c{opyright} 20XX IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2409.12419v3","updated":"2025-05-18T17:21:05Z","published":"2024-09-19T02:40:27Z","title":"Shape-Space Deformer: Unified Visuo-Tactile Representations for Robotic\n  Manipulation of Deformable Objects","summary":"  Accurate modelling of object deformations is crucial for a wide range of\nrobotic manipulation tasks, where interacting with soft or deformable objects\nis essential. Current methods struggle to generalise to unseen forces or adapt\nto new objects, limiting their utility in real-world applications. We propose\nShape-Space Deformer, a unified representation for encoding a diverse range of\nobject deformations using template augmentation to achieve robust, fine-grained\nreconstructions that are resilient to outliers and unwanted artefacts. Our\nmethod improves generalization to unseen forces and can rapidly adapt to novel\nobjects, significantly outperforming existing approaches. We perform extensive\nexperiments to test a range of force generalisation settings and evaluate our\nmethod's ability to reconstruct unseen deformations, demonstrating significant\nimprovements in reconstruction accuracy and robustness. Our approach is\nsuitable for real-time performance, making it ready for downstream manipulation\napplications.\n","authors":["Sean M. V. Collins","Brendan Tidd","Mahsa Baktashmotlagh","Peyman Moghadam"],"pdf_url":"https://arxiv.org/pdf/2409.12419v3.pdf","comment":"Accepted in ICRA2025"},{"id":"http://arxiv.org/abs/2505.09694v2","updated":"2025-05-18T17:18:18Z","published":"2025-05-14T18:00:19Z","title":"EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied\n  World Models","summary":"  Recent advances in creative AI have enabled the synthesis of high-fidelity\nimages and videos conditioned on language instructions. Building on these\ndevelopments, text-to-video diffusion models have evolved into embodied world\nmodels (EWMs) capable of generating physically plausible scenes from language\ncommands, effectively bridging vision and action in embodied AI applications.\nThis work addresses the critical challenge of evaluating EWMs beyond general\nperceptual metrics to ensure the generation of physically grounded and\naction-consistent behaviors. We propose the Embodied World Model Benchmark\n(EWMBench), a dedicated framework designed to evaluate EWMs based on three key\naspects: visual scene consistency, motion correctness, and semantic alignment.\nOur approach leverages a meticulously curated dataset encompassing diverse\nscenes and motion patterns, alongside a comprehensive multi-dimensional\nevaluation toolkit, to assess and compare candidate models. The proposed\nbenchmark not only identifies the limitations of existing video generation\nmodels in meeting the unique requirements of embodied tasks but also provides\nvaluable insights to guide future advancements in the field. The dataset and\nevaluation tools are publicly available at\nhttps://github.com/AgibotTech/EWMBench.\n","authors":["Hu Yue","Siyuan Huang","Yue Liao","Shengcong Chen","Pengfei Zhou","Liliang Chen","Maoqing Yao","Guanghui Ren"],"pdf_url":"https://arxiv.org/pdf/2505.09694v2.pdf","comment":"Website: https://github.com/AgibotTech/EWMBench"},{"id":"http://arxiv.org/abs/2505.12450v1","updated":"2025-05-18T14:42:28Z","published":"2025-05-18T14:42:28Z","title":"A Robot Simulation Environment for Virtual Reality Enhanced Underwater\n  Manipulation and Seabed Intervention Tasks","summary":"  This paper presents the (MARUN)2 underwater robotic simulator. The simulator\narchitecture enables seamless integration with the ROS-based mission software\nand web-based user interface of URSULA, a squid inspired biomimetic robot\ndesigned for dexterous underwater manipulation and seabed intervention tasks.\n(MARUN)2 utilizes the Unity game engine for physics-based rigid body dynamic\nsimulation and underwater environment modeling. Utilizing Unity as the\nsimulation environment enables the integration of virtual reality and haptic\nfeedback capabilities for a more immersive and realistic experience for\nimproved operator dexterity and experience. The utility of the simulator and\nimproved dexterity provided by the VR module is validated through user\nexperiments.\n","authors":["Sumey El-Muftu","Berke Gur"],"pdf_url":"https://arxiv.org/pdf/2505.12450v1.pdf","comment":"7 pages with 8 figures; presented in the AQ2UASIM: Advancing\n  Quantitative and Qualitative SIMulators for Marine Applications workshop held\n  as a part of the IEEE International Conference on Robotics and Automation\n  (ICRA) 2025"},{"id":"http://arxiv.org/abs/2505.12443v1","updated":"2025-05-18T14:33:17Z","published":"2025-05-18T14:33:17Z","title":"BadNAVer: Exploring Jailbreak Attacks On Vision-and-Language Navigation","summary":"  Multimodal large language models (MLLMs) have recently gained attention for\ntheir generalization and reasoning capabilities in Vision-and-Language\nNavigation (VLN) tasks, leading to the rise of MLLM-driven navigators. However,\nMLLMs are vulnerable to jailbreak attacks, where crafted prompts bypass safety\nmechanisms and trigger undesired outputs. In embodied scenarios, such\nvulnerabilities pose greater risks: unlike plain text models that generate\ntoxic content, embodied agents may interpret malicious instructions as\nexecutable commands, potentially leading to real-world harm. In this paper, we\npresent the first systematic jailbreak attack paradigm targeting MLLM-driven\nnavigator. We propose a three-tiered attack framework and construct malicious\nqueries across four intent categories, concatenated with standard navigation\ninstructions. In the Matterport3D simulator, we evaluate navigation agents\npowered by five MLLMs and report an average attack success rate over 90%. To\ntest real-world feasibility, we replicate the attack on a physical robot. Our\nresults show that even well-crafted prompts can induce harmful actions and\nintents in MLLMs, posing risks beyond toxic output and potentially leading to\nphysical harm.\n","authors":["Wenqi Lyu","Zerui Li","Yanyuan Qiao","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2505.12443v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.12428v1","updated":"2025-05-18T13:53:53Z","published":"2025-05-18T13:53:53Z","title":"Depth Transfer: Learning to See Like a Simulator for Real-World Drone\n  Navigation","summary":"  Sim-to-real transfer is a fundamental challenge in robot reinforcement\nlearning. Discrepancies between simulation and reality can significantly impair\npolicy performance, especially if it receives high-dimensional inputs such as\ndense depth estimates from vision. We propose a novel depth transfer method\nbased on domain adaptation to bridge the visual gap between simulated and\nreal-world depth data. A Variational Autoencoder (VAE) is first trained to\nencode ground-truth depth images from simulation into a latent space, which\nserves as input to a reinforcement learning (RL) policy. During deployment, the\nencoder is refined to align stereo depth images with this latent space,\nenabling direct policy transfer without fine-tuning. We apply our method to the\ntask of autonomous drone navigation through cluttered environments. Experiments\nin IsaacGym show that our method nearly doubles the obstacle avoidance success\nrate when switching from ground-truth to stereo depth input. Furthermore, we\ndemonstrate successful transfer to the photo-realistic simulator AvoidBench\nusing only IsaacGym-generated stereo data, achieving superior performance\ncompared to state-of-the-art baselines. Real-world evaluations in both indoor\nand outdoor environments confirm the effectiveness of our approach, enabling\nrobust and generalizable depth-based navigation across diverse domains.\n","authors":["Hang Yu","Christophe De Wagter","Guido C. H. E de Croon"],"pdf_url":"https://arxiv.org/pdf/2505.12428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12410v1","updated":"2025-05-18T13:22:34Z","published":"2025-05-18T13:22:34Z","title":"MTIL: Encoding Full History with Mamba for Temporal Imitation Learning","summary":"  Standard imitation learning (IL) methods have achieved considerable success\nin robotics, yet often rely on the Markov assumption, limiting their\napplicability to tasks where historical context is crucial for disambiguating\ncurrent observations. This limitation hinders performance in long-horizon\nsequential manipulation tasks where the correct action depends on past events\nnot fully captured by the current state. To address this fundamental challenge,\nwe introduce Mamba Temporal Imitation Learning (MTIL), a novel approach that\nleverages the recurrent state dynamics inherent in State Space Models (SSMs),\nspecifically the Mamba architecture. MTIL encodes the entire trajectory history\ninto a compressed hidden state, conditioning action predictions on this\ncomprehensive temporal context alongside current multi-modal observations.\nThrough extensive experiments on simulated benchmarks (ACT dataset tasks,\nRobomimic, LIBERO) and real-world sequential manipulation tasks specifically\ndesigned to probe temporal dependencies, MTIL significantly outperforms\nstate-of-the-art methods like ACT and Diffusion Policy. Our findings affirm the\nnecessity of full temporal context for robust sequential decision-making and\nvalidate MTIL as a powerful approach that transcends the inherent limitations\nof Markovian imitation learning\n","authors":["Yulin Zhou","Yuankai Lin","Fanzhe Peng","Jiahui Chen","Zhuang Zhou","Kaiji Huang","Hua Yang","Zhouping Yin"],"pdf_url":"https://arxiv.org/pdf/2505.12410v1.pdf","comment":"16 pages,6 figures,Submitted to IEEE RAL"},{"id":"http://arxiv.org/abs/2505.12384v1","updated":"2025-05-18T11:57:48Z","published":"2025-05-18T11:57:48Z","title":"Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey","summary":"  In embedded systems, robots must perceive and interpret their environment\nefficiently to operate reliably in real-world conditions. Visual Semantic SLAM\n(Simultaneous Localization and Mapping) enhances standard SLAM by incorporating\nsemantic information into the map, enabling more informed decision-making.\nHowever, implementing such systems on resource-limited hardware involves\ntrade-offs between accuracy, computing efficiency, and power usage.\n  This paper provides a comparative review of recent Semantic Visual SLAM\nmethods with a focus on their applicability to embedded platforms. We analyze\nthree main types of architectures - Geometric SLAM, Neural Radiance Fields\n(NeRF), and 3D Gaussian Splatting - and evaluate their performance on\nconstrained hardware, specifically the NVIDIA Jetson AGX Orin. We compare their\naccuracy, segmentation quality, memory usage, and energy consumption.\n  Our results show that methods based on NeRF and Gaussian Splatting achieve\nhigh semantic detail but demand substantial computing resources, limiting their\nuse on embedded devices. In contrast, Semantic Geometric SLAM offers a more\npractical balance between computational cost and accuracy. The review\nhighlights a need for SLAM algorithms that are better adapted to embedded\nenvironments, and it discusses key directions for improving their efficiency\nthrough algorithm-hardware co-design.\n","authors":["Calvin Galagain","Martyna Poreba","François Goulette"],"pdf_url":"https://arxiv.org/pdf/2505.12384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13532v1","updated":"2025-05-18T11:35:57Z","published":"2025-05-18T11:35:57Z","title":"Distributional Soft Actor-Critic with Harmonic Gradient for Safe and\n  Efficient Autonomous Driving in Multi-lane Scenarios","summary":"  Reinforcement learning (RL), known for its self-evolution capability, offers\na promising approach to training high-level autonomous driving systems.\nHowever, handling constraints remains a significant challenge for existing RL\nalgorithms, particularly in real-world applications. In this paper, we propose\na new safety-oriented training technique called harmonic policy iteration\n(HPI). At each RL iteration, it first calculates two policy gradients\nassociated with efficient driving and safety constraints, respectively. Then, a\nharmonic gradient is derived for policy updating, minimizing conflicts between\nthe two gradients and consequently enabling a more balanced and stable training\nprocess. Furthermore, we adopt the state-of-the-art DSAC algorithm as the\nbackbone and integrate it with our HPI to develop a new safe RL algorithm,\nDSAC-H. Extensive simulations in multi-lane scenarios demonstrate that DSAC-H\nachieves efficient driving performance with near-zero safety constraint\nviolations.\n","authors":["Feihong Zhang","Guojian Zhan","Bin Shuai","Tianyi Zhang","Jingliang Duan","Shengbo Eben Li"],"pdf_url":"https://arxiv.org/pdf/2505.13532v1.pdf","comment":"IEEE Intelligent Vehicles Symposium (IV 2025)"},{"id":"http://arxiv.org/abs/2505.12363v1","updated":"2025-05-18T10:57:33Z","published":"2025-05-18T10:57:33Z","title":"Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts","summary":"  While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research.\n","authors":["Qi Feng","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2505.12363v1.pdf","comment":"26 pages, 19 figures, 4 tables. Code, models, and dataset are\n  available at our project page: https://github.com/nkkbr/ViCA"},{"id":"http://arxiv.org/abs/2505.12361v1","updated":"2025-05-18T10:48:38Z","published":"2025-05-18T10:48:38Z","title":"Adaptive MPC-based quadrupedal robot control under periodic disturbances","summary":"  Recent advancements in adaptive control for reference trajectory tracking\nenable quadrupedal robots to perform locomotion tasks under challenging\nconditions. There are methods enabling the estimation of the external\ndisturbances in terms of forces and torques. However, a specific case of\ndisturbances that are periodic was not explicitly tackled in application to\nquadrupeds. This work is devoted to the estimation of the periodic disturbances\nwith a lightweight regressor using simplified robot dynamics and extracting the\ndisturbance properties in terms of the magnitude and frequency. Experimental\nevidence suggests performance improvement over the baseline static disturbance\ncompensation. All source files, including simulation setups, code, and\ncalculation scripts, are available on GitHub at\nhttps://github.com/aidagroup/quad-periodic-mpc.\n","authors":["Elizaveta Pestova","Ilya Osokin","Danil Belov","Pavel Osinenko"],"pdf_url":"https://arxiv.org/pdf/2505.12361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12354v1","updated":"2025-05-18T10:37:27Z","published":"2025-05-18T10:37:27Z","title":"A universal policy wrapper with guarantees","summary":"  We introduce a universal policy wrapper for reinforcement learning agents\nthat ensures formal goal-reaching guarantees. In contrast to standard\nreinforcement learning algorithms that excel in performance but lack rigorous\nsafety assurances, our wrapper selectively switches between a high-performing\nbase policy -- derived from any existing RL method -- and a fallback policy\nwith known convergence properties. Base policy's value function supervises this\nswitching process, determining when the fallback policy should override the\nbase policy to ensure the system remains on a stable path. The analysis proves\nthat our wrapper inherits the fallback policy's goal-reaching guarantees while\npreserving or improving upon the performance of the base policy. Notably, it\noperates without needing additional system knowledge or online constrained\noptimization, making it readily deployable across diverse reinforcement\nlearning architectures and tasks.\n","authors":["Anton Bolychev","Georgiy Malaniya","Grigory Yaremenko","Anastasia Krasnaya","Pavel Osinenko"],"pdf_url":"https://arxiv.org/pdf/2505.12354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12350v1","updated":"2025-05-18T10:30:24Z","published":"2025-05-18T10:30:24Z","title":"Multi-CALF: A Policy Combination Approach with Statistical Guarantees","summary":"  We introduce Multi-CALF, an algorithm that intelligently combines\nreinforcement learning policies based on their relative value improvements. Our\napproach integrates a standard RL policy with a theoretically-backed\nalternative policy, inheriting formal stability guarantees while often\nachieving better performance than either policy individually. We prove that our\ncombined policy converges to a specified goal set with known probability and\nprovide precise bounds on maximum deviation and convergence time. Empirical\nvalidation on control tasks demonstrates enhanced performance while maintaining\nstability guarantees.\n","authors":["Georgiy Malaniya","Anton Bolychev","Grigory Yaremenko","Anastasia Krasnaya","Pavel Osinenko"],"pdf_url":"https://arxiv.org/pdf/2505.12350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12337v1","updated":"2025-05-18T10:02:57Z","published":"2025-05-18T10:02:57Z","title":"Structureless VIO","summary":"  Visual odometry (VO) is typically considered as a chicken-and-egg problem, as\nthe localization and mapping modules are tightly-coupled. The estimation of\nvisual map relies on accurate localization information. Meanwhile, localization\nrequires precise map points to provide motion constraints. This classical\ndesign principle is naturally inherited by visual-inertial odometry (VIO).\nEfficient localization solution that does not require a map has not been fully\ninvestigated. To this end, we propose a novel structureless VIO, where the\nvisual map is removed from the odometry framework. Experimental results\ndemonstrated that, compared to the structure-based VIO baseline, our\nstructureless VIO not only substantially improves computational efficiency but\nalso has advantages in accuracy.\n","authors":["Junlin Song","Miguel Olivares-Mendez"],"pdf_url":"https://arxiv.org/pdf/2505.12337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12327v1","updated":"2025-05-18T09:44:57Z","published":"2025-05-18T09:44:57Z","title":"Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion\n  Predictions","summary":"  We describe a robust planning method for autonomous driving that mixes normal\nand adversarial agent predictions output by a diffusion model trained for\nmotion prediction. We first train a diffusion model to learn an unbiased\ndistribution of normal agent behaviors. We then generate a distribution of\nadversarial predictions by biasing the diffusion model at test time to generate\npredictions that are likely to collide with a candidate plan. We score plans\nusing expected cost with respect to a mixture distribution of normal and\nadversarial predictions, leading to a planner that is robust against\nadversarial behaviors but not overly conservative when agents behave normally.\nUnlike current approaches, we do not use risk measures that over-weight\nadversarial behaviors while placing little to no weight on low-cost normal\nbehaviors or use hard safety constraints that may not be appropriate for all\ndriving scenarios. We show the effectiveness of our method on single-agent and\nmulti-agent jaywalking scenarios as well as a red light violation scenario.\n","authors":["Albert Zhao","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2505.12327v1.pdf","comment":"IEEE International Conference on Robotics and Automation (ICRA) 2025"},{"id":"http://arxiv.org/abs/2505.12312v1","updated":"2025-05-18T08:55:02Z","published":"2025-05-18T08:55:02Z","title":"Visuospatial Cognitive Assistant","summary":"  Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence.\n","authors":["Qi Feng","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2505.12312v1.pdf","comment":"31 pages, 10 figures, 6 tables. The implementation and fine-tuned\n  model (ViCA-7B) are publicly available at https://huggingface.co/nkkbr/ViCA.\n  The ViCA-322K dataset can be found at\n  https://huggingface.co/datasets/nkkbr/ViCA-322K, and the ViCA-Thinking-2.68K\n  dataset is at https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k"},{"id":"http://arxiv.org/abs/2505.12311v1","updated":"2025-05-18T08:54:38Z","published":"2025-05-18T08:54:38Z","title":"Scene-Adaptive Motion Planning with Explicit Mixture of Experts and\n  Interaction-Oriented Optimization","summary":"  Despite over a decade of development, autonomous driving trajectory planning\nin complex urban environments continues to encounter significant challenges.\nThese challenges include the difficulty in accommodating the multi-modal nature\nof trajectories, the limitations of single expert in managing diverse\nscenarios, and insufficient consideration of environmental interactions. To\naddress these issues, this paper introduces the EMoE-Planner, which\nincorporates three innovative approaches. Firstly, the Explicit MoE (Mixture of\nExperts) dynamically selects specialized experts based on scenario-specific\ninformation through a shared scene router. Secondly, the planner utilizes\nscene-specific queries to provide multi-modal priors, directing the model's\nfocus towards relevant target areas. Lastly, it enhances the prediction model\nand loss calculation by considering the interactions between the ego vehicle\nand other agents, thereby significantly boosting planning performance.\nComparative experiments were conducted using the Nuplan dataset against the\nstate-of-the-art methods. The simulation results demonstrate that our model\nconsistently outperforms SOTA models across nearly all test scenarios.\n","authors":["Hongbiao Zhu","Liulong Ma","Xian Wu","Xin Deng","Xiaoyao Liang"],"pdf_url":"https://arxiv.org/pdf/2505.12311v1.pdf","comment":"Main text 9 pages, appendices 2 pages with 7 figures"},{"id":"http://arxiv.org/abs/2505.12310v1","updated":"2025-05-18T08:50:54Z","published":"2025-05-18T08:50:54Z","title":"DNOI-4DRO: Deep 4D Radar Odometry with Differentiable\n  Neural-Optimization Iterations","summary":"  A novel learning-optimization-combined 4D radar odometry model, named\nDNOI-4DRO, is proposed in this paper. The proposed model seamlessly integrates\ntraditional geometric optimization with end-to-end neural network training,\nleveraging an innovative differentiable neural-optimization iteration operator.\nIn this framework, point-wise motion flow is first estimated using a neural\nnetwork, followed by the construction of a cost function based on the\nrelationship between point motion and pose in 3D space. The radar pose is then\nrefined using Gauss-Newton updates. Additionally, we design a dual-stream 4D\nradar backbone that integrates multi-scale geometric features and\nclustering-based class-aware features to enhance the representation of sparse\n4D radar point clouds. Extensive experiments on the VoD and Snail-Radar\ndatasets demonstrate the superior performance of our model, which outperforms\nrecent classical and learning-based approaches. Notably, our method even\nachieves results comparable to A-LOAM with mapping optimization using LiDAR\npoint clouds as input. Our models and code will be publicly released.\n","authors":["Shouyi Lu","Huanyu Zhou","Guirong Zhuo"],"pdf_url":"https://arxiv.org/pdf/2505.12310v1.pdf","comment":"16 pages,10 figures"},{"id":"http://arxiv.org/abs/2505.12294v1","updated":"2025-05-18T08:15:11Z","published":"2025-05-18T08:15:11Z","title":"PartDexTOG: Generating Dexterous Task-Oriented Grasping via\n  Language-driven Part Analysis","summary":"  Task-oriented grasping is a crucial yet challenging task in robotic\nmanipulation. Despite the recent progress, few existing methods address\ntask-oriented grasping with dexterous hands. Dexterous hands provide better\nprecision and versatility, enabling robots to perform task-oriented grasping\nmore effectively. In this paper, we argue that part analysis can enhance\ndexterous grasping by providing detailed information about the object's\nfunctionality. We propose PartDexTOG, a method that generates dexterous\ntask-oriented grasps via language-driven part analysis. Taking a 3D object and\na manipulation task represented by language as input, the method first\ngenerates the category-level and part-level grasp descriptions w.r.t the\nmanipulation task by LLMs. Then, a category-part conditional diffusion model is\ndeveloped to generate a dexterous grasp for each part, respectively, based on\nthe generated descriptions. To select the most plausible combination of grasp\nand corresponding part from the generated ones, we propose a measure of\ngeometric consistency between grasp and part. We show that our method greatly\nbenefits from the open-world knowledge reasoning on object parts by LLMs, which\nnaturally facilitates the learning of grasp generation on objects with\ndifferent geometry and for different manipulation tasks. Our method ranks top\non the OakInk-shape dataset over all previous methods, improving the\nPenetration Volume, the Grasp Displace, and the P-FID over the state-of-the-art\nby $3.58\\%$, $2.87\\%$, and $41.43\\%$, respectively. Notably, it demonstrates\ngood generality in handling novel categories and tasks.\n","authors":["Weishang Wu","Yifei Shi","Zhizhong Chen","Zhipong Cai"],"pdf_url":"https://arxiv.org/pdf/2505.12294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12278v1","updated":"2025-05-18T07:33:31Z","published":"2025-05-18T07:33:31Z","title":"Emergent Active Perception and Dexterity of Simulated Humanoids from\n  Visual Reinforcement Learning","summary":"  Human behavior is fundamentally shaped by visual perception -- our ability to\ninteract with the world depends on actively gathering relevant information and\nadapting our movements accordingly. Behaviors like searching for objects,\nreaching, and hand-eye coordination naturally emerge from the structure of our\nsensory system. Inspired by these principles, we introduce Perceptive Dexterous\nControl (PDC), a framework for vision-driven dexterous whole-body control with\nsimulated humanoids. PDC operates solely on egocentric vision for task\nspecification, enabling object search, target placement, and skill selection\nthrough visual cues, without relying on privileged state information (e.g., 3D\nobject positions and geometries). This perception-as-interface paradigm enables\nlearning a single policy to perform multiple household tasks, including\nreaching, grasping, placing, and articulated object manipulation. We also show\nthat training from scratch with reinforcement learning can produce emergent\nbehaviors such as active search. These results demonstrate how vision-driven\ncontrol and complex tasks induce human-like behaviors and can serve as the key\ningredients in closing the perception-action loop for animation, robotics, and\nembodied AI.\n","authors":["Zhengyi Luo","Chen Tessler","Toru Lin","Ye Yuan","Tairan He","Wenli Xiao","Yunrong Guo","Gal Chechik","Kris Kitani","Linxi Fan","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.12278v1.pdf","comment":"Project page: https://zhengyiluo.github.io/PDC"},{"id":"http://arxiv.org/abs/2505.12267v1","updated":"2025-05-18T07:12:51Z","published":"2025-05-18T07:12:51Z","title":"Real-Time Spatial Reasoning by Mobile Robots for Reconstruction and\n  Navigation in Dynamic LiDAR Scenes","summary":"  Our brain has an inner global positioning system which enables us to sense\nand navigate 3D spaces in real time. Can mobile robots replicate such a\nbiological feat in a dynamic environment? We introduce the first spatial\nreasoning framework for real-time surface reconstruction and navigation that is\ndesigned for outdoor LiDAR scanning data captured by ground mobile robots and\ncapable of handling moving objects such as pedestrians. Our\nreconstruction-based approach is well aligned with the critical cellular\nfunctions performed by the border vector cells (BVCs) over all layers of the\nmedial entorhinal cortex (MEC) for surface sensing and tracking. To address the\nchallenges arising from blurred boundaries resulting from sparse single-frame\nLiDAR points and outdated data due to object movements, we integrate real-time\nsingle-frame mesh reconstruction, via visibility reasoning, with robot\nnavigation assistance through on-the-fly 3D free space determination. This\nenables continuous and incremental updates of the scene and free space across\nmultiple frames. Key to our method is the utilization of line-of-sight (LoS)\nvectors from LiDAR, which enable real-time surface normal estimation, as well\nas robust and instantaneous per-voxel free space updates. We showcase two\npractical applications: real-time 3D scene reconstruction and autonomous\noutdoor robot navigation in real-world conditions. Comprehensive experiments on\nboth synthetic and real scenes highlight our method's superiority in speed and\nquality over existing real-time LiDAR processing approaches.\n","authors":["Pengdi Huang","Mingyang Wang","Huan Tian","Minglun Gong","Hao Zhang","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2505.12267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11385v2","updated":"2025-05-18T07:11:35Z","published":"2024-07-16T05:05:02Z","title":"Omnigrasp: Grasping Diverse Objects with Simulated Humanoids","summary":"  We present a method for controlling a simulated humanoid to grasp an object\nand move it to follow an object's trajectory. Due to the challenges in\ncontrolling a humanoid with dexterous hands, prior methods often use a\ndisembodied hand and only consider vertical lifts or short trajectories. This\nlimited scope hampers their applicability for object manipulation required for\nanimation and simulation. To close this gap, we learn a controller that can\npick up a large number (>1200) of objects and carry them to follow randomly\ngenerated trajectories. Our key insight is to leverage a humanoid motion\nrepresentation that provides human-like motor skills and significantly speeds\nup training. Using only simplistic reward, state, and object representations,\nour method shows favorable scalability on diverse objects and trajectories. For\ntraining, we do not need a dataset of paired full-body motion and object\ntrajectories. At test time, we only require the object mesh and desired\ntrajectories for grasping and transporting. To demonstrate the capabilities of\nour method, we show state-of-the-art success rates in following object\ntrajectories and generalizing to unseen objects. Code and models will be\nreleased.\n","authors":["Zhengyi Luo","Jinkun Cao","Sammy Christen","Alexander Winkler","Kris Kitani","Weipeng Xu"],"pdf_url":"https://arxiv.org/pdf/2407.11385v2.pdf","comment":"Project page: https://www.zhengyiluo.com/Omnigrasp/"},{"id":"http://arxiv.org/abs/2503.00923v3","updated":"2025-05-18T06:27:35Z","published":"2025-03-02T14:55:22Z","title":"HWC-Loco: A Hierarchical Whole-Body Control Approach to Robust Humanoid\n  Locomotion","summary":"  Humanoid robots, capable of assuming human roles in various workplaces, have\nbecome essential to embodied intelligence. However, as robots with complex\nphysical structures, learning a control model that can operate robustly across\ndiverse environments remains inherently challenging, particularly under the\ndiscrepancies between training and deployment environments. In this study, we\npropose HWC-Loco, a robust whole-body control algorithm tailored for humanoid\nlocomotion tasks. By reformulating policy learning as a robust optimization\nproblem, HWC-Loco explicitly learns to recover from safety-critical scenarios.\nWhile prioritizing safety guarantees, overly conservative behavior can\ncompromise the robot's ability to complete the given tasks. To tackle this\nchallenge, HWC-Loco leverages a hierarchical policy for robust control. This\npolicy can dynamically resolve the trade-off between goal-tracking and safety\nrecovery, guided by human behavior norms and dynamic constraints. To evaluate\nthe performance of HWC-Loco, we conduct extensive comparisons against\nstate-of-the-art humanoid control models, demonstrating HWC-Loco's superior\nperformance across diverse terrains, robot structures, and locomotion tasks\nunder both simulated and real-world environments.\n","authors":["Sixu Lin","Guanren Qiao","Yunxin Tai","Ang Li","Kui Jia","Guiliang Liu"],"pdf_url":"https://arxiv.org/pdf/2503.00923v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12231v1","updated":"2025-05-18T04:30:36Z","published":"2025-05-18T04:30:36Z","title":"Design of a 3-DOF Hopping Robot with an Optimized Gearbox: An\n  Intermediate Platform Toward Bipedal Robots","summary":"  This paper presents a 3-DOF hopping robot with a human-like lower-limb joint\nconfiguration and a flat foot, capable of performing dynamic and repetitive\njumping motions. To achieve both high torque output and a large hollow shaft\ndiameter for efficient cable routing, a compact 3K compound planetary gearbox\nwas designed using mixed-integer nonlinear programming for gear tooth\noptimization. To meet performance requirements within the constrained joint\ngeometry, all major components-including the actuator, motor driver, and\ncommunication interface-were custom-designed. The robot weighs 12.45 kg,\nincluding a dummy mass, and measures 840 mm in length when the knee joint is\nfully extended. A reinforcement learning-based controller was employed, and\nrobot's performance was validated through hardware experiments, demonstrating\nstable and repetitive hopping motions in response to user inputs. These\nexperimental results indicate that the platform serves as a solid foundation\nfor future bipedal robot development.\n","authors":["JongHun Choe","Gijeong Kim","Hajun Kim","Dongyun Kang","Min-Su Kim","Hae-Won Park"],"pdf_url":"https://arxiv.org/pdf/2505.12231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05530v2","updated":"2025-05-18T04:20:01Z","published":"2024-07-08T00:28:41Z","title":"This&That: Language-Gesture Controlled Video Generation for Robot\n  Planning","summary":"  Clear, interpretable instructions are invaluable when attempting any complex\ntask. Good instructions help to clarify the task and even anticipate the steps\nneeded to solve it. In this work, we propose a robot learning framework for\ncommunicating, planning, and executing a wide range of tasks, dubbed This&That.\nThis&That solves general tasks by leveraging video generative models, which,\nthrough training on internet-scale data, contain rich physical and semantic\ncontext. In this work, we tackle three fundamental challenges in video-based\nplanning: 1) unambiguous task communication with simple human instructions, 2)\ncontrollable video generation that respects user intent, and 3) translating\nvisual plans into robot actions. This&That uses language-gesture conditioning\nto generate video predictions, as a succinct and unambiguous alternative to\nexisting language-only methods, especially in complex and uncertain\nenvironments. These video predictions are then fed into a behavior cloning\narchitecture dubbed Diffusion Video to Action (DiVA), which outperforms prior\nstate-of-the-art behavior cloning and video-based planning methods by\nsubstantial margins.\n","authors":["Boyang Wang","Nikhil Sridhar","Chao Feng","Mark Van der Merwe","Adam Fishman","Nima Fazeli","Jeong Joon Park"],"pdf_url":"https://arxiv.org/pdf/2407.05530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10260v2","updated":"2025-05-18T03:41:45Z","published":"2023-08-20T13:08:16Z","title":"Hand Dominance and Congruence for Wrist-worn Haptics using Custom\n  Voice-Coil Actuation","summary":"  During virtual interactions, rendering haptic feedback on a remote location\n(like the wrist) instead of the fingertips freeing users' hands from mechanical\ndevices. This allows for real interactions while still providing information\nregarding the mechanical properties of virtual objects. In this paper, we\npresent CoWrHap -- a novel wrist-worn haptic device with custom-made voice coil\nactuation to render force feedback. Then, we investigate the impact of asking\nparticipants to use their dominant or non-dominant hand for virtual\ninteractions and the best mapping between the active hand and the wrist\nreceiving the haptic feedback, which can be defined as hand-wrist congruence\nthrough a user experiment based on a stiffness discrimination task. Our results\nshow that participants performed the tasks (i) better with non-congruent\nmapping but reported better experiences with congruent mapping, and (ii) with\nno statistical difference in terms of hand dominance but reported better user\nexperience and enjoyment using their dominant hands. This study indicates that\nparticipants can perceive mechanical properties via haptic feedback provided\nthrough CoWrHap.\n","authors":["Ayoade Adeyemi","Umit Sen","Samet Mert Ercan","Mine Sarac"],"pdf_url":"https://arxiv.org/pdf/2308.10260v2.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.12214v1","updated":"2025-05-18T03:15:31Z","published":"2025-05-18T03:15:31Z","title":"Behavior Synthesis via Contact-Aware Fisher Information Maximization","summary":"  Contact dynamics hold immense amounts of information that can improve a\nrobot's ability to characterize and learn about objects in their environment\nthrough interactions. However, collecting information-rich contact data is\nchallenging due to its inherent sparsity and non-smooth nature, requiring an\nactive approach to maximize the utility of contacts for learning. In this work,\nwe investigate an optimal experimental design approach to synthesize robot\nbehaviors that produce contact-rich data for learning. Our approach derives a\ncontact-aware Fisher information measure that characterizes information-rich\ncontact behaviors that improve parameter learning. We observe emergent robot\nbehaviors that are able to excite contact interactions that efficiently learns\nobject parameters across a range of parameter learning examples. Last, we\ndemonstrate the utility of contact-awareness for learning parameters through\ncontact-seeking behaviors on several robotic experiments.\n","authors":["Hrishikesh Sathyanarayan","Ian Abraham"],"pdf_url":"https://arxiv.org/pdf/2505.12214v1.pdf","comment":"In Robotics Science and Systems 2025"},{"id":"http://arxiv.org/abs/2409.15832v2","updated":"2025-05-18T02:27:17Z","published":"2024-09-24T07:57:21Z","title":"PseudoNeg-MAE: Self-Supervised Point Cloud Learning using Conditional\n  Pseudo-Negative Embeddings","summary":"  We propose PseudoNeg-MAE, a novel self-supervised learning framework that\nenhances global feature representation of point cloud masked autoencoder by\nmaking them both discriminative and sensitive to transformations. Traditional\ncontrastive learning methods focus on achieving invariance, discarding\ntransformation-specific information. Recent approaches incorporate\ntransformation sensitivity by explicitly modeling relationships between\noriginal and transformed inputs. However, they report an invariant-collapse\nphenomenon, where the predictor degenerates into identity mappings, resulting\nin latent representations that have limited variation across transformations.\nWe propose a novel loss that explicitly penalizes invariant collapse, enabling\nthe network to capture richer transformation cues while preserving\ndiscriminative representations. PseudoNeg-MAE uses a parametric network COPE,\nwhich learns the localized displacements caused by transformations within the\nlatent space. However, jointly training COPE with the MAE leads to undesirable\ntrivial solutions where COPE outputs collapse to an identity. To address this,\nwe propose a loss that uses transformation-conditioned pseudo-negatives, to\npenalize such trivial invariant solutions. We validate PseudoNeg-MAE on shape\nclassification and relative pose estimation tasks, where it achieves\ncompetitive performance on the ModelNet40 and ScanObjectNN datasets under\nchallenging evaluation protocols and demonstrates superior accuracy in\nestimating relative poses compared to supervised methods.\n","authors":["Sutharsan Mahendren","Saimunur Rahman","Piotr Koniusz","Tharindu Fernando","Sridha Sridharan","Clinton Fookes","Peyman Moghadam"],"pdf_url":"https://arxiv.org/pdf/2409.15832v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12194v1","updated":"2025-05-18T02:07:55Z","published":"2025-05-18T02:07:55Z","title":"Spatial-LLaVA: Enhancing Large Language Models with Spatial Referring\n  Expressions for Visual Understanding","summary":"  Multimodal large language models (MLLMs) have demonstrated remarkable\nabilities in comprehending visual input alongside text input. Typically, these\nmodels are trained on extensive data sourced from the internet, which are\nsufficient for general tasks such as scene understanding and question\nanswering. However, they often underperform on specialized tasks where online\ndata is scarce, such as determining spatial relationships between objects or\nlocalizing unique target objects within a group of objects sharing similar\nfeatures. In response to this challenge, we introduce the SUN-Spot v2.0\ndataset1, now comprising a total of 90k image-caption pairs and additional\nannotations on the landmark objects. Each image-caption pair utilizes\nSet-of-Marks prompting as an additional indicator, mapping each landmark object\nin the image to the corresponding object mentioned in the caption. Furthermore,\nwe present Spatial-LLaVA, an MLLM trained on conversational data generated by a\nstate-of-the-art language model using the SUNSpot v2.0 dataset. Our approach\nensures a robust alignment between the objects in the images and their\ncorresponding object mentions in the captions, enabling our model to learn\nspatial referring expressions without bias from the semantic information of the\nobjects. Spatial-LLaVA outperforms previous methods by 3.15% on the zero-shot\nVisual Spatial Reasoning benchmark dataset. Spatial-LLaVA is specifically\ndesigned to precisely understand spatial referring expressions, making it\nhighly applicable for tasks in real-world scenarios such as autonomous\nnavigation and interactive robotics, where precise object recognition is\ncritical.\n","authors":["Xuefei Sun","Doncey Albin","Cecilia Mauceri","Dusty Woods","Christoffer Heckman"],"pdf_url":"https://arxiv.org/pdf/2505.12194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21415v2","updated":"2025-05-18T01:41:47Z","published":"2024-10-28T18:13:15Z","title":"Deploying Ten Thousand Robots: Scalable Imitation Learning for Lifelong\n  Multi-Agent Path Finding","summary":"  Lifelong Multi-Agent Path Finding (LMAPF) repeatedly finds collision-free\npaths for multiple agents that are continually assigned new goals when they\nreach current ones. Recently, this field has embraced learning-based methods,\nwhich reactively generate single-step actions based on individual local\nobservations. However, it is still challenging for them to match the\nperformance of the best search-based algorithms, especially in large-scale\nsettings. This work proposes an imitation-learning-based LMAPF solver that\nintroduces a novel communication module as well as systematic single-step\ncollision resolution and global guidance techniques. Our proposed solver,\nScalable Imitation Learning for LMAPF (SILLM), inherits the fast reasoning\nspeed of learning-based methods and the high solution quality of search-based\nmethods with the help of modern GPUs. Across six large-scale maps with up to\n10,000 agents and varying obstacle structures, SILLM surpasses the best\nlearning- and search-based baselines, achieving average throughput improvements\nof 137.7% and 16.0%, respectively. Furthermore, SILLM also beats the winning\nsolution of the 2023 League of Robot Runners, an international LMAPF\ncompetition. Finally, we validated SILLM with 10 real robots and 100 virtual\nrobots in a mock warehouse environment.\n","authors":["He Jiang","Yutong Wang","Rishi Veerapaneni","Tanishq Duhan","Guillaume Sartoretti","Jiaoyang Li"],"pdf_url":"https://arxiv.org/pdf/2410.21415v2.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2505.12176v1","updated":"2025-05-18T00:16:13Z","published":"2025-05-18T00:16:13Z","title":"Towards Robust Autonomous Landing Systems: Iterative Solutions and Key\n  Lessons Learned","summary":"  Uncrewed Aerial Vehicles (UAVs) have become a focal point of research, with\nboth established companies and startups investing heavily in their development.\nThis paper presents our iterative process in developing a robust autonomous\nmarker-based landing system, highlighting the key challenges encountered and\nthe solutions implemented. It reviews existing systems for autonomous landing\nprocesses, and through this aims to contribute to the community by sharing\ninsights and challenges faced during development and testing.\n","authors":["Sebastian Schroder","Yao Deng","Alice James","Avishkar Seth","Kye Morton","Subhas Mukhopadhyay","Richard Han","Xi Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.12176v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2411.06256v3","updated":"2025-05-18T23:52:24Z","published":"2024-11-09T19:07:58Z","title":"Annotative Indexing","summary":"  This paper introduces annotative indexing, a novel framework that unifies and\ngeneralizes traditional inverted indexes, column stores, object stores, and\ngraph databases. As a result, annotative indexing can provide the underlying\nindexing framework for databases that support retrieval augmented generation,\nknowledge graphs, entity retrieval, semi-structured data, and ranked retrieval.\nWhile we primarily focus on human language data in the form of text, annotative\nindexing is sufficiently general to support a range of other datatypes, and we\nprovide examples of SQL-like queries over a JSON store that includes numbers\nand dates. Taking advantage of the flexibility of annotative indexing, we also\ndemonstrate a fully dynamic annotative index incorporating support for ACID\nproperties of transactions with hundreds of multiple concurrent readers and\nwriters.\n","authors":["Charles L. A. Clarke"],"pdf_url":"https://arxiv.org/pdf/2411.06256v3.pdf","comment":"Code at https://github.com/claclark/Cottontail"},{"id":"http://arxiv.org/abs/2505.12574v1","updated":"2025-05-18T23:22:53Z","published":"2025-05-18T23:22:53Z","title":"PoisonArena: Uncovering Competing Poisoning Attacks in\n  Retrieval-Augmented Generation","summary":"  Retrieval-Augmented Generation (RAG) systems, widely used to improve the\nfactual grounding of large language models (LLMs), are increasingly vulnerable\nto poisoning attacks, where adversaries inject manipulated content into the\nretriever's corpus. While prior research has predominantly focused on\nsingle-attacker settings, real-world scenarios often involve multiple,\ncompeting attackers with conflicting objectives. In this work, we introduce\nPoisonArena, the first benchmark to systematically study and evaluate competing\npoisoning attacks in RAG. We formalize the multi-attacker threat model, where\nattackers vie to control the answer to the same query using mutually exclusive\nmisinformation. PoisonArena leverages the Bradley-Terry model to quantify each\nmethod's competitive effectiveness in such adversarial environments. Through\nextensive experiments on the Natural Questions and MS MARCO datasets, we\ndemonstrate that many attack strategies successful in isolation fail under\ncompetitive pressure. Our findings highlight the limitations of conventional\nevaluation metrics like Attack Success Rate (ASR) and F1 score and underscore\nthe need for competitive evaluation to assess real-world attack robustness.\nPoisonArena provides a standardized framework to benchmark and develop future\nattack and defense strategies under more realistic, multi-adversary conditions.\nProject page: https://github.com/yxf203/PoisonArena.\n","authors":["Liuji Chen","Xiaofang Yang","Yuanzhuo Lu","Jinghao Zhang","Xin Sun","Qiang Liu","Shu Wu","Jing Dong","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.12574v1.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2505.12570v1","updated":"2025-05-18T23:12:10Z","published":"2025-05-18T23:12:10Z","title":"Batched Self-Consistency Improves LLM Relevance Assessment and Ranking","summary":"  Given some information need, Large Language Models (LLMs) are increasingly\nused for candidate text relevance assessment, typically using a one-by-one\npointwise (PW) strategy where each LLM call evaluates one candidate at a time.\nMeanwhile, it has been shown that LLM performance can be improved through\nself-consistency: prompting the LLM to do the same task multiple times\n(possibly in perturbed ways) and then aggregating the responses. To take\nadvantage of self-consistency, we hypothesize that batched PW strategies, where\nmultiple passages are judged in one LLM call, are better suited than one-by-one\nPW methods since a larger input context can induce more diverse LLM sampling\nacross self-consistency calls. We first propose several candidate batching\nstrategies to create prompt diversity across self-consistency calls through\nsubset reselection and permutation. We then test our batched PW methods on\nrelevance assessment and ranking tasks against one-by-one PW and listwise LLM\nranking baselines with and without self-consistency, using three passage\nretrieval datasets and GPT-4o, Claude Sonnet 3, and Amazon Nova Pro. We find\nthat batched PW methods outperform all baselines, and show that batching can\ngreatly amplify the positive effects of self-consistency. For instance, on our\nlegal search dataset, GPT-4o one-by-one PW ranking NDCG@10 improves only from\n44.9% to 46.8% without self-consistency vs. with 15 self consistency calls,\nwhile batched PW ranking improves from 43.8% to 51.3%, respectively.\n","authors":["Anton Korikov","Pan Du","Scott Sanner","Navid Rekabsaz"],"pdf_url":"https://arxiv.org/pdf/2505.12570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05572v2","updated":"2025-05-18T19:44:50Z","published":"2024-11-08T13:51:37Z","title":"Why These Documents? Explainable Generative Retrieval with Hierarchical\n  Category Paths","summary":"  Generative retrieval has recently emerged as a new alternative of traditional\ninformation retrieval approaches. However, existing generative retrieval\nmethods directly decode docid when a query is given, making it impossible to\nprovide users with explanations as an answer for \"Why this document is\nretrieved?\". To address this limitation, we propose Hierarchical Category\nPath-Enhanced Generative Retrieval(HyPE), which enhances explainability by\ngenerating hierarchical category paths step-by-step before decoding docid. HyPE\nleverages hierarchical category paths as explanation, progressing from broad to\nspecific semantic categories. This approach enables diverse explanations for\nthe same document depending on the query by using shared category paths between\nthe query and the document, and provides reasonable explanation by reflecting\nthe document's semantic structure through a coarse-to-fine manner. HyPE\nconstructs category paths with external high-quality semantic hierarchy,\nleverages LLM to select appropriate candidate paths for each document, and\noptimizes the generative retrieval model with path-augmented dataset. During\ninference, HyPE utilizes path-aware reranking strategy to aggregate diverse\ntopic information, allowing the most relevant documents to be prioritized in\nthe final ranked list of docids. Our extensive experiments demonstrate that\nHyPE not only offers a high level of explainability but also improves the\nretrieval performance in the document retrieval task.\n","authors":["Sangam Lee","Ryang Heo","SeongKu Kang","Susik Yoon","Jinyoung Yeo","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2411.05572v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13538v1","updated":"2025-05-18T17:25:34Z","published":"2025-05-18T17:25:34Z","title":"RAGXplain: From Explainable Evaluation to Actionable Guidance of RAG\n  Pipelines","summary":"  Retrieval-Augmented Generation (RAG) systems show promise by coupling large\nlanguage models with external knowledge, yet traditional RAG evaluation methods\nprimarily report quantitative scores while offering limited actionable guidance\nfor refining these complex pipelines. In this paper, we introduce RAGXplain, an\nevaluation framework that quantifies RAG performance and translates these\nassessments into clear insights that clarify the workings of its complex,\nmulti-stage pipeline and offer actionable recommendations. Using LLM reasoning,\nRAGXplain converts raw scores into coherent narratives identifying performance\ngaps and suggesting targeted improvements. By providing transparent\nexplanations for AI decision-making, our framework fosters user trust-a key\nchallenge in AI adoption. Our LLM-based metric assessments show strong\nalignment with human judgments, and experiments on public question-answering\ndatasets confirm that applying RAGXplain's actionable recommendations\nmeasurably improves system performance. RAGXplain thus bridges quantitative\nevaluation and practical optimization, empowering users to understand, trust,\nand enhance their AI systems.\n","authors":["Dvir Cohen","Lin Burg","Gilad Barkan"],"pdf_url":"https://arxiv.org/pdf/2505.13538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13259v3","updated":"2025-05-18T16:10:35Z","published":"2023-09-23T04:46:28Z","title":"EMelodyGen: Emotion-Conditioned Melody Generation in ABC Notation with\n  the Musical Feature Template","summary":"  The EMelodyGen system focuses on emotional melody generation in ABC notation\ncontrolled by the musical feature template. Owing to the scarcity of\nwell-structured and emotionally labeled sheet music, we designed a template for\ncontrolling emotional melody generation by statistical correlations between\nmusical features and emotion labels derived from small-scale emotional symbolic\nmusic datasets and music psychology conclusions. We then automatically\nannotated a large, well-structured sheet music collection with rough emotional\nlabels by the template, converted them into ABC notation, and reduced label\nimbalance by data augmentation, resulting in a dataset named Rough4Q. Our\nsystem backbone pre-trained on Rough4Q can achieve up to 99% music21 parsing\nrate and melodies generated by our template can lead to a 91% alignment on\nemotional expressions in blind listening tests. Ablation studies further\nvalidated the effectiveness of the feature controls in the template. Available\ncode and demos are at https://github.com/monetjoe/EMelodyGen.\n","authors":["Monan Zhou","Xiaobing Li","Feng Yu","Wei Li"],"pdf_url":"https://arxiv.org/pdf/2309.13259v3.pdf","comment":"6 pages, 4 figures, accepted by ICMEW2025"},{"id":"http://arxiv.org/abs/2505.13535v1","updated":"2025-05-18T15:49:17Z","published":"2025-05-18T15:49:17Z","title":"Information Extraction from Visually Rich Documents using LLM-based\n  Organization of Documents into Independent Textual Segments","summary":"  Information extraction (IE) from Visually Rich Documents (VRDs) containing\nlayout features along with text is a critical and well-studied task.\nSpecialized non-LLM NLP-based solutions typically involve training models using\nboth textual and geometric information to label sequences/tokens as named\nentities or answers to specific questions. However, these approaches lack\nreasoning, are not able to infer values not explicitly present in documents,\nand do not generalize well to new formats. Generative LLM-based approaches\nproposed recently are capable of reasoning, but struggle to comprehend clues\nfrom document layout especially in previously unseen document formats, and do\nnot show competitive performance in heterogeneous VRD benchmark datasets. In\nthis paper, we propose BLOCKIE, a novel LLM-based approach that organizes VRDs\ninto localized, reusable semantic textual segments called $\\textit{semantic\nblocks}$, which are processed independently. Through focused and more\ngeneralizable reasoning,our approach outperforms the state-of-the-art on public\nVRD benchmarks by 1-3% in F1 scores, is resilient to document formats\npreviously not encountered and shows abilities to correctly extract information\nnot explicitly present in documents.\n","authors":["Aniket Bhattacharyya","Anurag Tripathi","Ujjal Das","Archan Karmakar","Amit Pathak","Maneesh Gupta"],"pdf_url":"https://arxiv.org/pdf/2505.13535v1.pdf","comment":"Accepted to ACL Main 2025"},{"id":"http://arxiv.org/abs/2505.12452v1","updated":"2025-05-18T15:04:02Z","published":"2025-05-18T15:04:02Z","title":"Introspective Growth: Automatically Advancing LLM Expertise in\n  Technology Judgment","summary":"  Large language models (LLMs) increasingly demonstrate signs of conceptual\nunderstanding, yet much of their internal knowledge remains latent, loosely\nstructured, and difficult to access or evaluate. We propose self-questioning as\na lightweight and scalable strategy to improve LLMs' understanding,\nparticularly in domains where success depends on fine-grained semantic\ndistinctions. To evaluate this approach, we introduce a challenging new\nbenchmark of 1.3 million post-2015 computer science patent pairs, characterized\nby dense technical jargon and strategically complex writing. The benchmark\ncenters on a pairwise differentiation task: can a model distinguish between\nclosely related but substantively different inventions? We show that prompting\nLLMs to generate and answer their own questions - targeting the background\nknowledge required for the task - significantly improves performance. These\nself-generated questions and answers activate otherwise underutilized internal\nknowledge. Allowing LLMs to retrieve answers from external scientific texts\nfurther enhances performance, suggesting that model knowledge is compressed and\nlacks the full richness of the training data. We also find that\nchain-of-thought prompting and self-questioning converge, though\nself-questioning remains more effective for improving understanding of\ntechnical concepts. Notably, we uncover an asymmetry in prompting: smaller\nmodels often generate more fundamental, more open-ended, better-aligned\nquestions for mid-sized models than large models with better understanding do,\nrevealing a new strategy for cross-model collaboration. Altogether, our\nfindings establish self-questioning as both a practical mechanism for\nautomatically improving LLM comprehension, especially in domains with sparse\nand underrepresented knowledge, and a diagnostic probe of how internal and\nexternal knowledge are organized.\n","authors":["Siyang Wu","Honglin Bao","Nadav Kunievsky","James A. Evans"],"pdf_url":"https://arxiv.org/pdf/2505.12452v1.pdf","comment":"We commit to fully open-source our patent dataset"},{"id":"http://arxiv.org/abs/2505.13534v1","updated":"2025-05-18T13:13:51Z","published":"2025-05-18T13:13:51Z","title":"InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in\n  Structured Biomedical Data","summary":"  Finding interesting phenomena is the core of scientific discovery, but it is\na manual, ill-defined concept. We present an integrative pipeline for\nautomating the discovery of interesting simple hypotheses (feature-target\nrelations with effect direction and a potential underlying mechanism) in\nstructured biomedical data. The pipeline combines machine learning, knowledge\ngraphs, literature search and Large Language Models. We formalize\n\"interestingness\" as a combination of novelty, utility and plausibility. On 8\nmajor diseases from the UK Biobank, our pipeline consistently recovers risk\nfactors years before their appearance in the literature. 40--53% of our top\ncandidates were validated as interesting, compared to 0--7% for a SHAP-based\nbaseline. Overall, 28% of 109 candidates were interesting to medical experts.\nThe pipeline addresses the challenge of operationalizing \"interestingness\"\nscalably and for any target. We release data and code:\nhttps://github.com/LinialLab/InterFeat\n","authors":["Dan Ofer","Michal Linial","Dafna Shahaf"],"pdf_url":"https://arxiv.org/pdf/2505.13534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12396v1","updated":"2025-05-18T12:50:36Z","published":"2025-05-18T12:50:36Z","title":"LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group\n  Policy Optimization","summary":"  Graph neural networks (GNNs) have advanced recommender systems by modeling\ninteraction relationships. However, existing graph-based recommenders rely on\nsparse ID features and do not fully exploit textual information, resulting in\nlow information density within representations. Furthermore, graph contrastive\nlearning faces challenges. Random negative sampling can introduce false\nnegative samples, while fixed temperature coefficients cannot adapt to the\nheterogeneity of different nodes. In addition, current efforts to enhance\nrecommendations with large language models (LLMs) have not fully utilized their\nChain-of-Thought (CoT) reasoning capabilities to guide representation learning.\nTo address these limitations, we introduces LGHRec (LLM-CoT Enhanced Graph\nNeural Recommendation with Harmonized Group Policy Optimization). This\nframework leverages the CoT reasoning ability of LLMs to generate semantic IDs,\nenriching reasoning processes and improving information density and semantic\nquality of representations. Moreover, we design a reinforcement learning\nalgorithm, Harmonized Group Policy Optimization (HGPO), to optimize negative\nsampling strategies and temperature coefficients in contrastive learning. This\napproach enhances long-tail recommendation performance and ensures optimization\nconsistency across different groups. Experimental results on three datasets\ndemonstrate that LGHRec improves representation quality through semantic IDs\ngenerated by LLM's CoT reasoning and effectively boosts contrastive learning\nwith HGPO. Our method outperforms several baseline models. The code is\navailable at: https://anonymous.4open.science/r/LLM-Rec.\n","authors":["Hailong Luo","Bin Wu","Hongyong Jia","Qingqing Zhu","Lianlei Shan"],"pdf_url":"https://arxiv.org/pdf/2505.12396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12283v1","updated":"2025-05-18T07:45:46Z","published":"2025-05-18T07:45:46Z","title":"Addressing Missing Data Issue for Diffusion-based Recommendation","summary":"  Diffusion models have shown significant potential in generating oracle items\nthat best match user preference with guidance from user historical interaction\nsequences. However, the quality of guidance is often compromised by\nunpredictable missing data in observed sequence, leading to suboptimal item\ngeneration. Since missing data is uncertain in both occurrence and content,\nrecovering it is impractical and may introduce additional errors. To tackle\nthis challenge, we propose a novel dual-side Thompson sampling-based Diffusion\nModel (TDM), which simulates extra missing data in the guidance signals and\nallows diffusion models to handle existing missing data through extrapolation.\nTo preserve user preference evolution in sequences despite extra missing data,\nwe introduce Dual-side Thompson Sampling to implement simulation with two\nprobability models, sampling by exploiting user preference from both item\ncontinuity and sequence stability. TDM strategically removes items from\nsequences based on dual-side Thompson sampling and treats these edited\nsequences as guidance for diffusion models, enhancing models' robustness to\nmissing data through consistency regularization. Additionally, to enhance the\ngeneration efficiency, TDM is implemented under the denoising diffusion\nimplicit models to accelerate the reverse process. Extensive experiments and\ntheoretical analysis validate the effectiveness of TDM in addressing missing\ndata in sequential recommendations.\n","authors":["Wenyu Mao","Zhengyi Yang","Jiancan Wu","Haozhe Liu","Yancheng Yuan","Xiang Wang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2505.12283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12279v1","updated":"2025-05-18T07:36:43Z","published":"2025-05-18T07:36:43Z","title":"A Survey on Side Information-driven Session-based Recommendation: From a\n  Data-centric Perspective","summary":"  Session-based recommendation is gaining increasing attention due to its\npractical value in predicting the intents of anonymous users based on limited\nbehaviors. Emerging efforts incorporate various side information to alleviate\ninherent data scarcity issues in this task, leading to impressive performance\nimprovements. The core of side information-driven session-based recommendation\nis the discovery and utilization of diverse data. In this survey, we provide a\ncomprehensive review of this task from a data-centric perspective.\nSpecifically, this survey commences with a clear formulation of the task. This\nis followed by a detailed exploration of various benchmarks rich in side\ninformation that are pivotal for advancing research in this field. Afterwards,\nwe delve into how different types of side information enhance the task,\nunderscoring data characteristics and utility. Moreover, we discuss the usage\nof various side information, including data encoding, data injection, and\ninvolved techniques. A systematic review of research progress is then\npresented, with the taxonomy by the types of side information. Finally, we\nsummarize the current limitations and present the future prospects of this\nvibrant topic.\n","authors":["Xiaokun Zhang","Bo Xu","Chenliang Li","Bowei He","Hongfei Lin","Chen Ma","Fenglong Ma"],"pdf_url":"https://arxiv.org/pdf/2505.12279v1.pdf","comment":"This work has been accepted by IEEE TKDE as a survey paper"},{"id":"http://arxiv.org/abs/2505.12260v1","updated":"2025-05-18T06:51:21Z","published":"2025-05-18T06:51:21Z","title":"LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x\n  Faster Query Inference","summary":"  Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode\nqueries and documents into low-dimensional dense or high-dimensional sparse\nvectors. It retrieves documents relevant to search queries based on vector\nsimilarities. Documents are pre-encoded offline, while queries arrive in\nreal-time, necessitating an efficient online query encoder. Although LLMs\nsignificantly enhance retrieval capabilities, serving deeply parameterized LLMs\nslows down query inference throughput and increases demands for online\ndeployment resources. In this paper, we propose LightRetriever, a novel\nLLM-based hybrid retriever with extremely lightweight query encoders. Our\nmethod retains a full-sized LLM for document encoding, but reduces the workload\nof query encoding to no more than an embedding lookup. Compared to serving a\nfull-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for\nquery inference with GPU acceleration, and even a 20x speedup without GPU.\nExperiments on large-scale retrieval benchmarks demonstrate that our method\ngeneralizes well across diverse retrieval tasks, retaining an average of 95%\nfull-sized performance.\n","authors":["Guangyuan Ma","Yongliang Ma","Xuanrui Gou","Zhenpeng Su","Ming Zhou","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2505.12260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08478v3","updated":"2025-05-18T06:26:48Z","published":"2024-10-11T03:10:09Z","title":"Dynamic Fusion Strategies for Federated Multimodal Recommendations","summary":"  Delivering deeply personalized recommendations necessitates understanding\nuser interactions with diverse multimedia features, but achieving this within\nthe constraints of Federated Recommendation Systems (FedRec) is severely\nhampered by communication bottlenecks, user heterogeneity, and the complexity\nof privacy-preserving multimodal fusion. To this end, we propose FedMR, a novel\nmultimodal FedRec framework centered around the Mixing Feature Fusion Module\n(MFFM). FedMR employs a two-stage process: (1) Server-side centralized\nmultimedia content processing provides rich, shared item context using\npre-trained models, mitigating limitations from client sparsity and resource\nconstraints efficiently. (2) Client-Side Personalized Refinement, where the\nMFFM dynamically adapts these server-provided multimodal representations based\non client-specific interaction patterns, effectively tailoring recommendations\nand resolving heterogeneity in user preferences towards different modalities.\nExtensive experiments validate that FedMR seamlessly enhances existing ID-based\nFedRecs, effectively transforming them into high-performing federated\nmultimodal systems.\n","authors":["Zhiwei Li","Guodong Long","Jing Jiang","Chengqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.08478v3.pdf","comment":"17 pages, 8 figures, 6 tables, conference"},{"id":"http://arxiv.org/abs/2503.01711v4","updated":"2025-05-18T06:19:50Z","published":"2025-03-03T16:24:36Z","title":"MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation\n  Alignment","summary":"  Personalized product search aims to retrieve and rank items that match users'\npreferences and search intent. Despite their effectiveness, existing approaches\ntypically assume that users' query fully captures their real motivation.\nHowever, our analysis of a real-world e-commerce platform reveals that users\noften engage in relevant consultations before searching, indicating they refine\nintents through consultations based on motivation and need. The implied\nmotivation in consultations is a key enhancing factor for personalized search.\nThis unexplored area comes with new challenges including aligning contextual\nmotivations with concise queries, bridging the category-text gap, and filtering\nnoise within sequence history. To address these, we propose a Motivation-Aware\nPersonalized Search (MAPS) method. It embeds queries and consultations into a\nunified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE)\nto prioritize critical semantics, and introduces dual alignment: (1)\ncontrastive learning aligns consultations, reviews, and product features; (2)\nbidirectional attention integrates motivation-aware embeddings with user\npreferences. Extensive experiments on real and synthetic data show MAPS\noutperforms existing methods in both retrieval and ranking tasks.\n","authors":["Weicong Qin","Yi Xu","Weijie Yu","Chenglei Shen","Ming He","Jianping Fan","Xiao Zhang","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2503.01711v4.pdf","comment":"accepted to ACL 2025 main conference"},{"id":"http://arxiv.org/abs/2505.13528v1","updated":"2025-05-18T04:40:34Z","published":"2025-05-18T04:40:34Z","title":"LLM-Based User Simulation for Low-Knowledge Shilling Attacks on\n  Recommender Systems","summary":"  Recommender systems (RS) are increasingly vulnerable to shilling attacks,\nwhere adversaries inject fake user profiles to manipulate system outputs.\nTraditional attack strategies often rely on simplistic heuristics, require\naccess to internal RS data, and overlook the manipulation potential of textual\nreviews. In this work, we introduce Agent4SR, a novel framework that leverages\nLarge Language Model (LLM)-based agents to perform low-knowledge, high-impact\nshilling attacks through both rating and review generation. Agent4SR simulates\nrealistic user behavior by orchestrating adversarial interactions, selecting\nitems, assigning ratings, and crafting reviews, while maintaining behavioral\nplausibility. Our design includes targeted profile construction, hybrid memory\nretrieval, and a review attack strategy that propagates target item features\nacross unrelated reviews to amplify manipulation. Extensive experiments on\nmultiple datasets and RS architectures demonstrate that Agent4SR outperforms\nexisting low-knowledge baselines in both effectiveness and stealth. Our\nfindings reveal a new class of emergent threats posed by LLM-driven agents,\nunderscoring the urgent need for enhanced defenses in modern recommender\nsystems.\n","authors":["Shengkang Gu","Jiahao Liu","Dongsheng Li","Guangping Zhang","Mingzhe Han","Hansu Gu","Peng Zhang","Ning Gu","Li Shang","Tun Lu"],"pdf_url":"https://arxiv.org/pdf/2505.13528v1.pdf","comment":"11 pages, under review"},{"id":"http://arxiv.org/abs/2505.13526v1","updated":"2025-05-18T03:20:20Z","published":"2025-05-18T03:20:20Z","title":"Geography-Aware Large Language Models for Next POI Recommendation","summary":"  The next Point-of-Interest (POI) recommendation task aims to predict users'\nnext destinations based on their historical movement data and plays a key role\nin location-based services and personalized applications. Accurate next POI\nrecommendation depends on effectively modeling geographic information and POI\ntransition relations, which are crucial for capturing spatial dependencies and\nuser movement patterns. While Large Language Models (LLMs) exhibit strong\ncapabilities in semantic understanding and contextual reasoning, applying them\nto spatial tasks like next POI recommendation remains challenging. First, the\ninfrequent nature of specific GPS coordinates makes it difficult for LLMs to\nmodel precise spatial contexts. Second, the lack of knowledge about POI\ntransitions limits their ability to capture potential POI-POI relationships. To\naddress these issues, we propose GA-LLM (Geography-Aware Large Language Model),\na novel framework that enhances LLMs with two specialized components. The\nGeographic Coordinate Injection Module (GCIM) transforms GPS coordinates into\nspatial representations using hierarchical and Fourier-based positional\nencoding, enabling the model to understand geographic features from multiple\nperspectives. The POI Alignment Module (PAM) incorporates POI transition\nrelations into the LLM's semantic space, allowing it to infer global POI\nrelationships and generalize to unseen POIs. Experiments on three real-world\ndatasets demonstrate the state-of-the-art performance of GA-LLM.\n","authors":["Zhao Liu","Wei Liu","Huajie Zhu","Jianxing Yu","Jian Yin","Wang-Chien Lee","Shun Wang"],"pdf_url":"https://arxiv.org/pdf/2505.13526v1.pdf","comment":"9 pages, 7figures"}]},"2025-05-17T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.05477v2","updated":"2025-05-17T22:33:52Z","published":"2024-03-08T17:38:55Z","title":"Take Your Best Shot: Sampling-Based Next-Best-View Planning for\n  Autonomous Photography & Inspection","summary":"  Autonomous mobile robots (AMRs) equipped with high-quality cameras have\nrevolutionized the field of inspections by providing efficient and\ncost-effective means of conducting surveys. The use of autonomous inspection is\nbecoming more widespread in a variety of contexts, yet it is still challenging\nto acquire the best inspection information autonomously. In situations where\nobjects may block a robot's view, it is necessary to use reasoning to determine\nthe optimal points for collecting data. Although researchers have explored\ncloud-based applications to store inspection data, these applications may not\noperate optimally under network constraints, and parsing these datasets can be\nmanually intensive. Instead, there is an emerging requirement for AMRs to\nautonomously capture the most informative views efficiently. To address this\nchallenge, we present an autonomous Next-Best-View (NBV) framework that\nmaximizes the inspection information while reducing the number of pictures\nneeded during operations. The framework consists of a formalized evaluation\nmetric using ray-tracing and Gaussian process interpolation to estimate\ninformation reward based on the current understanding of the partially-known\nenvironment. A derivative-free optimization (DFO) method is used to sample\ncandidate views in the environment and identify the NBV point. The proposed\napproach's effectiveness is shown by comparing it with existing methods and\nfurther validated through simulations and experiments with various vehicles.\n","authors":["Shijie Gao","Lauren Bramblett","Nicola Bezzo"],"pdf_url":"https://arxiv.org/pdf/2403.05477v2.pdf","comment":"For code and videos, see https://www.bezzorobotics.com/sg-lb-icra25"},{"id":"http://arxiv.org/abs/2412.01168v4","updated":"2025-05-17T22:13:03Z","published":"2024-12-02T06:10:23Z","title":"On the Surprising Effectiveness of Spectrum Clipping in Learning Stable\n  Linear Dynamics","summary":"  When learning stable linear dynamical systems from data, three important\nproperties are desirable: i) predictive accuracy, ii) provable stability, and\niii) computational efficiency. Unconstrained minimization of reconstruction\nerrors leads to high accuracy and efficiency but cannot guarantee stability.\nExisting methods to enforce stability often preserve accuracy, but do so only\nat the cost of increased computation. In this work, we investigate if a\nstraightforward approach can simultaneously offer all three desiderata of\nlearning stable linear systems. Specifically, we consider a post-hoc approach\nthat manipulates the spectrum of the learned system matrix that was computed\nusing unconstrained least squares. We call this approach spectrum clipping (SC)\nas it involves eigen decomposition and subsequent reconstruction of the system\nmatrix after clipping any eigenvalues that are larger than one to one (without\naltering the eigenvectors). Through comprehensive experiments involving two\ndifferent applications and publicly available benchmark datasets, we show that\nthis simple technique can efficiently learn highly-accurate linear systems that\nare provably-stable. Notably, we find that SC can match or outperform strong\nbaselines while being orders-of-magnitude faster. We also show that SC can be\nreadily combined with Koopman operators to learn stable nonlinear dynamics,\nsuch as those underlying complex dexterous manipulation skills involving\nmulti-fingered robotic hands. Finally, we find that SC can learn stable robot\npolicies even when the training data includes unsuccessful or truncated\ndemonstrations. Our codes and dataset can be found at\nhttps://github.com/GT-STAR-Lab/spec_clip.\n","authors":["Hanyao Guo","Yunhai Han","Harish Ravichandar"],"pdf_url":"https://arxiv.org/pdf/2412.01168v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10542v2","updated":"2025-05-17T22:09:51Z","published":"2025-05-15T17:53:11Z","title":"AORRTC: Almost-Surely Asymptotically Optimal Planning with RRT-Connect","summary":"  Finding high-quality solutions quickly is an important objective in motion\nplanning. This is especially true for high-degree-of-freedom robots.\nSatisficing planners have traditionally found feasible solutions quickly but\nprovide no guarantees on their optimality, while almost-surely asymptotically\noptimal (a.s.a.o.) planners have probabilistic guarantees on their convergence\ntowards an optimal solution but are more computationally expensive.\n  This paper uses the AO-x meta-algorithm to extend the satisficing RRT-Connect\nplanner to optimal planning. The resulting Asymptotically Optimal RRT-Connect\n(AORRTC) finds initial solutions in similar times as RRT-Connect and uses any\nadditional planning time to converge towards the optimal solution in an anytime\nmanner. It is proven to be probabilistically complete and a.s.a.o.\n  AORRTC was tested with the Panda (7 DoF) and Fetch (8 DoF) robotic arms on\nthe MotionBenchMaker dataset. These experiments show that AORRTC finds initial\nsolutions as fast as RRT-Connect and faster than the tested state-of-the-art\na.s.a.o. algorithms while converging to better solutions faster. AORRTC finds\nsolutions to difficult high-DoF planning problems in milliseconds where the\nother a.s.a.o. planners could not consistently find solutions in seconds. This\nperformance was demonstrated both with and without single instruction/multiple\ndata (SIMD) acceleration.\n","authors":["Tyler Wilson","Wil Thomason","Zachary Kingston","Jonathan Gammell"],"pdf_url":"https://arxiv.org/pdf/2505.10542v2.pdf","comment":"Submitted to IEEE Robotics and Automation Letters (RA-L). Manuscript\n  #25-1915. 8 pages, 4 figures, 1 table. A video of AORRTC can be found at\n  https://www.youtube.com/watch?v=j1itxP3KuiM . Information on the\n  implementation of AORRTC is available at https://robotic-esp.com/code/aorrtc/"},{"id":"http://arxiv.org/abs/2505.12153v1","updated":"2025-05-17T22:02:44Z","published":"2025-05-17T22:02:44Z","title":"Federated Deep Reinforcement Learning for Privacy-Preserving\n  Robotic-Assisted Surgery","summary":"  The integration of Reinforcement Learning (RL) into robotic-assisted surgery\n(RAS) holds significant promise for advancing surgical precision, adaptability,\nand autonomous decision-making. However, the development of robust RL models in\nclinical settings is hindered by key challenges, including stringent patient\ndata privacy regulations, limited access to diverse surgical datasets, and high\nprocedural variability. To address these limitations, this paper presents a\nFederated Deep Reinforcement Learning (FDRL) framework that enables\ndecentralized training of RL models across multiple healthcare institutions\nwithout exposing sensitive patient information. A central innovation of the\nproposed framework is its dynamic policy adaptation mechanism, which allows\nsurgical robots to select and tailor patient-specific policies in real-time,\nthereby ensuring personalized and Optimised interventions. To uphold rigorous\nprivacy standards while facilitating collaborative learning, the FDRL framework\nincorporates secure aggregation, differential privacy, and homomorphic\nencryption techniques. Experimental results demonstrate a 60\\% reduction in\nprivacy leakage compared to conventional methods, with surgical precision\nmaintained within a 1.5\\% margin of a centralized baseline. This work\nestablishes a foundational approach for adaptive, secure, and patient-centric\nAI-driven surgical robotics, offering a pathway toward clinical translation and\nscalable deployment across diverse healthcare environments.\n","authors":["Sana Hafeez","Sundas Rafat Mulkana","Muhammad Ali Imran","Michele Sevegnani"],"pdf_url":"https://arxiv.org/pdf/2505.12153v1.pdf","comment":"11 pages, 7 figures, conference"},{"id":"http://arxiv.org/abs/2412.16635v2","updated":"2025-05-17T21:14:51Z","published":"2024-12-21T14:00:41Z","title":"Task-Driven Co-Design of Mobile Manipulators","summary":"  Recent interest in mobile manipulation has resulted in a wide range of new\nrobot designs. A large family of these designs focuses on modular platforms\nthat combine existing mobile bases with static manipulator arms. They combine\nthese modules by mounting the arm in a tabletop configuration. However, the\noperating workspaces and heights for common mobile manipulation tasks, such as\nopening articulated objects, significantly differ from tabletop manipulation\ntasks. As a result, these standard arm mounting configurations can result in\nkinematics with restricted joint ranges and motions. To address these problems,\nwe present the first Concurrent Design approach for mobile manipulators to\noptimize key arm-mounting parameters. Our approach directly targets task\nperformance across representative household tasks by training a powerful\nmultitask-capable reinforcement learning policy in an inner loop while\noptimizing over a distribution of design configurations guided by Bayesian\nOptimization and HyperBand (BOHB) in an outer loop. This results in novel\ndesigns that significantly improve performance across both seen and unseen test\ntasks, and outperform designs generated by heuristic-based performance indices\nthat are cheaper to evaluate but only weakly correlated with the motions of\ninterest. We evaluate the physical feasibility of the resulting designs and\nshow that they are practical and remain modular, affordable, and compatible\nwith existing commercial components. We open-source the approach and generated\ndesigns to facilitate further improvements of these platforms.\n","authors":["Raphael Schneider","Daniel Honerkamp","Tim Welschehold","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2412.16635v2.pdf","comment":"Accepted for publication at RA-L. Project website:\n  https://moma-codesign.cs.uni-freiburg.de/"},{"id":"http://arxiv.org/abs/2412.10917v2","updated":"2025-05-17T21:14:03Z","published":"2024-12-14T18:04:18Z","title":"Adaptive Reward Design for Reinforcement Learning","summary":"  There is a surge of interest in using formal languages such as Linear\nTemporal Logic (LTL) to precisely and succinctly specify complex tasks and\nderive reward functions for Reinforcement Learning (RL). However, existing\nmethods often assign sparse rewards (e.g., giving a reward of 1 only if a task\nis completed and 0 otherwise). By providing feedback solely upon task\ncompletion, these methods fail to encourage successful subtask completion. This\nis particularly problematic in environments with inherent uncertainty, where\ntask completion may be unreliable despite progress on intermediate goals. To\naddress this limitation, we propose a suite of reward functions that\nincentivize an RL agent to complete a task specified by an LTL formula as much\nas possible, and develop an adaptive reward shaping approach that dynamically\nupdates reward functions during the learning process. Experimental results on a\nrange of benchmark RL environments demonstrate that the proposed approach\ngenerally outperforms baselines, achieving earlier convergence to a better\npolicy with higher expected return and task completion rate.\n","authors":["Minjae Kwon","Ingy ElSayed-Aly","Lu Feng"],"pdf_url":"https://arxiv.org/pdf/2412.10917v2.pdf","comment":"UAI 2025 Camera Ready Version"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2505.12102v1","updated":"2025-05-17T18:10:32Z","published":"2025-05-17T18:10:32Z","title":"Scalable Time-Tagged Data Acquisition for Entanglement Distribution in\n  Quantum Networks","summary":"  In distributed quantum applications such as entanglement distribution,\nprecise time synchronization and efficient time-tagged data handling are\nessential. Traditional systems often suffer from overflow, synchronization\ndrift, and storage inefficiencies. We propose a modular Time Tagging (TT) agent\nthat uses a 1 pulse per second (PPS) signal from White Rabbit (WR) devices to\nachieve network-wide synchronization, while applying real-time calibration,\noverflow mitigation, and compression. A live two-lab entanglement distribution\nexperiment validated the system's performance, achieving synchronized\ncoincidence detection at 25,000 counts/sec.\n","authors":["Abderrahim Amlou","Thomas Gerrits","Anouar Rahmouni","Amar Abane","Mheni Merzouki","Ya-Shian Li-Baboud","Ahmed Lbath","Abdella Battou","Oliver Slattery"],"pdf_url":"https://arxiv.org/pdf/2505.12102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12065v1","updated":"2025-05-17T16:07:01Z","published":"2025-05-17T16:07:01Z","title":"Demystifying and Enhancing the Efficiency of Large Language Model Based\n  Search Agents","summary":"  Large Language Model (LLM)-based search agents have shown remarkable\ncapabilities in solving complex tasks by dynamically decomposing problems and\naddressing them through interleaved reasoning and retrieval. However, this\ninterleaved paradigm introduces substantial efficiency bottlenecks. First, we\nobserve that both highly accurate and overly approximate retrieval methods\ndegrade system efficiency: exact search incurs significant retrieval overhead,\nwhile coarse retrieval requires additional reasoning steps during generation.\nSecond, we identify inefficiencies in system design, including improper\nscheduling and frequent retrieval stalls, which lead to cascading latency --\nwhere even minor delays in retrieval amplify end-to-end inference time. To\naddress these challenges, we introduce SearchAgent-X, a high-efficiency\ninference framework for LLM-based search agents. SearchAgent-X leverages\nhigh-recall approximate retrieval and incorporates two key techniques:\npriority-aware scheduling and non-stall retrieval. Extensive experiments\ndemonstrate that SearchAgent-X consistently outperforms state-of-the-art\nsystems such as vLLM and HNSW-based retrieval across diverse tasks, achieving\nup to 3.4$\\times$ higher throughput and 5$\\times$ lower latency, without\ncompromising generation quality. SearchAgent-X is available at\nhttps://github.com/tiannuo-yang/SearchAgent-X.\n","authors":["Tiannuo Yang","Zebin Yao","Bowen Jin","Lixiao Cui","Yusen Li","Gang Wang","Xiaoguang Liu"],"pdf_url":"https://arxiv.org/pdf/2505.12065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13520v1","updated":"2025-05-17T13:23:54Z","published":"2025-05-17T13:23:54Z","title":"Beyond Retrieval: Joint Supervision and Multimodal Document Ranking for\n  Textbook Question Answering","summary":"  Textbook question answering (TQA) is a complex task, requiring the\ninterpretation of complex multimodal context. Although recent advances have\nimproved overall performance, they often encounter difficulties in educational\nsettings where accurate semantic alignment and task-specific document retrieval\nare essential. In this paper, we propose a novel approach to multimodal\ntextbook question answering by introducing a mechanism for enhancing semantic\nrepresentations through multi-objective joint training. Our model, Joint\nEmbedding Training With Ranking Supervision for Textbook Question Answering\n(JETRTQA), is a multimodal learning framework built on a retriever--generator\narchitecture that uses a retrieval-augmented generation setup, in which a\nmultimodal large language model generates answers. JETRTQA is designed to\nimprove the relevance of retrieved documents in complex educational contexts.\nUnlike traditional direct scoring approaches, JETRTQA learns to refine the\nsemantic representations of questions and documents through a supervised signal\nthat combines pairwise ranking and implicit supervision derived from answers.\nWe evaluate our method on the CK12-QA dataset and demonstrate that it\nsignificantly improves the discrimination between informative and irrelevant\ndocuments, even when they are long, complex, and multimodal. JETRTQA\noutperforms the previous state of the art, achieving a 2.4\\% gain in accuracy\non the validation set and 11.1\\% on the test set.\n","authors":["Hessa Alawwad","Usman Naseem","Areej Alhothali","Ali Alkhathlan","Amani Jamal"],"pdf_url":"https://arxiv.org/pdf/2505.13520v1.pdf","comment":"14 pages, 16 figure"},{"id":"http://arxiv.org/abs/2505.11946v1","updated":"2025-05-17T10:24:08Z","published":"2025-05-17T10:24:08Z","title":"Let's have a chat with the EU AI Act","summary":"  As artificial intelligence (AI) regulations evolve and the regulatory\nlandscape develops and becomes more complex, ensuring compliance with ethical\nguidelines and legal frameworks remains a challenge for AI developers. This\npaper introduces an AI-driven self-assessment chatbot designed to assist users\nin navigating the European Union AI Act and related standards. Leveraging a\nRetrieval-Augmented Generation (RAG) framework, the chatbot enables real-time,\ncontext-aware compliance verification by retrieving relevant regulatory texts\nand providing tailored guidance. By integrating both public and proprietary\nstandards, it streamlines regulatory adherence, reduces complexity, and fosters\nresponsible AI development. The paper explores the chatbot's architecture,\ncomparing naive and graph-based RAG models, and discusses its potential impact\non AI governance.\n","authors":["Adam Kovari","Yasin Ghafourian","Csaba Hegedus","Belal Abu Naim","Kitti Mezei","Pal Varga","Markus Tauber"],"pdf_url":"https://arxiv.org/pdf/2505.11946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11944v1","updated":"2025-05-17T10:15:05Z","published":"2025-05-17T10:15:05Z","title":"Basic model for ranking microfinance institutions","summary":"  This paper discusses the challenges encountered in building a ranking model\nfor aggregator site products, using the example of ranking microfinance\ninstitutions (MFIs) based on post-click conversion. We suggest which features\nof MFIs should be considered, and using an algorithm based on Markov chains, we\ndemonstrate the ``usefulness'' of these features on real data. The ideas\ndeveloped in this work can be applied to aggregator websites in microinsurance,\nespecially when personal data is unavailable. Since we did not find similar\ndatasets in the public domain, we are publishing our dataset with a detailed\ndescription of its attributes.\n","authors":["Dmitry Dudukalov","Evgeny Prokopenko"],"pdf_url":"https://arxiv.org/pdf/2505.11944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11933v1","updated":"2025-05-17T09:36:05Z","published":"2025-05-17T09:36:05Z","title":"Conversational Recommendation System using NLP and Sentiment Analysis","summary":"  In today's digitally-driven world, the demand for personalized and\ncontext-aware recommendations has never been greater. Traditional recommender\nsystems have made significant strides in this direction, but they often lack\nthe ability to tap into the richness of conversational data. This paper\nrepresents a novel approach to recommendation systems by integrating\nconversational insights into the recommendation process. The Conversational\nRecommender System integrates cutting-edge technologies such as deep learning,\nleveraging machine learning algorithms like Apriori for Association Rule\nMining, Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN),\nand Long Short-Term Memory (LTSM). Furthermore, sophisticated voice recognition\ntechnologies, including Hidden Markov Models (HMMs) and Dynamic Time Warping\n(DTW) algorithms, play a crucial role in accurate speech-to-text conversion,\nensuring robust performance in diverse environments. The methodology\nincorporates a fusion of content-based and collaborative recommendation\napproaches, enhancing them with NLP techniques. This innovative integration\nensures a more personalized and context-aware recommendation experience,\nparticularly in marketing applications.\n","authors":["Piyush Talegaonkar","Siddhant Hole","Shrinesh Kamble","Prashil Gulechha","Deepali Salapurkar"],"pdf_url":"https://arxiv.org/pdf/2505.11933v1.pdf","comment":"Presented in ISETE conference (International Conference on Artificial\n  Intelligence, Machine Learning and Big Data Engineering 2024)"},{"id":"http://arxiv.org/abs/2505.11932v1","updated":"2025-05-17T09:36:03Z","published":"2025-05-17T09:36:03Z","title":"Neuro-Symbolic Query Compiler","summary":"  Precise recognition of search intent in Retrieval-Augmented Generation (RAG)\nsystems remains a challenging goal, especially under resource constraints and\nfor complex queries with nested structures and dependencies. This paper\npresents QCompiler, a neuro-symbolic framework inspired by linguistic grammar\nrules and compiler design, to bridge this gap. It theoretically designs a\nminimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize\ncomplex queries. Unlike previous methods, this grammar maintains completeness\nwhile minimizing redundancy. Based on this, QCompiler includes a Query\nExpression Translator, a Lexical Syntax Parser, and a Recursive Descent\nProcessor to compile queries into Abstract Syntax Trees (ASTs) for execution.\nThe atomicity of the sub-queries in the leaf nodes ensures more precise\ndocument retrieval and response generation, significantly improving the RAG\nsystem's ability to address complex queries.\n","authors":["Yuyao Zhang","Zhicheng Dou","Xiaoxi Li","Jiajie Jin","Yongkang Wu","Zhonghua Li","Qi Ye","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2505.11932v1.pdf","comment":"Findings of ACL2025, codes are available at this url:\n  https://github.com/YuyaoZhangQAQ/Query_Compiler"},{"id":"http://arxiv.org/abs/2505.11900v1","updated":"2025-05-17T08:32:05Z","published":"2025-05-17T08:32:05Z","title":"Recursive Question Understanding for Complex Question Answering over\n  Heterogeneous Personal Data","summary":"  Question answering over mixed sources, like text and tables, has been\nadvanced by verbalizing all contents and encoding it with a language model. A\nprominent case of such heterogeneous data is personal information: user devices\nlog vast amounts of data every day, such as calendar entries, workout\nstatistics, shopping records, streaming history, and more. Information needs\nrange from simple look-ups to queries of analytical nature. The challenge is to\nprovide humans with convenient access with small footprint, so that all\npersonal data stays on the user devices. We present ReQAP, a novel method that\ncreates an executable operator tree for a given question, via recursive\ndecomposition. Operators are designed to enable seamless integration of\nstructured and unstructured sources, and the execution of the operator tree\nyields a traceable answer. We further release the PerQA benchmark, with\npersona-based data and questions, covering a diverse spectrum of realistic user\nneeds.\n","authors":["Philipp Christmann","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2505.11900v1.pdf","comment":"Accepted at ACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2112.07620v4","updated":"2025-05-17T07:52:11Z","published":"2021-12-12T00:19:47Z","title":"Tree-based Focused Web Crawling with Reinforcement Learning","summary":"  A focused crawler aims at discovering as many web pages and web sites\nrelevant to a target topic as possible, while avoiding irrelevant ones.\nReinforcement Learning (RL) has been a promising direction for optimizing\nfocused crawling, because RL can naturally optimize the long-term profit of\ndiscovering relevant web locations within the context of a reward. In this\npaper, we propose TRES, a novel RL-empowered framework for focused crawling\nthat aims at maximizing both the number of relevant web pages (aka\n\\textit{harvest rate}) and the number of relevant web sites (\\textit{domains}).\nWe model the focused crawling problem as a novel Markov Decision Process (MDP),\nwhich the RL agent aims to solve by determining an optimal crawling strategy.\nTo overcome the computational infeasibility of exhaustively searching for the\nbest action at each time step, we propose Tree-Frontier, a provably efficient\ntree-based sampling algorithm that adaptively discretizes the large state and\naction spaces and evaluates only a few representative actions. Experimentally,\nutilizing online real-world data, we show that TRES significantly outperforms\nand Pareto-dominates state-of-the-art methods in terms of harvest rate and the\nnumber of retrieved relevant domains, while it provably reduces by orders of\nmagnitude the number of URLs needed to be evaluated at each crawling step.\n","authors":["Andreas Kontogiannis","Dimitrios Kelesis","Vasilis Pollatos","George Giannakopoulos","Georgios Paliouras"],"pdf_url":"https://arxiv.org/pdf/2112.07620v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11856v1","updated":"2025-05-17T05:46:30Z","published":"2025-05-17T05:46:30Z","title":"Telco-oRAG: Optimizing Retrieval-augmented Generation for Telecom\n  Queries via Hybrid Retrieval and Neural Routing","summary":"  Artificial intelligence will be one of the key pillars of the next generation\nof mobile networks (6G), as it is expected to provide novel added-value\nservices and improve network performance. In this context, large language\nmodels have the potential to revolutionize the telecom landscape through intent\ncomprehension, intelligent knowledge retrieval, coding proficiency, and\ncross-domain orchestration capabilities. This paper presents Telco-oRAG, an\nopen-source Retrieval-Augmented Generation (RAG) framework optimized for\nanswering technical questions in the telecommunications domain, with a\nparticular focus on 3GPP standards. Telco-oRAG introduces a hybrid retrieval\nstrategy that combines 3GPP domain-specific retrieval with web search,\nsupported by glossary-enhanced query refinement and a neural router for\nmemory-efficient retrieval. Our results show that Telco-oRAG improves the\naccuracy in answering 3GPP-related questions by up to 17.6% and achieves a\n10.6% improvement in lexicon queries compared to baselines. Furthermore,\nTelco-oRAG reduces memory usage by 45% through targeted retrieval of relevant\n3GPP series compared to baseline RAG, and enables open-source LLMs to reach\nGPT-4-level accuracy on telecom benchmarks.\n","authors":["Andrei-Laurentiu Bornea","Fadhel Ayed","Antonio De Domenico","Nicola Piovesan","Tareq Si Salem","Ali Maatouk"],"pdf_url":"https://arxiv.org/pdf/2505.11856v1.pdf","comment":"12 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2505.11795v1","updated":"2025-05-17T02:49:15Z","published":"2025-05-17T02:49:15Z","title":"The Effects of Demographic Instructions on LLM Personas","summary":"  Social media platforms must filter sexist content in compliance with\ngovernmental regulations. Current machine learning approaches can reliably\ndetect sexism based on standardized definitions, but often neglect the\nsubjective nature of sexist language and fail to consider individual users'\nperspectives. To address this gap, we adopt a perspectivist approach, retaining\ndiverse annotations rather than enforcing gold-standard labels or their\naggregations, allowing models to account for personal or group-specific views\nof sexism. Using demographic data from Twitter, we employ large language models\n(LLMs) to personalize the identification of sexism.\n","authors":["Angel Felipe Magnossão de Paula","J. Shane Culpepper","Alistair Moffat","Sachin Pathiyan Cherumanal","Falk Scholer","Johanne Trippas"],"pdf_url":"https://arxiv.org/pdf/2505.11795v1.pdf","comment":"Accepted at SIGIR'25, Padua, Italy"},{"id":"http://arxiv.org/abs/2412.10381v5","updated":"2025-05-17T01:06:52Z","published":"2024-11-28T04:06:02Z","title":"Supervised Learning-enhanced Multi-Group Actor Critic for Live Stream\n  Allocation in Feed","summary":"  In the context of a short video & live stream mixed recommendation scenario,\nthe live stream recommendation system (RS) decides whether to allocate at most\none live stream into the video feed for each user request. To maximize\nlong-term user engagement, it is crucial to determine an optimal live stream\npolicy for accurate live stream allocation. The inappropriate live stream\nallocation policy can significantly affect the duration of the usage app and\nuser retention, which ignores the long-term negative impact of live stream\nallocation. Recently, reinforcement learning (RL) has been widely applied in\nrecommendation systems to capture long-term user engagement. However,\ntraditional RL algorithms often face divergence and instability problems, which\nrestricts the application and deployment in the large-scale industrial\nrecommendation systems, especially in the aforementioned challenging scenario.\nTo address these challenges, we propose a novel Supervised Learning-enhanced\nMulti-Group Actor Critic algorithm (SL-MGAC). Specifically, we introduce a\nsupervised learning-enhanced actor-critic framework that incorporates variance\nreduction techniques, where multi-task reward learning helps restrict\nbootstrapping error accumulation during critic learning. Additionally, we\ndesign a multi-group state decomposition module for both actor and critic\nnetworks to reduce prediction variance and improve model stability. We also\npropose a novel reward function to prevent overly greedy live stream\nallocation. Empirically, we evaluate the SL-MGAC algorithm using offline policy\nevaluation (OPE) and online A/B testing. Experimental results demonstrate that\nthe proposed method not only outperforms baseline methods under the\nplatform-level constraints but also exhibits enhanced stability in online\nrecommendation scenarios.\n","authors":["Jingxin Liu","Xiang Gao","Yisha Li","Xin Li","Haiyang Lu","Ben Wang"],"pdf_url":"https://arxiv.org/pdf/2412.10381v5.pdf","comment":null}]},"2025-05-16T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2505.11687v1","updated":"2025-05-16T20:48:59Z","published":"2025-05-16T20:48:59Z","title":"Second SIGIR Workshop on Simulations for Information Access (Sim4IA\n  2025)","summary":"  Simulations in information access (IA) have recently gained interest, as\nshown by various tutorials and workshops around that topic. Simulations can be\nkey contributors to central IA research and evaluation questions, especially\naround interactive settings when real users are unavailable, or their\nparticipation is impossible due to ethical reasons. In addition, simulations in\nIA can help contribute to a better understanding of users, reduce complexity of\nevaluation experiments, and improve reproducibility. Building on recent\ndevelopments in methods and toolkits, the second iteration of our Sim4IA\nworkshop aims to again bring together researchers and practitioners to form an\ninteractive and engaging forum for discussions on the future perspectives of\nthe field. An additional aim is to plan an upcoming TREC/CLEF campaign.\n","authors":["Philipp Schaer","Christin Katharina Kreutz","Krisztian Balog","Timo Breuer","Andreas Konstantin Kruff"],"pdf_url":"https://arxiv.org/pdf/2505.11687v1.pdf","comment":"Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '25), July 13--18,\n  2025, Padua, Italy"},{"id":"http://arxiv.org/abs/2505.11672v1","updated":"2025-05-16T20:17:10Z","published":"2025-05-16T20:17:10Z","title":"Terminators: Terms of Service Parsing and Auditing Agents","summary":"  Terms of Service (ToS) documents are often lengthy and written in complex\nlegal language, making them difficult for users to read and understand. To\naddress this challenge, we propose Terminators, a modular agentic framework\nthat leverages large language models (LLMs) to parse and audit ToS documents.\nRather than treating ToS understanding as a black-box summarization problem,\nTerminators breaks the task down to three interpretable steps: term extraction,\nverification, and accountability planning. We demonstrate the effectiveness of\nour method on the OpenAI ToS using GPT-4o, highlighting strategies to minimize\nhallucinations and maximize auditability. Our results suggest that structured,\nagent-based LLM workflows can enhance both the usability and enforceability of\ncomplex legal documents. By translating opaque terms into actionable,\nverifiable components, Terminators promotes ethical use of web content by\nenabling greater transparency, empowering users to understand their digital\nrights, and supporting automated policy audits for regulatory or civic\noversight.\n","authors":["Maruf Ahmed Mridul","Inwon Kang","Oshani Seneviratne"],"pdf_url":"https://arxiv.org/pdf/2505.11672v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2505.11651v1","updated":"2025-05-16T19:22:19Z","published":"2025-05-16T19:22:19Z","title":"MIRACL-VISION: A Large, multilingual, visual document retrieval\n  benchmark","summary":"  Document retrieval is an important task for search and Retrieval-Augmented\nGeneration (RAG) applications. Large Language Models (LLMs) have contributed to\nimproving the accuracy of text-based document retrieval. However, documents\nwith complex layout and visual elements like tables, charts and infographics\nare not perfectly represented in textual format. Recently, image-based document\nretrieval pipelines have become popular, which use visual large language models\n(VLMs) to retrieve relevant page images given a query. Current evaluation\nbenchmarks on visual document retrieval are limited, as they primarily focus\nonly English language, rely on synthetically generated questions and offer a\nsmall corpus size. Therefore, we introduce MIRACL-VISION, a multilingual visual\ndocument retrieval evaluation benchmark. MIRACL-VISION covers 18 languages, and\nis an extension of the MIRACL dataset, a popular benchmark to evaluate\ntext-based multilingual retrieval pipelines. MIRACL was built using a\nhuman-intensive annotation process to generate high-quality questions. In order\nto reduce MIRACL-VISION corpus size to make evaluation more compute friendly\nwhile keeping the datasets challenging, we have designed a method for\neliminating the \"easy\" negatives from the corpus. We conducted extensive\nexperiments comparing MIRACL-VISION with other benchmarks, using popular public\ntext and image models. We observe a gap in state-of-the-art VLM-based embedding\nmodels on multilingual capabilities, with up to 59.7% lower retrieval accuracy\nthan a text-based retrieval models. Even for the English language, the visual\nmodels retrieval accuracy is 12.1% lower compared to text-based models.\nMIRACL-VISION is a challenging, representative, multilingual evaluation\nbenchmark for visual retrieval pipelines and will help the community build\nrobust models for document retrieval.\n","authors":["Radek Osmulsk","Gabriel de Souza P. Moreira","Ronay Ak","Mengyao Xu","Benedikt Schifferer","Even Oldridge"],"pdf_url":"https://arxiv.org/pdf/2505.11651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11471v1","updated":"2025-05-16T17:26:16Z","published":"2025-05-16T17:26:16Z","title":"CRISP: Clustering Multi-Vector Representations for Denoising and Pruning","summary":"  Multi-vector models, such as ColBERT, are a significant advancement in neural\ninformation retrieval (IR), delivering state-of-the-art performance by\nrepresenting queries and documents by multiple contextualized token-level\nembeddings. However, this increased representation size introduces considerable\nstorage and computational overheads which have hindered widespread adoption in\npractice. A common approach to mitigate this overhead is to cluster the model's\nfrozen vectors, but this strategy's effectiveness is fundamentally limited by\nthe intrinsic clusterability of these embeddings. In this work, we introduce\nCRISP (Clustered Representations with Intrinsic Structure Pruning), a novel\nmulti-vector training method which learns inherently clusterable\nrepresentations directly within the end-to-end training process. By integrating\nclustering into the training phase rather than imposing it post-hoc, CRISP\nsignificantly outperforms post-hoc clustering at all representation sizes, as\nwell as other token pruning methods. On the BEIR retrieval benchmarks, CRISP\nachieves a significant rate of ~3x reduction in the number of vectors while\noutperforming the original unpruned model. This indicates that learned\nclustering effectively denoises the model by filtering irrelevant information,\nthereby generating more robust multi-vector representations. With more\naggressive clustering, CRISP achieves an 11x reduction in the number of vectors\nwith only a $3.6\\%$ quality loss.\n","authors":["João Veneroso","Rajesh Jayaram","Jinmeng Rao","Gustavo Hernández Ábrego","Majid Hadian","Daniel Cer"],"pdf_url":"https://arxiv.org/pdf/2505.11471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11582v1","updated":"2025-05-16T17:06:35Z","published":"2025-05-16T17:06:35Z","title":"Comparing Lexical and Semantic Vector Search Methods When Classifying\n  Medical Documents","summary":"  Classification is a common AI problem, and vector search is a typical\nsolution. This transforms a given body of text into a numerical representation,\nknown as an embedding, and modern improvements to vector search focus on\noptimising speed and predictive accuracy. This is often achieved through neural\nmethods that aim to learn language semantics. However, our results suggest that\nthese are not always the best solution. Our task was to classify\nrigidly-structured medical documents according to their content, and we found\nthat using off-the-shelf semantic vector search produced slightly worse\npredictive accuracy than creating a bespoke lexical vector search model, and\nthat it required significantly more time to execute. These findings suggest\nthat traditional methods deserve to be contenders in the information retrieval\ntoolkit, despite the prevalence and success of neural models.\n","authors":["Lee Harris","Philippe De Wilde","James Bentham"],"pdf_url":"https://arxiv.org/pdf/2505.11582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11388v1","updated":"2025-05-16T15:51:52Z","published":"2025-05-16T15:51:52Z","title":"The Future is Sparse: Embedding Compression for Scalable Retrieval in\n  Recommender Systems","summary":"  Industry-scale recommender systems face a core challenge: representing\nentities with high cardinality, such as users or items, using dense embeddings\nthat must be accessible during both training and inference. However, as\nembedding sizes grow, memory constraints make storage and access increasingly\ndifficult. We describe a lightweight, learnable embedding compression technique\nthat projects dense embeddings into a high-dimensional, sparsely activated\nspace. Designed for retrieval tasks, our method reduces memory requirements\nwhile preserving retrieval performance, enabling scalable deployment under\nstrict resource constraints. Our results demonstrate that leveraging sparsity\nis a promising approach for improving the efficiency of large-scale\nrecommenders. We release our code at https://github.com/recombee/CompresSAE.\n","authors":["Petr Kasalický","Martin Spišák","Vojtěch Vančura","Daniel Bohuněk","Rodrigo Alves","Pavel Kordík"],"pdf_url":"https://arxiv.org/pdf/2505.11388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14662v2","updated":"2025-05-16T15:43:24Z","published":"2025-02-20T15:58:25Z","title":"iAgent: LLM Agent as a Shield between User and Recommender Systems","summary":"  Traditional recommender systems usually take the user-platform paradigm,\nwhere users are directly exposed under the control of the platform's\nrecommendation algorithms. However, the defect of recommendation algorithms may\nput users in very vulnerable positions under this paradigm. First, many\nsophisticated models are often designed with commercial objectives in mind,\nfocusing on the platform's benefits, which may hinder their ability to protect\nand capture users' true interests. Second, these models are typically optimized\nusing data from all users, which may overlook individual user's preferences.\nDue to these shortcomings, users may experience several disadvantages under the\ntraditional user-platform direct exposure paradigm, such as lack of control\nover the recommender system, potential manipulation by the platform, echo\nchamber effects, or lack of personalization for less active users due to the\ndominance of active users during collaborative learning. Therefore, there is an\nurgent need to develop a new paradigm to protect user interests and alleviate\nthese issues. Recently, some researchers have introduced LLM agents to simulate\nuser behaviors, these approaches primarily aim to optimize platform-side\nperformance, leaving core issues in recommender systems unresolved. To address\nthese limitations, we propose a new user-agent-platform paradigm, where agent\nserves as the protective shield between user and recommender system that\nenables indirect exposure.\n","authors":["Wujiang Xu","Yunxiao Shi","Zujie Liang","Xuying Ning","Kai Mei","Kun Wang","Xi Zhu","Min Xu","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.14662v2.pdf","comment":"Findings of ACL 2025 and WWW2025@HCRS"},{"id":"http://arxiv.org/abs/2505.11318v1","updated":"2025-05-16T14:41:57Z","published":"2025-05-16T14:41:57Z","title":"On the Role of Weight Decay in Collaborative Filtering: A Popularity\n  Perspective","summary":"  Collaborative filtering (CF) enables large-scale recommendation systems by\nencoding information from historical user-item interactions into dense\nID-embedding tables. However, as embedding tables grow, closed-form solutions\nbecome impractical, often necessitating the use of mini-batch gradient descent\nfor training. Despite extensive work on designing loss functions to train CF\nmodels, we argue that one core component of these pipelines is heavily\noverlooked: weight decay. Attaining high-performing models typically requires\ncareful tuning of weight decay, regardless of loss, yet its necessity is not\nwell understood. In this work, we question why weight decay is crucial in CF\npipelines and how it impacts training. Through theoretical and empirical\nanalysis, we surprisingly uncover that weight decay's primary function is to\nencode popularity information into the magnitudes of the embedding vectors.\nMoreover, we find that tuning weight decay acts as a coarse, non-linear knob to\ninfluence preference towards popular or unpopular items. Based on these\nfindings, we propose PRISM (Popularity-awaRe Initialization Strategy for\nembedding Magnitudes), a straightforward yet effective solution to simplify the\ntraining of high-performing CF models. PRISM pre-encodes the popularity\ninformation typically learned through weight decay, eliminating its necessity.\nOur experiments show that PRISM improves performance by up to 4.77% and reduces\ntraining times by 38.48%, compared to state-of-the-art training strategies.\nAdditionally, we parameterize PRISM to modulate the initialization strength,\noffering a cost-effective and meaningful strategy to mitigate popularity bias.\n","authors":["Donald Loveland","Mingxuan Ju","Tong Zhao","Neil Shah","Danai Koutra"],"pdf_url":"https://arxiv.org/pdf/2505.11318v1.pdf","comment":"Accepted at SIGKDD 2025"},{"id":"http://arxiv.org/abs/2505.11271v1","updated":"2025-05-16T14:04:31Z","published":"2025-05-16T14:04:31Z","title":"Semantic Caching of Contextual Summaries for Efficient\n  Question-Answering with Language Models","summary":"  Large Language Models (LLMs) are increasingly deployed across edge and cloud\nplatforms for real-time question-answering and retrieval-augmented generation.\nHowever, processing lengthy contexts in distributed systems incurs high\ncomputational overhead, memory usage, and network bandwidth. This paper\nintroduces a novel semantic caching approach for storing and reusing\nintermediate contextual summaries, enabling efficient information reuse across\nsimilar queries in LLM-based QA workflows. Our method reduces redundant\ncomputations by up to 50-60% while maintaining answer accuracy comparable to\nfull document processing, as demonstrated on NaturalQuestions, TriviaQA, and a\nsynthetic ArXiv dataset. This approach balances computational cost and response\nquality, critical for real-time AI assistants.\n","authors":["Camille Couturier","Spyros Mastorakis","Haiying Shen","Saravan Rajmohan","Victor Rühle"],"pdf_url":"https://arxiv.org/pdf/2505.11271v1.pdf","comment":"Preprint. Paper accepted at ICCCN 2025, the final version will appear\n  in the proceedings"},{"id":"http://arxiv.org/abs/2505.11198v1","updated":"2025-05-16T12:56:40Z","published":"2025-05-16T12:56:40Z","title":"User-centric Music Recommendations","summary":"  This work presents a user-centric recommendation framework, designed as a\npipeline with four distinct, connected, and customizable phases. These phases\nare intended to improve explainability and boost user engagement.\n  We have collected the historical Last.fm track playback records of a single\nuser over approximately 15 years. The collected dataset includes more than\n90,000 playbacks and approximately 14,000 unique tracks.\n  From track playback records, we have created a dataset of user temporal\ncontexts (each row is a specific moment when the user listened to certain music\ndescriptors). As music descriptors, we have used community-contributed Last.fm\ntags and Spotify audio features. They represent the music that, throughout\nyears, the user has been listening to.\n  Next, given the most relevant Last.fm tags of a moment (e.g. the hour of the\nday), we predict the Spotify audio features that best fit the user preferences\nin that particular moment. Finally, we use the predicted audio features to find\ntracks similar to these features. The final aim is to recommend (and discover)\ntracks that the user may feel like listening to at a particular moment.\n  For our initial study case, we have chosen to predict only a single audio\nfeature target: danceability. The framework, however, allows to include more\ntarget variables.\n  The ability to learn the musical habits from a single user can be quite\npowerful, and this framework could be extended to other users.\n","authors":["Jaime Ramirez Castillo","M. Julia Flores","Ann E. Nicholson"],"pdf_url":"https://arxiv.org/pdf/2505.11198v1.pdf","comment":"Accepted for the 16th Bayesian Modelling Applications Workshop\n  (@UAI2022) (BMAW 2022)"},{"id":"http://arxiv.org/abs/2505.11180v1","updated":"2025-05-16T12:31:29Z","published":"2025-05-16T12:31:29Z","title":"mmRAG: A Modular Benchmark for Retrieval-Augmented Generation over Text,\n  Tables, and Knowledge Graphs","summary":"  Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the capabilities of large language models. However, existing RAG\nevaluation predominantly focuses on text retrieval and relies on opaque,\nend-to-end assessments of generated outputs. To address these limitations, we\nintroduce mmRAG, a modular benchmark designed for evaluating multi-modal RAG\nsystems. Our benchmark integrates queries from six diverse question-answering\ndatasets spanning text, tables, and knowledge graphs, which we uniformly\nconvert into retrievable documents. To enable direct, granular evaluation of\nindividual RAG components -- such as the accuracy of retrieval and query\nrouting -- beyond end-to-end generation quality, we follow standard information\nretrieval procedures to annotate document relevance and derive dataset\nrelevance. We establish baseline performance by evaluating a wide range of RAG\nimplementations on mmRAG.\n","authors":["Chuan Xu","Qiaosheng Chen","Yutong Feng","Gong Cheng"],"pdf_url":"https://arxiv.org/pdf/2505.11180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11176v1","updated":"2025-05-16T12:20:31Z","published":"2025-05-16T12:20:31Z","title":"From Intent Discovery to Recognition with Topic Modeling and Synthetic\n  Data","summary":"  Understanding and recognizing customer intents in AI systems is crucial,\nparticularly in domains characterized by short utterances and the cold start\nproblem, where recommender systems must include new products or services\nwithout sufficient real user data. Customer utterances are characterized by\ninfrequent word co-occurences and high term variability, which poses\nsignificant challenges for traditional methods in specifying distinct user\nneeds and preparing synthetic queries. To address this, we propose an agentic\nLLM framework for topic modeling and synthetic query generation, which\naccelerates the discovery and recognition of customer intents. We first apply\nhierarchical topic modeling and intent discovery to expand a human-curated\ntaxonomy from 36 generic user intents to 278 granular intents, demonstrating\nthe potential of LLMs to significantly enhance topic specificity and diversity.\nNext, to support newly discovered intents and address the cold start problem,\nwe generate synthetic user query data, which augments real utterances and\nreduces dependency on human annotation, especially in low-resource settings.\nTopic model experiments show substantial improvements in coherence and\nrelevance after topic expansion, while synthetic data experiments indicate that\nin-class few-shot prompting significantly improves the quality and utility of\nsynthetic queries without compromising diversity. We also show that\nLLM-generated intent descriptions and keywords can effectively substitute for\nhuman-curated versions when used as context for synthetic query generation. Our\nresearch underscores the scalability and utility of LLM agents in topic\nmodeling and highlights the strategic use of synthetic utterances to enhance\ndataset variability and coverage for intent recognition. We present a\ncomprehensive and robust framework for online discovery and recognition of new\ncustomer intents in dynamic domains.\n","authors":["Aaron Rodrigues","Mahmood Hegazy","Azzam Naeem"],"pdf_url":"https://arxiv.org/pdf/2505.11176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16594v2","updated":"2025-05-16T10:53:01Z","published":"2024-07-23T15:53:17Z","title":"Flexible Generation of Preference Data for Recommendation Analysis","summary":"  Simulating a recommendation system in a controlled environment, to identify\nspecific behaviors and user preferences, requires highly flexible synthetic\ndata generation models capable of mimicking the patterns and trends of real\ndatasets. In this context, we propose HYDRA, a novel preferences data\ngeneration model driven by three main factors: user-item interaction level,\nitem popularity, and user engagement level. The key innovations of the proposed\nprocess include the ability to generate user communities characterized by\nsimilar item adoptions, reflecting real-world social influences and trends.\nAdditionally, HYDRA considers item popularity and user engagement as mixtures\nof different probability distributions, allowing for a more realistic\nsimulation of diverse scenarios. This approach enhances the model's capacity to\nsimulate a wide range of real-world cases, capturing the complexity and\nvariability found in actual user behavior. We demonstrate the effectiveness of\nHYDRA through extensive experiments on well-known benchmark datasets. The\nresults highlight its capability to replicate real-world data patterns,\noffering valuable insights for developing and testing recommendation systems in\na controlled and realistic manner. The code used to perform the experiments is\npublicly available at https://github.com/SimoneMungari/HYDRA.\n","authors":["Simone Mungari","Erica Coppolillo","Ettore Ritacco","Giuseppe Manco"],"pdf_url":"https://arxiv.org/pdf/2407.16594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13504v1","updated":"2025-05-16T09:46:10Z","published":"2025-05-16T09:46:10Z","title":"An agentic system with reinforcement-learned subsystem improvements for\n  parsing form-like documents","summary":"  Extracting alphanumeric data from form-like documents such as invoices,\npurchase orders, bills, and financial documents is often performed via vision\n(OCR) and learning algorithms or monolithic pipelines with limited potential\nfor systemic improvements. We propose an agentic AI system that leverages Large\nLanguage Model (LLM) agents and a reinforcement learning (RL) driver agent to\nautomate consistent, self-improving extraction under LLM inference uncertainty.\nOur work highlights the limitations of monolithic LLM-based extraction and\nintroduces a modular, multi-agent framework with task-specific prompts and an\nRL policy of rewards and penalties to guide a meta-prompting agent to learn\nfrom past errors and improve prompt-based actor agents. This self-corrective\nadaptive system handles diverse documents, file formats, layouts, and LLMs,\naiming to automate accurate information extraction without the need for human\nintervention. Results as reported on two benchmark datasets of SOIRE, and CORD,\nare promising for the agentic AI framework.\n","authors":["Ayesha Amjad","Saurav Sthapit","Tahir Qasim Syed"],"pdf_url":"https://arxiv.org/pdf/2505.13504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02844v2","updated":"2025-05-16T08:41:00Z","published":"2024-06-05T01:35:50Z","title":"Item-Language Model for Conversational Recommendation","summary":"  Large-language Models (LLMs) have been extremely successful at tasks like\ncomplex dialogue understanding, reasoning and coding due to their emergent\nabilities. These emergent abilities have been extended with multi-modality to\ninclude image, audio, and video capabilities. Recommender systems, on the other\nhand, have been critical for information seeking and item discovery needs.\nRecently, there have been attempts to apply LLMs for recommendations. One\ndifficulty of current attempts is that the underlying LLM is usually not\ntrained on the recommender system data, which largely contains user interaction\nsignals and is often not publicly available. Another difficulty is user\ninteraction signals often have a different pattern from natural language text,\nand it is currently unclear if the LLM training setup can learn more\nnon-trivial knowledge from interaction signals compared with traditional\nrecommender system methods. Finally, it is difficult to train multiple LLMs for\ndifferent use-cases, and to retain the original language and reasoning\nabilities when learning from recommender system data. To address these three\nlimitations, we propose an Item-Language Model (ILM), which is composed of an\nitem encoder to produce text-aligned item representations that encode user\ninteraction signals, and a frozen LLM that can understand those item\nrepresentations with preserved pretrained knowledge. We conduct extensive\nexperiments which demonstrate both the importance of the language-alignment and\nof user interaction knowledge in the item encoder.\n","authors":["Li Yang","Anushya Subbiah","Hardik Patel","Judith Yue Li","Yanwei Song","Reza Mirghaderi","Vikram Aggarwal","Qifan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.02844v2.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.10900v1","updated":"2025-05-16T06:07:19Z","published":"2025-05-16T06:07:19Z","title":"Explain What You Mean: Intent Augmented Knowledge Graph Recommender\n  Built With LLM","summary":"  Interaction sparsity is the primary obstacle for recommendation systems.\nSparsity manifests in environments with disproportional cardinality of\ngroupings of entities, such as users and products in an online marketplace. It\nalso is found for newly introduced entities, described as the cold-start\nproblem. Recent efforts to mitigate this sparsity issue shifts the performance\nbottleneck to other areas in the computational pipeline. Those that focus on\nenriching sparse representations with connectivity data from other external\nsources propose methods that are resource demanding and require careful domain\nexpert aided addition of this newly introduced data. Others that turn to Large\nLanguage Model (LLM) based recommenders will quickly encounter limitations\nsurrounding data quality and availability. In this work, we propose LLM-based\nIntent Knowledge Graph Recommender (IKGR), a novel framework that leverages\nretrieval-augmented generation and an encoding approach to construct and\ndensify a knowledge graph. IKGR learns latent user-item affinities from an\ninteraction knowledge graph and further densifies it through mutual intent\nconnectivity. This addresses sparsity issues and allows the model to make\nintent-grounded recommendations with an interpretable embedding translation\nlayer. Through extensive experiments on real-world datasets, we demonstrate\nthat IKGR overcomes knowledge gaps and achieves substantial gains over\nstate-of-the-art baselines on both publicly available and our internal\nrecommendation datasets.\n","authors":["Wenqing Zheng","Noah Fatsi","Daniel Barcklow","Dmitri Kalaev","Steven Yao","Owen Reinert","C. Bayan Bruss","Daniele Rosa"],"pdf_url":"https://arxiv.org/pdf/2505.10900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10870v1","updated":"2025-05-16T05:22:42Z","published":"2025-05-16T05:22:42Z","title":"Improve Rule Retrieval and Reasoning with Self-Induction and Relevance\n  ReEstimate","summary":"  This paper systematically addresses the challenges of rule retrieval, a\ncrucial yet underexplored area. Vanilla retrieval methods using sparse or dense\nretrievers to directly search for relevant rules to support downstream\nreasoning, often suffer from low accuracy. This is primarily due to a\nsignificant semantic gap between the instantiated facts in the queries and the\nabstract representations of the rules. Such misalignment results in suboptimal\nretrieval quality, which in turn negatively impacts reasoning performance. To\novercome these challenges, we propose Self-Induction Augmented Retrieval\n(SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce\npotential inferential rules that might offer benefits for reasoning by\nabstracting the underlying knowledge and logical structure in queries. These\ninduced rules are then used for query augmentation to improve retrieval\neffectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a\nmethod that re-estimates the relevance of retrieved rules by assessing whether\nthe abstract knowledge they contain can be instantiated to align with the facts\nin the queries and the helpfulness for reasoning. Extensive experiments across\nvarious settings demonstrate the effectiveness and versatility of our proposed\nmethods.\n","authors":["Ziyang Huang","Wangtao Sun","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2505.10870v1.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2405.12892v2","updated":"2025-05-16T01:02:49Z","published":"2024-05-21T16:02:06Z","title":"Retrievable Domain-Sensitive Feature Memory for Multi-Domain\n  Recommendation","summary":"  With the increase in the business scale and number of domains in online\nadvertising, multi-domain ad recommendation has become a mainstream solution in\nthe industry. The core of multi-domain recommendation is effectively modeling\nthe commonalities and distinctions among domains. Existing works are dedicated\nto designing model architectures for implicit multi-domain modeling while\noverlooking an in-depth investigation from a more fundamental perspective of\nfeature distributions. This paper focuses on features with significant\ndifferences across various domains in both distributions and effects on model\npredictions. We refer to these features as domain-sensitive features, which\nserve as carriers of domain distinctions and are crucial for multi-domain\nmodeling. Experiments demonstrate that existing multi-domain modeling methods\nmay neglect domain-sensitive features, indicating insufficient learning of\ndomain distinctions. To avoid this neglect, we propose a domain-sensitive\nfeature attribution method to identify features that best reflect domain\ndistinctions from the feature set. Further, we design a memory architecture\nthat extracts domain-specific information from domain-sensitive features for\nthe model to retrieve and integrate, thereby enhancing the awareness of domain\ndistinctions. Extensive offline and online experiments demonstrate the\nsuperiority of our method in capturing domain distinctions and improving\nmulti-domain recommendation performance.\n","authors":["Yuang Zhao","Zhaocheng Du","Qinglin Jia","Linxuan Zhang","Zhenhua Dong","Ruiming Tang"],"pdf_url":"https://arxiv.org/pdf/2405.12892v2.pdf","comment":null}]}}